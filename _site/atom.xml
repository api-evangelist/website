<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>API Evangelist</title>
  <updated>2019-08-22T02:00:00Z</updated>
  <link rel="self" href="https://apievangelist.com/atom.xml"/>
  <author><name>Kin Lane</name></author>
  <id>https://apievangelist.com/atom.xml</id>
	<entry>
    <title>An API Policy Domain Specialist At Twitter</title>
    <link href="http://apievangelist.com/2019/08/22/an-api-policy-domain-specialist-at-twitter/"/>
    <updated>2019-08-22T02:00:00Z</updated>
    <content><![CDATA[
There are some jobs on the Internet I apply for no matter what my current situation is, and an API policy domain specialist at Twitter was one of them that popped up recently. I applied for the job within the first couple of hours after it came out, but haven’t heard from them. I can speculate on the reasons why, but I think a story about the job posting itself is actually more interesting, so I’ll focus there. It is the first time I’ve seen a job posting for this role, but I think it will eventually become a required role in the future for any company with a public API—-that is, if companies want avoid the trouble Twitter is going through right now, which again, is making Twitter the poster child for how to do APIs both right and wrong.

To highlight what this role is all about, I think Twitter’s own posting sums it up well, so let’s start by just reviewing what you’ll be doing if you get this job at Twitter:



We’re looking for an experienced, proactive, and detail-oriented team member to help develop and enforce Twitter’s data, developer, and API policies. Individuals in this role will work with teams across Twitter, as well as with members of the Twitter community, to empower thousands of developers worldwide to safely harness the power of Twitter data.

In this role, you will support Twitter’s “Know Your Customer” program for developers using our Enterprise APIs. This includes process development, auditing, and enforcement for Twitter’s data business and our public APIs to help ensure that Twitter data is being used in a manner consistent with Twitter policies. You’ll provide clear, actionable policy advice to the developer community and to internal stakeholders, and help identify win-win solutions to urgent customer issues.


  Develop and implement data and API policies across Twitter’s standard and Enterprise APIs. Handle incoming queries about API policy areas with clear, helpful, and complete responses and feedback.
  Lead investigative efforts into potential API abuse, including using internal resources, external and publicly-available sources, and audits of customer activity.
  Conduct hands-on reviews of the product and service offerings of users of Twitter’s APIs to validate policy compliance.
  Develop and implement policies, procedures and tools for conducting investigations of alleged policy or use case violations, including developing new tools and processes to facilitate investigations as appropriate.
  Develop an audit plan to follow up on clear evidence of policy or use case violations, conduct internal audits and oversee independent third party audits.
  Continuously evaluate, propose, and execute on improvements for existing processes and policies.


This role is based in San Francisco, reporting to Twitter’s Head of Site Integrity.&amp;lt;/em&amp;gt;



One of the most powerful phrases in this description for me is that you will “develop and enforce Twitter’s data, developer, and API policies”. That is both a very important, and daunting challenge to take on. Ideally, this is something companies are doing from the beginning and weaving into their regular API management practices, but many companies like Twitter were born in the “growth at all cost” age, and after a decade of this, the bill is finally coming due on having to clean thing up the environment. The second part of this that grabs me is, “empower thousands of developers worldwide to safely harness the power of Twitter data” — which is something that is not going to be easy, with so many opinionated voices out there who are looking to be “empowered”, and with such a varied opinion on exactly what “safely harness” means.

When it comes to learning from Twitter and other industries in all of this, I think it is important that we continue highlighting, formalizing, and white labeling “Know Your Customer” practices. As Twitter states, “this includes process development, auditing, and enforcement for Twitter’s data business and our public APIs to help ensure that Twitter data is being used in a manner consistent with Twitter policies.” Again, this is something that every API provider should have in place early on, and be maturing and growing as their community matures and grows. Instituting a know your customer program after you’ve grown beyond 10K or 100K developers is going to be a pretty daunting task for any company, and will be much more affordable, realistic, and impactful if you initiate early-on, and evolve along with your API community.

Having an API policy role as part of your API management team should exist for every partner and public API effort. Granted, not every group will need to have a single person dedicated to this role, but it is a hat that everyone in the team should be putting on regularly, and a formal set of practices, and known your customer program should always be in place. The reason Twitter is in the current situation they are is because they’ve neglected this role over the last decade—-I am guessing me being opinionated about these things over the years is one of the reason Twitter hasn’t called me. (Call me maybe?) Honestly, I’m not 100% sure I’d be up for the job, but I’d be damn willing to try. Developing and enforcing policy at this scale, at the front lines where Twitter operates is going to a massive undertaking, and there will be not clear wins, with many, many, many, clear losses along the way. However, I think the learning experience that would come with those types of challenges would be a degree that you just couldn’t obtain at any University.

One thing missing from this job description for me is the public storytelling element. I think they nail the nuts and bolts of what has to happen, but the investigation and ultimate enforcement of policies is going to have to be more theatre than straightforward business. Whoever lands in this role is going to have to learn to become a master communicator and puppet master when it comes to public, partner, and government opinion. This isn’t just going to be about Twitter’s API policy, and is fast becoming about how Twitter’s policy will be dictated and regulated by federal and state policy—-if Twitter doesn’t get out ahead of every single area of policy enforcement. I mean, c’mon you run a social influence platform, you guys are going to have to get better at influencing public policy using your own channels. The public performance of all of this will be an essential part of your API policy enforcement strategy, and if you don’t address this early on, you will find yourself working against a pretty strong headwind, and eventually policy will be mandated by federal regulation to do things in a prescribed way, which won’t always be aligned with Twitter business objectives.

An API policy domain specialist at Twitter is probably the single most important and interesting API job that exists today. For me–it shows the power APIs have on politics, business, and almost every other aspect of our lives. I see API everywhere, but most days I just feel like the kid in the Sixth Sense. However, APIs are powering everything we do in our personal and professional lives, and like everything that operates at this scale, platform and government policy will continue to play an important role in shaping the positive or negative impact that APIs make on our worlds. I hope Twitter, and other API providers take the role of API policy seriously, and properly invest in this area. Prioritizing resources when it comes to understanding, developing, enforcing, and influencing API policy that guides the pipes behind the desktop, web, mobile, device, and network infrastructure that is increasingly dominating our lives.
]]></content>
    <id>http://apievangelist.com/2019/08/22/an-api-policy-domain-specialist-at-twitter/</id>
  </entry><entry>
    <title>Multiple Overlapping API Life Cycle(s)</title>
    <link href="http://apievangelist.com/2019/08/21/multiple-overlapping-api-life-cycles/"/>
    <updated>2019-08-21T02:00:00Z</updated>
    <content><![CDATA[
One of the toughest parts about teaching people about APIs is that there are many different views of what the API life cycle can be depending on who you are, and what your intentions are. As an advocate or evangelist for a single API you are speaking externally to the API consumption life cycle, but internally you are focused on the API delivery life cycle. As an API Evangelist for many APIs, targeting providers, consumers, and anyone else who comes along, I find it a constant challenge to properly speak to my intended audience. One problematic part of my storytelling I regularly see emerge is that I speak of a single API life cycle, where in reality there are many overlapping life cycles. So, to help me think through all of this I wanted to explore what these overlapping tracks might be—coming up with four distinct iterations of overlapping API building blocks.

The API Delivery Life Cycle 
The most common way we refer to the API life cycle is from the perspective of the API provider, where it is all about delivering an API. Referencing the stops along the life cycle that are most relevant to someone who is  delivering a new API, or might be moving an API forward as part of the evolution of an existing resource. From my vantage point, I consider these to be the most common stops along the API delivery life cycle.


  Definitions - Defining what an API does, crafting the JSON schema, OpenAPI, AsyncAPI, and other machine readable definitions of what is potentially being delivered, initializing the contract that will guide an API through the life cycle.
  Design - Stepping back and considering the healthiest API Design practices to apply, working from a diverse API toolbox, and ensuring you have a good handle on who your audience is, and the design patterns they’ll respond to.
  Mocking - Generating a mock instance of an API using the contract to test out design patterns, and begin socializing amongst technical and business stakeholders so that they can begin providing critical feedback on the design.
  Documentation - Making sure there is auto-generated documentation available for all providers and consumers from the agreed upon API contract definition, ensuring there is machine and human readable documentation available at all times.
  Testing - As the API contract stabilizes, tests can be generated to ensure that the contract is not veering of the intended course, providing guardrails that can be regularly rebuilt from the API contract, and ultimately used in production.
  Deployment - Publishing of an API either using an open source framework, gateway, or through hand coding of the interfaces, providing a production ready instance of an individual API contract that was defined early on.
  Management - Making sure an API requires proper authentication, has access levels and rate limits defined, as well as logging enabled, so that its access can be properly quantified, managed, and reported upon for provider and consumer.
  Monitoring - Ensuring that each API is being monitored in real time from multiple regions, ensuring that an API is up and available, as well as passing all tests that have been defined as part of the development of each API contract.
  Security - Knowing that requiring authentication will not be enough, and that all APIs are regularly scanned, probed, and pushed for any potential vulnerabilities, providing regular accounting of the security of al APIs in operation.


I can quickly add on another 10-20 stops along the API delivery life cycle, but I want to highlight the core building blocks of delivering an API, not exhaustively document every possible building block. These are the primary steps any API provider will need to master before they can begin competently delivering API resources for consumption internally, or externally to partners and the general public. Providing a basic checklist that can be use to flesh out the details needed before an API can be called production-ready, then begin it’s life as the back-end for web, mobile, device, and network applications.

API Consumption Life Cycle

Next, I wanted to explore the flip side of the API life cycle, and the view from the position of the API consumer. This is the second most common way I talk about the API life cycle, but I find that I often get things mixed up between provider and consumer, and I know from experience that I do not always ensure that things are made clear. To help me put myself into the shoes of API consumers, I wanted to explore what some of the stops along the API life cycle from purely a consumption standpoint—here are a few that I’m thinking about today.


  Discovery - Before you can consume an API you have to be able to find it, and be able to get what need to get up and running.
  Documentation - You will have to be able to understand the details of what an API does, and ultimately what it delivers via documentation.
  Plans - Once you understand what an API does, you will need to have a grasp on the tiers of access, and what you can afford to use.
  Account - After you decide to use an API you will need to be able to signup and obtain an account and credentials that will allow for access.
  Authentication - Now that you have keys, you will need to understand how you can use them as part of authentication for the APIs you need.
  Clients - There should always be client tooling that helps consumers get up and running without writing code, with as little friction as possible.
  Software Development Kits (SDK) - There should be SDKs available in a variety of programming languages allow consumers to seamlessly integrate.
  Command Line Interface (CLI) - Some developers will prefer using a CLI to integrate and automate using an API as part of their systems.
  Integration - Now we reach the point where we actually integrate with an API, something not all developers will successfully achieve as part of their own consumption life cycle.
  Support - Now that a consumer has integrated they will need to know where they can receive support as part of their application, and ongoing use of API resources.
  Engagement - There should be further opportunities for engagement beyond just the initial integration of an API, encouraging the deepening of the provider and consumer relationship.


This provides me with a simple slice of the API consumption side of things to focus on in my storytelling. Having simple, organized groups of API building blocks like this, wrapped in a variety of contexts helps me be more consistent in my writing and speaking. Using consistent wording combined with repetition is important, and thoughtfully defining the API life cycle from the provider, and well as consumer side, in ways that everyone can easily understand is very important to me.

API Maintenance Life Cycle
We aren’t done with APIs once they are delivered and being consumed. There are regular maintenance tasks that should occur, and these are things that won’t necessarily be handed by the same team that delivers the APIs. Opening up a whole other dimension of the API life cycle that cannot be ignored, otherwise we run the risk of offering a less than quality experience for our consumers. Here are the maintenance portions of the API life cycle that I am considering, and documenting as part of this work.


  Communication - An API is up and running and communication between provider and consumer is critical to ensure everyone is operating and doing what they need to be doing.
  Support - Making sure all of the consumers are supported and getting the attention they need, providing the critical feedback loop between provider and consumer.
  Issues - Publishing a real time accounting of open issues that exist to help API consumers understand the current status of the system, while also helping minimize support requests for known issues.
  Monitoring - Understanding the overall health and status of all APIs, and making sure all monitors, tests, and the platform is meeting their SLA when it comes providing APIs.
  Performance - Going beyond the monitoring and testing, and making sure a certain quality of service bar is met, and we have a complete grasp of how our customers experience API access.
  Security - Reviewing security practices, and auditing authentication, usage, logging, scanning, and other dimensions of security across all APIs in operation.
  Reporting - Staying in tune with API consumption, errors, and general access and usability across all API consumers, making sure an awareness is developed in how digital resources are used.
  Road Map - Publishing an accounting of what the future will hold for API consumers, regularly thinking about what future plans, or lack of future plans will have on API consumers.
  Evangelism - What is being doing to get the word out about the value APIs deliver, establishing a strategy for how you evangelize internally, with partners, and the public and help achieve platform goals.


Abandoned APIs that do not have anyone in the drive seat are the most common type of API I come across. It is pretty easy to tell when an API provider has moved on, and just left the API up and running. In other cases, teams are just under resourced, and do not have the bandwidth available to invest in the API maintenance portion of the API life cycle. We all get excited about the design and delivery of a new API, but few of us like doing the mundane aspects of API management—this is why you should invest in a good API management solution, because nobody should be reinventing the wheel in this area.

API Governance Life Cycle 

Lastly, and one of the most under-developed portions of the API life cycle, is the governance and oversight of things. This is the portion of the API life cycle where you go beyond the tactical day to day stuff and work to establish a strategy for how you deliver, consume, and maintain APIs. Getting formal about how we do what we do, and make sure all stakeholders, especially consumers have a good grasp on API policy, and how things get done. Here are some of the building blocks I consider at the API governance stops along the API life cycle.


  Definition - Which definitions are used? Where are the OpenAPI, schema, and other relevant patterns.
  Design - What design patterns are in play across the API definitions, and what is the meaning behind the design of all APIs.
  Deployment - What does deployment look like on-premise, in the cloud, and from region to region.
  Management - Quantify the standard approaches to managing APIs from on-boarding to analysis and reporting.
  Plans - How are access tiers and plans defined, providing 3rd party access to APIs, including that of aggregators and application developers.
  Monitoring - What does monitoring of web APIs look like, and how is data aggregated and shared.
  Testing - What does testing of web APIs look like, and how is data aggregated and shared.
  Performance - What does performance evaluation of web APIs look like, and how is data aggregated and shared.
  Security - What are the security practices in place for all APIs, and what is the current status of security.
  Breaches - When there is a breach, what is the protocol, and practices surrounding what should happen–where is the historical data as well.
  Terms of Service - What does terms of service across many APIs look like.
  Privacy Policy - How is privacy protected across API operations, and how is this impacting operations.
  Support - What are all the expected support channels, where are they located, and what the current metrics are.
  Training - A detailed training walk-through for anyone looking to understand governance.
  Certification - Providing certification for API publishers as well as API consumers when it comes to their platform participation.
  Executive - A robust walk-through of the concepts at play for an executive from the 100K view of things.


Your API delivery, consumption, and management house needs to be in order before you can begin realizing governance across everything. For many API providers it can be difficult to step back and look at things from the 100K level. However, increasingly it will become critical to make time to invest in API governance, otherwise you end up dealing with lower level fires that could have been addressed with a little more planning and coordination.

Many API Life Cycle(s)
I’ve historically visualized the API lifecycle as a set of linear stops along a single life cycle, however over the years, as the world of APIs have come into better focus, I’m seeing the life cycle as a set of many different life cycles that work in concert for API provider as well as API consumers—cause, we all should really be both. I acknowledge that there are many different possible iterations upon the different types of API life cycles that will exist across companies, organizations, institutions, and government agencies. It will all depend on the expertise, services, tooling, and resources groups have when it comes to delivering, consuming, managing, and governing the APIs they depend on. However, I’m at least looking to define at least a handful of templates that can be used to guide API operators who are wanting to see the bigger picture.

Next, I will work on a visual to go with these API lifecycle templates. I’m going to be sticking with my transit map approach to defining the API landscape, and plot these different API life cycles as different lines within the same overall landscape map. Working to show how they overlap and interact. My biggest challenge in all of this is to be able to articulate that this lifecycle is applied across many different APIs, and potentially many different teams publishing APIs, as well as many different types of consumers putting them to work. Historically I have tried to auto-generate these API transit maps, but for this work I think I will just hand-craft the maps to reflect the API life cycles I have defined. Providing a more universal way in which to help articulate, visualize, explore and learn more about how the API life cycle(s) work.
]]></content>
    <id>http://apievangelist.com/2019/08/21/multiple-overlapping-api-life-cycles/</id>
  </entry><entry>
    <title>The API Conferences I Am Tracking On For The Fall</title>
    <link href="http://apievangelist.com/2019/08/20/the-api-conferences-i-am-tracking-on-for-the-fall/"/>
    <updated>2019-08-20T02:00:00Z</updated>
    <content><![CDATA[As we approach the fall it is time to begin thinking about the conference season, and what the most relevant API conferences are. I haven’t been doing any events this year, but staying in tune with the conference circuit has always been important to my work. Who knows, maybe I will be spend some more time investing in API related events after taking a break for six month. When it comes to API events and conferences, here is what I am tracking on.




API City Conference
Seattle, WA United States
September 5, 2019

API City Conference is a non-profit community event that brings together a diverse set of developers and business people to network and learn about APIs and the business value it can bring to you and your company.

URL: https://2019.apicity.io/




The Postman User Conference
San Francisco, CA United States
September 11-12, 2019

This year’s POST/CON includes 4 tracks with excellent speakers from all over the world. They’ll be diving into advanced Postman use-cases and thought leadership in API testing and automation, API design and architecture, developer experience, and DevOps.

URL: https://www.getpostman.com/post-con-2019/




API Days: APIs for Consumer Applications
Barcelona, Spain
September 12-13, 2019

FinTech Banks, Transportations Companies, Booking firms, Food delivery services and even the Beauty sector are offering services for millions of people through APIs, posing some interesting challenges in terms of scaling, monitoring and changes deployment. We call these the APIs for consumers.

URL: https://www.apidays.co/barcelona




API Days: Connection, Automation, Intelligence
Melbourne, Australia
September 19-20, 2019

The Melbourne Australia edition of the popular API Days conference, focused on the connection, automation, and intelligence that APIs can deliver. Bringing together a variety of speakers and attendees from across Australia, New Zealand, and around the world to discuss how APIs are changing the way we do business.

URL: https://www.apidays.co/melbourne




RESTFest
Greenville, SC United States
September 25-28, 2019

Our objective is to give people interested in REST, Hypermedia APIs, crafting web service APIs, or any related topics a chance to get together in an informal setting to share ideas, trade stories, and show examples of current work.

URL: https://www.restfest.org/




API Specifications Conference (ASC)
Vancouver, British Columbia, Canada
October 15 - 17, 2019

The API Specifications Conference (ASC) is a place for API practitioners to come together and discuss the evolution of API technology. ASC includes cutting edge technology keynotes and sessions that chart the future of APIs, in-depth specification and standards discussions and an extensive tutorial track. The event is designed to be highly interactive with plenty of discussion time through workshop and un-conference style sessions in order to facilitate interaction.

URL: https://events.linuxfoundation.org/events/asc-2019/program/




API Days: Banking APIs and PSD2
London, England
November 13-14, 2019

What do you talk about when doing an API conference in London? Banking of course. This is the most recent, London UK edition of the popular API Days events. Keeping the momentum going when it comes to how APIs are changing the banking landscape, where there will be much discussion around banking API regulation and PSD2.

URL: https://www.apidays.co/london




API Days: From Legacy to Agile, From Product to Ecosystems
Paris, France
December 10-11, 2019

The original API Days location in Paris, France has grown into the biggest API gathering in the world. It is been happening since 2012, and attracts people from across the globe to talk APIs across many different industries. If you pick one conference to attend this fall, I recommend you make it to APIs Days, Paris.

URL: https://www.apidays.co/paris



These are all conferences I have been to, supported and helped grow throughout the years. If you are new to the world of APIs, or an experienced veteran, I recommend hitting as many of the events on this list as you can. While some of them have a more narrow focus, there is something to learn at each of these events on stage, and in the hallways talking with attendees.

It looks like a strong API conference season this fall. It definitely makes me miss being on the road, and able to hit all of these events. I’m happy to see all of these events continuing, but I’m most happy to see APIStrat be reborn as the API Specifications Conference, which I think is the most important subject we can be talking about in the API sector going into 2020.
]]></content>
    <id>http://apievangelist.com/2019/08/20/the-api-conferences-i-am-tracking-on-for-the-fall/</id>
  </entry><entry>
    <title>Human Empathy Is One Of My Most Important API Outreach Tools</title>
    <link href="http://apievangelist.com/2019/08/20/human-empathy-is-one-of-my-most-important-api-outreach-tools/"/>
    <updated>2019-08-20T02:00:00Z</updated>
    <content><![CDATA[
I am an empathic human being. It is one of my top strengths, as well as one of my top weaknesses. It is also one of the most important tools in my API toolbox. Being able to understand the API experience from the position of different people throughout the world of APIs is a cornerstone of the API Evangelist brand. Personally, I find APIs themselves to be empathy triggering, and something that has regularly forced me out of my silos, then allowing me t put myself in the shoes of my consumers. Something that when realized in a perpetual fashion can become a pretty powerful force for dialing in the services you offer, and establish, maintain, and strengthen connections with other people within the community.

Being able to listen to people in the hallways of conferences, and within the meeting rooms across enterprise, institutions, and government agencies, then internalize, process, and position my writing from what I learn from people is how I have written on API Evangelist for the last nine years. I rarely am positioning my narrative my own vantage point, or that of a company. Most of the time I am channeling someone I’ve met along the way, speaking from their perspective, and analyzing the world of APIs as they would see it. While I wish that the world always resembled my view of the API landscape, from experience I know better, and that there are many diverse ways of seeing the value or damage APIs are responsible for.

While API design, and the overall user experience around API service and tooling goes a long way to speak to end-users, I still think the human touch, and positioning our messaging from the vantage point of our consumers will have the greatest impact. Making a person connection will last much longer than any single blog post, advertisement, Tweet, image, video, or other common unit of engagement. Of course, what you gather from putting yourself in the shoes of your consumers should feed into all of these engagement areas, but ensuring they are rooted in the reality of consumers, possess the right amount of context, and speak in a personal tone will be critical to completing the empathic loop set into motion when talking with customers and conferences and within the companies, organizations, institutions, and government agencies where users work.

The downside of relying on empathy is it can be exhausting, and it is only something you can properly accomplish if you care, making it very challenging to scale. I think many people can be taught empathy, but some will never get it, and even fewer will be really good at it. Those who are best at it will burnout much quicker, and will need more careful oversight, as well as opportunities to recharge. However, if you can work to ensure your in-person evangelism approach is empathy centered, and you work to weave what you learn into your overall messaging, outreach, and engagement practices, it can make a serious impact. This type of outreach can’t be faked. It has to come from an honest place. Which can be hard to find depending on where you work, the type of environment that exists, as well as the industries you target. I know that many folks who read this will dismiss this as too simplistic, and not easily measured, but I know from experience it is the most important thing I can bring to the table when reaching out to my audience.
]]></content>
    <id>http://apievangelist.com/2019/08/20/human-empathy-is-one-of-my-most-important-api-outreach-tools/</id>
  </entry><entry>
    <title>Postman Collection As A Single Quantifiable, Shareable, Executable Unit Of Representation For Any Digital Capability</title>
    <link href="http://apievangelist.com/2019/08/19/postman-collection-as-a-single-quantifiable-shareable-executable-unit-of-representation-for-any-digital-capability/"/>
    <updated>2019-08-19T02:00:00Z</updated>
    <content><![CDATA[
In my world API definitions are more valuable than code. Code is regularly thrown away and rewritten. API definitions hold the persistent detail of what an API delivers, and contain all of the proprietary value when they are properly matured. OpenAPI has definitely risen to the top when it comes to which API definition formats you should be using, however, Postman Collections have one critical ingredient that makes them ultimately more usable, sharable, and meaningful to developers—-environmental context. This small but important difference is what makes Postman Collections so valuable as a single quantifiable, shareable, executable unit of representation for any digital capability.

Like OpenAPI, Postman Collections describe the surface area of a web API, but they have that added layer to describe the environment you are running in, which makes it much more of a run-time and execute-time experience. This may seem like a minor detail, but developers who want instant gratification, a Postman Collection bundled with the Postman API lifecycle tooling, makes for a pretty powerful representation of a company’s, organization’s, institutions’s, or government agency’s digital capability. Allowing for API providers (or consumers) to describe what an API does in a machine readable format, bundle with it the environment context to actually execute the digital capability, and enable the unit of value to be realized within the Postman API development ecosystem.

I can take any of my internal, or 3rd party public APIs I depend on, make successful calls to them, including authentication, tokens, and other environment variables, then export as a portable Postman Collection, and share with anyone I want using a simple URL, or by embedded the Run in Postman button within documentation, blog posts, and other resources. Then, any potential consumer can take that Postman Collection, load into their Postman client, and be able to realize the same digital capability I was using—-no documentation, on-boarding, or other friction required. You get instant gratification regarding putting the digital capability to work, exactly as I intended. This quantifiable, shareable, and executable nature of Postman Collections is what elevates them to a position held by no other API definition format out there.

It is this dance between machine readable API definition, and API lifecycle tooling (client, documentation, testing, mocking, etc.), linked together with the environment context that continues to ensure Postman captures my attention. Ensuring the technical details of your API is captured in a machine readable format is something I don’t think all API providers fully comprehend. Enabling developers to put an API to work for them in a single click, instead of the usual on-boarding dance, digestion of documentation, selection of relevant programming language SDK, and other cognitive load associated with API integration—-is critical! APIs represent the digital capabilities of your company, organization, institution, or government agency—-Postman Collections are how you ensure your capabilities are quantified, shared, and executed by internal and 3rd party developers.

Disclosure: Postman is an API Evangelist sponsor.
]]></content>
    <id>http://apievangelist.com/2019/08/19/postman-collection-as-a-single-quantifiable-shareable-executable-unit-of-representation-for-any-digital-capability/</id>
  </entry><entry>
    <title>A Second Wave of API Management is Going On</title>
    <link href="http://apievangelist.com/2019/08/19/a-second-wave-of-api-manageent-is-going-on/"/>
    <updated>2019-08-19T02:00:00Z</updated>
    <content><![CDATA[
I fully surfed the first wave of API management. API Evangelist began by researching what Mashery, Apigee, and 3Scale had set into motion. API Evangelist continued to has exist through funding from 3Scale, Mulesoft, WSO2, and continues to exist because of the support of next generation providers like Tyk. I intimately understand what API management is, and why it is valuable to both API providers and consumers. API management is so relevant as infrastructure it is now baked into the AWS, Azure, and Google Clouds. However, if you listen to technological winds blowing out there, you will mostly hear that the age of API management is over with, but in reality it is just getting started. The folks telling these tales are purely seeing the landscape from an investment standpoint, and not from an actual boots on the ground within mainstream enterprise perspective—something that is going to burn them from an investment standpoint, because they are going to miss out on the second wave of API management that is going on.

The basics of API haven’t changed from the first to the second wave, so let’s start with the fundamental building blocks of API management before I move into describing what the next wave will entail:


  Portal - A single URL to find out everything about an API, and get up and running working the resources that are available.
  On-Boarding - Think just about how you get a new developer to from landing on the home page of the portal to making their first API call, and then an application in production.
  Accounts - Allowing API consumers to sign up for an account, either for individual, or business access to API resources.
  Applications - Enable each account holder to register one or many applications which will be putting API resources to use.
  Authentication - Providing one, or multiple ways for API consumers to authenticate and get access to API resources.
  Services - Defining which services are available across one or many API paths providing HTTP access to a variety of business services.
  Logging - Every call to the API is logged via the API management layer, as well as the DNS, web server, file system, and database levels.
  Analysis - Understanding how APIs are being consumed, and how applications are putting API resources to use, identifying patterns across all API consumption.
  Usage - Quantifying usage across all accounts, and their applications, then reporting, billing, and reconciling usage with all API consumers.
  APIs - API access to accounts, authentication, services, logging, analysis, and usage of API resources.


Many believe API management is primarily about securing APIs, with others seeing it purely as monetization, when in reality API management is about awareness. Establishing, maintaining, and evolving an awareness of the API-driven digital capabilities you possess, and how these capabilities are being applied on the desktop, within mobile phones, on Internet-connected devices, and at the network layer. Without this awareness you will not remain competitive in the online global economy. The first wave of API management was about selling these essential building blocks to the growing number of startups, and handful of progressive enterprise. The second wave of API management is about selling these building blocks to mainstream enterprises across staple industries like healthcare, banking, education, and beyond.

API management remains the cornerstone of the API lifecycle, and while the first wave of API management providers will benefit from the second wave, it is the next generation of API management providers like Tyk and Kong who will truly reap the benefits. They are the ones who will be agile enough, aware enough, and innovative enough to meet the demands of mainstream enterprise companies, SMBs, and startups when it comes to delivering APIs at scale throughout their API journey. The core API management features will remain the essential building blocks that API management service providers will bring to the table, but there will be other areas in which rise to the occasion and serve.


  Discovery - Helping enterprise make sense of the growing number of digital resources they possess.
  Service Mesh - Establishing a fabric of services that are resilient, scalable, and meet consumer needs.
  Micro - Possessing a light footprint so it can be deployed anywhere, by anyone looking to put to work.
  Regional - the ability to rapidly deploy and scale to meet the specific needs of regional use cases.
  Transformations - Leaning on the API management to evolve, transform, and move APIs forward.
  Extensible - The ability to extend the capabilities within the API management layer for the long tail.


These are just a handful of the API management features that next generation API service providers will need to possess. While the established API management providers will be able to deliver in some of these areas, they ultimately will not be able to move fast enough, and direct investment properly within these areas. I also worry that even some of the next generation solutions won’t be able to get the investment they need with the current perspective being that we exist in a post API management phase. Sadly, I think this is the reality of a world that is heavily influenced, driven, and captured by investment ideology. The result is that the solutions being delivered are out of touch with what enterprises actually need on the ground, and startups that chase the investment money are always on to the next big thing, leaving significant chunks of change on the table.

For me, not much has changed since 2010 when it comes to APIs. I’d say the API lifecycle has expanded and come into focus a little bit more, but nothing revolutionary and only a handful of things that are evolutionary have actually emerged. The major shift in the landscape that has occurred is that in 2014 I was still talking to mostly startup, and in 2019, I’m mostly talking to enterprise. The mainstream enterprise has woken up to the potential of APIs, and they are needing the expertise, tooling, and services to get the job done. They need the awareness that API management brings, and to make sense of their digital capabilities. API management is a fundamental building block of the API economy, and just because the leading providers have been acquired and baked into the cloud doesn’t mean the opportunity is gone and over with. If you survived the SOA evolution, you know that there are plenty of building blocks being repurposed for a cloud, mobile, and device world—this is a world where API awareness is essential, and API management is how you achieve this level of awareness.

Disclosure: Tyk is an API Evangelist sponsor.
]]></content>
    <id>http://apievangelist.com/2019/08/19/a-second-wave-of-api-manageent-is-going-on/</id>
  </entry><entry>
    <title>Seeing API Consumers As Just The Other Ones</title>
    <link href="http://apievangelist.com/2019/08/16/seeing-api-consumers-as-just-the-other-ones/"/>
    <updated>2019-08-16T02:00:00Z</updated>
    <content><![CDATA[
As API providers, it can be easy to find ourselves in a very distant position from the consumers of our APIs. In recent weeks I have been studying the impacts of behavioral approaches to putting technology to work, something that has led me to the work of Max Meyer, and his Psychology of the Other-One (1921). I haven’t read his book yet, but have finished other works citing his work on how to “properly” study how animals (including humans) behave. While the psychological impact of all of this interests me, I’m most interested in how this perspective has amplified and driven how we use technology, and specifically how APIs can be used to create or bridge the divide between us (API providers) and our (API consumers).

While web and mobile technology is often portrayed as connecting and bringing people together, it also can be used to establish a separation between providers and consumers. We often get caught up in the scale and growth of delivering API infrastructure, and we forget that our API consumers are humans, and we can end up just seeing them as personas, humans, or just a demographic. Of course, as API providers, we can’t be expected to make a direct connection with every single consumer, but we also have to be wary of becoming so distant from their reality that we can’t make a connection with them at all. Leaving our products, services, and tooling something that doesn’t serve them in any way, and we fail to meet our own business objectives behind what we were building in the first place.

There will aways be some distance between API provider and consumer. However, we have to regularly work to narrow this divide, otherwise negative forces can make their way in between us and consumers. If we simply see your API consumers and end-users as the “other ones”, it will make supporting, and investing in their success much more difficult. Trust with API consumers, and the end-users of the applications they develop is tough to achieve, and even harder to maintain-—something that is increasingly more difficult when you simply see them as the other ones, those over there, and just nameless faceless database entries. It is our job as API community managers, customer success engineers, evangelists, and marketers to ensure that this all to common divide doesn’t grow between us and our consumers.

Technology has the potential to bring us together, and connect us in many new and interesting ways-—APIs are at the center of this technological evolution. However, without the proper care and attention, it also has the potential to push us further apart. Dehumanizing people along the way, and reducing them simply to database entries or just a series of transactions. As evangelists, we can’t let this happen. We have to work extra hard to get to know our consumers, and reach out to better understand who they are, what they need, and ensure we are in tune with their view of the API resources we are making available. This is something that applies to our intentional, unintentional, and malicious API consumers. The more visibility we have into who our API consumers are, and what they are up to, the more success we will have in achieving our objectives, and sensibly scaling our communities-—keeping the balance between us the API provider, and our consumers, and seeing them as more than just the “other ones”.
]]></content>
    <id>http://apievangelist.com/2019/08/16/seeing-api-consumers-as-just-the-other-ones/</id>
  </entry><entry>
    <title>Four Phases Of Internal API Evangelism</title>
    <link href="http://apievangelist.com/2019/08/16/four-phases-of-internal-api-evangelism/"/>
    <updated>2019-08-16T02:00:00Z</updated>
    <content><![CDATA[
General evangelism around what APIs are, as well as more precise advocacy around specific APIs or groups of API resources takes a lot of work, and repetition. Even as a seasoned API evangelist I can never assume my audience will receive and understand what it is that I am evangelizing, and I regularly find myself having to reassess the impact (or lack of) that I’m making, retool, refresh, and repeat my messaging to get the coverage and saturation I’m looking for. After a decade of doing this, I cannot tell which is more difficult, internal or external public evangelism, but I do find that after almost 10 years, I’m still learning from each audience I engage with—-proving to me that no single evangelism strategy will ever reliably work, and I need a robust, constantly evolving toolbox if I am going to be successful.

I have many different tools in my internal evangelism toolbox, but I find that the most important aspect of what I do is repetition. I never assume that my audience understand what I’m saying after a single session, or simply by reading one wiki page, blog post, or white paper. Quality internal evangelism requires regular delivery and enforcement, and an acknowledgement that my first engagements with teams will not have the impact I desire. When it comes to internal evangelism, I tend to encounter the following phases when engaging with internal teams:


  Silence - The first time I present material to internal teams I almost always encounter silence. Teams will often listen dutifully, but rarely will engage with me, ask questions, and want to get to the details of what is going on. I can rarely assess the state of things, and find that the silence stems from a range of emotions, ranging from not caring at all, to being very distrustful of what I am presenting. I can never assume that teams will care, trust me, and that silence is always a sign of the work that lies ahead of me, and I immediately get to work scheduling additional sessions with each team I’m trying to reach.
  Challenges - Usually by our second or third encounter with internal teams I will begin to get a little more than just silence, however it almost always comes in some aggressive form. Developers love to challenge what you are proposing, tearing things down before they ever understand what is happening. It is actually part of the natural cycle for them. Technical folks are used to taking things apart, ripping them into pieces, to see what they are all about, and what they are made of. It is easy to take this type of response in a negative and personal way, but it is the way technical minded folks see the world, especially when faced with something they do not understand, and are uncomfortable with. To survive this phase you have to have a lot of self-confidence and know your stuff, otherwise you will be eaten alive.
  Questions - After surviving the aggressive challenges about what APIs are all about and what they mean to a company, organization, institution, or government agency, I usually begin getting some more constructive questions. Moving beyond the desire to rip you to pieces, and actually start the process of actually understanding what is happening when it comes to providing and consuming of APIs. Again, you have to really know your stuff to be able to survive this line of questioning from often very smart, very technical, and inquisitive folks. However, if you come prepared, this is where you start seeing the ROI on your internal evangelism efforts.
  Engagement - I usually do not see real engagement from teams for at least 1-6 sessions. Depending on the team, they’ll take more or less time to get through the painful aspects of understanding what is going on. Depending on the type of development team, what their experience levels are, and the environments they’ve been working in, they will respond very differently. You will have to be patient to be able to reach the phase where teams actually become engaged, are able to contribute to the conversation, and take what you’ve presented and apply it to their daily work. Moving beyond just evangelism, and actually beginning to see real business value from sharing of API knowledge.


Depending on the organization culture, these four phases could take days, weeks, or months. Not all teams will be ready for what you are evangelizing. You cannot assume that teams understand what you are talking about, and that they see you as a messenger of a positive future. I’d say about 70% of the time I am seen as a hostile actor, coming in to disrupt, change ,and mess with people’s world. I don’t care how well honed my message, materials, and vision is, if I cannot connect with teams on a human and professional level–I will fail. I’ve been practicing my delivery of 101, intermediate, and advanced API material for a decade, and I still get eaten alive on a regular basis within startups, the enterprise, government agencies, and at conferences. There is no amount of preparation that will shield you from the intensity that internal development teams can bring to the table–this game isn’t for rookies.

Internal API evangelism and advocacy within the enterprise is not something you can do from high up on the mountain. You have to be able to come down from your management, architect, and executive horse, and be able to see things from the view of those in the trenches trying to get work done on a daily basis. If you can’t be seen as someone looking to build bridges between management and development ranks, your information will never be received. No amount of evangelism will cross the Grand Canyon that exists between business and technical groups in some organizations, if you aren’t willing to build bridges. Something that will take some serious planning, repetition, and tactical agility when it comes to the delivery of relevant information that is tailored for your intended audience. Internal evangelism is hard work, and something that will take regular auditing and retooling before you will ever see the impact you desire.
]]></content>
    <id>http://apievangelist.com/2019/08/16/four-phases-of-internal-api-evangelism/</id>
  </entry><entry>
    <title>An Observable Regulatory Provider Or Industry API Management Gateway</title>
    <link href="http://apievangelist.com/2019/08/15/an-observable-regulatory-provider-or-industry-api-management-gaeway/"/>
    <updated>2019-08-15T02:00:00Z</updated>
    <content><![CDATA[
I wrote a separate piece on an API gateway specification standard recently, merging several areas of my research and riffing on a recent tweet from Sukanya Santhanam (@SukanyaSanthan1). I had all these building blocks laying around as part of my research on API gateways, but also from the other areas of the API lifecycle that I track on. Her tweet happened to coincide with other thoughts I had simmering, so I wanted to jump on the opportunity to publish some posts, and see if I could slow jam a conversation in this area. Now, after I defined what I’d consider to be a core API gateway set of building blocks, I wanted to take another crack at refining my vision for how we make it more observable and something that could be used as a tech sector regulatory framework.

Copying and pasting from my API gateway core specification, here is what my v1 draft vision for an API gateway might be:


  Paths - Allowing many different API paths to exist.
  Schema - Allowing me to manage all of my schema.
  Integrations - Providing backend lego architecture.
    
      Resource - Allow for integration with other APIs.
      Database - Provide a stack of database integrations.
      Other - Define whole buffet of integration definitions.
    
  
  Requests - Define all of my HTTP 1.1 requests
    
      Methods - Providing me with my HTTP verbs.
      Path Parameters - Able to define path parameters.
      Query Parameters - Able to define query parameters.
      Bodies - Providing control over the request body.
      Headers - Full management of HTTP request headers.
      Encoding - Defining the media types in in use for requests.
      Validate - Providing validation for all incoming requests.
      Mappings - Allowing for mapping of requests to backend.
      Transformations - Transformation before sending to backend..
      Examples - Ensuring there are samples for each request.
      Schema - Able to reference all schema used in requests.
      Tags - Being able to organize API requests using tags.
    
  
  Responses - Define all of my HTTP 1.1 responses.
    
      Status Codes - Providing the ability to define HTTP status codes.
      Headers - Full management of all HTTP response headers.
      Encoding - Defining the media types in in use for responses.
      Schema - Able to reference all schema used in responses.
      Examples - Ensuring there are samples for each response.
    
  
  Stages - Able to stage APIs under any platform defined environment.
  Publishing - Allowing for conscious publishing of APIs into production.
  Versioning - Providing semantic versioning as header or in the path.
  Policies - Defining policies for API, and schema access by consumers.
  Licensing - Ensure that data and APIs are properly licensed for consumption.
  Plans - Crafting a handful of standard access tiers for different consumers.
  Rate Limiting - Define the rate limits for all APIs within each plan offered.
  Domains - Allow for default and custom domains associated with APIs.
  Certificates - Provide management and usage of certificates for encryption.
  Tags - Allow APIs, as well as their individual paths, and requests to be tagged.
  Dependencies - Inform on the dependencies between APIs, including 3rd party.
  Regions - Allow for multi-region deployment of APIs, with full DNS support.
  Contact - Ensure there is contact information for every API owner.
  Logging - Standardize the logging for all API traffic to one or many locations.
  Monitoring - Provide basic monitoring of all APIs from alternate locations.
  Status - Offer a real time status dashboard and notification for all APIs.
  Terms of Service - Allow for the publishing of one or many TOS applying to APIs.
  Authentication - Provide a handful of standard authentication mechanisms.
  Authorization - Enable fine grade authorization across APIs and schema.
  Consumers - Allow for consumers to sign up and maintain access accounts.
  Keys - Require consumers define their applications and use API keys with API calls.
  Documentation - Automatically publish documentation for all APIs that are published.
  Reporting - Provide reporting on all gateway activity across each API and the lifecycle.
    
      Platform - Deliver platform specific API consumption report.
      Consumer - Provide consumer specific API consumption reports.
    
  


That is a pretty long list. I know there are other building blocks missing, but this represents my first pass through my API research. It reflects the building blocks I want available when I put an API gateway to work in any cloud, on-premise, or on-device use case. However, in this post I wanted to go beyond what I’d consider the core API gateway, and talk about how we make it more observable, and something that could be applied to regulate an industry. Not something that happens behind the scenes, but something that happens out in the open, bringing in some sunlight, and pulling back the curtain on the black boxes some companies enjoy operating. While this won’t solve all our problems, I think it will provide a pretty good first step towards bringing some much needed observability to the tech sector, using common solutions that already exist. Here are a handful of building blocks I think could contribute to this conversation.


  Open Source - Ensure that the gateway is open source, and something that is developed out in the open, and can be forked an operated by anyone.
  Organizational - Provide the suggested framework for the operating organization, and what staffing and other resources are required to operate an instance of this model, providing a (hopefully) neutral entity to bring to life.
  Provider Directory - Require that all providers who have APIs published as part of the platform be profiled and available within a single directory, breaking down the resources they have published, as well as usage, monitoring, and other relevant information.
  Consumer Directory - Require that all consumers who have access to the platform publish a profile, share how their are using APIs, and offer a summary of their authentication, authorization, usage levels, and data points.
  Research Access - Provide access to researchers as part of the consumer management, but offering wider access to platform data and consumption based upon agreed upon plans.
  Media Access - Allow for access my media organization who are looking to understand what is happening across a platform, and the impact on consumers via applications.
  Industry Access - Provide access to industry analysts as part of the consumer management, but offering wider access to platform data and consumption based upon agreed upon plans.
  Auditor Access - Provide complete auditor access to the entire platform, allowing certified auditors to come in and review resources, consumers, logs, and any other aspects of the gateway operations.
  Schema Catalog - A complete catalog of all the types of data that is tracked and made available across all APIs, and used in desktop, web, mobile, and device applications.
  Monetization - Allow for the monetization of provider participation, consumer, researcher, and industry access, allowing for well defined plans, rate limits, and observability into revenue that is generated.
  Issues - Provide the ability for the public, consumers, researchers, and industry to submit issues in a safe, moderated, and accessible space, ensuring there is observability into all issues across the gateway.
  Disputes -  Ensure there are trained professionals to help address disputes on the platform rising from issues reported by the public, consumers, researchers, and industry analysts using the gateway.
  Reporting - Provide reporting to key stakeholders.
    
      Private - Provide comprehensive reporting accessible to trusted internal stakeholders.
      Public - Publish a line of regular public reports on platform usage and consumption.
      Researcher - Provide a set of reports just for researchers based upon plans and agreements.
      Media - Provide a rich set of reports that help keep media understanding what is going on.
      Auditors - Give auditors a full set of reports, including summary and detail access.
    
  


This model isn’t without precedent. Last year I spent a couple months studying the approach by UK regulators to bring more observability into the banking sector, and the formation of Open Banking organization (https://www.openbanking.org.uk/). Learning more about what open banking was in the UK (http://apievangelist.com/2018/02/21/what-is-open-banking-in-the-uk/),  how it provides a common API definition (http://apievangelist.com/2018/02/21/open-banking-in-the-uk-openapi-template/), who the common stakeholders were (http://apievangelist.com/2018/02/26/the-banking-api-actors-in-the-uk/), and exploring how I could emulate this approach as an open source API industry template (http://open.banking.blueprint.apievangelist.com/). In 2019, I want to go even further with this open source API Blueprint, understand how it can be used to define a common open source API gateway specification, while adding the necessary building blocks to ensure there is observability at the industry level.

I am proposing that this model be defined, standardized, and applied at the single provider, or industry level. If required, an independent entity can be setup to operate the API gateway as an independent platform, funded by regulators, and the monetization layer that leverages a mix of providers, consumers, researchers, and industry analyst access to the platform. While there will be private layers of the platform, the goal is to provide as much as possible out in the open, operating as many public API platforms have been operating for the last 20 years. Ironically, some of the worst behaved technology platforms have operated using this model in the past, but sadly have been actively tightening down access levels. I’m proposing we take their formula, open source it, and operate it independently, out in the open, and make them pay for it.

As I said before, this will not solve all problems. However it will define a layer that ALL desktop, web, mobile, and device applications can be required to go through. Requiring that provides only develop applications on top of APIs deployed within regulated API gateways. Requiring that all internal, partner, and 3rd party public consumers access API via the single, or regionally distributed gateways. If auditors ever find that an API provider is leverage APIs not listed in the API gateway directory, or their partners and the public have access to data that is not defined in schema within the API gateway—then there is a problem, and enforcement can follow. Of course, all of this isn’t as simple as proposed in this post, but it provides the scaffolding and blueprint for how it can be applied. There is no reason this can’t be applied to regulate technology companies, and applied to existing industries that are already regulated, helping bring more observability into already existing regulatory practices.

Nothing I’m proposing here is rocket science, or theoretical. It is based upon proven practices of tech companies. All I’m saying now, is rather than just advising companies, organizations, institutions, and government follow these best practices, I’m saying that we should begin working to establish a standard, and craft policy that requires everyone to participate. I know many tech folks don’t like the idea of regulation, but for certain industries, and certain platforms, it might be a positive thing to inject some regulation into the equation, and begin doing things out in the open, rather than behind the curtain. If you have any questions or comments on this blueprint, feel free to email me at info@apievangelist.com, or submit an issue on the GitHub repository for this blueprint.
]]></content>
    <id>http://apievangelist.com/2019/08/15/an-observable-regulatory-provider-or-industry-api-management-gaeway/</id>
  </entry><entry>
    <title>An API Platform Reliability Engineer At Stripe</title>
    <link href="http://apievangelist.com/2019/08/15/an-api-platform-reliability-engineer-at-stripe/"/>
    <updated>2019-08-15T02:00:00Z</updated>
    <content><![CDATA[
I find that the most interesting and telling API building blocks come out of the companies who are furthest along in their API journey, and have made the conscious effort to heavily invest in their platforms. I try to regularly visit API platforms who are doing the most interesting work on a regular basis, because I am almost always able to find some gem of an approach that I can showcase here on the blog.

This weeks gem is from API rockstar Stripe, and their posting for a reliability engineer for their API platform. Here is a summary from their job posting:

As a Reliability Engineer, you’ll help lead an active area of high impact by defining and building proactive ways to further hone the reliability of our API. You’ll collaborate with team members across Engineering, as well as with our business, sales and operations teams to determine areas of greatest leverage.

You Will:


  Work with engineers across the company to identify key areas for reliability improvement.
  Gather requirements and make thoughtful tradeoffs to ensure we are focusing our efforts on the most impactful projects.
  Work on services and tools to proactively improve the quality and reliability of our production API.
  Debug production issues across services and multiple levels of the stack. Improve operational standards, tooling, and processes.


I’ve studied thousands of APIs over almost a decade, and seeing a company invest this heavily in API reliability is a rare thing. For me, this demonstrates two things, that Stripe takes their API seriously, and that it takes a huge amount of investment and resources to do APIs right. Something I don’t think many API providers realize as they try to do APIs as a side project, and wonder why they aren’t seeing the results they want.

I find that the job postings for API providers is one of the most telling signals I can harvest to understand how committed a company is to their APIs. Human Resources is one of the most areas to be investing in when it comes to your API operations, and the frequency and type of API job postings tells a lot about the API journey a company is on. Hiring API engineers is an important role to be hiring for, but it will also take hiring someone dedicated to reliability to make the impact with your platform that you desire.
]]></content>
    <id>http://apievangelist.com/2019/08/15/an-api-platform-reliability-engineer-at-stripe/</id>
  </entry><entry>
    <title>Some Of The API Gateway Building Blocks</title>
    <link href="http://apievangelist.com/2019/08/05/some-of-the-api-gateway-building-blocks/"/>
    <updated>2019-08-05T02:00:00Z</updated>
    <content><![CDATA[
The inspiration for this post wasn’t fully mine, I’m borrowing and building upon what Sukanya Santhanam (@SukanyaSanthan1) tweeted out the other day. It is a good idea, and something that should be open sourced and moved forward. I’ve been studying with API management since 2010, and using gateways for over 15 years. I’ve watched gateways evolve, and partnered regularly with API management and gateway providers (Shout out to Tyk). Modern API gateways aren’t your grandfather’s SOA tooling, they’ve definitely gone through several iterations. While I still prefer hand rolling and forging my APIs out back in my woodshed on an anvil, I find myself working with a lot of different API gateways lately.

I’ve kept feeling like I needed to map out the layers of what I’d consider to be a modern API gateway, and begin providing links to the most relevant API gateways out there, and the most common building blocks for an API gateway. Now that you can find API gateways baked into the fabric of the cloud, it is time that we work to standardize the definition of what they can deliver. I’m not looking to change what already is. Actually, I’m looking to just document and build on what already is. As with every other stop along the API lifecycle I’m looking to just map out the common building blocks, and establish a blueprint going forward the might influence existing API gateway providers, as well as any newcomers.

After going through my API gateway research for a while, I quickly sketched out these common building blocks for helping deploy, manage, monitor, and secure your APIs:


  Paths - Allowing many different API paths to exist.
  Schema - Allowing me to manage all of my schema.
  Integrations - Providing backend lego architecture.
    
      Resource - Allow for integration with other APIs.
      Database - Provide a stack of database integrations.
      Other - Define whole buffet of integration definitions.
    
  
  Requests - Define all of my HTTP 1.1 requests
    
      Methods - Providing me with my HTTP verbs.
      Path Parameters - Able to define path parameters.
      Query Parameters - Able to define query parameters.
      Bodies - Providing control over the request body.
      Headers - Full management of HTTP request headers.
      Encoding - Defining the media types in in use for requests.
      Validate - Providing validation for all incoming requests.
      Mappings - Allowing for mapping of requests to backend.
      Transformations - Transformation before sending to backend..
      Examples - Ensuring there are samples for each request.
      Schema - Able to reference all schema used in requests.
      Tags - Being able to organize API requests using tags.
    
  
  Responses - Define all of my HTTP 1.1 responses.
    
      Status Codes - Providing the ability to define HTTP status codes.
      Headers - Full management of all HTTP response headers.
      Encoding - Defining the media types in in use for responses.
      Schema - Able to reference all schema used in responses.
      Examples - Ensuring there are samples for each response.
    
  
  Stages - Able to stage APIs under any platform defined environment.
  Publishing - Allowing for conscious publishing of APIs into production.
  Versioning - Providing semantic versioning as header or in the path.
  Policies - Defining policies for API, and schema access by consumers.
  Licensing - Ensure that data and APIs are properly licensed for consumption.
  Plans - Crafting a handful of standard access tiers for different consumers.
  Rate Limiting - Define the rate limits for all APIs within each plan offered.
  Domains - Allow for default and custom domains associated with APIs.
  Certificates - Provide management and usage of certificates for encryption.
  Tags - Allow APIs, as well as their individual paths, and requests to be tagged.
  Dependencies - Inform on the dependencies between APIs, including 3rd party.
  Regions - Allow for multi-region deployment of APIs, with full DNS support.
  Contact - Ensure there is contact information for every API owner.
  Logging - Standardize the logging for all API traffic to one or many locations.
  Monitoring - Provide basic monitoring of all APIs from alternate locations.
  Status - Offer a real time status dashboard and notification for all APIs.
  Terms of Service - Allow for the publishing of one or many TOS applying to APIs.
  Authentication - Provide a handful of standard authentication mechanisms.
  Authorization - Enable fine grade authorization across APIs and schema.
  Consumers - Allow for consumers to sign up and maintain access accounts.
  Keys - Require consumers define their applications and use API keys with API calls.
  Documentation - Automatically publish documentation for all APIs that are published.
  Reporting - Provide reporting on all gateway activity across each API and the lifecycle.
    
      Platform - Deliver platform specific API consumption report.
      Consumer - Provide consumer specific API consumption reports.
    
  


I’m going to add these to my API gateway research. I’m sure there are other building blocks out there, but I think this is a good start. It reflects what I think makes API gateways different from API management. It has the design, deployment, and backend integration portion of the conversation, as well as the key API management features expected. I see API gateways as a Venn diagram of API lifecycle features. Providing a single blueprint, tooling, and appliance that will help you deliver, manage, distribute, and scale your API infrastructure.

You can find this outline published over at GitHub. I will me managing it as a living document and opening up to feedback via GitHub issues. I’m going to evolve this as a core API gateway specification—eventually defining APIs and schema for each layer of the stack. This will take some time because I will have to profile several of the existing API gateway APIs and mine them for logical patterns. Borrowing from their API designs, and schema, layering them together to create a common set of building blocks. I’m going to also begin iterating upon what I’d call an observable regulatory set of API gateway building blocks to augment this existing list. Establishing my vision of not just an API gateway standard, which can be used internally, as well as openly as part of an industry-wide effort to deliver consistent APIs for a collective purpose.
]]></content>
    <id>http://apievangelist.com/2019/08/05/some-of-the-api-gateway-building-blocks/</id>
  </entry><entry>
    <title>A Look At Behavioral API Patents</title>
    <link href="http://apievangelist.com/2019/08/05/a-look-at-behavioral-api-patents/"/>
    <updated>2019-08-05T02:00:00Z</updated>
    <content><![CDATA[
I have been studying uses of behavioral technology lately. Riffing off my partner in crimes work on the subject, but putting my own spin on it, and adding APIs (of course) into the mix. Applying on of my classic techniques, I figured I’d start with a patent search for “behavioral application programming interfaces”. I find patents to be a “wonderful” source of inspiration and understanding when it comes to what is going on with technology. Here are the top results for my patent search, with title, abstract, and a link to understand more about each patent.

User-defined coverage of media-player devices on online social networks
In one embodiment, a method includes detecting, by a media-player device including multiple antennas, a client system of a user is within a wireless communication range of the media-player device. In response to the detection, the media-player device broadcasts an authentication key for the user of the client system. The media-player device then registers the user to the media-player device based on the authentication key being verified by the client system. The media-player device further receives from the client system instructions to adjust a power level of each of the multiple antennas. The instructions are determined based on broadcast signals received at the client system and on a respective position of the client system associated with each received broadcast signal. The respective position of the client system is determined with respect to a position of the media-player device.

Controlling use of vehicular Wi-Fi hotspots by a handheld wireless device
A system and method of controlling use of vehicular Wi-Fi hotspots by a handheld wireless device includes: detecting that the handheld wireless device is communicating via a Wi-Fi hotspot; determining at the handheld wireless device that that the Wi-Fi hotspot is provided by a vehicle; and enabling one or more default prohibitions against transmitting low-priority data from the handheld wireless device via a cellular wireless carrier system while the handheld wireless device communicates with the Wi-Fi hotspot provided by the vehicle.

System and method for collecting data
The passive data collection method is sometime more reliable because the direct query method might not be available or possible through a 3.sup.rd party channel application. Accordingly, an improved data collection method is provided. The method includes: running a channel application located on a first layer of an operating system of a user device; receiving an application interface (API) call, from the channel application, for a graphic rendering module located on a second layer of the operating system, wherein the graphic rendering module is a non-video playback module; intercepting metadata sent to the graphic rendering module; determining identifying information of a content based on the intercepted metadata; and storing the determined identifying information of the content.

System and method for analytics with automated whisper mode
A service session is facilitated via a packet switched network; in the service session, user equipment participates in an interactive communication exchange with an agent via a first interaction mode, and the interactive communication exchange is based on a user inquiry. The interactive communication exchange is monitored and a determination is made that a consultation service would facilitate resolution of the user inquiry. A service resource is associated with the service session responsive to determining that the consultation service would facilitate the resolution; the service resource provides consultation to the agent via a second interaction mode without exposing the consultation to the user equipment. The consultation elevates an experience level employed in the first service session towards resolution of the user inquiry.

Method, device, and system for displaying information associated with a web page
Embodiments of the present application relate to a method, device, and system for displaying information. The method includes receiving a web page access request, in response to receiving the web page access request, displaying a first web page and obtaining designated information associated with the first web page, the first web page being associated with the web page access request and the designated information including content of the first web page, receiving an instruction to navigate to a second web page, in response to receiving the instruction to navigate to the second web page, communicating the designated information to a server associated with the second web page, and displaying the second web page, the second web page including information communicated by the server associated with the second web page.

Large-scale page recommendations on online social networks
In one embodiment, a method includes accessing user-concept scores for a first set of users, wherein each user-concept score is associated with a user-concept pair; calculating recommended user-concept scores for a subset of user-concept pairs in a second set of users. The first set of users may be discrete from the second set of users. A recommendation-algorithm may compute the recommended user-concept scores for a user-concept pair by optimizing an objective function comprising a plurality of predicted rating functions. Each predicted rating function may be determined using a user score, a concept score, a user-bias value associated with the user, as well as a concept-bias value associated with the concept. Finally, the method may include sending recommendations for one or more concepts based on the recommended user-concept scores for the second set of users.

Technologies for secure personalization of a security monitoring virtual network function
Technologies for secure personalization of a security monitoring virtual network function (VNF) in a network functions virtualization (NFV) architecture include various security monitoring components, including a NFV security services controller, a VNF manager, and a security monitoring VNF. The security monitoring VNF is configured to receive provisioning data from the NFV security services controller and perform a mutually authenticated key exchange procedure using at least a portion of the provisioning data to establish a secure communication path between the security monitoring VNF and a VNF manager. The security monitoring VNF is further configured to receive personalization data from the VNF manager via the secure communication path and perform a personalization operation to configure one or more functions of the security monitoring VNF based on the personalization data. Other embodiments are described and claimed.

Systems and methods for implementing intrusion prevention
System and methods are provided for implementing an intrusion prevention system in which data collected at one or more remote computing assets is analyzed against a plurality of workflow templates. Each template corresponding to a different threat vector and comprises: (i) a trigger definition, (ii) an authorization token, and (iii) an enumerated countermeasure responsive to the corresponding threat vector. When a match between the data collected at the one or more remote computing assets and a trigger definition of a corresponding workflow template is identified, an active threat is deemed to be identified. When this occurs the authorization token of the corresponding workflow template is enacted by obtaining authorization from at least two authorization contacts across established trust channels for the at least two authorization contacts. Responsive to obtaining this authorization, the enumerated countermeasure of the corresponding workflow template is executed.

Identity and trustworthiness verification using online and offline components
Methods and systems for verifying the identity and trustworthiness of a user of an online system are disclosed. In one embodiment, the method comprises receiving online and offline identity information for a user and comparing them to a user profile information provided by the user. Furthermore, the user's online activity in a third party online system and the user's offline activity are received. Based on the online activity and the offline activity a trustworthiness score may be calculated.

Protecting sensitive information from a secure data store
In embodiments of the present invention improved capabilities are described for the steps of receiving an indication that a computer facility has access to a secure data store, causing a security parameter of a storage medium local to the computer facility to be assessed, determining if the security parameter is compliant with a security policy relating to computer access of the remote secure data store, and in response to an indication that the security parameter is non-compliant, cause the computer facility to implement an action to prevent further dissemination of information, to disable access to network communications, to implement an action to prevent further dissemination of information, and the like.

Local data aggregation repository
Apparatuses, systems, methods, and computer program products are disclosed for a local repository of aggregated data. A hardware device comprises a local repository of data aggregated, for a user, from a plurality of third party service providers. A hardware device comprises a local authentication module configured to secure, on the hardware device, aggregated data and electronic credentials of a user for a plurality of third party service providers. A hardware device comprises an interface module configured to provide access controls to a user defining which of a plurality of other third party service providers the user authorizes to access aggregated data, and to provide the aggregated data to the authorized other third party service providers.

Mapping and display for evidence based performance assessment
A computer-implemented method for providing a user with a performance indicator score includes receiving a first transaction message that includes historical clinical-trial performance data from one or more processors at a clinical research organization and receiving a second transaction message with health records data with parameters indicative of insurance claims data. The received historical clinical-trial performance data and the prescription data is translated into an updated database. Related records within the updated database are identified and one or more key performance indicators included in the data at the updated database for a first physician are identified. A score for each of the one or more key performance indicators are calculated and a performance indicator score record for the first physician is generated based on the calculated scores for each of the one or more key performance indicators. A multi-dimensional chart for organizing and evaluating investigators is generated.

Programmable write word line boost for low voltage memory operation
A system and method for efficient power, performance and stability tradeoffs of memory accesses under a variety of conditions are described. A system management unit in a computing system interfaces with a memory and a processing unit, and uses boosting of word line voltage levels in the memory to assist write operations. The computing system supports selecting one of multiple word line boost values, each with an associated cross-over region. A cross-over region is a range of operating voltages for the memory used for determining whether to enable or disable boosting of word line voltage levels in the memory. The system management unit selects between enabling and disabling the boosting of word line voltage levels based on a target operational voltage for the memory and the cross-over region prior to updating the operating parameters of the memory to include the target operational voltage.

Ray compression for efficient processing of graphics data at computing devices
A mechanism is described for facilitating ray compression for efficient graphics data processing at computing devices. A method of embodiments, as described herein, includes forwarding a set of rays to a ray compression unit hosted by a graphics processor at a computing device, and facilitating the ray compression unit to compress the set of rays, wherein the set of rays are compressed into a compressed representation.

Transactionally deterministic high speed financial exchange having improved, efficiency, communication, customization, performance, access,  trading opportunities, credit controls, and fault tolerance
The disclosed embodiments relate to implementation of a trading system, which may also be referred to as a trading system architecture, having improved performance which further assures transactional determinism under increasing processing transaction loads while providing improved trading opportunities, fault tolerance, low latency processing, high volume capacity, risk mitigation and market protections with minimal impact, as well as improved and equitable access to information and opportunities.

Product notification and recommendation technology utilizing detected activity
An exemplary system and method provides a product notification and recommendation technology for monitoring a computing device to detect particular use-cases of device activity and providing a notification through a user interface that indicates at least one product corresponding to the detected particular use-cases. In this way, the product notification and recommendation technology adds a new dimension of usage-based personalization to targeted advertising that results in timely product and service recommendations.

Presentation of content items in view of commerciality
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for enhancing selecting relevant and diverse advertisements. In one aspect, a method includes receiving an initial query, selecting one or more additional queries relating to the initial query, including selecting additional queries having a greatest commerciality, identifying one or more content items for each of the additional queries, the one or more content items forming a content block, and providing a content block and an associated additional query to a client device to be displayed along with search results associated with the initial query.

Protecting privacy of personally identifying information when delivering targeted assets
Techniques are disclosed herein for protecting personally identifying information (PII) and behavioral data while delivering targeted assets. In one aspect, a profile is created based on a template and desired characteristics of users to receive one or more targeted assets. The template provides a framework for the user characteristics. One or more clients are provided the template. A manifest that identifies the targeted assets is encrypted based on the profile. The encrypted manifest is sent to the one or more clients. A user profile is generated at a client based on a template. The client attempts to decrypt the encrypted manifest based on the profile created at the client. The client sends a request for any targeted assets that were identified through the attempt to decrypt the encrypted manifest.

Vector-based characterizations of products and individuals with respect to customer service agent assistance
Systems, apparatuses, and methods are provided herein for providing customer service agent assistance. A system for providing customer service agent assistance comprises a customer profile database storing customer partiality vectors for a plurality of customers, the customer partiality vectors comprise customer value vectors, a communication device, and a control circuit. The control circuit being configured to: provide a customer service agent user interface on a user device associated with a customer service agent, associate a particular customer with the customer service agent, retrieving at least one customer value vector for the particular customer from the customer profile database, and cause, via the communication device, the at least one customer value vector of the particular customer to be displayed on the customer service agent user interface of the user device.

Sparse neural control
Aspects herein describe new methods of determining optimal actions to achieve high-level goals with minimum total future cost. At least one high-level goal is inputted into a user device along with various observational data about the world, and a computational unit determines, though a method comprising backward and forward sweeps, an optimal course of action as well as emotions. In one embodiment a user inputs a high-level goal into a cell phone which senses observational data. The cell phone communicates with a server that provides instructions. The server determines an optimal course of action via the method of backward and forward sweeps, and the cell phone then displays the instructions and emotions to the user.

Framework for classifying an object as malicious with machine learning for deploying updated predictive models
According to one embodiment, an apparatus comprises a first analysis engine and a second analysis engine. The first analysis engine analyzes an object to determine if the object is malicious. The second analysis engine is configured to (i) receive results of the analysis of the object from the first analysis engine and (ii) analyze, based at least in part on the analysis by the first analysis engine, whether the object is malicious in accordance with a predictive model. Responsive to the first analysis engine and the second analysis engine differing in determinations as to whether the object is malicious, information associated with an analysis of the object by at least one of the first analysis engine and the second analysis engine is uploaded for determining whether an update of the predictive model is to occur. An update of the predictive model is subsequently received by the classification engine.

Continuous user authentication
A method of enabling continuous user authentication, comprising: setting up an authentication server to provide authentication data to an enterprise server in parallel to a remote user session with the enterprise server, when the user is using a touch screen device; extracting samples from a user's behavior, to build a library of user specific parameters; and tracking user behavior to authenticate the user, the tracking comprises initial identification of a user of the touch screen device when starting a session with the enterprise server and continuous authentication of the user during the session with the enterprise server.

System and method for decentralized autonomous healthcare economy platform
A system and method for a decentralized autonomous healthcare economy platform are provided. The system and method aggregates all of the healthcare data into a global graph-theoretic topology and processes the data via a hybrid federated and peer to peer distributed processing architectures.

Safety features for high level design
This disclosure relates generally to electronic design automation using high level synthesis techniques to generate circuit designs that include safety features. The algorithmic description representation can be specified in a first language and include at least one programming language construct associated with a first safety data type. Compiling the algorithmic description may involve identifying the at least one construct, accessing a first safety data type definition associated with the first safety data type, and generating a second representation of the circuit design based on the algorithmic description representation and the first safety data type definition. The second representation can be provided in a second language and include at least one safety feature for a portion of the circuit design associated with the at least one construct.

Configuring a programmable device using high-level language
A method of preparing a programmable integrated circuit device for configuration using a high-level language includes compiling a plurality of virtual programmable devices from descriptions in said high-level language. That compiling includes compiling configurations of configurable routing resources from programmable resources of said programmable integrated circuit device, and compiling configurations of a plurality of complex function blocks from programmable resources of said programmable integrated circuit device. A machine-readable data storage medium may be encoded with a library of such compiled configurations. A virtual programmable device may include a stall signal network and routing switches of the virtual programmable device may include stall signal inputs and outputs.

System and process for simulating the behavioral effects of timing violations between unrelated clocks
According to one aspect, embodiments of the invention provide a CDC simulation system comprising a timing analysis module configured to receive a circuit design, analyze the circuit design to identify at least one CDC, and generate a report including information related to the at least one CDC, a CDC simulation module configured to communicate with the timing analysis module and to receive the report from the timing analysis module, and a test bench module configured to communicate with the CDC simulation module, to receive the circuit design, and to operate a test bench code to simulate the operation of the circuit design, wherein the CDC simulation module is further configured to edit a top level of the test bench code, based on the received report, such that the test bench module is configured to identify timing violations in the circuit design due to the at least one CDC.

Deep compositional frameworks for human-like language acquisition in virtual environments
Described herein are systems and methods for human-like language acquisition in a compositional framework to implement object recognition or navigation tasks. Embodiments include a method for a model to learn the input language in a grounded and compositional manner, such that after training the model is able to correctly execute zero-shot commands, which have either combination of words in the command never appeared before, and/or new object concepts learned from another task but never learned from navigation settings. In embodiments, a framework is trained end-to-end to learn simultaneously the visual representations of the environment, the syntax and semantics of the language, and outputs actions via an action module. In embodiments, the zero-shot learning capability of a framework results from its compositionality and modularity with parameter tying.

Efficient word encoding for recurrent neural network language models
Systems and processes for efficient word encoding are provided. In accordance with one example, a method includes, at an electronic device with one or more processors and memory, receiving a user input including a word sequence, and providing a representation of a current word of the word sequence. The representation of the current word may be indicative of a class of a plurality of classes and a word associated with the class. The method further includes determining a current word context based on the representation of the current word and a previous word context, and providing a representation of a next word of the word sequence. The representation of the next word of the word sequence may be based on the current word context. The method further includes displaying, proximate to the user input, the next word of the word sequence.

Displaying temporary profile content items on communication networks
In one embodiment, a method includes accessing, from a data store of the communication network, user information associated with a first user of the communication network, identifying one or more entities associated with the communication network that are relevant to the first user based on the user information, and retrieving, for each identified entity, one or more content frames associated with the entity. The method includes ranking the one or more content frames based on the user information. The method also includes sending, to a client device of the first user, one or more of the content frames for display to the first user in ranked order, wherein each content frame is selectable by the first user to display the selected content frame in association with a particular content item for a specified period of time.

Method and system for mining frequent and in-frequent items from a large transaction database
The technique relates to a system and method for mining frequent and in-frequent items from a large transaction database to provide the dynamic recommendation of items. The method involves determining user interest for an item by monitoring short item behavior of at least one user then selecting a local category, a neighborhood category and a disjoint category with respect to the item clicked by the at least one user based on long term preferences data of a plurality of users of the ecommerce environment thereafter selecting one or more frequent and infrequent items from each of the selected local, neighborhood and disjoint category items and finally generating one or more dynamic recommendations based on the one or more items selected from the local category, the neighborhood category and the disjoint category and the one or more selected frequent and infrequent items.

Clustering based process deviation detection
Systems and methods for data analysis include correlating event data to provide process instances. The process instances are clustered, using a processor, by representing the process instances as strings and determining distances between strings to form a plurality of clusters. One or more metrics are computed on the plurality of clusters to monitor deviation of the event data.

Automated system and method to customize and install virtual machine configurations for hosting in a hosting environment
Some embodiments provide a method for automated configuration of a set of resources for hosting a virtual machine at a particular node in a hosting system. The hosting system includes several nodes for hosting virtual machines. The method, at a first virtual machine operating using a first set of resources of the particular node, receives a user-specified virtual machine configuration for a second virtual machine to be hosted on a second set of resources of the particular node. The method retrieves, to the first virtual machine, a software image from a computer readable hardware medium storing several software images based on the user-specified virtual machine configuration. The method modifies the retrieved software image according to the user-specified virtual machine configuration. The method configures the second set resources using the modified software image.

Systems and methods of monitoring a network topology
The technology disclosed relates to maintaining up to date software version data in a network. In particular, it relates to accessing a network topology that records node data and connection data including processes running on numerous hosts grouped into local services on the hosts, the local services running on multiple hosts grouped into service clusters and sub-clusters of service clusters, and network connections used by the service clusters to connect the hosts grouped into service connections. It further relates to collecting current software version information for the processes, updating the network topology with the current software version for particular process running on a particular host when it differs from a stored software version in the network topology, reassigning the particular host to a sub-cluster within the service cluster according to the current software version, and monitoring the updated sub-cluster within the service cluster.

Power management of memory chips based on working set size
Briefly, in accordance with one or more embodiments, an apparatus comprises a memory comprising one or more physical memory chips, and a processor to implement a working set monitor to monitor a working set resident in the one or more physical memory chips. The working set monitor is to adjust a number of the physical memory chips that are powered on based on a size of the working set.

Mobile communication terminal providing adaptive sensitivity of a click event
A mobile communication terminal has a controller and a touch display. The touch display is arranged to display at least a first graphical object and a second graphical object, receive a touch, and determine a touch position and a touch duration for the touch. The controller is configured to receive the touch position and the touch duration, determine a graphical object, among the first graphical object and the second graphical object, corresponding to the touch position, determine if the touch duration exceeds a reference time threshold, and if so, generate a click event for the corresponding graphical object. The first graphical object is associated with a first time threshold, and the second graphical object is associated with a second time threshold. The first time threshold is different from the second time threshold. The controller is further configured to retrieve the first time threshold if the corresponding graphical object is the first graphical object and use the first time threshold as the reference time threshold, and retrieve the second time threshold if the corresponding graphical object is the second graphical object and use the second time threshold as the reference time threshold. The first time threshold is either higher or lower than said second time threshold depending on one or more of the following: a size, shape or/and color of the corresponding graphical object, a distance from the corresponding graphical object to a neighboring graphical object, a relative location of the corresponding graphical object in a touch area of the touch display, and a level of the corresponding graphical object in a menu hierarchy.

Control system user interface
Embodiments include systems and methods comprising a gateway located at a premise forming at least one network on the premise that includes a plurality of premise devices. A sensor user interface (SUI) is coupled to the gateway and presented to a user via a remote device. The SUI includes at least one display element. The at least one display element includes a floor plan display that represents at least one floor of the premise. The floor plan display visually and separately indicates a location and a current state of each premise device of the plurality of premise devices.

Optimizing transportation networks through dynamic user interfaces
 The present disclosure relates to providing a dynamic graphical user interface for efficiently presenting users with relevant ride information throughout the fulfillment of a ride request. In some embodiments, the system detects a trigger event during a ride, and based on detecting the trigger event, the system expands or collapses an information portion within a graphical user interface. When in a collapsed state, for example, the information portion of the graphical user interfaces includes a first set of content. Upon detecting a trigger event, the system dynamically expands the information portion to provide a second set of content that includes information associated with the detected trigger.

Human-to-mobile interfaces
A method of character recognition for a personal computing device comprising a user interface capable of receiving inputs that are to be recognized through data input means which are receptive to keyed, tapped or a stylus input, said device being adapted to facilitate a reduction in the number of physical keying actions, tapping actions or gestures required to create a data string to less than the number of characters within said data string: storing a set of data strings each with a priority indicator associated therewith, wherein the indicator is a measure of a plurality of derivatives associated with the data string; recognizing an event; looking up the most likely subsequent data string to follow the event from the set of data strings based on one or more of the plurality of derivatives; ordering the data strings for display based on the priority indicator of that data string.

Personalized autonomous vehicle ride characteristics
Systems and method are provided for controlling a vehicle. In one embodiment, a method includes: obtaining ride preference information associated with a user, identifying a current vehicle pose, determining a motion plan for the vehicle along a route based at least in part on the ride preference information, the current vehicle pose, and vehicle kinematic and dynamic constraints associated with the route, and operating one or more actuators onboard the vehicle in accordance with the motion plan. The user-specific ride preference information influences a rate of vehicle movement resulting from the motion plan.

Feasible lane routing
 Systems and method are provided for controlling a vehicle. The systems and methods calculate lane plan data including a set of lane plans defining a route from a start location to a destination location, solve a motion planning algorithm to produce solved lane plan data defining a solved lane plan and a trajectory therefor, receive forthcoming distance data representing a forthcoming distance, determine a feasible lane based on the solved lane plan data within the forthcoming distance, remove a lane plan from the lane plan data to produce feasible lane plan data including a feasible lane plan defining a route from the start location to the destination location, and control motion of the vehicle based on the feasible lane plan data.

Methods and systems for diagnosing eyes using aberrometer
Configurations are disclosed for a health system to be used in various healthcare applications, e.g., for patient diagnostics, monitoring, and/or therapy. The health system may comprise a light generation module to transmit light or an image to a user, one or more sensors to detect a physiological parameter of the user's body, including their eyes, and processing circuitry to analyze an input received in response to the presented images to determine one or more health conditions or defects.

Mobile localization using sparse time-of-flight ranges and dead reckoning
Mobile localization of an object having an object positional frame of reference using sparse time-of-flight data and dead reckoning can be accomplished by creating a dead reckoning local frame of reference, including an estimation of object position with respect to known locations from one or more Ultra Wide Band transceivers. As the object moves along its path, a determination is made using the dead-reckoning local frame of reference. When the object is within a predetermine range of one or more of the Ultra Wide Band transceivers, a &quot;conversation&quot; is initiated, and range data between the object and the UWB transceiver(s) is collected. Using multiple conversations to establish accurate range and bearing information, the system updates the object's position based on the collected data.

Method and apparatus for identifying defects in a chemical sensor array
An apparatus including an array of sensors including a plurality of chemical sensors and a plurality of reference sensors, each chemical sensor coupled to a corresponding reaction region for receiving at least one reactant, and each reference sensor comprising a field effect transistor having a gate coupled to a corresponding reference line and an access circuit for accessing the chemical sensors and the reference sensors and a controller to apply bias voltages to the reference lines to select corresponding reference sensors, acquire output signals from the selected reference sensors, and identify one or more defects in the access circuit based on differences between the acquired output signals and expected output signals.

Systems and methods for restoring cognitive function
Systems and methods for restoring cognitive function are disclosed. In some implementations, a method includes, at a computing device, separately stimulating one or more of lateral and medial entorhinal afferents and other structures connecting to a hippocampus of an animal subject in accordance with a plurality of predefined stimulation patterns, thereby attempting to restore object-specific memories and location-specific memories; collecting a plurality of one or more of macro- and micro-recordings of the stimulation of hippocampalentorhinal cortical (HEC) system; and refining the computational model for restoring individual memories in accordance with a portion of the plurality of one or more of macro- and micro-recordings.

Personal emergency response (PER) system
A system includes one or more sensors to detect activities of a mobile object; and a processor coupled to the sensor and the wireless transceiver to classify sequences of motions into groups of similar postures each represented by a model and to apply the models to identify an activity of the object.

Providing visualization data to a co-located plurality of mobile devices
A computer-implemented method according to one embodiment includes identifying a plurality of mobile devices, determining a relative location of each of the plurality of mobile devices, and assigning visualization data to each of the plurality of mobile devices, based on the relative location of each of the plurality of mobile devices.

Systems and methods for automatically detecting users within detection regions of media devices
 Systems and methods are presented for detecting users within a range of a media device. A detection region may be defined that is within the range of the media device and smaller than the range. The detection region may be stored. It may be determined whether a user is within the detection region. The media device may be activated and settings associated with the user may be applied when a user is within the detection region. In some embodiments, settings associated with a user may be compared to provided media content when the user is within the detection region. The content may change when the settings conflict with the media content. Reminders may be provided to or directed to a plurality of users within the range of the media device.

Fractional-readout oversampled image sensor
 Signals representative of total photocharge integrated within respective image-sensor pixels are read out of the pixels after a first exposure interval that constitutes a first fraction of a frame interval. Signals in excess of a threshold level are read out of the pixels after an ensuing second exposure interval that constitutes a second fraction of the frame interval, leaving residual photocharge within the pixels. After a third exposure interval that constitutes a third fraction of the frame interval, signals representative of a combination of at least the residual photocharge and photocharge integrated within the pixels during the third exposure interval are read out of the pixels.

Fraud detection in interactive voice response systems
Systems and methods for call detail record (CDR) analysis to determine a risk score for a call and identify fraudulent activity and for fraud detection in Interactive Voice Response (IVR) systems. An example method may store information extracted from received calls. Queries of the stored information may be performed to select data using keys, wherein each key relates to one of the received calls, and wherein the queries are parallelized. The selected data may be transformed into feature vectors, wherein each feature vector relates to one of the received calls and includes a velocity feature and at least one of a behavior feature or a reputation feature. A risk score for the call may be generated during the call based on the feature vectors.

The why and how of behavioral for each of these technological approaches varies. However, it does provide a nice slice of the pie when it comes to the different angles of how behavioral approaches is being applied. I purposely left the company name off of each of these to make folks click in to see who is behind each one. I’ll spoil it a little, and say the usual suspects like Facebook are behind some of them, but others are behind these different ways in which technological is being used to understand and shift our behavior. Of course, for good. ;-)

I’ll be doing more work to understand what is behind the intent of these patents. My concern around understanding more about how and what is considered behavioral in the API space is more about surveillance, than ever is about truly understanding what the good and bad of behavioral is. This is just the first of many patent searches that I will conduct. I’ll keep evolving my vocabulary for searching and discovering APIs in this area, evolving my results over time, and learning from what companies are up to when it comes to behavioral manipulation via APIs.
]]></content>
    <id>http://apievangelist.com/2019/08/05/a-look-at-behavioral-api-patents/</id>
  </entry><entry>
    <title>Reverse Engineering Mobile APIs To Show A Company Their Public APIs</title>
    <link href="http://apievangelist.com/2019/08/02/reverse-engineering-mobile-apis-to-show-a-company-their-public-apis/"/>
    <updated>2019-08-02T02:00:00Z</updated>
    <content><![CDATA[
One story I tell a lot when talking to folks about APIs, is how you can reverse engineer a mobile phone to map out the APIs being used. As the narrative goes, many companies that I talk with claim they do not have any public APIs when first engage with them. Then I ask them, “Do you have any mobile applications?”. To which the answer is almost always, “Yes!”.  Having anticipated this regular conversation, if I am looking to engage with a company in the area of API consulting, I will have made the time to reverse engineer their application to produce a set of OpenAPI definitions that I can then share with them, showing that they indeed have public APIs.

The process isn’t difficult, and I’ve written about this several times. To reverse engineer a mobile application, all you have to do is download the targeted application to your phone, configure your phone to use your laptop as a proxy, and be running Charles Proxy on your laptop. I’m not going to share a walkthrough of this, it is easy enough to Google and find the technical details of doing it, I’m more looking to just educate the average business person that this is possible. Once you have your mobile phone proxied through Charles, it will capture every call made home to the mother ship, logging the request and response structure of all APIs being used by the mobile application-—which uses public DNS for routing, making it a public API.

I have an API that I developed which I can upload the resulting Charles Proxy output file, and convert all the API calls into an OpenAPI. Making quick work of documenting the APIs behind a mobile application. Which, when you share with someone who is under the belief that their mobile APIs are private APIs, it can make quite a splash. Usually you get a response, like “well, we don’t allow access to the general public for our APIs”. To which I respond, any consumer of your application is a consumer of your APIs. If you use public DNS to handle the transport for your APIs-—they are public APIs. There is no secret force field created by mobile applications to keep your APIs secure or protected. Some applications have invested in SSL pinning to prevent proxying with tooling like Charles Proxy, but there is still ways around—-albeit it requires a significant more work.

As I’m spending more time crafting API discovery tooling and agents, I’m revisiting my work to reverse engineer mobile applications and generate OpenAPI from proxy logs. I’d like to find a way to emulate mobile applications and script the exploration of them. I find having to click through every option and feature within an application pretty mind numbing, and I’d like to automate it a little more. I feel like mobile, browsers, and  internet of things API discovery will be one of the next frontiers when it comes to mapping out the API landscape. I’m guessing there will always be truly public APIs launching, but the majority of APIs will continue to operate in the shadows of our browsers, mobile phones, and other devices that are becoming ubiquitous in our lives.
]]></content>
    <id>http://apievangelist.com/2019/08/02/reverse-engineering-mobile-apis-to-show-a-company-their-public-apis/</id>
  </entry><entry>
    <title>Didn’t We Already Do That?</title>
    <link href="http://apievangelist.com/2019/08/02/didnt-we-already-do-that/"/>
    <updated>2019-08-02T02:00:00Z</updated>
    <content><![CDATA[
When you are in the API game you hear this phrase a lot, “didn’t we already do that?”. It is a common belief system that because something was already done, that it means it will not work ever again. When you are operate solely in a computational world, you tend to see things as 1s and 0s, and if something was tried and “didn’t work”, there is no resetting of that boolean switch for some reason. We excel at believing things are done, without ever unpacking why something failed, or how the context may have changed.

One of the things I’m going to do over the next couple months is go through the entire SOA toolbox and take accounting of everything we threw away, and evaluate what the possible reasoning were behind it—-good and bad. I strongly believe that many folks who abandoned SOA, willingly or unwillingly, and especially the people who enjoy saying, “didn’t we already do that”, haven never spent time unpacking why we did, why it didn’t work, let alone whether or not it might work today. I think this paradigm reflects one of the fundamental illnesses we encounter in the tech sector-—we have a dysfunctional and distorted relationship with the past (this is by design).

I’ve written about this before, but I’ll say it again. Can you imagine saying, “didn’t we already do that” about other non-technical things. Fashion. Art. Music. Stories. Law. Why do we say it in technology? When it comes to SOA, there are many reasons why it didn’t work overall, and most of the reasons were not technical. So why would we not want to re-evaluate some of the technologies and practices to see if there would be a new opportunity to apply an old pattern or approach? This question isn’t just something I’ve heard a handful of times. There have been literally a hundred plus blog posts on API Evangelist where people have commented this—-especially when it comes to JSON Schema and OpenAPI.

Anyways. I’m going to revisit my SOA history. My therapist said enough time has past and I’ve healed properly, so I can begin digging around in this part of my past. I’m guessing there are a lot of practices, tooling, and patterns we can rethink in a JSON, YAML, containerized, CI/CD, cloudy world. Alongside this work I’m going to be assessing what the preferred open source tool are for the API lifecycle, making sure it represents my vision of a diverse API toolbox, tracking on tooling for JSON Schema, OpenAPI, and AsyncAPI that will help us service the entire API lifecycle for both internal and external API delivery. There is a lot of work that has been done in the past that we should be learning from, and I’m more than happy to dive in, do the research, and shine a light on what we’ve left behind.
]]></content>
    <id>http://apievangelist.com/2019/08/02/didnt-we-already-do-that/</id>
  </entry><entry>
    <title>The Future Of APIs Will Be In The Browser</title>
    <link href="http://apievangelist.com/2019/08/01/the-future-of-apis-will-be-in-the-browser/"/>
    <updated>2019-08-01T02:00:00Z</updated>
    <content><![CDATA[
I have been playing with the new browser reporting API lately, and while it isn’t widely supported, it does work in Chrome, and soon Firefox. I won’t go into too much technical detail, but the API provides an interesting look at reporting on APIs usage in the browser. Offering a unique view into the shadows of what is happening behind the curtain in our browser when we are using common web applications each day. I have been proxying my web traffic for a long time to produce a snapshot at the domains who are operating beneath the covers, but it is interesting for browsers to begin baking in a look at the domains who are violating, generating errors, and other shenanigans.

As I’m contemplating the API discovery universe I can’t help but think of the how “API innovation” is occurring within the browser.  When I say “API innovation”, I don’t mean the kind that got us all excited from 2005 through 2010, or the golden days from 2010 through 2015-—I am talking the exploitative kind. Serving advertisers, trackers, and other exploitative practices. Most people would scoff at me calling these things APIs, but they are using the web to deliver machine readable information, so they are APIs. I’ve been tracking on the APIs I use behind the scene in my browser using Charles Proxy for a while now, but I’m feeling I should formalize my analysis.

I’m thinking I’ll take a sampling of domain, maybe 250+, and automate the browsing of each page, while also running through Charles Proxy. Then aggregate all of the domains that are loaded, and categorize them by media type–just to give me a sampling of the APIs in operation behind the scenes of some common sites. I’m sure most are advertising or social related, but I’m guessing there are a lot of other surprises in there. While some of the APIs will be publicly showcased in some way, there are no doubt a number of APIs being used that do not have a public presence, documentation, or other visible element. While I am interested in learning how the public APIs I track on are used, I’m more interested in painting a picture of the shadow APIs that are running behind the JavaScript libraries, and other embeddable in use across the web.

Developer portals and API documentation are not the only way to find APIs. Reverse engineering mobile and web applications will continue to be a significant player when it comes to understanding the next generation of APIs. When I look across the web, all I see are APIs. I know I’m biased, but I think there is something to this. I don’t think all companies are interested in doing APIs the same way many of us API evangelist, pundits, analysts and believers are. They like APIs, but aren’t that interested in showcasing their practices, and sharing patterns with the rest of the community. I’m guessing they are more interested in penetrating our worlds via our browsers, and capturing some of the valuable behavioral exhaust we all produce on a daily basis.
]]></content>
    <id>http://apievangelist.com/2019/08/01/the-future-of-apis-will-be-in-the-browser/</id>
  </entry><entry>
    <title>About Giving Away API Knowledge For Free</title>
    <link href="http://apievangelist.com/2019/08/01/giving-away-api-knowledge-for-free/"/>
    <updated>2019-08-01T02:00:00Z</updated>
    <content><![CDATA[
I’m in the business of providing access to the API knowledge accumulated over the last decade. Despite what many people claim, I do not know everything about APIs, but after a decade I have picked up a few things along the way. Historically, I have really enjoyed sharing my knowledge with people, but I’m increasingly becoming weary of sharing to openly because of the nature of how business gets conducted online. Specifically, that there are endless waves of folks who want to tap what I know without giving anything in return, who work at companies who enjoy a lot of resources. I know people have read the startup handbook, which tells them to reach out to people who know and get their view, but when everyone does this, and doesn’t give anything in return, it is, well…I guess business as usual? Right? ;-(

Taking a sampling from the usual week in my inbox, I’ll get a handful of requests reflecting these personas:


  Analysts / Consultants - Other analysts reaching out to share information, and get my take on what I’m seeing. There is usually reciprocity here, so I’m usually happy to engage, especially if I know them personally, and have worked with them before.
  Startup Founders - I get a wide range of startup founders reaching out, many of which I do not know, wanting to get validation of their idea, and understand the marketplace they are targeting—usually if I know them, or they come with a reference I’ll engage.
  Venture Capitalists - There is a regular stream of VCs wanting to know what is happening, get my take on things, but they usually are just interested  listin validating what they already know, and get introduced to some new concepts.
  Students - There is a growing number of students reaching out, and increasingly PHD students who are working on something API related as part of their studies.


This represents the usual suspects. There are plenty of other outliers, but this represent the regular drumbeat of people making their way into my inbox. Depending on the day, my mood, and the way in which they reach out, I’ll decide to engage or not engage. However, as things are getting much tighter, especially as my time is at a premium each week, and my patience for the API sector decreases, I’m beginning to push back more. One of my biggest pet peeves is when people who have funding, or venture capitalists want to tap my knowledge without compensation. I’m guessing the privilege level with these folks is pretty high, and they are just used to engaging with other people of means—-completely oblivious to the fact that some of us don’t come from wealthy families, and are just making things work on our own.

In coming months I’ll be publishing a range of guides, white papers, and blueprints for people to purchase. Also, as usual I am open to paid consulting time. Beyond that, you’ll have to gather what you can from my short form blog posts, and the research I openly publish across my network of sites. Don’t expect much from me if you cold email me-—however, feel free to do so. The more creative the outreach, and value demonstrated by your pitch might just influence me. However, if you are just cold emailing or calling me, without understanding that I am trying to piece together a living from my work, your outreach will probably not get the response you are looking for. I’m sorry to be so cold about this, but we can’t all just be giving away our knowledge for free, and a little support of my work goes a long way–it shows that you understand and respect how much time I’ve invested in what I know about the space.
]]></content>
    <id>http://apievangelist.com/2019/08/01/giving-away-api-knowledge-for-free/</id>
  </entry><entry>
    <title>The Challenges Of API Discovery Conversations Being Purely Technical</title>
    <link href="http://apievangelist.com/2019/07/31/the-challenges-of-api-discovery-conversations-being-purely-technical/"/>
    <updated>2019-07-31T02:00:00Z</updated>
    <content><![CDATA[
Ironically one of the biggest challenges facing API discovery on the web, as well as within the enterprise, is that most conversations focus purely on the technical, rather than the human and often business implications of finding and putting APIs to work. The biggest movement in the realm of API discovery in the last couple years has been part of the service mesh evolution of API infrastructure, and how your gateways “discover” and understand the health of APIs or microservices that provide vital services to applications and other systems. Don’t get me wrong, this is super critical, but it is more about satisfying a technical need, which is also being fueled by an investment wave-—it won’t contribute to much to the overall API discovery and search conversation because of it’s limited view of the landscape.

Runtime API discovery is critical, but there are so many other times we need API discovery to effectively operate the enterprise. Striving for technical precision at runtime is a great goal, but enabling all your groups, both technical and business to effectively find, understand, engage, and evolve with existing APIs should also be a priority. It can be exciting to focus on the latest technological trends, but doing the mundane profiling, documentation, and indexing of existing API infrastructure can have a much larger business impact. Defining the technical details of your API Infrastructure using OpenAPI, Postman, and other machine readable formats is just the beginning, ideally you are also working define the business side of things along the way.

I find that defining APIs using OpenAPI and JSON Schema to be grueling work. However, I find documenting the teams and owners behind APIs, the licensing, dependencies (both technical and business), pricing, and other business aspects of an API to be even more difficult. Over the last decade we’ve gotten to work standardizing how define the technical surface area of our APIs, but we’ve done very little work to standardize how we license, price, own, collaborate, and track on the other business implications of delivering APIs. This is one reason Steve Willmott and I created the APIs.json format, to help drive this discussion. Providing a machine readable API format to transcend the technical details of APIs, and allow us to better define the operational side of making sure APIs are discoverable.

APIs.json is about defining everything about your APIs that JSON Schema, OpenAPI, and AsyncAPI will not. Where your documentation is, how to find SDKs, what the terms and conditions are, or maybe the licensing behind your API. We designed the API specification to be flexible, and something that can be extended. There are a handful of default property types you can use when applying the format, but ultimately it is about pushing you to define your own using x- extensions. Helping API providers think through what the common building blocks of their API operations are, and provide them with a simple JSON or YAML format for indexing all of these elements for use in your API catalog, or publishing to the root of your developer portal. Helping augment what OpenAPI, JSON Schema, and AsyncAPI have done, but providing a single place for you to hang all of your API artifacts.

I’m working hard to continue refining my catalog of 3K+ APIs.json files. I’m working on better ways to validate or invalidate what I have indexed, and provide a single search interface for them. Once I’ve refreshed the catalog, and synced them with the evolution of the available APIs over at APIs.io, I will publish a fresh list of the companies I’m tracking on. I feel like one of the most critical business aspects of API discovery we consistently overlook, ignore, or are in denial of, is whether an API is still active, and anyone is home. This is a rampant illness in the cataloging of public APIs, but also something that you can find all over the enterprise. We need to do a better job of understand where are APIs are, but also be more honest about which APIs are used, do not have an owner, or are straight 404’ing and shouldn’t be listed in any active API catalog.
]]></content>
    <id>http://apievangelist.com/2019/07/31/the-challenges-of-api-discovery-conversations-being-purely-technical/</id>
  </entry><entry>
    <title>Differences Between API Observability Over Monitoring, Testing, Reliability, and Performance</title>
    <link href="http://apievangelist.com/2019/07/31/differences-between-api-observability-over-monitoring-testing-reliability-and-performance/"/>
    <updated>2019-07-31T02:00:00Z</updated>
    <content><![CDATA[
I’ve been watching the API observability coming out of Stripe, as well as Honeycomb for a couple years now. Then observability of systems is not a new concept, but it is one that progressive API providers have embraced to help articulate the overall health and reliability of their systems. In control theory, observability is a measure of how well internal states of a system can be inferred from knowledge of its external outputs. Everyone (except me) focuses only on the modern approaches for monitoring, testing, performance, status, and other common API infrastructure building blocks to define observability. I insist on adding the layers of transparency and communication, which I feel are the critical aspects of observability—-I mean if you aren’t transparent and communicative about your monitoring, testing, performance, and status, does it really matter?

I work to define observability as a handful of key API building blocks that every API provider should be investing in:


  Monitoring - Actively monitoring ALL of your APIs to ensure they are up and running.
  Testing - Performing tests to ensure APIs aren’t just up but also doing what they are intended to.
  Performance - Adding an understanding of how well your APIs are delivering to ensure they perform as expected.
  Security - Actively locking down, scanning, and ensuring all your API infrastructure is secure.


Many folks rely on the outputs from these areas to define observability, but there are a couple more ingredients needed to make it observable:


  Transparency - Sharing the practices and results from each of these areas is critical.
  Communication - If you aren’t talking about these things regularly they do not exist.
  Status - Providing real time status updates for al these areas is essential.


You can be actively observing the outputs from monitoring, testing, performance, and security operations, but if this data isn’t accessible to other people on your team, within or company, partners, and for the public as required, then things aren’t observable. Of course, I’m not talking about making ALL API activity public, but I’m saying, if you are a public API, and you aren’t providing transparency, communication, and status of your monitoring, testing, performance, and security—-then you aren’t observable.

I know many folks will disagree with me on this part, but that is ok. I am used to it. So far, I haven’t seen much embrace of the observability concept, with many providers either not understanding it, or not grasping the meaningful impact it will have on their operations. So I’m not holding my breath that folks will buy into my portion of it. However it is my self appointed role to make sure the bar is high, even if nobody adopts the same set of rules. In the end, API observability isn’t some new trendy buzzword, it is one of a handful of meaningful constructs that exist to help make us all better, but most likely will get lost in the shuffle of doing APIs each day. :-(
]]></content>
    <id>http://apievangelist.com/2019/07/31/differences-between-api-observability-over-monitoring-testing-reliability-and-performance/</id>
  </entry><entry>
    <title>Peer API Design Review Sessions</title>
    <link href="http://apievangelist.com/2019/07/30/peer-api-design-review-sessions/"/>
    <updated>2019-07-30T02:00:00Z</updated>
    <content><![CDATA[
Public APIs have always benefitted from something that internal APIs do not always received—-feedback from other people. While the whole public API thing didn’t play out as I originally imagined, there is still a lot of benefit in letting other see, and provide open feedback on your API. It is painful for many developers to receive feedback on their API, and it is something that can be even more painful when it is done publicly. This is why so many of us shy away from establishing feedback loops around our APIs. It is hard work to properly design, develop, and then articulate our API vision to an external audience. It is something I understand well, but still suffer from when it comes to properly accessioning peer review and feedback on my API designs.

I prefer opening up to peer reviews of my API designs while they are still just mocks. I’m less invested in them at this point, and it is easier to receive feedback on them. It is way less painful to engage in an ongoing discussion fo what an API should (and shouldn’t) do early on, then it is to define the vision, deliver an API as code or within a gateway, and then have people comment on your baby that you have given birth to. It hurts to have people question your vision, and what you’ve put forth. Especially for us fragile white men who who aren’t often very good at accepting critical feedback, and want to just be left to our own devices. I’d much prefer just being a coder, but around 2008 through 2010 I saw the benefits to my own personal development when I opened up my work to my peers and let a little sunlight in. I am a better developer because of it.

One tool in my API toolbox that is growing in importance is the peer, and open API design review sessions. Taking an OpenAPI draft, loading it into Swagger Editor, firing up a Zoom or Google Hangout, and inviting others to openly share in the design of an API. I find it isn’t something everyone is equipped to do, but many are open to learning, or at least curious about how it works. Curious is good. It is a start. I think many folks aren’t fluent in the API design process, and are often afraid to appear like they don’t know what they are doing, and having an open discussion throughout the API design process helps them learn out in the open. Using a process that helps everyone involved learn together, and lower their guard a little bit when it comes to new ideas, new ways of doing things, and discussing the overall developer experience (DX) of delivering a quality API.

Peer API Design reviews is something I’d love to see more API design tooling support. If nothing else, more people just doing it with existing tools. You may not have fully embraced a complete API design first approach within your enterprise group, but openly discussing API design patterns is important. It is critical for any API developer to receive feedback on their design from other stakeholders, and other API development peers. It is important that we allow ourselves to open up to this feedback and sometimes criticism of our designs, based upon what others know, sharing potential views on how an API can reduce friction for consumers. Ideally, this process is also made accessible to non-developer stakeholders, and even business owners, but I’m thinking this is another post all by itself—-for right now, I just want to advocate for more peer API design review sessions.
]]></content>
    <id>http://apievangelist.com/2019/07/30/peer-api-design-review-sessions/</id>
  </entry><entry>
    <title>API For Processing Common Logging Formats And Generating OpenAPI Definitions</title>
    <link href="http://apievangelist.com/2019/07/30/api-for-processing-common-logging-formats-and-generating-openapi-definitions/"/>
    <updated>2019-07-30T02:00:00Z</updated>
    <content><![CDATA[
I’ve invested a lot of time in the last six months into various research, scripts, and tooling to help me with finding APIs within the enterprise. This work is not part my current role, but as a side project to help me get into the mindset of how to help the enterprise understand where their APIs are, and what APIs they are using. Almost every enterprise group I have consulted for has trouble keeping tabs on what APIs are being consumed across the enterprise, and I’m keen on helping understand what the best ways are to help them get their API houses in order.

While there are many ways to trace out how APIs are being consumed across the enterprise, I want to start with some of the basics, or the low hanging when it came to API logging within the enterprise. I’m sure there are a lot of common logging locations to tackle, but my list began with some of the common cloud platforms in use for logging of operations to begin my work—focusing on the following three cloud logging solutions:


  Amazon CloudFront - Beginning with the cloud leader, and looking at how the enterprise is centralizing their logs with CloudFront.
  Google StackDriver - Next, I found Google’s multi-platform approach interesting and worth evaluating as part of this work.
  Azure Logging - Of course, I have to include Azure in all of this as they are a fast growing competitor to Amazon in this space.


After establishing a short list of cloud platforms logging solutions, I began looking at which of the common web server formats I should be looking for within these aggregate logging locations, trying to map out how the enterprise is logging web traffic. Providing me with a short list of the three most common web server formats I should be looking at when it comes to mapping the enterprise API landscape—-providing artifacts of the APIs that enterprise groups are operating.


  Apache Log File - The most ubiquitous open source web server out there is the default for many API providers.
  NGINX Log File - The next most ubiquitous open source web server is definitely something I should be looking for.
  IIS Log File - Then of course, many Microsoft web server folks are still using IIS to serve up their API infrastructure.


These three web server logging formats represent a significant slice of the API logging pie. If I can identify these logging formats across common cloud logging locations, I feel that I can provide a pretty significant solution for finding the APIs that are in use across the enterprise. However, I didn’t just want to be looking a the web server logging for understanding what APIs are being served up, I also wanted to look at the exhaust from how APIs are being consume by looking at these two web browser and proxy traffic formats:


  HAR File - Allowing for the discovery of APIs that are used in web and browser applications across common use cases.
  Charles Proxy JSON Session - Using a common proxy application to reverse engineer web and mobile application API calls.


These cloud logging solutions, web server formats, as well as browser and proxy solutions give me a pretty interesting look at the API discovery pie. I have scripts to help identify these common formats, and then automatically produce OpenAPI definitions from them. It is pretty easy to run these scripts in a variety of ways to help automatically produce a catalog of OpenAPI definitions from them, automating the mapping of the API landscape within he enterprise. I have all of these scripts working for me in a variety of capacities, the next step is to further automate them, organize them into more of a usable suite of API tooling, then unleash them on a larger set of enterprise logs.

All of my scripts currently run as APIs, as I’m API-first, but I’m currently exploring ways in which I can better execute them at the command line, and as autonomous solutions that can be installed within the enterprise, without any external connections or dependencies. I have a list of ways in which I want to add more value on top of these API discovery solutions, allowing me to generate revenue from them. However right now, I am more interested in ensuring they help automate the API landscape across the majority of enterprise logging solutions. Once I dial this in, I will be looking for more ways to implement the existing functionality, as well as evolve to cover other platforms and formats. I’m just looking to deliver a basic solution for understanding where the hell all the APIs are in the enterprise, before I look to bake in more advanced features.
]]></content>
    <id>http://apievangelist.com/2019/07/30/api-for-processing-common-logging-formats-and-generating-openapi-definitions/</id>
  </entry><entry>
    <title>API Storytelling Within The Enterprise</title>
    <link href="http://apievangelist.com/2019/07/28/api-storytelling-within-the-enterprise/"/>
    <updated>2019-07-28T02:00:00Z</updated>
    <content><![CDATA[
Storytelling is important. Storytelling within the enterprise is hard. Reaching folks on the open web is hard work to, but there is usually an audience that will eventually tune in, and over time you can develop and cultivate that audience. The tools you have at your disposal within the enterprise are much more prescribed and often times dictated–even controlled. I also find that they aren’t always as effective as they are made out to be, with the perception being one thing, and the reach, engagement, and noise being then much harder realities you face when trying to get a message out.

Email might seem like a good idea, and is definitely a critical tool when reaching specific individuals or groups, but as a general company wide thing, it quickly becomes exponentially ineffective with each person you add on as CC. I’d say that you are better off creating a daily or weekly email newsletter if you are going to be sending across large groups of the enterprise rather than participating in the constant email barrage that occurs on a daily basis. Email is an effective tool when used properly, but I’d say I haven’t perfected the art of using email to reach my intended audience within the enterprise.

My preferred storytelling format is relatively muted within the enterprise — people rarely read blogs in this world. Blog reading is something you do out on the web apparently. This means I have to get pretty creative when it comes to getting your stories out. It doesn’t mean you shouldn’t be using this format of storytelling, but you just can’t count on folks to regularly consume a blog, or subscribe to an RSS feed. You can still have a blog, but you have to find other ways of slipping the links into existing conversations, documentation, and other avenues in which people consume information within the enterprise.

I would say this reality of reading within the enterprise is why I try to write more white papers and guides. I know that many folks across the enterprise prefer to consume their reading materials as a PDF on their laptop, desktop, or tablet. While this is definitely not my preferred way of consuming information, I have to remember that it is the primary way in which enterprise folks can cut through the noise, and find some quiet time to digest 6-8 pages of API blah blah blah during their busy day. While I will keep pumping out short form content on the blog, I will also be investing much more into creating longer form white papers and guides that have a greater opportunity of penetrating the enterprise.

I know that enterprise folks are caught up in the daily shit-storm and can’t always get to my blog, or spend too much time on Twitter. Making content more portable, and something they can email around, download and potentially consume later is important. As I work within the enterprise more I am realizing how critical this is for folks, including myself. I found myself firing back up my Pocket app on my iPad, so that I can queue things up for later. Reminding how difficult it is to tell stories within the enterprise and that I cannot discount tools like the PDF when it comes to reaching my intended audience. You really have to understand your audience, and work to meet them at their level, regardless of the tools you use to get information and be influenced by the deluge of storytelling we are inundated with on a daily basis.
]]></content>
    <id>http://apievangelist.com/2019/07/28/api-storytelling-within-the-enterprise/</id>
  </entry><entry>
    <title>APIs and Browser Bookmarklets</title>
    <link href="http://apievangelist.com/2019/07/24/apis-and-browser-bookmarklets/"/>
    <updated>2019-07-24T02:00:00Z</updated>
    <content><![CDATA[
I have quite a few API driven bookmarklets I use to profile APIs. I recently quit using Google Chrome, so I needed to migrate all of them to Firefox. I saw this work as an opportunity to better define and organize them, as they had accumulated over the years without any sort of strategy. When I need some new functionality in my browser I would create a new API, and craft a bookmarklet that would accomplish whatever I needed. I wish I had more browser add-on development skills, something I regular try to invest in, but I find that bookmarklets are the next best thing when it comes to browser and API interactions.

There are a number of tasks I am looking to accomplish when I’m browsing the web pages of an API provider. The first thing I want to do is record their domain, then retrieve as much intelligence about the company behind the domain in a single click of the bookmarklet. This was the first bookmarklet and API I developed. Since then, I’ve made numerous others to record the pricing page, parse the terms of service, OpenAPI, and other valuable API artifacts from across the landscape. Bookmarklets are a great way to provide just a little more context combined with a URL pointer, for harvesting, processing, and possibly some human review. Allowing me to augment, enrich, and automate how I consume information as I’m roaming around the web, researching specific topics, and do what I do.

At this point I am actually glad I didn’t invest a lot of energy into developing Chrome browser extension, because it wouldn’t have easily translated to a Firefox world. Since I have been investing in APIs plus bookmarklets, I can easily import, or copy and paste my bookmarklets over. I”m spending the time to go through them, inventory them, and better organize them for optimal usage, so the migration is a little more work than just import and export. Another aspect of this work that I am thankful for is that I abstracted away is the usage of other 3rd party APIs. My very first bookmarklet which profiles the domain of the website I’m looking at has used several different business intelligence solutions, all of which I have been priced out of using, so I’ve resorted to other ways to obtain the profile information I need–the API continues to work despite the APIs I use under the hood.

Browsers are an area of my API research that is significantly deficient. I am working to invest a little more time here, focusing on the migration and evolution of my API driven bookmarklets, but also playing around with the Browser Reporting API, which is some pretty interesting HEADER voodoo. I can’t help but feel like the browsers will continue to play an increasingly important role when it comes to APIs. Not just because of browser APIs like the Reporting API, but also because of the hidden APIs web and mobile applications use, as well as the above the tables APIs we leverage within the browser—-like my bookmarklets. I find the browser a more interesting place to study how APIs are being put to work than with startups these days. I feel like it is where the “innovation” is occurring these days, and sadly, it isn’t the good kind of “innovation” everyone so passionately believes in—-it is the more exploitative ad-driven “innovation” that is pretty invasive in our lives.
]]></content>
    <id>http://apievangelist.com/2019/07/24/apis-and-browser-bookmarklets/</id>
  </entry><entry>
    <title>Absolutism Around API Tools Increases Friction And Failure</title>
    <link href="http://apievangelist.com/2019/07/24/absolutism-around-api-tools-increases-friction-and-failure/"/>
    <updated>2019-07-24T02:00:00Z</updated>
    <content><![CDATA[
I know you believe your tools are the best. I mean, from your vantage point, they are. I also know that when you are building a new API tool, your investors want you to position your tooling as the best. The one solution to rule them all. They want you to work hard to make your tools the best, but also make sure and demonize other tooling as being inferior, out of date, and something the dinosaurs use. I know this absolute belief in your tooling feels good and right, but you are actually creating friction for your users, and potentially failure or at least conflict within their teams. Absolutism, along with divide and conquer strategies for evangelizing API tooling works for great short term financial strategies, but doesn’t do much to help us on the ground actually developing, managing, and sustaining APIs.

Ironically, there are many divers factors that contribute to why API tooling providers and their followers resort to absolutism when it comes to marketing and evangelizing their tools. Much of which has very little to do with the merits of the tools being discussed, and everything about those who are making the tools. I wanted to explore a few of them so they are available on the tip of my tongue while working within the enterprise.


  No Due Diligence On What Is Out There - Most startups who are developing API tooling do not spend the time understanding what already exists across the landscape, and get outside of the echo chamber to learn what real world companies are using to get the job done each day.
  No Learning Around Using Existing Tools - Even if startups are aware of existing tools, patterns, and processes, they rarely invest the time to actually understand what existing tools deliver—spending time to deeply understand how existing tools are being put to use by their would-be customers.
  Lack Of Awareness Around The Problem - There is a reason investors prefer young engineers when it comes to developing the next set of disruptive tooling, because they rarely understand the scope of problems being solved, and provide great fuel for short to mid-term growth strategies.
  Aggressive Male Dominated Environment - Young white men are perfect for this approach to delivering tooling that isn’t about the tool, but about a larger economic strategy, putting us passionate, privileged souls at the helm, and push them to do the disruptive bidding with very little awareness of the big picture.
  No Empathy For Others You Encounter - API tooling that takes an absolutist approach is rarely about empowering others, or understanding and providing solutions to their problems—lacking in empathy for other tooling providers, tooling consumers, or the companies left with each round of tech debt.
  Lack Of Diverse Experience In Industry - Entrepreneurs who ride each wave of API tooling absolutism and state their API tool is the one solution often lack experience in a variety of industries, and rarely have diverse experience outside of the - Silicon Valley echo chamber, and across multiple industries or geographic regions.
  VC Backed With Aggressive Growth - The aggressive absolutist approach of each wave of API tooling is almost always fueled by aggressive funding cycles, and have very little to do with the actual application of API tooling—operating the puppet strings which most API tooling providers and consumers on the front line do not see.


If you are in the business of tearing down someone else to deliver your tool, your tool will die by the same approach–someday. There is always a better funded, more aggressive solution to emerge on the market. Even if your tool has managed to achieve some level of market success, there will be a time when you let your guard down, and someone comes along to begin taking jabs at you. With each cycle of absolutism assault, the merits of the tooling mean very little. Perception always trumps reality, and there are always armies of developers waiting by in the wings to adopt what is new, and begin raising a pitchfork to attack what was. There is no allegiance and loyalty in this game.

I know. I know. This is just business. I just don’t get the game. Smart people have to make money! Yes, there are also many of us who are responsible for keeping the lights on. That aren’t as disloyal as you are, willing to jump from job to job, startup to startup. There are many of us who have been doing this a lot longer than you, and are willing to be responsible for the tech debt we incur along the way, and we do not mind doing the hard work to clean up your messes. I know that API tool absolutism makes you feel knowledgable and in control now, but just wait until you’ve ridden a few waves, and you’ve had many of your valuable tooling taken away from you because of this game. Then you will begin to see the other side of this, and better understand the toll of this business approach. Eventually you will grow weary of it, but fortunate for you, there will always be a fresh crop of recruits to wage this battle, and there is no rest for the wicked. #liveByDisruption #DieByDisruption
]]></content>
    <id>http://apievangelist.com/2019/07/24/absolutism-around-api-tools-increases-friction-and-failure/</id>
  </entry><entry>
    <title>The Higher Level Business Politics That I Am Not Good At Seeing In The API Space</title>
    <link href="http://apievangelist.com/2019/07/23/the-higher-level-business-poitics-that-i-am-not-good-at-seeing-in-the-api-space/"/>
    <updated>2019-07-23T02:00:00Z</updated>
    <content><![CDATA[
I have built successful startups. I’m good at the technology of delivering new solutions. I am decent at understanding and delivering much of business side of bringing new technological solutions to market. What I’m not good at is the higher level business politics that occur. These are the invisible forces behind businesses that I’m good at seeing, playing in, and almost always catch me off guard, too late, or just simply piss me off that they are going on behind the scenes. Unfortunately it is in this realm where most of the money is to be made doing APIs, resulting in me being written out of numerous successful API efforts, because I’m not up to speed on what is going on.

Startups are great vehicles for higher level economic strategies. They are the playground of people with access to resources, and have economic visions that rarely jive with what is happening on the ground floors. Startup strategies count on a handful at the top understanding the vision, with most at the bottom levels not being able to see the vision, and small group of disposable stooges in the middle, ensuring that the top level vision is realized—at all costs. You can work full time at a startup, and even enjoy a higher level position, and still never see the political goings on that are actually motivating the investment in your startup. This is by design. The whole process depends on the lower levels working themselves to the bone, working on, marketing, and selling one vision, while there are actually many other things going on above, usually with a whole other set of numbers.

After 30 years of playing in this game I still stuck at seeing the higher level influences. I’ve seen shiny API tooling solution after shiny API tooling solution arrive on the market, and I still fall for the shine. Ooooohhh, look at that. It will solve X, or Y problem. I really like the vision of those team members. Their timing is perfect. They seem to have the right funding, and mindshare of developers. Then I begin to see some of the usual tell-tale signs of direction coming from up above. It will be subtle signals, like the change in pricing tiers, a quickness to support a standard on import, but very slow to support export. A shift in the marketing strategy. A public “pivot”. There are a diverse of signals you can tune into that will help predict where an API startup is headed, often times away from the original tooling vision, and the needs of the end-users.

With so much experience, you’d think I’d be better at this. I’m not good at it, because I hate playing these games. I like making money, but not in the way that follow the usual VC fueled playbook. To make money at scale you have to be willing to play by multiple playbooks, keeping one or more of them secret from your teams and end-users. This just isn’t me. I prefer being more transparent. I like building real businesses. I like developing real tools. This is what I’m good at. I’m not good at the higher level games required to build wealth for myself or others. It is this reality that leaves me so reluctant to share my knowledge with VCs, talk to and support new startups, and leaves me so cranky on a regular basis when I tell stories in the space. I know y’all think this is business as usual, and are looking to get your piece of the pie, but I operate at a different level, and refuse to go there.
]]></content>
    <id>http://apievangelist.com/2019/07/23/the-higher-level-business-poitics-that-i-am-not-good-at-seeing-in-the-api-space/</id>
  </entry><entry>
    <title>API Provider And Consumer Developer Portals</title>
    <link href="http://apievangelist.com/2019/07/23/api-provider-and-consumer-developer-portals/"/>
    <updated>2019-07-23T02:00:00Z</updated>
    <content><![CDATA[
I’ve been studying API developer portals for almost a decade. I’ve visited the landing pages, portals, websites, and other incarnations from thousands of API providers. I have an intimate understanding of what is needed for API providers to attract, support, and empower API consumers. One area I’m deficient in, and I also think it reflects a wider deficiency in the API space, is regarding how to you make an API portal service both API providers and API consumers. Providing a single portal within the enterprise where everyone can come and understand how to deliver or consume an API.

There are plenty of examples out there now when it comes to publishing an API portal for your consumers, but only a few that show you how to publish an API. I’d say the most common example are API marketplaces that allow both API consumers and providers to coexist, but this model isn’t exactly what you want within the enterprise. One thing the model lacks is the on-boarding of new developers when it comes to actually developing an API. Suffering from many of the same same symptoms API management service providers have historically suffered from—-not providing true assistance when it comes to delivering a quality API.

When I envision an API portal that serves both providers and consumers, either publicly or privately, I envision just as much assistance when it comes to delivering a new API as we provide for new consumers of an API. Helping with API definition, design, deployment, management, testing, monitoring, documentation, and other critical stops along the API lifecycle. We need to see more examples of the split between API provider and consumers, equally helping both sides of the coin get up to speed, and be successful with what they are looking to achieve. I think we’ve spend almost 15 years investing in perfecting and monetizing the API portal with a focus not he consumer, and now we need to invest on helping make the portal easier for new API providers to step up and learn how to properly publish their API.

The modern API management solution is still tailored for the mystical API provider who knows how do to everything, where most do not understand the full API lifecycle. It would be an opportunity for an API management provider to go beyond just one or a handful of stops along the API lifecycle, and properly invest in on boarding new APIs. I think one reason why all of this suffers is that venture capitalists have never prioritized education and training for both API providers or consumers—-directing API service providers to only lightly invest when it comes to these API educational resources. Now that APIs have gone mainstream, we are going to need an industrial grade enterprise solution for delivering API portals that help onboard both API providers and consumers, and provide them both with what they need to navigate the entire lifecycle of the API solutions they are providing and applying.
]]></content>
    <id>http://apievangelist.com/2019/07/23/api-provider-and-consumer-developer-portals/</id>
  </entry>
</feed>
