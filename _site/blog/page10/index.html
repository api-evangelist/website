<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
  <a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
  <ul class="icons">
    <li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
    <li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
    <li><a href="https://www.linkedin.com/organization/1500316/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
    <li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
  </ul>
</header>

    	        <section>
	<div class="content">

	<h3>The API Evangelist Blog</h3>
	<p>This blog is dedicated to understanding the world of APIs, exploring a wide range of topics from design to deprecation, and spanning the technology, business, and politics of APIs. <a href="https://github.com/kinlane/api-evangelist" target="_blank">All of this runs on Github, so if you see a mistake, you can either fix by submitting a pull request, or let us know by submitting a Github issue for the repository</a>.</p>
	<center><hr style="width: 75%;" /></center>
	
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/16/adwords-api-release-and-sunset-schedule-for-2018/">AdWords API Release and Sunset Schedule For 2018</a></h3>
        <span class="post-date">16 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/beach-rocks-currents_kand_two.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>APIs are not forever, and eventually will go away. The trick with API deprecation is to communicate clearly, and regularly with API consumers, making sure they are prepared for the future. I’ve been tracking on the healthy, and not so healthy practices when it comes to <a href="http://deprecation.apievangelist.com/">API deprecation</a> for some time now, but felt like Google had some more examples I wanted to add to our toolbox. Their approach to setting expectations around API deprecation is worthy of emulating, and making common practice across industries.</p>

<p><a href="http://googleadsdeveloper.blogspot.com/2017/10/adwords-api-release-and-sunset-schedule.html">The Google Adwords API team is changing their release schedule</a>, which in turns impacts the number of APIs they’ll support, and how quickly they will be deprecating their APIs. They will be releasing new versions of the API three times a year, in February, June and September. They will also be only supporting two releases concurrently at all times, and three releases for a brief period of four weeks, pushing the pace of API deprecation alongside each release. I think that Google’s approach provides a nice blueprint that other API provides might consider adopting.</p>

<p>Adopting an API release and sunset schedule helps communicate the changes on the horizon, but it also provides a regular rhythm that API consumers can learn to depend on. You just know that there will be three releases a year, and you have a quantified amount of time to invest in evolving integration before any API is deprecated. It’s not just the communication around the roadmap, it is about establishing the schedule, and establishing an API release and sunset cadence that API consumers can be in sync with. Something that can go a lot further than just publishing a road map, and tweeting things out.</p>

<p>I’ll add this example to my API deprecation research.  Unfortunately the topic is one that is widely communicated around in the API space, but Google has long a strong player when it comes to finding healthy API deprecation examples to follow. I’m hoping to get to the point soon where I can publish a simple guide to API deprecation. Something API providers can follow when they are defining and deploying their APIs, and establish a regular API release and deprecation approach that API developers can depend on. It can be easy to get excited about launching a new API, and forget all about it’s release and deprecation cycles, so a little guidance goes a long way to helping API providers think about the bigger picture.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/16/adwords-api-release-and-sunset-schedule-for-2018/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/16/operating-your-api-portal-using-github/">Operating Your API Portal Using Github</a></h3>
        <span class="post-date">16 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/jekyll/jekyllrb.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>Operating on Github is natural for me, but I am regularly reminded what a foreign concept it is for some of the API providers I’m speaking with. Github is the cheapest, easiest way to launch a public or private developer portal for your API. With the introduction of <a href="https://pages.github.com/">Github Pages</a>, each Github repository is turned into a place to host any API related project. In my opinion, every API should begin with Github, providing a place to put your API definition, portal, and other elements of your API operations.</p>

<p>If you are just getting going with understand how Github can be used to support your API operations, I wanted to provide a simple checklist of the concepts at play, that will lead you being able to publish your API portal to Github.</p>

<ul>
  <li><strong>Github Account</strong> - You will need an account to be able to use Github. Anything you do on Github that is public will be free. You can do private portals on Github, but this story is about using it for a public API portal.</li>
  <li><strong>Github Organization</strong> - I recommend starting an organization for your API operations, instead of under just a single users account. Then you can make the definition for the API the first repository, and possibly the portal your second repository you create.</li>
  <li><strong>Github Repo</strong> - A Github repository is basically a folder on the platform which you can start the code, pages, and other content used as part of API operations.</li>
  <li><a href="https://pages.github.com/"><strong>Github Pages</strong></a> - Each Github repository has the ability to turn on a public project site, which can be used as a hosting location for a developer portal.</li>
  <li><a href="https://jekyllrb.com/"><strong>Jekyll</strong></a> - Github Pages allows any Github repository to become a website hosting location which you can access via your Github user account, or even provide an address using your own domain.</li>
</ul>

<p>I recommend every API provider think about hosting their API portal on Github. The learning curve isn’t that significant to get up and running, and if your portal is public, it is free. You can version control, and leverage other key aspects of Github for evolving and managing your API portal. There are a growing number of examples of <a href="https://apievangelist.com/2017/06/14/gsa-api-standards-with-working-prototype-api-and-portal/">forkable API portals like from the GSA</a>, or <a href="http://portal.apievangelist.com/2017/09/05/a-new-minimumviable-documentation-jekyll-template-for-apis/">an interesting minimum viable API documentation template</a> from my friend James Higginbotham (<a href="https://twitter.com/launchany">@launchany</a>). Demonstrating that the practice is growing, with the number of healthy examples to build on diversifying.</p>

<p>If you need help understanding how to use Github for hosting your API developer portal, feel free to reach out. I am happy to see where I can help. Another thing to note is that this approach to running a Jekyll static website isn’t limited to Github. You can always start the project there, and move off to any Jekyll enabled hosting provider. I run my entire network of websites and API project this way, leveraging Github as my plan A, and AWS as my plan B, with a server image ready to go when I need. Github just provides a number of bells and whistles that make it much more usable, as well as something others can collaborate around, enjoying the network effects that come with using the platform.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/16/operating-your-api-portal-using-github/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/16/the-basics-of-api-management/">The Basics Of API Management</a></h3>
        <span class="post-date">16 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/42_16_600_400_0_max_1_1_1-5.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I am developing a basic API management strategy for one of my client’s API. With each area of their API strategy I am taking what I’ve learned monitoring the API sector, but pausing for a moment to think about again, and then applying to their operations. Over the years I have separated out many aspects of API management, distilling it down to a core set of elements that reflect the evolution of API management as its evolved into a digital commodity. It helps me to think through these aspects of API operations in general, but also applying to a specific API I am working on, helping me further refine my API strategy advice.</p>

<p>API management is the oldest area of my research. It has spawned every other area of the lifecycle I track on, but also is the most mature aspect of the API economy. This project I am working on gives me an opportunity to think about what is API management, and what should be spun off into separate areas of concern. I am looking to distill API management down to:</p>

<ul>
  <li><strong>Portal</strong> - A single URL to find out everything about an API, and get up and running working the resources that are available.</li>
  <li><strong>On-Boarding</strong> - Think just about how you get a new developer to from landing on the home page of the portal to making their first API call, and then an application in production.</li>
  <li><strong>Accounts</strong> - Allowing API consumers to sign up for an account, either for individual, or business access to API resources.</li>
  <li><strong>Applications</strong> - Enable each account holder to register one or many applications which will be putting API resources to use.</li>
  <li><strong>Authentication</strong> - Providing one, or multiple ways for API consumers to authenticate and get access to API resources.</li>
  <li><strong>Services</strong> - Defining which services are available across one or many API paths providing HTTP access to a variety of business services.</li>
  <li><strong>Logging</strong> - Every call to the API is logged via the API management layer, as well as the DNS, web server, file system, and database levels.</li>
  <li><strong>Analysis</strong> - Understanding how APIs are being consumed, and how applications are putting API resources to use, identifying patterns across all API consumption.</li>
  <li><strong>Usage</strong> - Quantifying usage across all accounts, and their applications, then reporting, billing, and reconciling usage with all API consumers.</li>
  <li><strong>APIs</strong> - API access to accounts, authentication, services, logging, analysis, and usage of API resources.</li>
</ul>

<p>There are other common elements bundled with API management, but this reflects the core of what API management is about–the business of APIs. Keeping track of who has access to what, and how much they are using. There are a number of other aspects of API management that many will consider under the API management umbrella, but I’ve elevated to being their own stops along the API lifecycle. Some areas are:</p>

<ul>
  <li><strong>Documentation</strong> - Static or interactive documentation for all available API paths, parameters, headers, and other details of the request and response surface area of the API.</li>
  <li><strong>Support</strong> - Self-service, or direct support channels that API consumers put to use to get help along the way.</li>
  <li><strong>SDKs</strong> - The SDKs, samples, libraries, and other supporting code elements for web, mobile, or other types of applications.</li>
  <li><strong>Road Map</strong> - Communicating what the future holds when it comes to the API.</li>
  <li><strong>Issues</strong>  - Notification of any open issues with the available of the API.</li>
  <li><strong>Change Log</strong> - A history of what has happened when it comes to changes to the API.</li>
</ul>

<p>These areas compliment API management, but should be approached beyond the day to day management aspects of API operations. I’d also consider authentication, logging, and analysis to be bigger than just about API management, as all three areas should cover more than just the API, but they are still very coupled to the core aspects of API management. In my definition, API management is very much about managing the consumption of resources, and not always the other aspects of API operations. This isn’t just my definition, it is what I’m seeing with the commodization of API management like we see over at Amazon Web Services.</p>

<p>AWS API Gateway is really about accounts, applications, authentication, and services. The logging, analysis are delivered by AWS CloudWatch. For this particular project I’m using Github and Jekyll as the portal, and custom delivering on-boarding, usage, and supporting APIs separately. Further narrowing down my definition of just what is API management. I’d say that AWS represents this evolution in API management well, with the decoupling of concerns between AWS API Gateway and AWS CloudWatch. If you apply <a href="https://aws.amazon.com/cognito/">AWS Cognito</a> to authentication, you can separate out another. I do not see any viable solution to handling usage, billing, and what used to be the business and monetization side of API management.</p>

<p>This API project I am working on is already using AWS for the backend of their operations, so I’m investing cycles into better understanding the moving parts of API management in context of the AWS platform. It makes sense to think some more about the decoupling of API management in context of AWS, since they are a major player in the commodization, and maturing of the concept. Once I’m done with AWS, I’m going to take another look at Google, then Azure, who are the other major players who are defining the future of API management.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/16/the-basics-of-api-management/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/12/bots-voice-and-conversational-apis-are-your-next-generation-of-api-clients/">Bots, Voice, And Conversational APIs Are Your Next Generation Of API Clients</a></h3>
        <span class="post-date">12 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-conversational-interfaces.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>Around 2010, the world of APIs began picking up speed with the introduction of the iPhone, and then Android mobile platforms. Web APIs had been used for delivering data and content to websites for almost a decade at that point, but their potential for delivering resources to mobile phones is what pushed APIs into the spotlight. The API management providers pushed the notion of being multi-channel, and being able to deliver to web and mobile clients, using a common stack of APIs. Seven years later, web and mobile are still the dominant clients for API resources, but we are seeing a next generation of clients begin to get more traction, which includes voice, bot, and other conversational interfaces.</p>

<p>If you deliver data and content to your customers via your website and mobile applications, the chance that you will also be delivering it to conversational interfaces, and the bots and assistants emerging via Alexa and Google Home, as well as on Slack, Facebook, Twitter, and other messaging platforms, is increasing. I’m not selling that everything will be done with virtual assistants, and voice commands in the near future, but as a client we will continue to see mainstream user adoption, and voice be used in automobiles, and other Internet connected devices emerging in our world. I am not a big fan of talking to devices, but I know many people who are.</p>

<p>I don’t think Siri, Alexa, and Google Home will live up to the hype, but there is enough resources being invested into these platforms, and the devices that they are enabling, that some of it will stick. In the cracks, interesting things will happen, and some conversational interfaces will evolve and become useful. In other cases, as a consumer, you won’t be able to avoid the conversational interfaces, and be required to engage with bots, and use voice enabled devices. This will push the need to have conversationally literate APIs that can deliver data to people in bite-size chunks. Sensors, cameras, drones, and other Internet-connected devices will increasingly be using APIs to do what they do, but voice, and other types of conversational interfaces will continue to evolve to become a common API client.</p>

<p>I am hoping at this point we begin to stop counting the different channels we deliver API data and content to. Despite many of the Alexa skills, and Slack bots I encounter being pretty yawn-worthy, I’m still keeping an eye on how APIs are being used by these platforms. Even if I don’t agree with all the uses of APIs, I still find the technical, business, and politics beyond them evolving worth tuning into. I tend to not emphasize to my clients that they work on voice or bot applications if they aren’t too far along their API journey, but I do make sure they understand one of the reasons they are doing APIs is to support a wide and evolving range of clients, and that at some point they’ll have to begin studying how voice, bots, and other conversational approaches will be a client they have to consider a little more in their overall strategy.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/12/bots-voice-and-conversational-apis-are-your-next-generation-of-api-clients/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/12/air-an-asthma-api/">Air, An Asthma API</a></h3>
        <span class="post-date">12 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/propeller/air-asthma-api-text-phone-aa3.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>You don’t find me showcasing specific APIs often. I’m usually talking about an API because of their approach to the technology, business, or politics of how they do APIs. It just isn’t my style to highlight APIs, unless I think they are interesting, and delivering value that is worth talking about, or possibly reflecting a meaningful trend that is going on. In this case it is a useful API that I think brings value, but also provides an example of an API I can showcase to non-developer folks as a meaningful example of an API.</p>

<p>The API I’m talking about today, is <a href="https://www.propellerhealth.com/air-by-propeller/">the Air API</a>, an asthma API from Propeller, which provides a set of free tools to help people understand current asthma conditions in their neighborhoods. The project is led by the Propeller data scientists and clinical researchers, looking to leverage Air API to help predict how asthma may be affected by local conditions, including a series of tools that share local asthma conditions, ranging from an email or text subscription, to an embeddable Air Widget for other websites.</p>

<p>The Air API provides an easy to explain example of what is possible with APIs. Environmental APIs will continue to be an important aspect of doing APIs. Aggregating sensor and other data to help us understand the air, water, weather, and other critical environmental factors that impact our lives each day. I like the idea of these APIs being open and available to 3rd party developers to build tools on top of them, while the platforms using them as a marketing vehicle for their other products and services, while making sure to keep the valuable data accessible to everyone.</p>

<p>I’ll put the Air API into my toolbox of APIs I use to help onboard folks with APIs. If they are impacted by asthma, or know someone who is, it helps make the personal connection, which can be important when on-boarding folks with the abstract concepts surrounding APIs. People tend to not care about technology until it makes an impact on their world. Which is one reason I think healthcare and environment APIs are going to play an important role in the sector for years to come. They provide a rich world of data, content, and algorithms, that can be exposed via APIs, and be applied in meaningful ways in people’s lives. Leaving a (hopefully) positive impression on folks about what APIs can do.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/12/air-an-asthma-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/12/everything-is-headless-in-an-api-world/">Everything Is Headless In A Decoupled API World</a></h3>
        <span class="post-date">12 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/catacombs_copper_circuit_2.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>It is always funny how long some concepts take to fully capture my attention. Sometimes I understand a concept on the surface, but never really invest the time into thinking deeply about how it actually fits into the big picture of my API research. One of these concepts is “headless”. Most commonly applied to the “headless CMS”. The wikipedia entry for headless CMS proclaims, “<a href="https://en.wikipedia.org/wiki/Headless_CMS">a Headless CMS is a back-end only content management system (CMS) built from the ground up as a content repository that makes content accessible via a RESTful API for display on any device.</a>”</p>

<p>A headless CMS is basically API-first, but instead of API being the first focus, the entire CMS, and administrative system for managing the content gets top billing, with API playing a critical supporting role. It’s what I am always prescribing as a way for API providers to consider the role between their applications, and their backend API resources. Decoupling your apps, from the backend resources. The administration interface for the content management system is one application, and each of your web, mobile, or other applications act as independent solutions. I see headless as just a business view of doing APIs, which makes sense when selling the concept to normals.</p>

<p>I’m adding a research area for headless that augments <a href="http://deployment.apievangelist.com/">my API deployment research</a>. Some of the open source implementations I’ve come across like <a href="https://getdirectus.com/">Directus</a> are pretty slick. It’s a pretty quick way to go from API to something that immediately benefits the average user in as short of time as possible. Headless is just another way to frame the the API conversation in the context of delivering an internal content management system, over getting 3rd party developers to build applications on top. It is something that is still possible because their is an API behind, but it is not the primary objective in any headless implementation.</p>

<p>While the main definition of headless is about having an API behind everything, I am also finding examples where Github is the backend, instead of just a RESTful API. <a href="http://prose.io/">Prose.io</a> which is actually the CMS I use for API Evangelist is considered a headless CMS, but Github acts my backend. Some of the headless CMS solutions I’ve used that depend on Github use Git as the connection, while others depend on the Github API for reading and writing data and content. This reflects how I use Github across my projects, beyond just Prose.io, except I am depending on web APIs, Github, as well as Google Sheets for the backend of Jekyll driven websites and applications.</p>

<p>I have been tracking on headless CMS stories for almost two years now, but after diving a little deeper this last week, it finally clicked for me. I feel headless is a good way to help the world move past WordPress, and embrace a more decoupled way of delivering websites, mobile, and other types of applications. I’m going to deploy Directus so that I can better understand headless as an approach to deploying APIs, and see about deploying basic demo implementations that I can point to. I’m hoping to use it as a way to introduce folks to APIs where public APIs is not the first objective, allowing them to get their feet wet with a simple API and website implementation. Helping deliver value without all the immediate risk, allowing folks to learn about how APIs can drive one or many applications in a more controlled internal environment.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/12/everything-is-headless-in-an-api-world/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/12/obfuscating-the-evolving-code-behind-my-api/">Obfuscating The Evolving Code Behind My API</a></h3>
        <span class="post-date">12 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/christianity-under-construction_copper_circuit.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’m dialing in a set of machine learning APIs that I use to obfuscate and distort the images I use across my storytelling. The code is getting a little more hardened, but there is still so much work ahead when it comes to making sure it does exactly what I needed it to do, with only dials and controls I need–nothing more. I’m the only consumer of my APIs, which I use them daily, with updates made to the code along the way, evolving the request and response structures until they meet my needs. Eventually the APIs will be done (enough), and I’ll stop messing with them, but that will take a couple months more of pushing forward the code.</p>

<p>While the code for these APIs are far from finished, I find the API helps obfuscate and diffuse the unfinished nature of things. The API maintains a single set of paths, and I might still evolve  the number of parameters it accepts, and the fields it outputs, the overall will keep a pretty tight surface area despite the perpetually unfinished backend. I like this aspect of operating APIs, and how they can be used as a facade, allowing you to maintain one narrative on the front-end, even with another occurring behind behind the scenes. I feel like API facades really fit with my style of coding. I’m not really an A grade programmer, more a B- level one, but I know how to get things to work–if I have a polished facade, things look good.</p>

<p>Honestly, I’m pretty embarrassed by my wrappers for TensorFlow. I’m still figuring everything out, and finding new ways of manipulating and applying Tensor Flow models, so my code is rapidly changing, maturing, and not always complete. When it comes to the API interface I am focused on not introducing breaking changes, and maintaining a coherent request and response structure for the image manipulation APIs. Even though the code will at some point be stabilized, with updates becoming less frequent, the code will probably not see the light of day on Github, but the API might eventually be shared beyond just me, providing a simple, usable interface someone else can use.</p>

<p>I wouldn’t develop APIs like this outside a controlled group of consumers, but I find being both API provider and consumer pushes me to walk a line of unfinished and evolving backend, with a stable, simple API, without any major breaking changes. The fewer fixes I have to do to my API clients, the more successful I’ve been pushing forward the backend, while keeping the API in forward motion without the friction. I guess I just feel like the API makes for a nice wrapper for code, acting as shock absorbers for ups and downs of evolving code, and getting it to do what you need. Providing a nice facade that obfuscates the evolving code behind my API.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/12/obfuscating-the-evolving-code-behind-my-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/11/provide-an-open-source-database-and-api-and-then-sell-the-data/">Provide An Open Source Threat Information Database And API Then Sell Premium Data Subscriptions</a></h3>
        <span class="post-date">11 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/old-door-lock_copper_circuit.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I was doing some <a href="http://security.apievangelist.com">API security research</a> and stumbled across <a href="https://github.com/toolswatch/vFeed">vFeed</a>, a “<a href="https://github.com/toolswatch/vFeed">Correlated Vulnerability and Threat Intelligence Database Wrapper</a>”, providing a JSON API of vulnerabilities from the vFeed database. The approach is a Python API, and not a web API, but I think provides an interesting blueprint for open source APIs. What I found interesting (somewhat) from the vFeed approach was the fact they provide an open source API, and database, but if you want a production version of the database with all the threat intelligence you have to pay for it.</p>

<p>I would say their technical and business approach needs a significant amount of work, but I think there is a workable version of it in there. First, I would create a Python, PHP, Node.js, Java, Go, Ruby version of the API, making sure it is a web API. Next, remove the production restriction on the database, allowing anyone to deploy a working edition, just minus all the threat data. There is a lot of value in there being an open source set of threat intelligence sharing databases and API. Then after that, get smarter about having a variety different free and paid data subscriptions, not just a single database–leverage the API presence.</p>

<p>You could also get smarter about how the database and API enables companies to share their threat data, plugging it into a larger network, making some of it free, and some of it paid–with revenue share all around. There should be a suite of open source threat information sharing databases and APIs, and a federated network of API implementations. Complete with a wealth of open data for folks to tap into and learn from, but also with some revenue generating opportunities throughout the long tail, helping companies fund aspects of their API security operations. Budget shortfalls are a big contributor to security incidents, and some revenue generating activity would be positive.</p>

<p>So, not a perfect model, but enough food for thought to warrant a half-assed blog post like this. Smells like an opportunity for someone out there. Threat information sharing is just one dimension of my API security research where I’m looking to evolve the narrative around how APIs can contribute to security in general. However, there is also an opportunity for enabling the sharing of API related security information, using APIs. Maybe also generating of revenue along the way, helping feed the development of tooling like this, maybe funding individual implementations and threat information nodes, or possibly even fund more storytelling around the concept of API security as well. ;-)</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/11/provide-an-open-source-database-and-api-and-then-sell-the-data/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/11/their-security-practices-are-questionable-but-their-communication-is-unacceptable/">Their Security Practices Are Questionable But Their Communication Is Unacceptable</a></h3>
        <span class="post-date">11 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/status-berlin_propaganda_leaflets.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I study the API universe every day of the week, looking for common patterns in the way people are using technology. <a href="http://apievangelist.com/api-lifecycle/">I study almost 100 stops along the API lifecycle</a>, looking for healthy practices that companies, organizations, institutions, and government agencies can follow when dialing in their API operations. Along the way I am also looking for patterns that aren’t so healthy, which are contributing to many of the problems we see across the API sector, but more importantly the applications and devices that they are delivering valuable data, content, media, and algorithms to.</p>

<p><a href="http://security.apievangelist.com/">One layer of my research is centered around studying API security</a>, which also includes keeping up with <a href="http://vulnerabilities.apievangelist.com/">vulnerabilities</a> and <a href="http://breaches.apievangelist.com/">breaches</a>. I also pay attention to <a href="http://cybersecurity.apievangelist.com/">cybersecurity</a>, which is a more theatrical version of regular security, with more drama, hype, and storytelling. I’ve been reading everything I can on the Equifax, Accenture, and other scary breaches, and like the other areas of the industry I track on, I’m beginning to see some common patterns emerge. It is something that starts with the way we use (or don’t use) technology, but then is significantly amplified by the human side of things.</p>

<p>There are a number of common patterns that contribute to these breaches on the technical side, such as not enough monitoring, logging, and redundancy in security practices. However, there are also many common patterns emerging from the business approach by leadership during security incidents, and breaches. These companies security practices are questionable, but I’d say the thing that is the most unacceptable about all of these is the communication around these security events. I feel like they demonstrate just how dysfunctional things are behind the scenes at these companies, but also demonstrate their complete lack of respect and concern for individuals who are impacted by these incidents.</p>

<p>I am pretty shocked by seeing how little some companies are investing in API security. The lack of conversation from API providers about their security practices, or lack of, demonstrates how much work we still have to do in the API space. It is something that leaves me concerned, but willing to work with folks to help find the best path forward. However, when I see companies do all of this, and then do not tell people for months, or years after a security breach, and obfuscate, and bungle the response to an incident, I find it difficult to muster up any compassion for the situations these companies have put themselves in. Their security practices are questionable, but their communication around security breaches is unacceptable.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/11/their-security-practices-are-questionable-but-their-communication-is-unacceptable/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/11/a-grpc-meetup-kit/">The gRPC Meetup Kit</a></h3>
        <span class="post-date">11 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/grpc/grpc-logo.png" align="right" width="40%" style="padding: 15px;" /></p>
<p><a href="http://apievangelist.com/2017/09/29/tyk-is-conducting-api-surgery-meetups/">I wrote about Tyk’s API surgery meetups last week</a>, and adding a new approach to our API event and workshop toolbox, and next I wanted to highlight <a href="https://github.com/grpc-ecosystem/meetup-kit">the gRPC Meetup Kit</a>, a resource for creating your own gRPC event. gRPC is an approach out of Google for designing, delivering, and operating high performance APIs. If you look at the latest wave of APIs out of Google you’ll see they are all REST and/or gRPC. Most of them are dual speed, providing both REST and gRPC. gRPC is an open source initiative, but very much a Google led effort that we’ve seen picking up momentum in 2017.</p>

<p>While I am keeping an eye on gRPC itself, this particular story is about the concept of providing a Meetup kit for your API related service or tooling, providing an “In a Box” solution that anyone can use to hold a Meetup. The gRPC team provides three groups of resources:</p>

<h3 id="grpc-101-presentation">gRPC 101 Presentation</h3>
<ul>
  <li><a href="https://www.youtube.com/watch?v=UVsIfSfS6I4">Talk</a> - A 15 minute course introduction video.</li>
  <li><a href="https://docs.google.com/presentation/d/1dgI09a-_4dwBMLyqfwchvS6iXtbcISQPLAXL6gSYOcc/edit?usp=sharing">Slides</a> - Slides that go along with the talk.</li>
  <li><a href="https://codelabs.developers.google.com/codelabs/cloud-grpc/index.html">Codelab</a> - A 45m codelab that attendees can complete using <a href="https://cloud.google.com/shell/">Cloud Shell</a>.</li>
</ul>

<h2 id="resources-and-community">Resources and community</h2>
<ul>
  <li><a href="http://www.grpc.io/"><strong>gRPC Website</strong></a></li>
  <li><strong>Codelab</strong>
    <ul>
      <li><a href="https://codelabs.developers.google.com/codelabs/cloud-grpc/index.html">Building a gRPC service with Node.js</a></li>
      <li><a href="https://codelabs.developers.google.com/codelabs/cloud-grpc-csharp/index.html">Building a gRPC service with C#</a></li>
    </ul>
  </li>
  <li><a href="https://github.com/grpc"><strong>GitHub Source</strong></a>
    <ul>
      <li><a href="https://github.com/grpc-ecosystem">Extended gRPC Ecosystem</a></li>
      <li><a href="http://www.grpc.io/blog/">Blog</a></li>
      <li><a href="https://www.youtube.com/channel/UCrnk1HWelWnYtF78YZX80fg">Youtube Channel</a></li>
    </ul>
  </li>
  <li><strong>Ask Questions</strong>
    <ul>
      <li><a href="https://gitter.im/grpc/grpc">Gitter Chat</a></li>
      <li><a href="https://groups.google.com/forum/#!forum/grpc-io">Google Group</a></li>
      <li><a href="http://stackoverflow.com/tags/grpc/">Stack Overflow</a></li>
    </ul>
  </li>
  <li><strong>Keep in Touch</strong>
    <ul>
      <li><a href="https://twitter.com/grpcio">Twitter</a></li>
    </ul>
  </li>
</ul>

<h2 id="request-support-for-your-event">Request Support for Your Event</h2>
<ul>
  <li><a href="https://goo.gl/forms/C3TCtFdobz4ippty2">gRPC Stickers</a></li>
  <li><a href="https://goo.gl/forms/pvxNwWExr5ApbNst2">Sign up for office hours with gRPC team</a></li>
</ul>

<p>It provides a nice blueprint for what is needed when crafting your own Meetup Kit as well as some material you could weave into any other type of Meetup, or workshop that might contain gRPC. Maybe an API design and protocol workshop, where you cover all of the existing approaches out there today like REST, Hypermedia, gRPC, GraphQL, and others. If nothing else the gRPC Meetup Kit provides a nice forkable project, that you could use as scaffolding for your own kit.</p>

<p>As I mentioned in my piece about Tyk, I don’t think Meetups are going anywhere as a tool for API providers, and API service providers to reach a developer audience. However, I think we are going to have to get more creative about how we organize, produce, and incentivize others to put them on. They are a great vehicle for brining together folks in a community to learn about technology, but we have to make sure they are delivering value for people who show up. I am guessing that a little planning, and evolving a toolkit using Github is a good way to approach putting on Meetups, workshops, and other small events around your products, services, and tooling.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/11/a-grpc-meetup-kit/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/11/the-api-coaches-at-capital-one/">The API Coaches At Capital One</a></h3>
        <span class="post-date">11 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/capital-one/21586757_10155715320589813_1876210064026688571_o.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>API evangelism and even advocacy at many organizations has always been a challenge to introduce, because many groups aren’t really well versed in the discipline, and often times it tends to take on a more marketing or even sales like approach, which can hurt its impact. I’ve worked with groups to rebrand, and change how they evangelize APIs internally, with partners, and the public, trying to ensure the efforts are more effective. While I still bundle all of this under <a href="http://evangelism.apievangelist.com/">my API evangelism research</a>, I am always looking for new approaches that push the boundaries, and evolve what we know as API evangelism, advocacy, outreach, and other variations.</p>

<p>I was introduced to a new variation of the internal API evangelism concept a few weeks back while at Capital One talking with my friend Matthew Reinbold(<a href="https://twitter.com/libel_vox">@libel_vox</a>) about their approach to API governance. His team at the Capital One API Center of Excellence has the concept of the API coach, and I think Matt’s description from <a href="https://matthewreinbold.com/2017/10/03/API-Governance-Blueprint/">his recent API governance blueprint story</a> sums it up well:</p>

<blockquote>
  <p><em>At minimum, the standards must be a journey, not a destination. A key component to “selective standardization” is knowing what to select. It is one thing for us in our ivory tower to throw darts at market forces and team needs. It is entirely another to repeatedly engage with those doing the work.<br /><br />
Our coaching effort identifies those passionate practitioners throughout our lines of business who have raised their hands and said, “getting this right is important to my teams and me”. Coaches not only receive additional training that they then apply to their teams. They also earn access to evolving our standards.<br /><br />
In this way, standards aren’t something that are dictated to teams. Teams drive the standards. These aren’t alien requirements from another planet. They see their own needs and concerns reflected back at them. That is an incredibly powerful motivator toward acceptance and buy-in.</em></p>
</blockquote>

<p>A significant difference here between internal API evangelism and API coaching is you aren’t just pushing the concept of APIs (evangelizing), you are going the extra mile to focus on healthy practices, standards, and API governance. Evangelism is often seen as an API provider to API consumer effort, which doesn’t always translate to API governance internally across organizations who are developing, deploying, and managing APIs. API coaches aren’t just developing API awareness across organizations, they are cultivating a standardized, bottom up, as well as top down awareness around providing and consuming APIs. Providing a much more advanced look at what is needed across larger organizations, when it comes to outreach and communication.</p>

<p>Another interesting aspect of Capital One’s approach to API coaching, is that this isn’t just about top down governance, it has a bottom up, team-centered, and very organic approach to API governance. It is about standardizing, and evolving culture across many organizations, but in a way that allows team to have a voice, and not just be mandated what the rules are, and required to comply. The absence of this type of mindset is the biggest contributor to a lack of API governance we see across the API community today. The is what I consider the politics of APIs, something that often trumps the technology of all of this.</p>

<p>API coaching augments <a href="http://evangelism.apievangelist.com/">my API evangelism research</a> in a new and interesting way. It also dovetails with my <a href="http://design.apievangelist.com/">API design research</a>, as well as begins rounding off a new area I’ve wanted to add for some time, but just have not see enough activity in to warrant doing so–API governance. I’m not a big fan of the top down governance that was given to us by our SOA grandfathers, and the API space has largely been doing alright without the presence of API governance, but I feel like it is approaching the phase where a lack of governance will begin to do more harm than good. It’s a drum I will start beating, with the help of Matt and his teams work at Capital One. I’m going to reach out to some of the other folks I’ve talked with about API governance in the past, and see if I can produce enough research to get the ball rolling.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/11/the-api-coaches-at-capital-one/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/10/explore-download-api-and-share-data/">Explore, Download, API, And Share Data</a></h3>
        <span class="post-date">10 Oct 2017</span>
        <p><a href="https://nycopendata.socrata.com/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9"><img src="https://s3.amazonaws.com/kinlane-productions/nyc-open-data/nyc-opendata-explore-download-api.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I’m regularly looking through API providers, service providers, and open data platforms looking for interesting ways in which folks are exposing APIs. I have written about <a href="http://apievangelist.com/2017/10/05/show-that-api-call-behind-that-dashboard-visualization/">Kentik exposing the API call behind each dashboard visualization for their networking solution</a>, as well as <a href="https://apievangelist.com/2016/10/24/the-api-behind-every-feature-in-the-user-interface/">CloudFlare providing an API link for each DNS tool available via their platform</a>. All demonstrating healthy way we can show how APIs are right behind everything we do, and <a href="https://nycopendata.socrata.com/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9">today’s example of how to provide API access is out of New York Open Data, providing access to 311 service requests made available via the Socrata platform</a>.</p>

<p>The page I’m showcasing provides access 311 service requests from 2010 to present, with all the columns and meta data for the dataset, complete with a handy navigation toolbar that lets you view data in Carto or Plot.ly, download the full dataset, access via API, or simply share via Twitter, Facebook, or email. It is a pretty simple example of offering up multiple paths for data consumers to get what they want from a dataset. Not everyone is going to want the API. Depending on who you are you might go straight for the download, or opt to access via one of the visualization and charting tools. Depending on who you are targeting with your data, the list of tools might vary, but the NYC OpenData example via Socrata provides a nice example to build upon. With the most important message being do not provide only the options you would choose–get to know your consumers, and deliver solutions they will also need.</p>

<p>It provides a different approach to making APIs behind available to users than the Kentik or CloudFlare approaches do, but it adds to the number of examples I have to show people how APIs and API enabled integration can be exposed through the UI, helping educate the massess about what is possible. I could see standardized buttons, drop downs, and other embeddable tooling emerge for helping deliver solutions like this for providers. Something like we are seeing with <a href="https://apievangelist.com/2017/06/05/extending-your-apps-using-embeddable-serverless-webhooks/">the serverless webhooks out Auth0 Extensions</a>. Some sort of API-enabled goodness that triggers something, and can be easily embedded directly into any existing web or mobile application, or possibly a browser toolbar–opening up API enabled solutions to the average user.</p>

<p>One of the reasons I keep showcasing examples like this is that I want to keep pushing back on the notion that APIs are just for developers. Simple, useful, and relevant APIs are not beyond what the average web application user can grasp. They should be present behind every action, visualization, and dataset made available online. When you provide useful integration and interoperability examples that make sense to the average user, and give them easy to engage buttons, drop downs, and workflows for implementing, more folks will experience the API potential in their world. The reasons us developers and IT folk keep things complex, and outside the realm of the normal folk is more about us, our power plays, as well as our inability to simplify things so that they are accessible beyond those in the club.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/10/explore-download-api-and-share-data/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/10/connecting-service-level-agreement-to-api-monitoring/">Connecting Service Level Agreements To API Monitoring</a></h3>
        <span class="post-date">10 Oct 2017</span>
        <p><a href="https://apimetrics.io"><img src="https://s3.amazonaws.com/kinlane-productions/apimetrics/api-metrics-latency-sla.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>Monitoring your API availability should be standard practice for internal and external APIs. If you have the resources to custom build API monitoring, testing, and performance infrastructure, I am guessing you already have some pretty cool stuff in place. If you don’t, then you should not be reinventing the wheel out there, and you should be leveraging one of the existing API monitoring services out there on the market. When you are getting started with monitoring your APIs I recommend you begin with uptime and downtime, and once you deliver successfully on that front, I recommend you work on API performance, and the responsiveness of your APIs.</p>

<p>You should begin with making sure you are delivering the service level agreement you have in place with your API consumers. What, you don’t have a service level agreement? No better time to start than now. If you don’t already have an explicitly stated SLA in place, I recommend creating one internally, and see what you can do to live up to your API SLA, then once you ensure things are operating at acceptable levels, you share with your API consumers. I am guessing they will be pretty pleased to hear that you are taking the initiative to offer an SLA, and are committed enough to your API to work towards such a high bar for API operations.</p>

<p>To help you manage defining, and then ultimately monitoring and living up to your API SLA, I recommend taking a look at <a href="https://apimetrics.io">APIMetrics</a>, who is obsessively focused on API quality, performance, and reliability. They spend a lot of time monitoring public APIs, and have <a href="https://apievangelist.com/2017/04/24/a-ranking-score-to-determine-if-your-api-was-sla-compliant/">developed a pretty sophisticated approach to ranking and scoring your API to ensure you meet your SLA</a>. As you can see in the picture for this story, the APIMetrics administrative dashboard provides a pretty robust way for you to measure any API you want, and establish metrics and triggers that let you know if you’ve met or failed to meet your SLA requirements. As I said, you could start out by monitoring internally if you are nervous about the results, but once you are ready to go prime time you have the tools to help you regularly reporting internally, as well as externally to your API consumers.</p>

<p>I wish that every stop along the life cycle had a common definition for defining a specific aspect of service level agreements, and was something that multiple API providers could measure and report upon similar to what APIMetrics does for monitoring and performance. I’d like to see API design begin to have a baseline definition, that was verifiable through a common set of machine readable API assertions. I’d love for API plans, pricing, and even terms of service measurable, reportable, in a similar way. These are all things that should be observable through existing outputs, and reflected as part of service level agreements. I’d love to see the concept of the SLA evolve to cover all aspects of the quality of service beyond just availability. APIMetrics provides a good look at how the services we use to manage our APIs can be used to define the level of service we provide, something that we can be emulating more across our API operations.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/10/connecting-service-level-agreement-to-api-monitoring/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/10/algorithmic-observability-should-work-like-machine-readable-food-labels/">Algorithmic Observability Should Work Like Machine Readable Food Labels</a></h3>
        <span class="post-date">10 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algorithms/new-food-labels.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’ve been doing a lot of thinking about <a href="https://apievangelist.com/2016/08/04/pushing-for-more-algorithmic-transparency-using-apis/">algorithmic transparency</a>, as well as a more evolved version of it I’ve labeled as <a href="https://apievangelist.com/2017/06/28/algorithmic-transparency-in-policing/">algorithmic observability</a>. Many algorithmic developers feel their algorithms should remain black boxes, usually due to intellectual property concerns, but in reality the reasons will vary. My stance is that algorithms should be open source, or at the very least have some mechanisms for auditing, assessing, and verifying that algorithms are doing what they promise, and that algorithms aren’t doing harm behind the scenes.</p>

<p>This is a concept I know algorithm owners and creators will resist, but algorithms observability should work like food labels, but work in a more machine readable way, allowing them to be validated by other external (or internal) systems. Similar to food you buy in the store, you shouldn’t have to give away the whole recipe and secret sauce behind your algorithm, but there should be all the relevant data points, inputs, outputs, and other “ingredients” or “nutrients” that go into the resulting algorithm. <a href="http://apievangelist.com/2016/01/04/api-definition-origin-validation-and-attribution/">I talked about algorithm attribution before</a>, and I think there should be some sort of algorithmic observability manifest, which provides the “label” for an algorithm in a machine readable format. It should give all the relevant sources, attribution, as well as input and outputs for an algorithm–with different schema for different industries.</p>

<p>In addition to there being an algorithmic observability “label” available for all algorithms, there should be live, or at least virtualized, sandboxed instances of the algorithm for verification, and auditing of what is provided on the label. As we saw with the Volkswagen emissions scandal, algorithm owners could cheat, but it would provide an important next step for helping us understand the performance, or lack of performance when it comes to the algorithms we are depending on. Why I call this algorithmic observability, instead of algorithmic transparency, is each algorithm should be observable using it’s existing inputs and outputs (API), and not just be a “window” you can look through. It should be machine readable, and audit-able by other systems in real time, and at scale. Going beyond just being able to see into the black box, but also be able to assess, and audit what is occurring in real time.</p>

<p>Algorithmic observability regulations would work similar to what we see with food and drugs, where if you make claims about your algorithms, they should have to stand up to scrutiny. Meaning there should be standardized algorithmic observability controls for government regulators, industry analysts, and even the media to step up and assess whether or not an algorithm lives up to the hype, or is doing some shady things behind the scenes. Ideally this would be something that technology companies would do on their own, but based upon my current understanding of the landscape, I’m guessing that is highly unlikely, and will be something that has to get mandated by government in a variety of critical industries incrementally. If algorithms are going to impacting our financial markets, elections, legal systems, education, healthcare, and other essential industries, we are going to have to begin the hard work of establishing some sort of framework to ensure they are doing what is being sold, and not hurting or exploiting people along the way.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/10/algorithmic-observability-should-work-like-machine-readable-food-labels/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/10/offering-a-guest-blogger-program-to-create-unique-content/">A Guest Blogger Program To Create Unique Content For Your API</a></h3>
        <span class="post-date">10 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/runscope/runscope-featured-guest-series.png" align="right" width="40%" style="padding: 15p;" /></p>
<p>Creating regular content for your blog is essential to maintaining a presence. If you don’t publish regularly, and refresh your content, you will find your SEO, and wider presence quickly becoming irrelevant. I understand that unlike me, many of you have jobs, and responsibilities when it comes to operating your APIs, and carving out the time to craft regular blog posts can be difficult. To help you in your storytelling journey I am always looking for other stories to help alleviate your pain, while helping keep your blog active, and ensure folks will continue stumbling across your API, or API service, while Google, or on social media.</p>

<p>Another interesting example of how to keep your blog fresh came from my partners over at Runscope, <a href="https://blog.runscope.com/writing-for-runscope">who conducted a featured guest blog post series</a>, where they were paying API community leaders to help “create an incredible resource of blog posts about APIs, microservices, DevOps, and QA.” Which has produced a handful of interesting posts:</p>

<ul>
  <li><a href="https://blog.runscope.com/?offset=1493310540201">Monolith to Microservices: Transforming a web-scale, real-world e-commerce platform using the Strangler Pattern</a></li>
  <li><a href="https://blog.runscope.com/posts/you-might-not-need-graphql">You Might Not Need GraphQL</a></li>
  <li><a href="https://blog.runscope.com/posts/3-easy-steps-to-cloud-operational-excellence-devops-pagerduty-statuspage">3 Easy Steps to Cloud Operational Excellence</a></li>
  <li><a href="https://blog.runscope.com/posts/building-steam-powered-iot-api-thingsboard-raspberry-pi">Building a Steam Powered IoT API with Thingsboard</a></li>
</ul>

<p>One thing to note is that Runscope paid $500.00 per post to help raise the bar when it comes to the type of author that will step up for such an effort. I’ve seen companies try to do this before, offering gift cards, swag, and even nothing in return, with varying grades of success and failure. I’m not saying a guest author program for your blog will always yield the results you are looking for, but it is a good way to help build relationships with your community, and help augment your existing workload, with some regular storytelling on the blog.</p>

<p>A guest blogger program is a tool I will be adding to <a href="http://communications.apievangelist.com/">my API communications research</a>, expanding on the tools API operators have in their toolbox to keep their communication strategies active. An active blog does more than just educate your community, and boost your SEO. An active blog, that is informative, and relevant shows that there is someone home behind an API, and that they are investing in the platform. While there are exceptions, the clearest sign that an API will soon be deprecated, or does not have the resources to support consumers properly is when the blog hasn’t been updated in the last six months. While I’m reviewing, indexing, and learning about different APIs, when I come across an inactive blog, or Twitter account for an API, I’ll almost always keep moving, feeling like there really isn’t much worthwhile there, as it will soon be gone.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/10/offering-a-guest-blogger-program-to-create-unique-content/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/09/treating-your-apis-like-they-are-infrastructure/">Treating Your APIs Like They Are Infrastructure</a></h3>
        <span class="post-date">09 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/stripe/stripe-api-versioning.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>We all (well most of us) strive to deliver as stable of an API presence as we possibly can. It is something that is easier said than done. It is something that takes caring, as well as the right resources, experience, team, management, and budget to do APIs just right. It is something the API idols our there make look easy, when they really have invested a lot of time and energy into developing a agile, yet scalable approach to ensuring APIs stay up and running. Something that you might able to achieve with a single API, but can easily be lost between each API version, as we steer the ship forward.</p>

<p>I spend a lot of time at the developer portals of these leading API providers looking for interesting insight into how they are operating, and I though <a href="https://stripe.com/blog/api-versioning">Stripe’s vision around versioning their API is worth highlighting</a>. Specifically their quote about treating your API like they are real life physical infrastructure.</p>

<p><em>“Like a connected power grid or water supply, after hooking it up, an API should run without interruption for as long as possible.Our mission at Stripe is to provide the economic infrastructure for the internet. Just like a power company shouldn’t change its voltage every two years, we believe that our users should be able to trust that a web API will be as stable as possible.”</em></p>

<p>This is possible. This is how I view Amazon S3, and Pinboard. These are two APIs I depend on to make my business work. Storage and bookmarking are two essential resources in my world, and both these APIs have consistently delivered stable API infrastructure, that I know I can depend on. I think it is also interesting to note that one is a tech giant, while the other is a viable small business (not startup). Demonstrating for me that there isn’t a single path to being a reliable, stable, API provider, despite what some folks might believe.</p>

<p>I am spending a lot of time lately thinking of API infrastructure along the lines of our energy grid, and transit system. The analogies are not perfect, but I do feel like as time moves on, some of our API infrastructure will continue to become commoditized, deemed essential, and something we depend on just as much as power, gas, and other utilities. These are the APIs that will be sticking around, the ones that can prove their usefulness, and deliver reliable integrations that do not change with each funding season, or technology trend. I’m looking forward to getting beyond the wild west days of APIs, and moving into the stage where APIs are treated like they are infrastructure, not just some toy, or latest fad, and we can truly depend on them and build our businesses around.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/09/treating-your-apis-like-they-are-infrastructure/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/09/api-governance-in-the-capital-one-devexchange/">Learning About API Governance From Capital One DevExchange</a></h3>
        <span class="post-date">09 Oct 2017</span>
        <p><a href="https://medium.com/capital-one-developers/api-lifecycle-governance-best-practices-56a1ebbb4f1b"><img src="https://cdn-images-1.medium.com/max/800/0*DOAJXcR7VcCN3iOh.jpg" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I am still working through my notes from a recent visit to Capital One, where I spent time talking with Matthew Reinbold (<a href="https://twitter.com/libel_vox">@libel_vox</a>) about their API governance strategy. I was given a walk through their approach to defining API standards across groups, as well as how they incentivize, encourage, and even measure what is happening. I’m still processing my notes from our talk, and waiting to see Matt publish more on his work, before I publish too many details, but I think it is worth looking at from a high level view, setting the bar for other API governance conversations I am engaging in.</p>

<p>First, what is API governance. I personally know that many of my readers have a lot of misconceptions about what it is, and what it isn’t. I’m not interesting in defining a single definition of API governance. I am hoping to help define it so that you can find it a version of it that you can apply across your API operations. API governance is at its simplest form, about ensuring consistency in how you do API across your development groups, and a more robust definition might be about having an individual or team dedicated to establishing organization-wide API standards, helping train, educate, enforce, and in the case of capital one, measure their success.</p>

<p>Before you can begin thinking about API governance, you need to start establishing what your API standards are. In my experience this usually begins with API design, but should also quickly also be about consistent, API deployment, management, monitoring, testing, SDKs, clients, and every other stop along the API lifecycle. Without well-defined, and properly socialized API standards, you won’t be able to establish any sort of API governance that has any sort of impact. I know this sounds simple, but I know more API providers who do not have any kind of API design, or other guide for their operations, than I know API providers who have consistent guides to design, and other stops along their API lifecycle.</p>

<p>Many API providers are still learning about what consistent API design, deployment, and management looks like. In the API industry we need to figure out how to help folks begin establishing organizational-wide API design guides, and get them on the road towards being able to establish an API governance program–it is something we suck at currently. Once API design, then deployment and management practices get defined we can begin to realize some standard approaches to monitoring, testing, and measuring how effective API operations are. This is where organizations will begin to see the benefits of doing API governance, and it not just being a pipe dream. Something you can’t ever realize if you don’t start with the basics like establishing an API design guide for your group. Do you have an API design guide for your group?</p>

<p>While talking with Matt about their approach at Capital One, he asked if it was comparable to what else I’ve seen out there. I had to be honest. I’ve never come across someone who had established API design, deployment, and management practices. Were actively educating and training their staff. Then actually measuring the impact and performance of APIs, and the teams behind them. I know there are companies who are doing this, but since I tend to talk to more companies who are just getting started on their API journey, I’m not seeing anyone organization who is this advanced. Most companies I know do not even have an API design guide, let alone measuring the success of their API governance program. It is something I know a handful of companies would like to strive towards, but at the moment API governance is more talk than it is ever a reality.</p>

<p>If you are talking API governance at your organization, I’d love to learn more about what you are up to. No matter where you are at in your journey. I’m going to be mapping out what I’ve learned from Matt, and compare with I’ve learned from other organizations. I will be publishing it all as stories here on API Evangelist, but will also look to publish a guide and white papers on the subject, as I learn more. I’ve worked with some universities, government agencies, as well as companies on their API governance strategies. API governance is something that I know many API providers are working on, but Capital One was definitely the furthest along in their journey that I have come across to date. <a href="https://medium.com/capital-one-developers/api-lifecycle-governance-best-practices-56a1ebbb4f1b">I’m stoked that they are willing to share their story</a>, and don’t see it as their secret sauce, as it is something that doesn’t just need sharing, it is something we need leaders to step up and show everyone else how it can be done.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/09/api-governance-in-the-capital-one-devexchange/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/09/publishing-your-api-road-map-using-trello/">Publishing Your API Road Map Using Trello</a></h3>
        <span class="post-date">09 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/tyk/tyk-road-map.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I consider a road map for any API to be an essential building block, whether it is a public API or not. You should be in the business of planning the next steps for your API in an organized way, and you should be sharing that with your API consumers so that they can stay up to speed on what is right around the corner. If you want to really go the extra mile I recommend following what Tyk is up to, with <a href="https://trello.com/b/59d5kAZ5/tyk-api-platform-roadmap">their public road map using Trello</a>.</p>

<p>With the API management platform Tyk, you don’t just see a listing of <a href="https://trello.com/b/59d5kAZ5/tyk-api-platform-roadmap">their API road map</a>, you see all the work and conversation behind the road ma using the visual collaboration platform Trello. Using their road map you can see proposed features, which is great to see if something you want has already been suggested, and you can get at a list of what the next minor releases will contain. Plus using the menu bar you can get at a history of the changes the Tyk team has made to the platform, going back for the entire history of the Trello board.</p>

<p>Using Trello you can subscribe to, or vote up any of the message boards. If you want to submit something you need to sign-up and post something to <a href="https://community.tyk.io/">the Tyk community</a>. Then they’ll consider adding it to the proposed road map features. It is a pretty low cost, easy to make public, approach to delivering a road map. Sometimes this stuff doesn’t need a complex solution, just one that provides some transparency, and help your customers understand what is next. Tyk provides a nice way to provide a road map that any other API provider, or service provider can follow.</p>

<p>Another interesting approach to delivering an API road map that I can add to my research.  I’m a big fan of having many different ways of delivering the essential building blocks of API operations, using a variety of simple free or paid SaaS tools. You’d be surprised at how useful an open road map can be for your API. Even if you aren’t adding too many new features, or have a huge number of people participating, it provides an easy reference showing what is next for an API. It also shows someone is home behind the scenes, and that an API is actually active, alive, and something you should be using.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/09/publishing-your-api-road-map-using-trello/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/09/communicating-sections-of-your-api-documentation/">Communication Strategy Filler Using Sections Of Your API Documentation</a></h3>
        <span class="post-date">09 Oct 2017</span>
        <p><a href="https://twitter.com/AWSSecurityInfo/status/868059146280550400"><img src="https://s3.amazonaws.com/kinlane-productions/aws/aws-tweet-documentation-links.png" align="right" width="35%" style="padding: 15px;" /></a>&lt;/a&gt;</p>
<p>Coming up with things creative things to write about regularly on the blog, and on Twitter when you are operating an API is hard. It has taken a lot of discipline to keep posts going up on API Evangelist regularly for the last seven years–totaling almost 3K total stories told so far. I don’t expect every API provider to have the same obsessive compulsive disorder that I do, so I’m always looking for innovative things that they can do to communicate with their API communities–something that Amazon Web Services is always good at providing healthy examples that I feel I can showcase.</p>

<p>One thing the AWS team does on a regular basis is tweeting out links to specific areas of their documentation, that helps users accomplish specific things with AWS APIs. The AWS security team is great at doing this, with recent examples focusing on securing things with <a href="https://twitter.com/AWSSecurityInfo/status/868059146280550400">the AWS Directory Service</a>, and <a href="https://twitter.com/AWSSecurityInfo/status/867711869900935169">API Organizations</a>. Each contains a useful description, attractive looking image, and a link to a specific page in the documentation that helps you learn more about what is possible.</p>

<p>I have been pushing myself to make sure all headers, and sub headers in my API documentation have anchors, so that I can not just link to a specific page, but I can link to a specific section, path, or other relevant item within my API documentation. This helps me in my storytelling when I’m looking to reference specific topics, and would help when it comes to tweeting out regular elements across my documentation in tweets. I’m slowly going to push out some of the lower grade tweets of curated news that I push out, and replace with relevant work I do in specific areas of my research–using my own work to fill the cracks over less than exciting things I may come across in the API space.</p>

<p>Tweeting out what is possible with your API, with links to specific sections of your API documentation is something I’m going to add to my <a href="http://communications.apievangelist.com/">API communication research</a>. Providing a common building block that other API providers, or even API service providers can consider when looking for things to fill the cracks in their platform communication strategy. It is simple, useful to API consumers, and is an easy way to keep the Tweet stream regularly flowing, and helping developers understand what is possible. I feel it is also something that should impact how we craft our documentation, as well as our communication strategies, making sure we are publishing with appropriate titles and anchors so we can easily reference and cite the valuable information we are making available across our platforms.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/09/communicating-sections-of-your-api-documentation/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/06/publish-share-monetize-machine-learning-apis/">Publish, Share, Monetize Machine Learning APIs</a></h3>
        <span class="post-date">06 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/azure/azure-publish-share-monetize-ml.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’ve been playing with Tensor Flow for over a year now, specifically when it comes to working with images and video, but it has been something that has helped me understand what things looks like behind the algorithmic curtain that seems to be part of a growing number of tech marketing strategies right now. Part of this learning is exploring beyond Google’s approach, who is behind Tensor Flow, and understand what is going on at AWS, as well as Azure. I’m stil getting my feet wet learning about what Microsoft is up to with their platform, but I did notice one aspect of the <a href="https://azure.microsoft.com/en-us/services/machine-learning/">Azure Machine Learning Studio</a> emphasized developers to, <em>“publish, share, monetize”</em> their ML models. While I’m sure there will be a lot of useless vapor ware being sold within this realm, I’m simply seeing it as the next step in API monetization, and specifically the algorithmic evolution of being an API provider.</p>

<p>As the label says in the three ML models for sale in the picture, this is all experimental. Nobody knows what will actually work, or even what the market will bear. However, this is something APIs, and the business of APIs excel at. Making a digital resource available to consumers in a retail, or even wholesale way via marketplaces like Azure and AWS, then playing around with features, pricing, and other elements, until you find the sweet spot. This is how Amazon figured out the whole cloud computing game, and became the leader. It is how Twilio, Stipe and other API as a product companies figured out what developers needed, and what these markets would bear. This will play out in marketplaces like Azure and Google, as well as startup players like <a href="https://algorithmia.com/">Algorithmia</a>–which is where I’ve been cutting my teeth, and learning about ML.</p>

<p>The challenge for ML API entrepreneurs will be helping consumers understand what their models do, or do not do. I see it as an opportunity, because there will be endless amounts of vapor ware, ML voodoo, and smoke and mirrors trying to trick consumers into buying something, as well as endless traps when it comes to keeping them locked in. If you are actually doing something interesting with ML, and it actually provides value in the business world, and you provide clear, concise, no BS language about what it does–you are going to do well. The challenge for you will be getting found in the mountains of crap that is emerging, and differentiating yourself from the smoke and mirrors that we are already seeing so much of. Another challenge you’ll face is navigating the vendor platform course set up by AWS, Google, and Azure as they battle it out for dominance–a game that many of us little guys will have very little power to change or steer.</p>

<p>It is a game that I will keep a close eye on. I’m even pondering publishing a handful of image manipulation models I’ve been working on. IDK. I do not think they are quite ready, and I’m not even entirely sure they are something I want widely used. I’m kind of enjoying using them in my own work, providing me with images I can use in my storytelling. I don’t think the ROI is there yet in the ML API game, and I’ll probably just keep being a bystander, and analyst on the sideline until I see just the right opportunity, or develop just the right model I think will stand out. After seven years of doing API Evangelist I’m pretty good at seeing through the BS, and I’m thinking this experience is going to come in handy in this algorithmic evolution of the API universe, where the magic of AI and ML put so many people under their spell.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/06/publish-share-monetize-machine-learning-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/06/thinking-about-why-we-rate-limit-our-apis/">Thinking About Why We Rate Limit Our APIs</a></h3>
        <span class="post-date">06 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/power-lines-empty-space_sunday.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I am helping a client think through their API management solution at the moment, so I’m working through all the moving parts of how, and why of API management solutions. The API management landscape has shifted since the last time I helped a small company navigate the process of getting up and running, so I wanted to work through each aspect and think critically before I make any recommendations. My client has a content API, which isn’t very complex, but possesses some pretty valuable data they’ve aggregated, curated, and are looking to make available via a simple web API. It is pretty clear that all developers will need a key to be access the API, but I wanted to pause for a moment and think more about API rate limiting.</p>

<p>Why do we rate limit? The primary reason is to help manage the compute resources available for all API consumers. You don’t want any single user hitting the server too hard, and taking things down for everyone else. I’d say after that, the next major reason is to enforce API access tiers, and ensure API consumers are only consuming what they should be. Which both seem like pretty dated concepts, that might need re-evaluation in general, but also in the context of this particular project. There is no free access to this API. I believe there will be a public account for test driving (making very limited # of calls), and some that drive their embeddable strategy, but for access to the majority of content, developers will have to register for a key, and provide a credit card to pay for their consumption. Which leaves me with the question, should we be rate limiting at all?</p>

<p>If users are paying for whatever they consume, and there is a credit card on file, do we want to rate limit? Why are we so worried about server capacity in a cloud world? It seems like rate limiting is a legacy constraint, that has continue to live on unquestioned, and even propped up by accounting and business decisions over simple technical ones. API access tiers with varying rate limits are sometime imposed as part of identity and access control, limiting what new users have access to, but often times they are used to corral and route users into specific, and measurable account plans, that help startups predict and articulate revenue to investors. I know many of my friends disagree with my thoughts on this, but I feel this accounting decision behind rate limiting are hurting the bottom line, more than they are helping. If you are focused on your API being the product it is hurting it, if you are focused on your API consumers being your product, then you are helping it.</p>

<p>My client in question is looking to build an actual business that sells a product to customers, without an exit strategy, so I want to do my best to help them understand how they can reduce technical and business complexity, while maximizing revenue around the API services they are offering. If we have the API properly resourced with scalable compute, load-balancing, monitoring, checks and balances. Then we also have a verified credit card on file for each API key holder. Why do we want to rate limit? It seems like it is an unnecessary complexity for API consumers to have to wrestle with. Let’s just allow them to register, make API calls, measure, and bill accordingly. Amazon provides a clear precedent for how this works, and from my experience I tend to spend more on my AWS bill then I do with services I use which keep me in tiered access plans. I’m not saying tiered access plans don’t have their place, I’m saying we should be questioning their value each time we are constructing them, and not just assuming they should be done my default.</p>

<p>A by-product of noticing how the API management landscape recently is helping me reassess each of the common building blocks of API management, and think more critically about the how and why behind their existence. There is a significant difference between rate limiting and metering API calls, and I don’t think we always have to do both. We still need the ability to turn off keys, and block specific user agents and IP addresses, but in some cases I think rate limiting shouldn’t be part of the API management operations. We have the compute, storage, and databases resources at our disposal to scale as we need to meet demand, and we have the credit card verified, and on file to bill against, let’s just get out of API consumers way. In the case of this particular project I’m working, I think this will be my recommendation. Focusing on reducing the amount of API management overhead, and simplifying the load for API consumers along the way.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/06/thinking-about-why-we-rate-limit-our-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/06/the-api-management-landscape-looks-different-than-i-anticipated/">The API Management Landscape Has Shifted More Than I Anticipated</a></h3>
        <span class="post-date">06 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/yellow-tree-in-the-rocks.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>It is interesting to take a fresh look at the API management landscape these days. It has been a while since I’ve looked through all the providers to see where their pricing is at, and what they offer. I’d say the space has definitely shifted from what things looked like 2012 through 2015. There are still a number of open source offerings, which there weren’t in 2012, but the old guard has solidly turned their attention to the enterprise. There are the cloud solutions like <a href="https://restlet.com/">Restlet</a>, ad <a href="http://www.slashdb.com/">SlashDB</a> which really help you get up and running from existing data sources in the cloud, but for this particular project I am looking for a simple proxy and connector approach to deploying on any infrastructure, and they don’t quite fit the requirements.</p>

<p>Apigee, and the other more enterprise offerings have always been out of my league, and 3Scale’s entry level package is up to $750, which is a little out of my reach, but I do know they are open sourcing their offering, now that they are part of Red Hat. There is <a href="http://nrel.github.io/api-umbrella/">API Umbrella</a>, <a href="http://www.apiman.io/">APIMan</a>, <a href="http://www.fusio-project.org/">Fusio</a>, <a href="http://www.monarchapis.com/">Monarch</a>, and handful of other solutions that will work, but they take certain platform, or specific language commitment that doesn’t work for this project. Everything else is of the enterprise caliber, nothing really that I would recommend to my customers who are just getting started on their API journey. I’m really left with the cloud giants, which I guess is one of the main reasons we are at this junction in the evolution of API management. API management becoming a commodity has really shifted the landscape, making it more difficult to be a strong player like <a href="https://s3.amazonaws.com/kinlane-productions/partners/tyk-logo.png">Tyk</a> and Kong are managing to pull off.</p>

<p>If my customer was looking to launch a data API from an existing database I’d point them to SlashDb or Restlet. If they are an enterprise customer I’d point them to 3Scale. Tyk is pretty much my goto person for the lower end of the market, with Kong as the alternate. If my customer is already running on Google, Azure, or AWS, then I’m pretty much telling them to stay put, and use the tooling that is available to them. Another thing I’m noticing has dropped out of prominence is the billing for API usage aspect of API management. It’s in Tyk, and 3Scale, but really wasn’t a component of many of the other solutions I’ve been looking at. Overall things seem scattered out there after the last round of API management acquisitions, the VC funding shifted, and the cloud giants stepped into their game with their solutions. I’m guessing that is the just the circle of life and all.</p>

<p>In this new landscape I am going to have to spend more time playing with the low-end solutions that are available. It is essential that there are solutions that are accessible to folks who are just starting on their API journey. It is something that requires a climb-able ladder. Meaning you need to be able to afford the reach to the next rung of the ladder, otherwise it can become quite a blocker. I was a big advocate for this in the early days, but stopped pushing on because there were so many options out there. It will take some playing around to get a better feeling about where we are, before I feel good about making recommendations to new players again. A process I should probably be repeating each year, because things seem to be shifting a little more than I anticipated.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/06/the-api-management-landscape-looks-different-than-i-anticipated/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/06/a-couple-more-questions-for-the-equifax-ceo-about-their-breach/">A Couple More Questions For The Equifax CEO About Their Breach</a></h3>
        <span class="post-date">06 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/digital-bits-capital-dc-flag-side-view.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p><a href="https://www.theverge.com/2017/10/3/16410806/equifax-ceo-blame-breach-patch-congress-testimony">Speaking to the House Energy and Commerce Committee, former Equifax CEO Richard Smith pointed the finger at a single developer who failed to patch the Apache Struts vulnerability</a>. Saying that protocol was followed, and a single developer was responsible, shifting the blame away from leadership. It sounds like a good answer, but when you operate in the space you understand that this was a systemic failure, and you shouldn’t be relying on a single individual, or even a single piece of scanning software to verify the patch was applied. You really should have many layers in place to help prevent breaches like we saw with Equifax.</p>

<p>If I was interviewing the CEO, I’d have a few other questions for him, getting at some of the other systemic and process failures based upon his lack of leadership, and awareness:</p>

<ul>
  <li><strong>API Monitoring &amp; Testing</strong> - You say the scanner for the Apache Struts vulnerability failed, but what about other monitoring and testing. The plugin in questions was a REST plugin, that allowed for API communication with your systems. Due to the vulnerability, extra junk information was allowed to get through. Where were your added API request and response integrity testing and monitoring process? Sure you were scanning for the vulnerability, but are you keeping an eye on the details of the data being passed back and forth? API monitoring &amp; testing has been around for many years, and service providers like <a href="http://apis.how/8nlsropidv">Runscope</a> do this for a living. What other layers of monitoring and testing were in place?</li>
  <li><strong>API Management</strong> - When you expose APIs like you did from Apache Struts, what does the standardized management approach look like? What sort of metering, logging, rate limiting, and analysis occurs on each endpoint, and verification occurs, ensuring that only required clients should have access? API management has been standard procedure for over a decade now for exposing APIs like this both internally and externally. Why didn’t your API management process stop this sort of breach after only a couple hundred record went out? API management is about awareness regarding access to all your resources. You should have a dashboard, or at least some reports that you view as a CEO on this aspect of operations.</li>
</ul>

<p>These are just two examples of systems and processes that should have been in place. You should not be depending on a single person, or a single tool to catch this type of security incident There should be many layers in place, with security triggers, and notifications in place. Your CTO should be in tune with all of these layers, and you as the CEO should be getting briefed on how they work, and have a hand in making sure they are in place. I’m guessing that your company is doing APIs, but is dramatically behind the times when it comes to commonplace API management practices. This is your fault as the CEO. This is not the fault of a single employee, or any group of employees.</p>

<p>I am guessing that as a CEO you are more concerned with the selling of this data, than you are of securing it in storage, or transit. I’m guessing you are intimately aware of the layers that enable you to generate revenue, but you are barely investing in the technology and processes to do this securely, while respecting the privacy of your users. They are just livestock to you. They are just products on a shelf. It shows your lack of leadership to point the finger at a single person, or single piece of technology. There should have been many layers in place to catch this type of breach beyond a single vulnerability. It demonstrates your lack of knowledge regarding modern trends in how we secure and provide access to data, and you should never have been put in charge of such a large data brokerage company.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/06/a-couple-more-questions-for-the-equifax-ceo-about-their-breach/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/05/teaching-my-client-three-approaches-to-modular-ui-design-using-their-apis/">Teaching My Client Three Approaches To Modular UI Design Using Their APIs</a></h3>
        <span class="post-date">05 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/holmes-county/holmes-county-listing.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I am working with a client to develop a simple user interface on top of a Human Services Data API (HSDA) I launched for them. They want a basic website for searching, browsing, and navigating the organizations, locations, and services available in their API. A part of this work is helping them understand how modular and configurable their web site is, with each page, or portion of a page being a simple API call. It is taking a while for them to fully understand what they have, and the potential of evolving a web application in this way, but I feel like they are beginning to understand, and are taking the reigns a little more when it comes to dictate what they want within this new world.</p>

<p>When I first published a basic listing of human services they were disappointed. They had envisioned a map of the listings, allowing users to navigate in a more visual way. I got to work helping them see the basic API call(s) behind the listing, and how we could use the JSON response in any way we wanted. I am looking to provide three main ways in which I can put the API data to work in a variety of web applications:</p>

<ul>
  <li><strong>Server-Side</strong> - A pretty standard PHP call to the API, taking the results and rendering to the page using HTML.</li>
  <li><strong>Client-Side</strong> - Leveraging JavaScript in the browser to call the API and then render to the page using Jquery.</li>
  <li><strong>Static Push</strong> - Calling the APPI using PHP, then publishing as YAML or JSON to a Jekyll site and rendering with Liquid and HTML.</li>
</ul>

<p>What the resulting HTML, CSS, and JavaScript looks like in all these scenarios is up to the individual who is in charge of dictating the user experience. In this case, it is my client. They just aren’t used to having this much control over dictating the overal user experience. Which path they choose depends on a few things like whether they want the content to be easily indexed by search engines, or if they want things to be more JavaScript enabled magic (ie. maps, transitions, flows). All the API does is gives them full access to the entire surface area of the human services schema they have stored in their database.</p>

<p>After moving past the public website, I’m beginning to show them the power of not just GETs via their API, I’m showing them the power of POST, PUT, and DELETE. I find it is easy to show them the potential in an administrative system using a basic form. I find people get forms. Once you show them that in order to POST they have to have a special token or key, otherwise the API will reject, they feel a whole lot better about the process. I find the form tends to put things into context for them, beyond displaying data and content, and allowing them to actually manage all this data and content. I find the modularity of an API really lends itself to giving business users more control over the user interface. They may not be able to do everything themselves, but they tend to be more invested in the process, and enjoy more ownership over the process–which is a good thing.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/05/teaching-my-client-three-approaches-to-modular-ui-design-using-their-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/05/how-api-evangelist-works/">How API Evangelist Works</a></h3>
        <span class="post-date">05 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/crypto-machine-bletchley_copper_circuit.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’ve covered this topic several times before, but I figured I’d share again for folks who might have just become readers int he last year. Providing an overview of how API Evangelist works, to help eliminate confusion as you are navigating around my site, as well as to help you find what you are looking for. First, API Evangelist was started in the summer of 2010 as a research site to help me better understand what is going on in the world of APIs. In 2017, it is still a research site, but it has grown and expanded pretty dramatically into a network of websites, driven by a data and a content core.</p>

<p>The most import thing to remember is that <a href="https://github.com/api-evangelist">all my sites run on Github</a>, which is my workbench in the the API Evangelist workshop. apievangelist.com is the front door of the workshop, with each area of my research existing as its own Github repository, at its own subdomain with the apievangelist domain. An example of this can be found in my API design research, where you will find at <a href="http://design.apievangelist.com/">design.apievangelist.com</a>. As I do my work each day, I publish my research to each of my domains, in the form of YAML data for one of these areas:</p>

<ul>
  <li><strong>Organizatons</strong> - Companies, organizations, institutions, programs, and government agencies doing anything interesting with APIs.</li>
  <li><strong>Individuals</strong> - The individual people at organizations, or independently doing anything interesting with APIs.</li>
  <li><strong>News</strong> - The interesting API related, or other news I curate and tag daily in my feed reader or as I browse the web.</li>
  <li><strong>Tools</strong> - The open source tooling I come across that I think is relevant to the API space in some way.</li>
  <li><strong>Building Blocks</strong> - The common building blocks I find across the organizations, and tooling I’m studying, showing the good and the bad of doing APIs.</li>
  <li><strong>Patents</strong> - The API related patents I harvest from the US Patent Office, showing how IP is impacting the world of APIs.</li>
</ul>

<p>You can find the data for each of my research areas in the _ data folder for each repository. Which is then rendered as HTML for each subdomain using Liquid via each Jekyll CMS driven website. All of this is research. It isn’t meant to be perfect, or a comprehensive directory for others to use. If you find value in it–great!! However, it is just my research laying on my workbench. It will change, evolve, and be remixed and reworked as I see fit, to support my view of the API sector. You are always welcome to learn from this research, or even fork and reuse it in your own work. You are also welcome to submit pull requests to add or update content that you come across about your organization or open source tool.</p>

<p>The thing to remember about API Evangelist is it exist primarily for me. It is about me learning. I take what I learn and publish as blog posts to API Evangelist. This is how I work through what I’m discovering as part of my research each day, and use as a vehicle to move my understanding of APIs forward. This is where it starts getting real. After seven years of doing this I am reaching 4K to 7K page views per day, and clearly other folks find value in reading, and sifting through my research. Because of this I have four partners, <a href="http://apis.how/ake3nxbapm">3Scale</a>, <a href="http://apis.how/5ytnitnakm">Restlet</a>, <a href="http://apis.how/8nlsropidv">Runscope</a>, and <a href="http://apis.how/zflfesymzk">Tyk</a> who pay me money to have their logo on the home page, in the navigation, and via a footer toolbar. Runscope also pays me to place a re-marketing tag on my site so they can show advertising to my users on other websites, and Facebook. This is how I pay my rent, an how I eat each month.</p>

<p>Beyond this base, I take my research and create API Evangelist industry guides. <a href="http://definitions.apievangelist.com/#Guide">API Definitions</a>, and <a href="http://design.apievangelist.com/#Guide">API Design</a> are the two most recent editions. I’m currently working on one for data, database, as well as deployment, and management. These guides are sometimes underwritten by my partners, but mostly they are just the end result of my API research. I also spend time and energy taking what I know and craft API strategy and white papers for clients, and occasionally I actually create APIs for people–mostly in the realm of human services, or other social good efforts. I’m not really interested in building APIs for startups, or in service of the Silicon Valley machine. Even tough I enjoy watching, studying, and learning from this world, because there are endless lessons regarding how we can use technology in this community, as well as how we should not be using technology.</p>

<p>That is a pretty basic walk through of API Evangelist works. It is important to remember I am doing this research for myself. To learn, and to make a living. API Evangelist is a production, a persona I created to help me wade through the technology, business, and politics of APIs. It reflects who I am, but honestly is increasingly more bullshit than it is reality, kind of like the API space. I hope you enjoy this work. I enjoy hearing from my readers, and hearing how my research impacts your world. It keeps me learning each day, and from ever having to go get a real job. It is always a work in progress and never done. Which I know frustrates some, but I find endlessly interesting, and is something that reflects the API journey, something you have to get used to if you are going to be successful doing APIs in this crazy world.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/05/how-api-evangelist-works/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/05/show-that-api-call-behind-that-dashboard-visualization/">Show The API Call Behind Each Dashboard Visualization</a></h3>
        <span class="post-date">05 Oct 2017</span>
        <p><a href="https://www.kentik.com/kentik-apis-enable-multi-solution-integration/"><img src="https://s3.amazonaws.com/kinlane-productions/kentik/kentik_API_menu-300w.png" align="right" width="35%" style="padding: 15px;" /></a></p>
<p>I am a big fan of user interfaces that bring APIs out of the shadows. Historically, APIs are often a footnote in the software as a service (SaaS) world, available as a link way down at the bottom of the page, in the settings, or help areas. Rarely, are APIs made a first class citizen in the operations of a web application, which really just perpetuates the myth that APIs aren’t for everybody, and the “normals” shouldn’t worry their little heads about it. When in reality, EVERYBODY should know about APIs, and have the opportunity to put them to work, so we should stop burying the links to our APIs, and our developer areas. If your API is too technical for a business user to understand what is going on, then you should probably get to work simplifying it, not burying it and keeping it in developer and IT realm.</p>

<p>I have written before about <a href="https://apievangelist.com/2016/10/24/the-api-behind-every-feature-in-the-user-interface/">how DNS provider CloudFlare provides an API behind every feature in their user interface</a>, and I’ve found <a href="https://www.kentik.com/kentik-apis-enable-multi-solution-integration/">another great example of this over at the network API provider Kentik</a>. In their network dashboard visualization tooling they provide a robust set of tooling for accessing the data behind the visuals, allowing you to export, view SQL, show API call, and enter share view. In their post, they proceed to instruction about how you can get your API key as part of your account, as well as providing a pretty robust introduction into why APIs are important. This is how ALL dashboards should work in my opinion. Any user should be introduced to APIs, and have the ability to get at the data behind, and export it, or directly make an API call in their browser or at the command line.</p>

<p>Developers like to think this stuff should be out of reach of the average user, but that is more about our own insecurities, and power trips, than it is about the average users ability to grasp this stuff. There is no reason why ALL user interfaces can’t be developed on top of APIs, with native functionality for getting at the API call, as well as the data, content, or algorithms behind the user interface feature. It makes for more powerful user interfaces, as well as more educated, literate, and efficient power users of our applications. If all web applications operated this way, we’d see a much more API literate business world, where users would be asking more questions, curious about how things work, and experimenting with ways they can be more successful in what they do. While I do see positive examples like Kentik out there, I also find that many web application developers are further retreating from APIs being front and center, preferring to keep them in the shadows of web and mobile applications, out of the reach of the average user. Something we need to reverse.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/05/show-that-api-call-behind-that-dashboard-visualization/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/05/big-data-is-not-abut-access-using-web-apis/">Big Data Is Not About Access Using Web APIs</a></h3>
        <span class="post-date">05 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/dragon_close-up_yellow_collage.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’m neck deep in research around data and APIs right now, and after <a href="http://apievangelist.com/2017/10/03/looking-at-the-37-apached-data-projects/">looking at 37 of the Apache data projects</a> it is pretty clear that web APIs are not a priority in this world. There are some of the projects that have web APIs, and there a couple projects that look to bridge several of the projects with an aggregate or gateway API, but you can tell that the engineers behind the majority of these open source projects are not concerned with access at this level. Many engineers will counter this point by saying that web APIs can’t handle the volume, and it shows that the concept isn’t applicable in all scenarios. I’m not saying web APIs should be used for the core functionality at scale, I’m saying that web APIs should be present to provide access to the result state of the core features for each of these platform, whatever that is, which something that web APIs excel at.</p>

<p>From my vantage point the lack of web APIs isn’t a technical one, it is a business and political motivation. When it comes to big data the objectives are always about access, and it definitely isn’t about the wide audience access that comes when you use HTTP, and the web for API access. The objective is to aggregate, move around, and work with as much data as you possibly can amongst a core group of knowledgable developers. Then you distribute awareness, access, and usage to designated parties via distilled analysis, visualizations, or in some cases to other systems where the result can be accessed and put to use. Wide access to this data is not the primary objective, paying forward much of the power and control we currently see around database to API efforts. Big data isn’t about democratization. Big Data is about aggregating as much as you can and selling the distilled down wisdom from analysis, or derived as part of machine learning efforts.</p>

<p>I am not saying there is some grand conspiracy here. It just isn’t the objective of big data folks. They have their marching orders, and the technology they develop reflect these marching orders. It reflects the influence money and investment has on the technology. The ideology that drives how the tech is engineered, and the algorithms handle specific inputs, and provide intended outputs. Big data is often sold as data liberation, democratization, and access to your data, building on much of what APIs have done in recent years. However, in the last couple of years the investment model has shifted, the clients who are purchasing and implementing big data have evolved, and they aren’t your API access type of people. They don’t see wide access to data as a priority. You are either in the club, and know how to use the Apache X technology, or you are sanctioned one of the dashboard analysis visualization machine learning wisdom drips from the big data. Reaching a wide audience is not necessary.</p>

<p>For me, this isn’t some amazing revelation. It is just watching power do what power does in the technology space. Us engineers like to think we have control over where technology goes, yet we are just cogs in the larger business wheel. We program the technology to do exactly what we are paid to do. We don’t craft liberating technology, or the best performing technology. We assume engineer roles, with paychecks, and bosses who tell us what we should be building. This is how web APIs will fail. This is how web APIs will be rendered yesterdays technology. Not because they fail technically, it is because the ideology of the hedge funds, enterprise groups, and surveillance capitalism organizations that are selling to law enforcement and the government will stop funding data systems that require wide access. The engineers will go along with it because it will be real time, evented, complex, and satisfying to engineer in our <a href="http://kinlane.com/2017/10/04/isolated-development-environments/">isolated development environments (IDE</a>). I’ve been doing data since the 1980s, and in my experience this is how data works. Data is widely seen as power, and all the technical elements, and many of the human elements involved often magically align themselves in service of this power, whether they realize they are doing it or not.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/05/big-data-is-not-abut-access-using-web-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/04/apis-used-to-give-access-to-resources-that-were-out-of-our-reach/">APIs Used To Give Us Access To Resources That Were Out Of Our Reach</a></h3>
        <span class="post-date">04 Oct 2017</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/mountainlake/clean_view/file-00_00_58_86.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I remember when almost all the APIs out there gave us developers access to things we couldn’t ever possibly get on our own. Some of it was about the network effect with the early Amazon and eBay marketplaces, or Flickr and Delicious, and then Twitter and Facebook. Then what really brought it home was going beyond the network effect, and delivering resources that were completely out of our reach like maps of the world around us, (seemingly) infinitely scalable compute and storage, SMS, and credit card payments. In the early days it really seemed like APIs were all about giving us access to something that was out of our reach as startups, or individuals.</p>

<p>While this still does exist, it seems like many APIs have flipped the table and it is all about giving them access to our personal and business data in ways that used to be out of their reach. Machine learning APIs are using parlour tricks to get access to our internal systems and databases. Voice enablement, entertainment, and cameras are gaining access to our homes, what we watch and listen to, and are able to look into the dark corners of our personal lives. Tinder, Facebook, and other platforms know our deep dark secrets, our personal thoughts, and have access to our email and intimate conversations. The API promise seems to have changed along the way, and stopped being about giving us access, and is now about giving them access.</p>

<p>I know it has always been about money, but the early vision of APIs seemed more honest. It seemed more about selling a product or service that people needed, and was more straight up. Now it just seems like APIs are invasive. Being used to infiltrate our professional and business worlds through our mobile phones. It feels like people just want access to us, purely so they can mine us and make more money. You just don’t see many Flickrs, Google Maps, or Amazon EC2s anymore. The new features in mobile devices we carry around, and the ones we install in our home don’t really benefit us in new and amazing ways. They seem to offer just enough to get us to adopt them, and install in our life, so they can get access to yet another data point. Maybe it is just because everything has been done, or maybe it is because it has all been taken over by the money people, looking for the next big thing (for them).</p>

<p>Oh no! Kin is ranting again. No, I’m not. I’m actually feeling pretty grounded in my writing lately, I’m just finding it takes a lot more work to find interesting APIs. I have to sift through many more emails from folks telling me about their exploitative API, before I come across something interesting. I go through 30 vulnerabilities posts in my feeds, before I come across one creative story about something platform is doing. There are 55 posts about ICOs, before I find an interesting investment in a startup doing something that matters. I’m willing to admit that I’m a grumpy API Evangelist most of the time, but I feel really happy, content, and enjoying my research overall. I just feel like the space has lost its way with this big data thing, and are using APIs to become more about infiltrating and extraction, that it is about delivering something that actually gives developers access to something meaningful. I just think we can do better. Something has to give, or this won’t continue to be sustainable much longer.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/04/apis-used-to-give-access-to-resources-that-were-out-of-our-reach/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/04/api-providers-should-provide-observability-into-government-developer-accounts/">API Providers Should Provide Observability Into Government Developer Accounts</a></h3>
        <span class="post-date">04 Oct 2017</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/api_evangelist_site/blog/white_house_window_propaganda_leaflets.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’ve talked about this before, but after reading several articles recently about various federal government agencies collecting, and using social media accounts for surveillance lately, it is a drum I will be beating a lot more regularly. <a href="http://transparency.apievangelist.com/2013/02/26/api-transparency-report-as-essential-building-block/">Along with the transparency reports we are beginning to see emerge from the largest platform providers</a>, I’d like to start seeing more observability regarding which accounts, both user and developer are out of government agencies. Some platforms are good at highlighting how government of all shapes and sizes are using their platform, and some government agencies are good at showcasing their social media usage, but I’d like to understand this from purely an API developer account perspective.</p>

<p>I’d like to see more observability into which government agencies are requesting API keys. Maybe not specific agencies ad groups, and account details, although that would be a good idea as well down the road. I am just looking for some breakdown of how many developer accounts on a platform are government and law enforcement. What does their API consumption look like? If there is Oauth via a platform, is there any bypassing of the usual authentication flows to get at data, any differently than regular developers would be accessing, or requiring user approval? From what I am hearing, I’m guessing that there are more government accounts out there than platforms either realize, or are willing to admit. It seems like now is a good time to start asking these questions.</p>

<p>I would add on another layer to this. If an application developer is developing applications on behalf of law enforcement, or as part of a project for a government agency, there should be some sort of disclosure at this level as well. I know I’m asking a lot, and a number of people will call me crazy, but with everything going on these days, I’m feeling like we need a little more disclosure regarding how government(s) are using our platforms, as well as their contractors. The transparency disclosure that platforms have been engaging is a good start to the legal side of this conversation, but I’m looking for the darker, more lower level surveillance that I know is going on behind the scenes. The data gathering on U.S. citizens that doesn’t necessarily violate any particular law, because this is such new territory, and the platform terms of service might sanction it in some loopholy kind of way.</p>

<p>This isn’t just a U.S. government type of thing. I want this to be standard practice for all forms of government on the platforms we use. A sort of UN level, General Data Protection Regulation (GDPR). Which reminds me. I am woefully behind on what GDPR outlines, and how the rolling out of it is going. Ok, I’ll quick ranting now, and get back to work. Overall, we are going to have to open up some serious observability into how the online platforms we are depending are being accessed and use by the government, both on the legal side of things, as well as just general usage. Seems like the default on the general usage should always be full disclosure, but I’m guessing it isn’t a conversation anyone is having yet, which is why I bring up. Now we are having it. Thanks.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/04/api-providers-should-provide-observability-into-government-developer-accounts/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/04/letting-go-in-an-api-world-is-hard-to-do/">Letting Go In An API World Is Hard To Do</a></h3>
        <span class="post-date">04 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/flower-barbed-wire_clean_view.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I encounter a number of folks who really, really, really want to do APIs. You know, because they are cool and all, but they just can’t do what it takes to let go a little, so that their valuable API resources can actually be put to use by other folks. Sometimes this happens because they don’t actually own the data, content, or algorithms they are serving up, but in other cases it is because they view their thing as being so valuable, and so important that they can’t share it openly enough, to be accessible via an API. Even if your APIs are private, you still have to document, and share access with folks, so they can understand what is happening, and have enough freedom to put to use in their application as part of their business, without too much constraint and restrictions.</p>

<p>Some folks tell me they want to do API, but I can usually tell pretty quickly that they won’t be able to go the distance. I find a lot of this has to do with perceptions of intellectual property, combined with a general distrust of EVERYONE. My thing is super valuable, extremely unique and original, and EVERYONE is going to want it. Which is why they want to do APIs, because EVERYONE will want it. Also, once it is available to EVERYONE via an API, competitors, and people we don’t want getting at it, will now be able to reverse engineer, and copy this amazing idea. However, if we don’t make accessible, we can’t get rich. Dilemna. Dilemna. Dilemna. What do we do? My answer is you probably that you shouldn’t be doing APIs.</p>

<p>You see, doing APIs, whether public or privately requires letting go a bit. Sure, you can dial in how much control you are willing to give up using API management solutions, but you still have to let go enough so that people can do SOMETHING with your valuable resource. If you can’t, APIs probably aren’t your jam. They just won’t work if you don’t give your API consumers enough room to breathe while developing and operating their integrations and applications. I understand if you can’t let go. The API game isn’t for everyone, or maybe there is some other data, content, and resources you don’t feel so strongly about that you could start with, and get the hang of doing APIs before you jump in with your prized possessions?</p>

<p>Another thing I might suggest, is that maybe you should twice about why these digital things are so important to you. Is it because they really matter to you, or is because you think they’ll make you a lot of money? If it is just the latter, they are probably not very valuable if you just keep them locked up. The best ideas are the ones that get used. The things that make the biggest impact get shared, and are usually pretty accessible. I’m guessing that most of your anxiety does not come from APIs, and what will happen when you launch them. I’m pretty sure it comes from some unhealthy views about what you have, the stories you’ve been told about intellectual property, and your obsession with getting rich. Again, which leaves us at the part of the story where you probably shouldn’t do APIs–I don’t think they are your jam.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/04/letting-go-in-an-api-world-is-hard-to-do/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/04/sharing-top-sections-from-your-api-documentation-as-part-of-your-communications-strategy/">Sharing Top Sections From Your API Documentation As Part Of Your Communications Strategy</a></h3>
        <span class="post-date">04 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/amazon/top-aws-iam-documentation-pages-so-far-in-2017.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’m always learning from the API communication practices from out of the different AWS teams. From <a href="http://communications.apievangelist.com/2016/10/04/amazon-alexa-as-an-example-when-it-comes-to-api-communications/">the regular storytelling coming out of the Alexa team</a>, to the <a href="https://apievangelist.com/2012/01/12/the-secret-to-amazons-success-internal-apis/">mythical tales of leadership at AWS that have contributed to the platform’s success</a>, the platform provides a wealth of examples that other API providers can emulate.</p>

<p>As I talked about last week, <a href="http://apievangelist.com/2017/09/29/developing-an-ability-to-repeat-the-same-api-stories-over-and-over-you-are-going-to-need-it/">finding creative ways to keep publishing interesting content to your blog as part of your API evangelism and communications strategy is hard</a>. It is something you have to work at. One way I find inspiration is by watching the API leaders, and learning from what they do. An interesting example I recently found out of the AWS security team, <a href="https://aws.amazon.com/blogs/security/the-top-20-aws-iam-documentation-pages-so-far-in-2017/">was their approach to showcasing the top 20 AWS IAM documentation pages so far in 2017</a>. It is a pretty simple, yet valuable way to deliver some content for your readers, that can also help you expose the dark corners of your API documentation, and other resources on your blog.</p>

<p>The approach from the AWS security team is a great way to generate content without having to come up with good ideas, but also will help with your SEO, especially if you can cross publish, or promote through other channels. It’s pretty basic content stuff, that helps with your overall SEO, and if you play it right, you could also get some SMM juice by tweeting out the store, as well as maybe a handful of the top links from your list. It is pretty listicle type stuff, but honestly if you do right, it will also deliver value. These are the top answers, in a specific category, that your API consumers are looking for answers in. Helping these answers rise to the top of your blog, search engine, and social media does your consumers good, as well as your platform.</p>

<p>One more tool for the API communications and evangelism toolbox. Something you can pull out when you don’t have any storytelling mojo. Which is something you will need on a regular basis as an API provider, or service provider. It is one of the tricks of trade that will keep your blog flowing, you readers reading, and hopefully your valuable API, products, services, and stories floating to the top of the heap. And that is what all of this is about–staying on top of the pile, keeping things relevant, valuable, and useful. If we can’t do that, it is time to go find something else to do.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/04/sharing-top-sections-from-your-api-documentation-as-part-of-your-communications-strategy/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/03/clearly-designate-api-bot-automation-accounts/">Clearly Designate API Bot Automation Accounts</a></h3>
        <span class="post-date">03 Oct 2017</span>
        <p><a href="https://www.shieldsquare.com/good-bots-and-bad-bots/"><img src="https://s3.amazonaws.com/kinlane-productions/shieldsquare/good-v-bad-bots.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I’m continuing <a href="http://apievangelist.com/2017/07/17/bot-observability-for-every-platform/">my research into bot platform observability</a>, and <a href="http://apievangelist.com/2017/08/15/which-platforms-have-control-over-the-conversations-around-their-bots/">how API platforms are handling (or not handling) bot automation on their platforms</a>, as I try to make sense of each <a href="http://apievangelist.com/2017/10/02/the-waves-of-api-driven-bots-invading-our-shores/">wave of the bot invasion on the shores of the API sector</a>. It is pretty clear that Twitter and Facebook aren’t that interested in taming automation on their platforms, unless there is more pressure applied to them externally. I’m looking to make sure there is a wealth of ideas, materials, and examples of how any API driven platform can (are) control bot automation on their platform, as the pressure from lawmakers, and the public intensifies.</p>

<p>Requiring users clearly identify automation accounts is a good first place to start. <a href="https://api.slack.com/bot-users">Establishing a clear designation for bot users has its precedents</a>, and requiring developers to provide an image, description, and some clear check or flag that identifies an account as automated just makes sense. <a href="https://medium.com/slack-developer-blog/the-bot-rulebook-a442d9fb21cb">Providing a clear definition of what a bot is, with articulate rules for what bots should and shouldn’t be doing</a> is next up on the checklist for API platforms. Sure, not all users will abide by this, but it is pretty easy to identify automated traffic versus human behavior, and having a clear separation allows accounts to automatically turned off when they fit a particular fingerprint, until a user can pass a sort of platform Turing test, or provide some sort of human identification.</p>

<p>Automation on API platforms has its uses. However, unmanaged automation via APIs has proven to be a problem. Platforms need to step up and manage this problem, or the government eventually will. Then it will become yet another burdensome regulation on business, and there will be nobody to blame except for the bad actors in the space (cough Twitter &amp; Facebook, cough, cough). Platforms tend to not see it as a problem because they aren’t the targets of harassment, and it tends to boost their metrics and bottom line when it comes to advertising and eyeballs. Platforms like Slack have a different business model, which dictates more control, otherwise it will run their paying customers off. The technology, and practices already exist for how to manage bot automation on API platforms effectively, they just aren’t ubiquitous because of the variances in how platforms generate their revenue.</p>

<p>I am going to continue to put together a bot governance package based upon <a href="http://bots.apievangelist.com/">my bot API research</a>. Regardless of the business model in place, ALL API platforms should have a public bot automation strategy in place, with clear guidelines for what is acceptable, and what is not. I’m looking to provide API platforms with a one-stop source for guidance on this journey. It isn’t rocket science, and it isn’t something that will take a huge investment if approached early on. Once it gets out of control, and you have congress crawling up your platforms ass on this stuff then it probably is going to get more expensive, and also bring down regulatory hammer on everyone else. So, as API platform operators let’s be proactive and take the problem of bot automation on directly, and learn from Twitter and Facebook’s pain.</p>

<p><em><strong>Photo Credit:</strong> <a href="https://www.shieldsquare.com/good-bots-and-bad-bots/">ShieldSquare</a></em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/03/clearly-designate-api-bot-automation-accounts/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/03/looking-at-the-37-apached-data-projects/">Looking At The 37 Apache Data Projects</a></h3>
        <span class="post-date">03 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/apache/apache-logo.png" align="right" width="30%" style="padding: 15px;" /></p>
<p>I’m spending time investing in <a href="http://data.apievangelist.com">my data</a>, as well as <a href="http://database.apievangelist.com">my database API research</a>. I’ll have guides, with accompanying stories coming out over the next couple weeks, but I want to take a moment to publish some of the raw research that I think paints an interesting picture about where things are headed.</p>

<p>When studying what is going on with data and APIs you can’t do any search without stumbling across an Apache project doing something or other with data. <a href="https://projects.apache.org/projects.html?category">I found 37 separate projects at Apache that were data related</a>, and wanted to publish as a single list I could learn from.</p>

<ul>
  <li><a href="https://projects.apache.org/project.html?airavata"><strong>Airvata</strong>**</a> - Apache Airavata is a micro-service architecture based software framework for executing and managing computational jobs and workflows on distributed computing resources including local clusters, supercomputers, national grids, academic and commercial clouds. Airavata is dominantly used to build Web-based science gateways and assist to compose, manage, execute, and monitor large scale applications (wrapped as Web services) and workflows composed of these services.</li>
  <li><a href="https://projects.apache.org/project.html?ambari"><strong>Ambari</strong></a> - Apache Ambari makes Hadoop cluster provisioning, managing, and monitoring dead simple.</li>
  <li><a href="https://projects.apache.org/project.html?apex"><strong>Apex</strong></a> - Apache Apex is a unified platform for big data stream and batch processing. Use cases include ingestion, ETL, real-time analytics, alerts and real-time actions. Apex is a Hadoop-native YARN implementation and uses HDFS by default. It simplifies development and productization of Hadoop applications by reducing time to market. Key features include Enterprise Grade Operability with Fault Tolerance, State Management, Event Processing Guarantees, No Data Loss, In-memory Performance &amp; Scalability and Native Window Support.</li>
  <li><a href="https://projects.apache.org/project.html?avro"><strong>Avro</strong></a> - Apache Avro is a data serialization system.</li>
  <li><a href="https://projects.apache.org/project.html?beam"><strong>Beam</strong></a> - Apache Beam is a unified programming model for both batch and streaming data processing, enabling efficient execution across diverse distributed execution engines and providing extensibility points for connecting to different technologies and user communities.</li>
  <li><a href="https://projects.apache.org/project.html?bigtop"><strong>Bigtop</strong></a> - Bigtop is a project for the development of packaging and tests of the Apache Hadoop ecosystem. The primary goal of Bigtop is to build a community around the packaging and interoperability testing of Hadoop-related projects. This includes testing at various levels (packaging, platform, runtime, upgrade, etc…) developed by a community with a focus on the system as a whole, rather than individual projects. In short we strive to be for Hadoop what Debian is to Linux.</li>
  <li><a href="https://projects.apache.org/project.html?bookkeeper"><strong>BookKeeper</strong></a> - BookKeeper is a reliable replicated log service. It can be used to turn any standalone service into a highly available replicated service. BookKeeper is highly available (no single point of failure), and scales horizontally as more storage nodes are added.</li>
  <li><a href="https://projects.apache.org/project.html?calcite"><strong>Calcite</strong></a> - Calcite is a framework for writing data management systems. It converts queries, represented in relational algebra, into an efficient executable form using pluggable query transformation rules. There is an optional SQL parser and JDBC driver. Calcite does not store data or have a preferred execution engine. Data formats, execution algorithms, planning rules, operator types, metadata, and cost model are added at runtime as plugins.</li>
  <li><a href="https://projects.apache.org/project.html?couchdb"><strong>CouchDB</strong></a> - Apache CouchDB is a database that completely embraces the web. Store your data with JSON documents. Access your documents with your web browser, via HTTP. Query, combine, and transform your documents with JavaScript. Apache CouchDB works well with modern web and mobile apps. You can even serve web apps directly out of Apache CouchDB. And you can distribute your data, or your apps, efficiently using Apache CouchDB’s incremental replication. Apache CouchDB supports master-master setups with automatic conflict detection.</li>
  <li><a href="https://projects.apache.org/project.html?crunch"><strong>Crunch</strong></a> - The Apache Crunch Java library provides a framework for writing, testing, and running MapReduce pipelines. Its goal is to make pipelines that are composed of many user-defined functions simple to write, easy to test, and efficient to run.</li>
  <li><a href="https://projects.apache.org/project.html?datafu"><strong>DataFu</strong></a> - Apache DataFu consists of two libraries: Apache DataFu Pig is a collection of useful user-defined functions for data analysis in Apache Pig. Apache DataFu Hourglass is a library for incrementally processing data using Apache Hadoop MapReduce. This library was inspired by the prevalence of sliding window computations over daily tracking data. Computations such as these typically happen at regular intervals (e.g. daily, weekly), and therefore the sliding nature of the computations means that much of the work is unnecessarily repeated. DataFu’s Hourglass was created to make these computations more efficient, yielding sometimes 50-95% reductions in computational resources.</li>
  <li><a href="https://projects.apache.org/project.html?drill"><strong>Drill</strong></a> - Apache Drill is a distributed MPP query layer that supports SQL and alternative query languages against NoSQL and Hadoop data storage systems. It was inspired in part by Google’s Dremel.</li>
  <li><a href="https://projects.apache.org/project.html?edgent"><strong>Edgent</strong></a> - Apache Edgent is a programming model and micro-kernel style runtime that can be embedded in gateways and small footprint edge devices enabling local, real-time, analytics on the continuous streams of data coming from equipment, vehicles, systems, appliances, devices and sensors of all kinds (for example, Raspberry Pis or smart phones). Working in conjunction with centralized analytic systems, Apache Edgent provides efficient and timely analytics across the whole IoT ecosystem: from the center to the edge.</li>
  <li><a href="https://projects.apache.org/project.html?falcon"><strong>Falcon</strong></a> - Apache Falcon is a data processing and management solution for Hadoop designed for data motion, coordination of data pipelines, lifecycle management, and data discovery. Falcon enables end consumers to quickly onboard their data and its associated processing and management tasks on Hadoop clusters.</li>
  <li><a href="https://projects.apache.org/project.html?flink"><strong>Flink</strong></a> - Flink is an open source system for expressive, declarative, fast, and efficient data analysis. It combines the scalability and programming flexibility of distributed MapReduce-like platforms with the efficiency, out-of-core execution, and query optimization capabilities found in parallel databases.</li>
  <li><a href="https://projects.apache.org/project.html?flume"><strong>Flume</strong></a> - Apache Flume is a distributed, reliable, and available system for efficiently collecting, aggregating and moving large amounts of log data from many different sources to a centralized data store</li>
  <li><a href="https://projects.apache.org/project.html?giraph"><strong>Giraph</strong></a> - Apache Giraph is an iterative graph processing system built for high scalability. For example, it is currently used at Facebook to analyze the social graph formed by users and their connections.</li>
  <li><a href="https://projects.apache.org/project.html?hama"><strong>Hama</strong></a> - The Apache Hama is an efficient and scalable general-purpose BSP computing engine which can be used to speed up a large variety of compute-intensive analytics applications.</li>
  <li><a href="https://projects.apache.org/project.html?helix"><strong>Helix</strong></a> - Apache Helix is a generic cluster management framework used for the automatic management of partitioned, replicated and distributed resources hosted on a cluster of nodes. Helix automates reassignment of resources in the face of node failure and recovery, cluster expansion, and reconfiguration.</li>
  <li><a href="https://projects.apache.org/project.html?ignite"><strong>Ignite</strong></a> - Apache Ignite In-Memory Data Fabric is designed to deliver uncompromised performance for a wide set of in-memory computing use cases from high performance computing, to the industry most advanced data grid, in-memory SQL, in-memory file system, streaming, and more.</li>
  <li><a href="https://projects.apache.org/project.html?kafka"><strong>Kafka</strong></a> - A single Kafka broker can handle hundreds of megabytes of reads and writes per second from thousands of clients. Kafka is designed to allow a single cluster to serve as the central data backbone for a large organization. It can be elastically and transparently expanded without downtime. Data streams are partitioned and spread over a cluster of machines to allow data streams larger than the capability of any single machine and to allow clusters of co-ordinated consumers. Kafka has a modern cluster-centric design that offers strong durability and fault-tolerance guarantees. Messages are persisted on disk and replicated within the cluster to prevent data loss. Each broker can handle terabytes of messages without performance impact.</li>
  <li><a href="https://projects.apache.org/project.html?knox"><strong>Knox</strong></a> - The Apache Knox Gateway is a REST API Gateway for interacting with Hadoop clusters. The Knox Gateway provides a single access point for all REST interactions with Hadoop clusters. In this capacity, the Knox Gateway is able to provide valuable functionality to aid in the control, integration, monitoring and automation of critical administrative and analytical needs of the enterprise.</li>
  <li><a href="https://projects.apache.org/project.html?lens"><strong>Lens</strong></a> - Lens provides an Unified Analytics interface. Lens aims to cut the Data Analytics silos by providing a single view of data across multiple tiered data stores and optimal execution environment for the analytical query. It seamlessly integrates Hadoop with traditional data warehouses to appear like one.</li>
  <li><a href="https://projects.apache.org/project.html?metamodel"><strong>MetaModel</strong></a> - With MetaModel you get a uniform connector and query API to many very different datastore types, including: Relational (JDBC) databases, CSV files, Excel spreadsheets, XML files, JSON files, Fixed width files, MongoDB, Apache CouchDB, Apache HBase, Apache Cassandra, ElasticSearch, OpenOffice.org databases, Salesforce.com, SugarCRM and even collections of plain old Java objects (POJOs). MetaModel isn’t a data mapping framework. Instead we emphasize abstraction of metadata and ability to add data sources at runtime, making MetaModel great for generic data processing applications, less so for applications modeled around a particular domain.</li>
  <li><a href="https://projects.apache.org/project.html?oozie"><strong>Oozie</strong></a> - Oozie is a workflow scheduler system to manage Apache Hadoop jobs. Oozie is integrated with the rest of the Hadoop stack supporting several types of Hadoop jobs out of the box (such as Java map-reduce, Streaming map-reduce, Pig, Hive, Sqoop and Distcp) as well as system specific jobs (such as Java programs and shell scripts).</li>
  <li><a href="https://projects.apache.org/project.html?orc"><strong>ORC</strong></a> - ORC is a self-describing type-aware columnar file format designed for Hadoop workloads. It is optimized for large streaming reads, but with integrated support for finding required rows quickly. Storing data in a columnar format lets the reader read, decompress, and process only the values that are required for the current query.</li>
  <li><a href="https://projects.apache.org/project.html?parquet"><strong>Parquet</strong></a> - Apache Parquet is a general-purpose columnar storage format, built for Hadoop, usable with any choice of data processing framework, data model, or programming language.</li>
  <li><a href="https://projects.apache.org/project.html?phoenix"><strong>Phoenix</strong></a> - Apache Phoenix enables OLTP and operational analytics for Apache Hadoop by providing a relational database layer leveraging Apache HBase as its backing store. It includes integration with Apache Spark, Pig, Flume, Map Reduce, and other products in the Hadoop ecosystem. It is accessed as a JDBC driver and enables querying, updating, and managing HBase tables through standard SQL.</li>
  <li><a href="https://projects.apache.org/project.html?reef"><strong>REEF</strong></a> - Apache REEF (Retainable Evaluator Execution Framework) is a development framework that provides a control-plane for scheduling and coordinating task-level (data-plane) work on cluster resources obtained from a Resource Manager. REEF provides mechanisms that facilitate resource reuse for data caching, and state management abstractions that greatly ease the development of elastic data processing workflows on cloud platforms that support a Resource Manager service.</li>
  <li><a href="https://projects.apache.org/project.html?samza"><strong>Samza</strong></a> - Apache Samza provides a system for processing stream data from publish-subscribe systems such as Apache Kafka. The developer writes a stream processing task, and executes it as a Samza job. Samza then routes messages between stream processing tasks and the publish-subscribe systems that the messages are addressed to.</li>
  <li><a href="https://projects.apache.org/project.html?spark"><strong>Spark</strong></a> - Apache Spark is a fast and general engine for large-scale data processing. It offers high-level APIs in Java, Scala and Python as well as a rich set of libraries including stream processing, machine learning, and graph analytics.</li>
  <li><a href="https://projects.apache.org/project.html?sqoop"><strong>Sqoop</strong></a> - Apache Sqoop(TM) is a tool designed for efficiently transferring bulk data between Apache Hadoop and structured datastores such as relational databases.</li>
  <li><a href="https://projects.apache.org/project.html?storm"><strong>Storm</strong></a> - Apache Storm is a distributed real-time computation system. Similar to how Hadoop provides a set of general primitives for doing batch processing, Storm provides a set of general primitives for doing real-time computation.</li>
  <li><a href="https://projects.apache.org/project.html?tajo"><strong>Tajo</strong></a> - The main goal of Apache Tajo project is to build an advanced open source data warehouse system in Hadoop for processing web-scale data sets. Basically, Tajo provides SQL standard as a query language. Tajo is designed for both interactive and batch queries on data sets stored on HDFS and other data sources. Without hurting query response times, Tajo provides fault-tolerance and dynamic load balancing which are necessary for long-running queries. Tajo employs a cost-based and progressive query optimization techniques for optimizing running queries in order to avoid the worst query plans.</li>
  <li><a href="https://projects.apache.org/project.html?tez"><strong>Tez</strong></a> - Apache Tez is an effort to develop a generic application framework which can be used to process arbitrarily complex directed-acyclic graphs (DAGs) of data-processing tasks and also a reusable set of data-processing primitives which can be used by other projects.</li>
  <li><a href="https://projects.apache.org/project.html?vxquery"><strong>VXQuery</strong></a> - Apache VXQuery will be a standards compliant XML Query processor implemented in Java. The focus is on the evaluation of queries on large amounts of XML data. Specifically the goal is to evaluate queries on large collections of relatively small XML documents. To achieve this queries will be evaluated on a cluster of shared nothing machines.</li>
  <li><a href="https://projects.apache.org/project.html?zeppelin"><strong>Zeppelin</strong></a> - Zeppelin is a modern web-based tool for the data scientists to collaborate over large-scale data exploration and visualization projects.</li>
</ul>

<p>There is a serious amount of overlap between these projects. Not all of these projects have web APIs, while some of them are all about delivering a gateway or aggregate API across projects. There is a lot to process here, but I think listing them out provides an easier way to understand the big data explosion of projects over at Apache.</p>

<p>It is tough to understand what each of these do without actually playing with them, but that is something I just don’t have the time to do, so next up I’ll be doing independent searches for these project names, and finding stories from across the space regarding what folks are doing with these data solutions. That should give me enough to go on when putting them into specific buckets, and finding their place in my data, and database API research.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/03/looking-at-the-37-apached-data-projects/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/03/temporal-logic-of-actions-for-apis/">Temporal Logic of Actions For APIs</a></h3>
        <span class="post-date">03 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/status-berlin_dark_dali.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’m evolving forward my thoughts on algorithmic <a href="http://apievangelist.com/2017/02/28/a-checklist-for-api-observability/">observability</a> and <a href="http://apievangelist.com/2016/08/04/pushing-for-more-algorithmic-transparency-using-apis/">transparency</a> using APIs, and <a href="https://en.wikipedia.org/wiki/TLA%2B">I was recently introduced to TLA+, or the Temporal Logic of Actions</a>. It is the closest I’ve come to what I’m seeing in my head when I think about how we can provide observability into algorithms through existing external outputs (APIs). As I do with all my work here on API I want to process TLA+ as part of my API research, and see how I can layer it in with what I already know.</p>

<p>TLA+ is a formal specification language developed by Leslie Lamport, which can be used to design, model, document, and verify concurrent systems. It has been described as exhaustively-testable pseudocode which can provide a blueprint for software systems. In the context of design and documentation, TLA+ can be viewed as informal technical specifications. However, since TLA+ specifications are written in a formal language of logic and mathematics it can be used to uncover design flaws before system implementation is underway, and are amenable to model checking for finding all possible system behaviours up to some number of execution steps, and examines them for violations. TLA+ specifications use basic set theory to define safety (bad things won’t happen) and temporal logic to define liveness (good things eventually happen).</p>

<p>TLA+ specifications are organized into modules.Although the TLA+ standard is specified in typeset mathematical symbols, existing TLA+ tools use symbol definitions in ASCII, using several terms which require further definition:</p>

<ul>
  <li><strong>State</strong> - an assignment of values to variables</li>
  <li><strong>Behaviour</strong> - a sequence of states</li>
  <li><strong>Step</strong> - a pair of successive states in a behavior</li>
  <li><strong>Stuttering Step</strong> - a step during which variables are unchanged</li>
  <li><strong>Next-State Rlation</strong> - a relation describing how variables can change in any step</li>
  <li><strong>State Function</strong> - an expression containing variables and constants that is not a next-state relation</li>
  <li><strong>State Predicate</strong> - a Boolean-valued state function</li>
  <li><strong>Invariant</strong> - a state predicate true in all reachable states</li>
  <li><strong>Temporal Formula</strong> - an expression containing statements in temporal logic</li>
</ul>

<p>TLA+ is concerned with defining the correct system behavior, providing with a set of operators for working through what is going on, as well as working with data structures. There is tooling that has been developed to support TLA+ including an IDE, model checker, and proof system. It is all still substantially over my head, but I get what is going on enough to warrant moving forward, and hopefully absorbing more on the subject. As with most languages and specifications I come across it will just take some playing with, and absorbing the concepts at play, before things will come into focus.</p>

<p>I’m going to pick up some of my previous work around <a href="http://apievangelist.com/2016/10/06/adding-behaviordriven-development-assertions-to-my-api-research/">behavior driven assertions</a>, and how <a href="http://apievangelist.com/2016/04/08/the-api-assertions-we-make-believe-in-and-require-for-our-business-contracts/">assertions can be though of in terms of the business contracts APIs put forward</a>, and see where TLA+ fits in. It’s all still fuzzy, but API assertions and TLA+ feels like where I want to go with this. I’m thinking about how we can wrap algorithms in APIs, write assertions for them, and validate across the entire surface area of an algorithm, or stack of API exposed algorithms using TLA+. Maybe I’m barking up the wrong tree, but if nothing else it will get me thinking more about this side of my API research, which will push forward my thoughts on algorithmic transparency, and audit-able observability.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/03/temporal-logic-of-actions-for-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/03/database-to-database-then-api-instead-of-directly-to-api/">Database To Database Then API, Instead Of Directly To API</a></h3>
        <span class="post-date">03 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/amazon/database-to-database-api.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I am working with a team to expose a database as an API. With projects like this there can be a lot of anxiety in exposing a database directly as an API. Security is the first one, but in my experience, most of the time security is just cover for anxiety about a messy backend. The group I’m working with has been managing the same database for over a decade, adding on clients, and making the magic happen via a whole bunch of databases and table kung fu. Keeping this monster up and running has been priority number one, and evolving, decentralizing, or decoupling has never quite been a priority.</p>

<p>The database team has learned the hard way, and they have the resources to keep things up and running, but never seem to have them when it comes to refactoring it and thinking differently, let alone tackling the delivery of a web API on top of things. There will need to be a significant amount of education and training around REST, and doing APIs properly before we can move forward, something there really isn’t a lot of time or interest in doing. To help bridge the gap I am suggesting that we do an entirely new API, with it’s own database, and we focus on database to database communication, since that is what the team knows. We can launch an Amazon RDS instance, with an EC2 instance running the API, and the database team can work directly with RDS (MySQL) which they are already familiar with.</p>

<p>We can have a dedicated API team handle the new API and database, and the existing team can handle the syncing from database to database. This also keeps the messy, aggregate, overworked database out of reach of the new API. We get an API. The database team anxiety levels are lowered. It balances things out a little. Sure there will still be some work between databases, but the API can be a fresh start, and it won’t be burdened by the legacy. The database to database connection can carry this load. Maybe once this pilot is done, the database team will feel a little better about doing APIs, and be a little more involved with the next one.</p>

<p>I am going to pitch this approach in coming weeks. I’m not sure if it will be well received, but I’m hoping it will help bridge the new to the old a little bit. I know the database team likes to keep things centralized, which is one reason they have this legacy beast, so there might be some more selling to occur on that front. Doing APIs isn’t always about the technical. It is often about the politics of how things get done on the ground. Many organizations have messy databases, which they worry will make them look bad when any of it is exposed as an API. I get it, we are all self-conscious about the way our backends look. However, sometimes we still need to find ways to move things forward, and find compromise. I hope this database to database, then to API does the trick.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/03/database-to-database-then-api-instead-of-directly-to-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/02/the-waves-of-api-driven-bots-invading-our-shores/">The Waves Of API Driven Bots Invading Our Shores</a></h3>
        <span class="post-date">02 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/beach-rocks-currents_blue_circuit_5.jpg" align="right" width="40%" style="padding: 15p;" /></p>
<p>As each wave of technology comes crashing on the shores of the API space you’ll mostly find me silent, listening and watching what is happening. Occasionally you’ll hear me grumble about the aggressiveness of a single wave, or how unaware each wave is of the rest of the beach, or of the waves that came before them. Mostly I am just yelling back to the waves that claim, “we are going to change the beach forever”, and “we are the wave that matters, better than all the waves that came before us”. Mostly, it is the hype, and the unrealistic claims being made by each wave that bothers me, not the waves themselves.</p>

<p>I do not think that technology won’t have an impact on the beach. I just think that us technologists tend to over-hype, and over-believe in the power each wave of technology, and that we do not consider the impact on the wider beach, and the amount of sand that ends up in everything. I don’t doubt that there will be some gems found in the sand, and that geologically speaking that the ocean plays a significant role in how the coastline is shaped. I’m just choosing to sit back on the bluff and enjoy my time on the beach, and not choosing to be a three year old playing in each of the waves, super excited by the sound each crash makes on the beach. I’m not saying that playing in the waves is wrong, I’m just choosing to look at the bigger picture from up here on the bluff.</p>

<p>You can see one such canvas being painted over the last couple of years with what has become to be known as “bots”. Little automated nuggets of tech goodness, or evil, depending on your location on the beach. People love saying that bots will change everything. They’ll be your assistant. They’ll do everything for you. They’ll automate your life. Take care of your parking tickets. Buy your groceries. Raise your children. Feed hungry people in Africa. When in reality, they tend to be annoying, harassing, and can be mess up an entire election kind of bad. They can DDoS. They can threaten to kill and rape you. But, hey, let’s keep investing in them, and building platforms that support them, without ever acknowledging the negative consequences they have on our beautiful beach.</p>

<p>Some days when I’m swimming in the bot waves I’ll be completely consumed. The undertow grabs me, spins me around, and I don’t know which way is up, and I end up with a mouth and ass-crack full of
sand before I can make my way to the beach. I felt this way over last Christmas as I tried to make sense of the fake news engine, and what was coming out of Russia. Other days I feel like I’m walking on the beach collecting agates, finding some polished glass, but occasionally also finding some really beautiful agates. Today is one of those days, and I seem to be finding more bots that are actually useful, and do one thing well, without all the bullshit, and hype. Showing me the potential of this technology, and the specs of usefulness it can bring to our silicon beach.</p>

<p>I’m finding useful bots that will convert a file for me. Transcribe a audio file. Notify me of a changes within my domain(s), which happens to be beach front real estate. Amidst all the sand and gravel I am seeing meaningful bot implementations. They aren’t anywhere near the automation we have been promised, but they are providing some value. Today I have a handful of these agates in the palm of my hand. We’ll see if I’m actually able to use these in my day to day world. Maybe hang one from the window by a string. Mount one in a piece of jewelry that will be worn infrequently. Who knows, maybe one will become something I hold in my regularly, rubbing, soothing me as I do what I do as the API Evangelist each day.</p>

<p>Even with all of this (potential) usefulness I am finding in today’s bot waves, I’m still reminded of the power of the ocean. The dangers of sneaker waves while I’m heads down looking for agates. The power of the undertow while swimming on a sunny day. The ability for thousands of waves to come in and take away the beach, the bluff, and destroy the house I’ve built, as well as my neighbors. I’m reminded that no matter how shiny each gem is that I find in the waves, or how much I love the sound each wave crashing on the beach, but I can never stop thinking about the power of the ocean at scale. I mean, as our president recently point out, we are all “Surrounded by water. Big water. Ocean water.” Let’s not be distracted by each wave, and make sure we are always paying attention to the bigger picture. I feel like we have to do this for the “kids” as well, so the waves don’t sneak up on them.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/02/the-waves-of-api-driven-bots-invading-our-shores/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/02/the-ca-acquisition-of-runscope/">The CA Acquisition Of Runscope</a></h3>
        <span class="post-date">02 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/runscope/ca-runscope-acquisitions.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>You won’t find me talking about the acquisition of API startups very often. I’m just not a fan of the game. I am not anti-venture capital, but I find the majority of investment in the API startup ecosystem works against everything we are trying to do with APIs. In my opinion, VC investment shouldn’t be the default, it should be an exception. There are other ways to build a business, and I see too many useful API tools get ruined while playing this game. With that said, I tend to not cover the topic, unless I get really pissed off, or the occasional investment or acquisition that I feel will result in a positive result.</p>

<p><a href="https://blog.runscope.com/posts/301">Last week we saw the Runscope acquisition by CA</a>. This is an acquisition that doesn’t leave me concerned. Runscope is a partner of mine, run by people I know and care about, and they offer a tool that is useful in the API sector. If they’d had been acquired by many other bigcos I would have been more concerned, or even upset (if it had been certain ones). However, I have experience with CA, and while they are an enterprise beast, I’ve seen them make acquisitions before that weren’t damaging to the services and tooling they acquired. I trust that CA isn’t acquiring Runscope to just eliminate a strong player from the sector, and that they are actually interested in what Runscope does.</p>

<p>I have seen CA’s role in the API space through <a href="http://www.apiacademy.co/">the lens of the API Academy team,</a> as well as through public and private conversations with other CA employees, on a variety of other teams. I’ve gone on-site and participated in API training session, and I have seen evidence that CA is invested in helping evolve their enterprise to be an API aware organization. Something that you can see reflected in how they approach doing business with their customers. I’m currently working to help move forward some API curriculum with the API academy team, which wouldn’t be happening if I didn’t feel they were committed to helping invest in API literacy across the API space.</p>

<p>The CA acquisition of Runscope doesn’t leave me nervous. I feel like it is a good match. Also, despite the CEO of Runscope, John Sheehan and I often butting heads about startup and VC culture, I feel like he has played the game in an honest and respectful way. He’s made the best choices he could have as a CEO in this game. He cares about making a high quality, useful API product. He genuinely cares about the API space. Even though I think he loves the startup and investment game a little more than he should. All of this leaves me without the indigestion that API startup investment and acquisitions usually leaves in my stomach. I don’t feel like we are losing yet another valuable tool. I feel like CA will be a good steward of Runscope, and the team will actually get the opportunity to evolve, grow, and do better things.</p>

<p>Nice work y’all! Here is to everything being 200 OK!</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/02/the-ca-acquisition-of-runscope/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/02/learning-about-api-design-with-resources-that-matter-to-you/">Learning About API Design With Resources That Matter To You</a></h3>
        <span class="post-date">02 Oct 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/hack-education/hack-education-data.png" align="right" width="35%" style="padding: 15px;" /></p>
<p>I have been helping my partner in crime Audrey Watters (<a href="https://twitter.com/audreywatters">@audreywatters</a>) evolve her data work as part of her Columbia Spencer Education Journalism Fellowship, where she is publishing <a href="https://github.com/hack-education-data/">a wealth of ed-tech funding data to Github</a>. I worked with her to evolve the schema she is using across the Google Sheet, and YAML data stores she is using. Something that will autogenerate APIs (well dynamic JSON) based upon the filename, and the fields she chooses as part of her data stores. I just planted the seeds, and she has been cranking away creating repos, and building data stores since this last summer.</p>

<p>She mentioned to me recently that she thought she had been being consistent in her naming conventions across her work, but had recently noticed some inconsistencies–realizing the importance of a consistent design and schema across the projects, something that really could become problematic at scale if she hadn’t caught. Luckily she was able to fix with some work, and was back on track. She isn’t as automated in the replication of data across her projects, but that is a good thing. It is forcing her to think more deeply about the naming and overall design of her static data APIs, which she uses across many repos, and displayed in a variety of lists, outlines, and stories she is telling around her work.</p>

<p>Audrey has spent seven years listening to me talk about API design blah blah blah, but until she was working with her own data, that she cared about, she didn’t fully grasp some of the API design and implications of working with the access, reusability, and maintenance of data at scale. I’ve offered to automate more of the maintenance, replication, and standardization of data across her repos, but she’s declined. She said she finds it valuable to work with the design, and naming of her data stores, for us in different projects. She likes keeping here YAML data stores in separate repos, and then working with them individually in specific uses cases. As part of her work, she has a master ed-tech investor data store, and API of investors behind each ed-tech company, but then when she aggregates for her wider ed-tech funding, she replicates and names (or renames) it to fit that project.</p>

<p>The work that she is doing is what I consider static API design, where the data is YAML or JSON on Github, but then each project dynamically generates JSON, XML, CSV, or RSS using Liquid, and then also generates HTML UI elements using Liquid as well. It’s not full blown API design, and deployment, but the same API definition, schema, and design concerns come into plays, because if she isn’t thoughtful, and consistent, she will feel the pain at some point at the client level (Liquid/Jekyll). Also, since she is doing this across so many repos, at some point she will begin feeling the pain at a pretty significant scale. However, since she actually cares about the data she is managing, it is important for her to take the time to do it right, and not always opt for automation, so that she can make sure she gets the design and schema details right. Something I wish more API data stewards would realize.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/02/learning-about-api-design-with-resources-that-matter-to-you/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/10/02/open-api-definitions-for-entire-schema-org-vocabulary-dont-reinvent-wheel/">OpenAPI Definitions For Entire Schema.org Vocabulary (Do Not Reinvent Wheel)</a></h3>
        <span class="post-date">02 Oct 2017</span>
        <p><a href="http://schema.org/"><img src="https://s3.amazonaws.com/kinlane-productions/schema-org/schema-org.png" align="right" width="30%" style="padding: 15px;" /></a></p>
<p>I am preparing my Schema.org Github repo with a variety of data sources for use across my API tooling and other projects. I’m trying to get better at using a common vocabulary, and not reinventing the wheel each time I start a new project. Schema.org has the most robust vocabulary of shared schema available today–so I am using this existing work as the core of mine.</p>

<p>I am slicing and dicing the schema.org vocabulary into several formats that I can use in my OpenAPI-driven editors, and other tooling. <a href="http://schema.org/docs/developers.html">I took the JSON-LD representation for Schema.org</a>, and published it as a simpler JSON schema definition format that can be applied quickly to an OpenAPI. It isn’t perfect, and you lose a lot of the semantics in the process, but I think it still provides an important base for API designers, architects, and developers to use across their OpenAPI.</p>

<p>It is pretty verbose, with over 150K lines, but it provides a fairly consolidated view of Schema.org classes, in a single set of definitions:</p>

<script src="https://gist.github.com/kinlane/a71967811f8ab47c15919d75107c5843.js"></script>

<p>You can download a copy via the Gist, or you can find as <a href="https://github.com/api-evangelist-tools/schema-org/blob/master/_data/definitions/index.json">JSON</a> and <a href="https://github.com/api-evangelist-tools/schema-org/blob/master/_data/definitions/index.yaml">YAML</a> in my Github repository for this work. I’m going to be creating complete OpenAPI for each Schema.org class, as well as individual JSON schema files for each class. I just haven’t to figure out how to decouple them into individual files, yet containing all the relevant schema. I have the code, I just need to dial it in, when I have more time.</p>

<p>I am going to use this Schema.org JSON schema as an autocomplete in my API design tooling, and using the OpenAPI as the source definition for my API deployment and testing tooling. I’ve been evolving <a href="http://org.open.referral.adopta.agency/">my Human Services Data API</a> work to easily generate server side code using OpenAPI, and I’m going to use the same code base to generate any Schema.org API, and deploy as AWS EC2 instance. I’m not looking to develop a SaaS solution, but a quick deploy solution for my own work, and projects I work on with my clients. As I work with more, I will validate that each of these definitions are 100% correct, and properly represent the Schema.org vocabulary.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/10/02/open-api-definitions-for-entire-schema-org-vocabulary-dont-reinvent-wheel/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/29/2017-the-api-stack-for-disrupting-the-world/">The API Stack For Disrupting The World</a></h3>
        <span class="post-date">29 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/fredericksburg-downtown-flag.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I know people don’t understand why I’m so obsessed with APIs. Sometimes I ask the same question. When I began in 2010, it was 75% about my belief in the good that APIs can do, and 25% about pushing back on the bad things being done with APIs. In 2017, it is 15% about the good, and 85% about pushing back on the bad things that APIs can do. API driven platforms are being used for some pretty shady things these days, and increasingly they are a force for disruption, and not about making the world a better place.</p>

<p>With this in mind, I wanted to take a moment to highlight the API stack right now that is being used to disrupt the world around us. These are the APIs that have shifted the political landscape in the U.S., and are being used to replicate, automate, and scale this disruption around the world.</p>

<ul>
  <li><strong>Facebook</strong> - The network effect is what brings the troublemakers to Facebook.They are on pace to have 2 billion active users. Something that has the potential to create quite a network effect when sharing stories and links, and when you seed that, target it, and grow it using the Facebook advertising engine–it makes for an excellent engine for disruption.</li>
  <li><strong>Twitter</strong> - Twitter is a different beast. Less of the mainstream population than Facebook enjoys, but still a sizable, and very public audience. You can use the Twitter engine to spin things up, get people sharing, do some of the same sharing of stories and links, seeding, targeting, and growing with advertising. Often times the viral nature will spread to Facebook on take on a life of its own.</li>
  <li><strong>Reddit</strong> - Now Reddit is entirely just an organic engine for disseminating information, which makes it great for propaganda, everything fake, and stoking the haters. The network effect that is Reddit, works very, very well will Twitter and Facebook, making for a perfect storm of virality that can spread like wildfire.</li>
  <li><strong>WordPress</strong> - WordPress is where the news, and other websites get published. Because WordPress is an open source solution, it can be installed anywhere. It can be installed as many times as you want, with no costs beyond your hosting. Each of those installations have an API, which allow you to easily publish across hundreds or thousands of installations. When you slap advertising on these beasts, and plant your seeds across Facebook, Twitter, and Reddit, you make for a pretty efficient propaganda machine.</li>
  <li><strong>Google</strong> - Google is the advertising engine for use on the WordPress sites. Google Adwords and Adsense are where disruptors buy the ads they need to plant seeds, but more importantly, it is how they generate revenue from the click throughs, and page views generated from the Facebook, Twitter, and Reddit network effects. Beyond advertising, the Google index provides another great way for the message to spread. All you have to do is play the SEO game, which is something that is greatly aided, and gamed, by the inbound traffic received from Facebook, Twitter, and Reddit.</li>
</ul>

<p>This is a pretty simplistic snapshot of the API surface area that is being used to mess with our realities right now, but if you break down the number API resources available for each of these platforms, you begin to see all the knobs and dials that can be turned at scale, to disrupt the world. I’ll write another post that walks through all the paths and endpoints, but I want to look at the problem from the 100K view, and point the finger at APIs. Without them, such a small group of people wouldn’t be able to do such a huge amount of damage.</p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/ellis-island-nazi-poster.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>The key driver of disruption here is the advertising engine brought to the table by Google, Facebook, and Twitter, which allows troublemakers to sew the seeds of destruction, as well as target, and grow revenue that supports their efforts. Secondarily, the network effect of Facebook, Twitter, and Reddit allow for the larger demographics to be stirred up, and mobilized, and potentially completing the loop for very disruptive memes. When advertising alone can’t drum up the attention needed, the API driven bot armies on Twitter, Facebook, and Reddit pick up the slack, get to work voting up, liking, sharing, until each platforms algorithms get triggered, and do the rest of the work. Not every message will enjoy the virality necessary to do damage, but every once in awhile the disruptors will hit the big time, and there message will take on a life of its own, with each platform doing the rest of the work for them.</p>

<p>All of this would be possible without APIs. However, it would take armies of people to do. APIs are the bullhorn, the amplification, the automation needed to scale this type of disruption. In my next post I will publish a complete list of the knobs and dials that can be turned by the disruptors, to crank up the volume on their campaigns, and to direct their bot armies in support of a specific message, or to attack a target. All of the advertising and targeting engines for the platforms above have APIs, allow campaigns to be automated, and scaled for both spending, and generating revenue. WordPress APIs make the open source platform into a kind of printing press for the propaganda machine, with Facebook, Twitter, Google, and Reddit APIs acting as the messenger boys. APIs are the key element, that makes all of this seem so deafening at the moment.</p>

<p>APIs are not evil, nor are they good, or even neutral. They are just a tool. They only do what the platform operators design them to do, and what the API consumer decides to put it to work doing. This is what has made APIs so interesting to me. When you set this stage, the possibilities for innovation have been great. However, recent history has shown, when advertising is the main revenue engine for both the platforms and consumers, the API providers are more than willing to ignore and look the other way at what is going on. This has made for a perfect storm of disruption at a scale we’ve never seen before, and because of the technical complexity, it is something that most people don’t even see happening. They can’t see the strings. They can’t see how their world is being disrupted. They don’t see their role in it. They don’t understand that things are being amplified and algorithmically distorted–they just think that is the way the world is.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/29/2017-the-api-stack-for-disrupting-the-world/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/29/developing-an-ability-to-repeat-the-same-api-stories-over-and-over-you-are-going-to-need-it/">Developing The Ability To Repeat The Same API Stories Over And Over</a></h3>
        <span class="post-date">29 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/kinlane-whiteboard-api-bw-artsy.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>After seven years of telling stories on API Evangelist I’ve had to repeat myself from time to time. Honestly, I repeat myself A LOT. Hopefully I do it in a way that some of you don’t notice, or at least you are good at filtering the stories you’ve already heard from your feed timeline. My primary target audience is the waves of new folks to the world of APIs I catch with the SEO net I’m casting and working on a daily basis. Secondarily, it is the API echo chamber, and folks who have been following me for a while. I try to write stories across the spectrum, speaking to the leading edge API conversations, as well as the 101 level, and everything in between.</p>

<p>Ask anyone doing API evangelism, advocacy, training, outreach, and leadership–and they’ll that you have to repeat yourself a lot. It is something you get pretty sick of, and if you don’t find ways to make things interesting, and change things up, you will burn out. To help tell the same story over and over I’m always looking for a slightly different angle. Let’s take API Meetups as an example. Writing a story about conducting an API Meetup has been done. Overdone. To write a new story about it I’ll evaluate what is happening at the Meetup that is different, or maybe the company, or the speaker. Diving into the background of what they are doing looking for interesting things they’ve done. You have to find an angle to wrap the boring in something of value.</p>

<p>API documentation is another topic I cover over, and over, and over. You can only talk about static or interactive API documentation so much. Then you move into the process behind. Maybe a list of other supporting elements like code samples, visualizations, or authentication. How was the onboarding process improved? How the open source solution behind it simplifies the process. You really have to work at this stuff. You have to explore, scratch, dig through your intended topic until you find an angle that you truly care about. Sure, it has to matter to your readers, but if you don’t care about it, the chances of writing an interesting story diminishes.</p>

<p>This process requires you to get to know a topic. Read other people’s writing on the topic. Study it. Spin it around. Dive into other angles like the company or people behind. Spend time learning the history of how we got here with the topic. If you do all this work, there is a greater chance you will be able to find some new angle that will be interesting. Also, when something new happens in any topical area, you have this wealth of knowledge about it, and you might find a new spark here as well. Even after all that, you still might not find what you are looking for. You still end up with many half finished stories in your notebook. It is just the way things go. It’s ok. Not everything you write has to see the light of day. Sometimes it will just be exercise for the next round of inspiration. That hard work you are experiencing to find a good story is what it takes to reach the point where you are able to discover the gems, those stories that people read, retweet, and talk about.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/29/developing-an-ability-to-repeat-the-same-api-stories-over-and-over-you-are-going-to-need-it/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/29/tyk-is-conducting-api-surgery-meetups/">Tyk Is Conducting API Surgery Meetups</a></h3>
        <span class="post-date">29 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/tyk/tyk-api-surgery-singapore.jpeg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I was having one of my regular calls with <a href="https://tyk.io/">the Tyk team</a> as part of our partnership, discussing what they are up to these days. I’m always looking to understand their road map, and see where I can discover any stories to tell about what they are up to. A part of their strategy to build awarness around their API management solution that I found was interesting, was <a href="https://www.meetup.com/Tyk-Users-Singapore/photos/28111064/463973670/">the API Surgery event they held in Singapore last month</a>, where they brought together API providers, developers, and architects to learn more about how Tyk can help them out in their operations.</p>

<p>API surgery seems like an interesting evolution in the Meetup formula. They have a lot of the same elements as a regular Meetup like making sure there was pizza and drinks, but instead of presentations, they ask folks to bring their APIs along, and they  walk them through setting up Tyk, and deliver an API management layer for their API operations. If they don’t have their own API, no problem. Tyk makes sure there are test APIs for them to use while learning about how things work. Helping them understand how to deliver API developer onboarding, documentation, authentication, rate limiting, monitoring, analytics, and the other features that Tyk delivers.</p>

<p>They had about 12 people show up to the event, with a handful of business users, as well as some student developers. They even got a couple of new clients from the event. It seems like a good way to not beat around the bush about what an API service provider is wanting from event attendees, and getting down to the business at hand, learning how to secure and manage your API. I think the Meetup format still works for API providers, and service providers looking to reach an audience, but I like hearing about evolutions in the concept, and doing things that might bring out a different type of audience, and cut out some of the same tactics we’ve seen play out over the last decade.</p>

<p>I could see Meetups like this working well at this scale. You don’t need to attract large audiences with this approach. You just need a handful of interested people, looking to learn about your solution, and understand how it solves a problem they have. Tyk doesn’t have to play games about why they are putting on the event, and people get the focus time with a single API service provider. Programming language meetups still make sense, but I think as the API sector continues to expand that API service provider, or even API provider focused gatherings can also make sense. I’m going to keep an eye on what Tyk is doing, and look for other examples of Meetups like this. It might reflect some positive changes out there on the landscape.</p>

<p><em><strong>Disclosure:</strong> Tyk is an API Evangelist partner.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/29/tyk-is-conducting-api-surgery-meetups/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/29/api-design-industry-guide-graphql-a-query-language-for-apis/">API Design Industry Guide: GraphQL, A Query Language For APIs</a></h3>
        <span class="post-date">29 Sep 2017</span>
        <p><a href="http://design.apievangelist.com/#Guide"><img src="https://s3.amazonaws.com/kinlane-productions/guides/definition/design/api-design-industry-guide-graphql.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p><em>This post is from the latest copy of my API Evangelist API Design Industry Guide, which provides a high level look at the API design layer of the industry. Providing a quick look at the services, tools, and some of the common building blocks of API design. The guide is heavily rooted in REST and hypermedia, but is working to track on the expansion of the space beyond just these formats. My industry guides change regularly, and I try to publish the articles from them here on the blog to increase their reach and exposure.</em></p>

<p>GraphQL is a query language designed by Facebook to build client applications using a flexible syntax and provide a system for describing the data requirements and interactions required by each application. GraphQL began as a Facebook project that soon began powering all their mobile applications. By 2015, became a formal specification. GraphQL provides a query language for your APIs that allows users to describe how they would like their API requests be fulfilled. The approach shifts the API design process to be more about request flexibility requiring API providers to design all API paths ahead of time. It opts for an augmented query language over investing in static schema that requires specific API paths.</p>

<p>REST APIs focus on paths to your resources, but GraphQL is all about fields and data types, with everything accessed through a single API path. GraphQL does a better job of providing a more comprehensive approach access to data stored in a database by offloading design to the query layer for interpretation at query render time. The ability to define what data is returned opens up some interesting approaches to delivering resources, especially when it comes to potentially constrained network environments.</p>

<p>When it comes to providing access to data used in responsive web and mobile applications, GraphQL can be successful in allowing application developers to get exactly what they need for an interface and nothing more. This can increase performance and give UI / UX designers more of a voice in what an API does. GraphQL has played a significant role in the evolution of React, Facebook’s open source framework for deploying user interfaces. React is well-known has achieved some significant traction in application development circles. This design approach to delivering data using APIs a natural fit for rapidly delivering web and mobile apps.</p>

<p>GraphQL has seen some significant adoption beyond Facebook, notably at Github and Pinterest. GraphQL strengths become clear when it is used to deliver complex data stores quickly and efficiently by developers that require a greater level of control what data they need. While GraphQL is not traditional API design, it is an important design constraint to consider when planning the future of your API design practices and toolbox.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/29/api-design-industry-guide-graphql-a-query-language-for-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/28/api-design-industry-guide-grpc-open-source-rpc-framework/">API Design Industry Guide: gRPC, Open Source RPC Framework</a></h3>
        <span class="post-date">28 Sep 2017</span>
        <p><a href="http://design.apievangelist.com/#Guide"><img src="https://s3.amazonaws.com/kinlane-productions/guides/definition/design/api-design-industry-guide-grpc-open-source-rpc-framework.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p><em>This post is from the latest copy of my API Evangelist API Design Industry Guide, which provides a high level look at the API design layer of the industry. Providing a quick look at the services, tools, and some of the common building blocks of API design. The guide is heavily rooted in REST and hypermedia, but is working to track on the expansion of the space beyond just these formats. My industry guides change regularly, and I try to publish the articles from them here on the blog to increase their reach and exposure.</em></p>

<p>gRPC is a high-performance open source remote procedure call (RPC) framework that is often used to deploy APIs across data centers that also supporting load balancing, tracing, health checks and authentication. While gRPC excels in more controlled, tightly coupled environments, it is also applicable for delivering resources to web, mobile, and other Internet connected devices.</p>

<p>When crafting gRPC APIs, you begin by defining the service using Protocol Buffers, a language and toolset for binary serialization that has support across 10 leading programming languages. Protocol Buffers can be used to generate client and server stubs in these programming languages with tight API/client  coupling  — delivering a higher level of performance than your average REST API and SDK can.</p>

<p>gRPC API design patterns takes advantage of HTTP/2 advances and uses authenticated bi-directional streaming to deliver APIs that can be scaled to millions of RPC calls per second. Its an effective  approach for larger, more demanding API platforms that have begun to see the performance limitations of a more RESTful API design approach. gRPC is not ideal for every API implementation, but is definitely an approach providers should consider when high volumes anticipated, especially within the data center or other tightly controlled environment.</p>

<p>Google has been using gRPC internally for over a decade now, but has recently committed to delivering all their public APIs using gRPC in addition to RESTful APIs, demonstrating that the API design patterns can coexist. This approach makes it a welcome addition to any microservice style architecture. It has  the added benefit of API management features like authentication, tracing, load balancing, and health checking that are required to deliver high  performance.</p>

<p>gRPC is definitely more of an industrial grade API design pattern, shifting APIs into the next gear when it comes to performance. It also leverages the next generation of the HTTP protocol, HTTP/2. While not an API design pattern that every API provider will be working with, they should be aware it exists so that they understand what it is and the role it plays in the space.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/28/api-design-industry-guide-grpc-open-source-rpc-framework/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/28/how-do-we-help-folks-understand-that-apis-are-a-journey/">How Do We Help Folks Understand That APIs Are A Journey?</a></h3>
        <span class="post-date">28 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/path-in-the-woods-black-white.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I was hanging out with my friend Mike Amundsen (@mamund) in Colorado last month and we ended up discussing folks uncertainty with APIs. You see, many folks that he has been talking to were extremely nervous about all the unknowns in the world of APIs, and were looking for more direction regarding what they should be doing (or not doing). Not all people thrive in a world of unknown unknowns, and not even in a world of known unknowns. Many just want a world of known knowns. This is something that makes the API landscape a very scary thing to some folk, and world where they will not thrive and be successful unless we can all begin to find a way to help them understand that this is all a journey.</p>

<p>I love figuring all of this API stuff out, and I know Mike does too. We like thinking about the lofty concepts, as well as figuring out how to piece all the technical elements together in ways that work in a variety of business sectors. Many folks we are pushing APIs on aren’t like us, and just want to be told what to do. They just want the technology solution to their problem. A template. A working blueprint. It freaks them out to have so many options, possibilities, patterns, and directions they take things. I feel like we are setting folks up for failure when we talk them into embarking on an API journey without the proper training, equipment, support, and guidance.</p>

<p>I think about the last seven years doing this, and how much I’ve learned. Realizing this makes me want to keep doing APIs, just so I can keep learning new things. I thought I understood REST when I started. I didn’t. I thought I understand the web when I started, I didn’t (still don’t). I was missing a lot of the basics, and no matter what folks told me, or how precise their language was, I still needed to bang my head on something over and over before I got it. I was missing a significant amount of why hypermedia can be a good approach without truly understanding content negotiation, and link relations. Realizing how much I still need to explore and learn has only emboldened me on my journey, but I’m not convinced this will be the case with everyone. We are wrong to assume everyone is like us.</p>

<p>As technologists and autodidacts we often overestimate our own ability, as well as what others are capable of. We realize APIs are not a destination, but a journey. However, we suck at explaining this to others. We are horrible at understanding all of the stepping stones that got us here, and recreating them for others. I put myself into this group. I think about this stuff full time, and I still regularly stumble when it comes to on-boarding folks with what API are, and properly helping them in their journey. I still do not have a proper set of on-boarding lessons for folks, <a href="http://101.apievangelist.com">beyond my API 101 stuff</a>. I talk a lot of talk about the API life cycle, the API economy, and all the business and politics of APIs, but I still can’t point folks to where the yellow brick road is. We have to get better at this if we expect folks to ever catch up.</p>

<p>This is one reason I feel <a href="http://zapier.com">Zapier</a>, and other <a href="http://ipaas.apievangelist.com">iPaaS</a> providers are so important. We should be helping people understand APIs and integration in context of the problems they are trying to solve, not in terms of REST, SDKs, or any of the other technical jargon we spew. With Zapier, folks can play with Zaps (recipes) that deliver meaningful API integration that actually solve a problem in their world. They can play with what is possible, without learning all the technical pieces first. They can evolve in their business world, while also progressing on their API journey. IDK. I’m just trying to find ways to help folks better understand what APIs are. I’ll never make everything known to them, but I’m hoping that I can help make folks a little less nervous about the known unknowns, and who knows maybe some day they’ll feel brave enough, and confident in their API awareness that they’ll be able to operate in a world of the unknown unknowns, and settle in on the perpetual journey that are APIs.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/28/how-do-we-help-folks-understand-that-apis-are-a-journey/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/28/api-design-industry-guide-the-restlet-platform-story/">API Design Industry Guide: The Restlet Platform Story</a></h3>
        <span class="post-date">28 Sep 2017</span>
        <p><a href="http://design.apievangelist.com/#Guide"><img src="https://s3.amazonaws.com/kinlane-productions/guides/definition/design/api-design-industry-guide-the-restlet-platform-story.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p><em>This post is from the latest copy of my API Evangelist API Design Industry Guide, which provides a high level look at the API design layer of the industry. Providing a quick look at the services, tools, and some of the common building blocks of API design. The guide is heavily rooted in REST and hypermedia, but is working to track on the expansion of the space beyond just these formats. My industry guides change regularly, and I try to publish the articles from them here on the blog to increase their reach and exposure.</em></p>

<p>Restlet began as an open source Java API framework over a decade ago and has evolved into an API studio, client, and cloud platform with an API design core. At the center of the API lifecycle management platform is its API designer which gives you a visual view of an API and an OpenAPI or RAML view, providing a machine readable accounting of each API’s contract.</p>

<p>The Restlet Studio allows you to design and document your APIs, starting from scratch, or import existing API design patterns using OpenAPI for RAML. Using the Restlet design UI you can shape the paths, parameters, headers and complete requests and responses for any API. Then, take the definition and actually put it to work in development, staging, or production environments.</p>

<p>Restlet demonstrates how API design is more than just a momentary phase where you are developing APIs and is actively defining every stop along the API lifecycle from design to deprecation. While designing an API in the Restlet API Studio, you can also work to test and automate using the client, helping ensure a usable and complete API is designed. The Restlet Client provides a dashboard to verify the desired API contract in a way that can be shared across teams, with clients, and across stakeholders.</p>

<p>Once the API design process has matured and evolved and is ready for deployment, Restlet empowers production deployment by, generating server and client side code with documentation and a landing page for consumers to access and put an API to work. The Restlet Cloud provides all the components you need to quickly deploy, manage, and scale an API, while using the API design studio as the central place where truth around the API is defined — touching every other aspect of API operations.</p>

<p>Less of a plug for Restlet, I am hoping it is a demonstration of how API design is central to every aspect of API operations and can be central to API service providers. API design isn’t just about the technical design of the surface area of API requests and responses. It is about designing and defining all aspects of doing business using APIs.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/28/api-design-industry-guide-the-restlet-platform-story/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/28/apistrat-and-the-openapi-initiative/">APIStrat And The OpenAPI Initiative</a></h3>
        <span class="post-date">28 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/openapi/openapi-membership-september-2017.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>We are getting closer to <a href="http://events.linuxfoundation.org/events/apistrat">APIStrat in Portland, Oregon, October 31st through November 2nd</a>. So I’m going to keep crafting stories that help convince you should be there. It is the first APIStrat conference as an OpenAPI event, operated by the Linux Foundation events team. Steve and I are still playing a big part, and will be MC’ing, but like OpenAPI, APIStrat has grown to the point where we need to let it become more than just something Steve, myself, and the 3Scale team can execute by ourselves.</p>

<p>APIStrat has always been a place where we gather and talk about OpenAPI, going back to when it was affectionately known as Swagger. Tony, and the team have spoken before, and there has been many other sessions, workshops, and keynotes involving the API specification format. This APIStrat is going to be no different, but there will be an even heavier presence for the specification. Since Tony Tam is stepping away, we are giving a full hour on mainstage for him and folks involved in the evolution of OpenAPI to share their story. Darrel Miller will be holding also be holding a workshop on the first day, where several folks involved in the OAI will be sharing knowledge.</p>

<p>There will also be an OAI booth presence, and I know that Jeff ErnstFriedman will be present for OAI membership discussions. If your company is investing in OpenAPI as part of your API operations, and developing tooling around the specification, you should be considering joining the OAI. <a href="https://www.openapis.org/membership/members">Take a look at the current membership list</a>. I’m a member, and so are other heavy hitters like Adobe, Google, Microsoft, IBM, and even my partner in crime 3Scale, and Tyk are present. As a member you get in on the Slack channel conversations, participate on marketing and governance calls, and you get invited to participate on the APIStrat crew (if you want).</p>

<p>Let me know if you are interested becoming a member, I can hook you up with Jeff. He’s the man. If you’d rather, head over to <a href="http://events.linuxfoundation.org/events/apistrat/attend/registration">APIStrat registration</a> and get signed up to join in on the conversation in Portland. I can make sure you get some dedicated time with Jeff there, and he can make sure you get hooked up as a member. If you are looking to be part of the conversation that is APIStrat, and help guide the direction OpenAPI is headed, this is where you need to be. So, I’ll see you in Portland next month, and look forward to hearing what you are up to with your API strategy.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/28/apistrat-and-the-openapi-initiative/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/27/moving-beyond-just-distributed-api-scale-towards-federated-api-scale/">Thinking Beyond Just Distributed API Scale Towards Federated API Scale</a></h3>
        <span class="post-date">27 Sep 2017</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/losangelescloudy/dali_three/file-00_00_35_50.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>You hear a lot about doing APIs at scale in our space. Many folks dismiss web APIs because they feel they won’t scale, and aren’t performing at the scale they envision. The majority of these discussions focus on how do you scale large operations of Twitter, Facebook, or Google scope. A single organization operating API infrastructure at scale, distributed across many geographical regions, supporting millions of users. There are plenty of discussions going on regarding the technology, business, and politics of doing APIs at this scale. I find myself thinking in similar ways, but more federated version of this, where the latest technology might not always be the right answer.</p>

<p><a href="http://org.open.referral.adopta.agency/">My Human Services Data API (HDSA) work</a> is the best example I have of this. Where I’m having to keep the technology, and API definition bar as low as possible to onboard as many people as I possibly can, but then eventually, be able to aggregate large amounts of data across many federated instance. I have 3,144 counties, and 19,354 cities to consider. They should all be speaking a common schema when it comes to the sharing of human services data. Something that is easier said, than done. When you get on the ground you realize many of them are stuck in 1990s, or early 2000s edition of the web, and just do not have the resources needed to move things forward. They can’t afford the latest SaaS service, and they can’t drop the ball, or thousands, or millions of people will suffer–the stakes are high.</p>

<p>When I go into large companies, who have a large teams, and significant number of resources, the conversation around scale is much different. Sure, there is distributed scale. Sure, there is volume scale. However, most times the distribution and volume exists within a single company or organization. A single command and control structure. However, I’m talking about federated distribution and volume, with no single command and control structure. I’m facing not just how you deliver technology across all these nodes, but how do you train, consider extremely short budgets, and other aspects of ensuring things get done consistently, reliably, without disruption. Cities aren’t the only example of this in our world. I’m also seeing the same across state and federal agencies, as well as k-12, and higher educational institutions. Again, where the technological bar is low, but the stakes are high.</p>

<p>This real world is just a different game than tech culture likes to admit. I can’t always take what I learn in the tech sector and immediately apply on the ground in the mainstream worlds. Some of it applies, but honestly I’ve had to throw a lot of seemingly sensible decision out lately, opting for much simpler, web based solutions, that I’m confident staff can be trained on, and can realistically be implemented in existing IT environments. If we keep moving fast and breaking things, and some point we are going to have to stop, pick up some of the pieces, and help take care of the folks we’ve left behind.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/27/moving-beyond-just-distributed-api-scale-towards-federated-api-scale/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/27/api-design-industry-guide-application-level-profile-semantics-alps/">API Design Industry Guide: Application-Level Profile Semantics (ALPS)</a></h3>
        <span class="post-date">27 Sep 2017</span>
        <p><a href="http://design.apievangelist.com/#Guide"><img src="https://s3.amazonaws.com/kinlane-productions/guides/definition/design/api-design-industry-guide-alps.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p><em>This post is from the latest copy of my <a href="http://design.apievangelist.com/#Guide">API Evangelist API Design Industry Guide</a>, which provides a high level look at the API design layer of the industry. Providing a quick look at the services, tools, and some of the common building blocks of API design. The guide is heavily rooted in REST and hypermedia, but is working to track on the expansion of the space beyond just these formats. My industry guides change regularly, and I try to publish the articles from them here on the blog to increase their reach and exposure.</em></p>

<p>Current API design focusses on using schema to help quantify the payload of the request and response structure of our APIs. JSON Schema, MSON, and other data specifications have emerged to help us quantify the bits we are passing back and forth with APIs. Alongside this evolution, another data format has emerged to help us define simple descriptions of our application-level semantics, similar to how we are using HTML microformats to share data on the web, Application-Level Profile Semantics (ALPS).</p>

<p>ALPS goes well beyond schema, which provides a representation of a plan or theory in the form of an outline or model. ALPS provides a way to define the meaning behind the data, content, and other resources you are making available via an API. ALPS seeks to establish a shared understanding by illuminating the meaning behind hypermedia interfaces (data and state transitions) such as HTML, Collection+JSON, HAL or Siren. It encourages reusability of common profile documents across the media types we are depending on.</p>

<p>Using ALPS you can easily define the common data elements we all use in our API like contacts, todo lists. It can even describe the structure of our APIS for verbose and more useful error responses. What really matters is that you can also define the transitions surrounding these data elements. You can get at the meaning and use behind them, like rolling dice, or playing with a deck of cards. It’s much more than just metadata describing the data elements at work.</p>

<p>If we want our APIs designs to reflect the meaning and interactions around the valuable resources we are serving up, we need to work hard to make sure we are all using common data formats and schema. Schema.org provides us with a good start, but we also need to invest in more in ALPS registries, directories, and dictionaries. These provide machine readable definitions of the common data elements exchanged between systems and applications, as well as the meaning, relationships, interactions, and transitions that make these data elements valuable in our digital worlds.</p>

<p>ALPS has been submitted as an Internet Engineering Task Force (IETF) draft, and provides one possible standard to consider when looking to define the semantics behind API operations.  Visit: http://apis.how/alps-io/ for more information.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/27/api-design-industry-guide-application-level-profile-semantics-alps/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/09/27/a-reminder-to-always-have-a-plan-b-for-our-api-related-github-infrastructure/">A Reminder To Always Have A Plan B For Our API Related Github Infrastructure</a></h3>
        <span class="post-date">27 Sep 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/github/github-api-evangelist-flagged.jpeg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I had a scare this last weekend regarding my Github infrastructure. My Github organization for API Evangelist was flagged as SPAM and taken down. The Github organization contains almost 100 repositories that I use across my platform. These repositories drive the public side of my research, but also contain YAML files that are used in automation across my entire platform, and network of websites. At about 12:00 PM on Saturday, everything came to a screeching halt, with all the data I depend on to make things go around becoming unavailable.</p>

<p>I have backups of all the data, and the website templates that produce the public side of API Evangelist. I also have a plan B in place for setting up a Jekyll instance that runs on Amazon EC2, but I hadn’t ever actually ran any drills on plan B. After submitting a ticket to Github, I got to work firing up the AWS EC2 instance, and unloading and unpacking the almost 100 website backups for my API Evangelist research. After getting things setup, and as I was preparing to switch over the DNS, I got an email from Github saying:</p>

<blockquote>
  <p>Sorry for the hassle! It appears your organization had been caught up in a spam filter and was flagged incorrectly. I’ve cleared that flag now, so your account should be back to normal. You shouldn’t see that message again, but let me know if I can help with anything else!</p>
</blockquote>

<p>Crisis averted. Luckily this was just my own company Github organization. I operate numerous other API developer portals, code repositories, documentation sites, and other API related projects and tooling that lives entirely on Github. If my personal account was frozen, or any of these organizations taken offline, I would have been in a lot more hot water, and accountable to my clients. Overall I was down for a little over six hours. It showed me the fragile nature of depending on Github, not just for my data driven project public presence, but also it being the center of so many workflows that depend on the YAML, JSON, and code that I publish there.</p>

<p>The experience has forced me to look at my backup process some more, and given me the opportunity to actually run a live drill on failing over to a secondary provider. Even with this failover I still wouldn’t have the Github API available as part of these data project workflows, an API that I depend on, which I really cannot replace. I can replace Git, and Jekyll, but not the Github API portion of the orchestration I depend on so heavily. I have to do some more meditating on this dependency in my world. Don’t get me wrong. I love me some Githubz, but this worries me. Any API dependency worries me if I can’t easily replicate and replace. But, I guess this is what the API game is often about right? Establishing dependencies that are difficult or impossible to walk away from. ;-(</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/09/27/a-reminder-to-always-have-a-plan-b-for-our-api-related-github-infrastructure/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  

	<table width="100%" border="1" style="background-color:#FFF; border: 0px #FFF;">
		<tr style="background-color:#FFF; border: 0px #FFF;">
			<td align="left">
				<a href="/blog/page9" class="button"><< Prev</a></li>
			</td>
			<td></td>
			<td align="right">
				<a href="/blog/page11" class="button">Next >></a>
			</td>
		</tr>
	</table>

  </div>
</section>

              
<footer>
  <hr>
  <div class="features">
    
    
      
      <article>
        <div class="content">
          <p align="center"><a href="https://www.getpostman.com/" target="_blank"><img src="https://apievangelist.com/images/postman-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
        </div>
      </article>
      
    
      
      <article>
        <div class="content">
          <p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://apievangelist.com/images/tyk-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
        </div>
      </article>
      
    
  </div>
  <hr>
  <p align="center">
    relevant work:
    <a href="http://apievangelist.com">apievangelist.com</a> |
    <a href="http://adopta.agency">adopta.agency</a>
  </p>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Homepage</a></li>
    <li><a href="http://101.apievangelist.com/">101</a></li>
    <li><a href="/blog/">Blog</a></li>
    <li><a href="http://history.apievangelist.com/">History of APIs</a></li>
    <li><a href="/#api-lifecycle">API Lifecycle</a></li>
    <li><a href="/search/">Search</a></li>
    <li><a href="/newsletters/">Newsletters</a></li>
    <li><a href="/images/">Images</a></li>
    <li><a href="/archive/">Archive</a></li>
  </ul>
</nav>

              <section>
  <div class="mini-posts">
    <header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
    
    
      
        <article style="display: inline;">
          <a href="https://www.getpostman.com/" class="image"><img src="https://apievangelist.com/images/postman-logo.png" alt="" width="50%" style="padding: 15px; border: 1px solid #000;" /></a>
        </article>
      
    
      
        <article style="display: inline;">
          <a href="https://tyk.io/" class="image"><img src="https://apievangelist.com/images/tyk-logo.png" alt="" width="50%" style="padding: 15px; border: 1px solid #000;" /></a>
        </article>
      
    
  </div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
