<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
  <a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
  <ul class="icons">
    <li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
    <li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
    <li><a href="https://www.linkedin.com/organization/1500316/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
    <li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
  </ul>
</header>

    	        <section>
	<div class="content">

	<h3>The API Evangelist Blog</h3>
	<p>This blog is dedicated to understanding the world of APIs, exploring a wide range of topics from design to deprecation, and spanning the technology, business, and politics of APIs. <a href="https://github.com/kinlane/api-evangelist" target="_blank">All of this runs on Github, so if you see a mistake, you can either fix by submitting a pull request, or let us know by submitting a Github issue for the repository</a>.</p>
	<center><hr style="width: 75%;" /></center>
	
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/08/01/the-trusted-automated-exchange-of-intelligence-information/">The Trusted Automated Exchange of Intelligence Information (TAXII)</a></h3>
        <span class="post-date">01 Aug 2017</span>
        <p><a href="https://oasis-open.github.io/cti-documentation/"><img src="https://s3.amazonaws.com/kinlane-productions/taxii/taxii-logo.png" align="right" width="30%" style="padding: 15px;" /></a></p>
<p>I recently wrote about <a href="http://apievangelist.com/2017/07/10/opportunity-to-develop-a-threat-intelligence-apis-json/">the opportunity around developing an aggregate threat information API</a>, and got some interest in both creating, as well as investing in some of the resulting products and services that would be derived from this security API work. As part of the feedback and interest on that post, I was pointed in the direction of <a href="https://oasis-open.github.io/cti-documentation/">the Trusted Automated Exchange of Intelligence Information (TAXII)</a>, as one possible approach to defining a common set of API definitions and tooling for the exchange of threat intelligence.</p>

<p>The description of TAXII from the project website describes it well:</p>

<blockquote>
  <p>Trusted Automated Exchange of Intelligence Information (TAXII) is an application layer protocol for the communication of cyber threat information in a simple and scalable manner. TAXII is a protocol used to exchange cyber threat intelligence (CTI) over HTTPS. TAXII enables organizations to share CTI by defining an API that aligns with common sharing models. TAXII is specifically designed to support the exchange of CTI represented in STIX.</p>
</blockquote>

<p>I breezed through <a href="https://docs.google.com/document/d/1eyhS3-fOlRkDB6N39Md6KZbvbCe3CjQlampiZPg-5u4/edit#heading=h.4do73o99e2l7">the documentation for TAXII version 2.0</a>, and it looks pretty robust, and a project that has made some significant inroads towards accomplishing what I’d like to see out there for sharing threat intelligence. I’m still understanding the overlap of TAXII, the transport mechanism for sharing cyber threat intelligence, and STIX, the structured language for cyber threat intelligence, but it looks like a robust, existing approach defining the schema and an API for sharing threat intelligence.</p>

<p>Next, I am going to gather my thoughts around both of these existing definitions, and look at establishing an OpenAPI that represents STIX and TAXII, providing a machine readable definition for sharing threat intelligence. I think having an OpenAPI will provide a blueprint that can be used to define a handful of server side implementations in a variety of programming languages. I was happy to be directed to this existing work, saving me significant time and energy when it comes to this conversation. Now I don’t have to jumpstart it, I just have to contribute to, and augment the work that is already going on.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/08/01/the-trusted-automated-exchange-of-intelligence-information/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/31/professional-api-deployment-templates/">Professional API Deployment Templates</a></h3>
        <span class="post-date">31 Jul 2017</span>
        <p><a href="http://apievangelist.com/2017/06/14/gsa-api-standards-with-working-prototype-api-and-portal/"><img src="https://s3.amazonaws.com/kinlane-productions/gsa/gsa-prototype-api-portal.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I wrote about <a href="http://apievangelist.com/2017/06/14/gsa-api-standards-with-working-prototype-api-and-portal/">the GSA API prototype the other day</a>. It is an API prototype developed by the GSA, providing an API that is designed in alignment with <a href="https://tech.gsa.gov/guides/API_standards/">GSA API design guidelines</a>, complete with an API portal for delivering documentation, and other essential resources any API deployment will need. The GSA provides us with an important approach to delivering open source blueprints that other federal agencies can fork, reverse engineer and deploy as their own custom API implementation.</p>

<p>We need more of this in the private sector. We need a whole buffet of APIs that do a variety of different things, in every language, and platform or stack that we can imagine. Need a contact directory API, or maybe a document storage API, URL shortener API–here is a forkable, downloadable, open source solution you can put to work immediately. We need the WordPress for APIs. Not the CMS interface WordPress is known for, just a simple API that is open source, and can be easily deployed by anyone in any common hosting, and serverless environments. Making the deployment of common API patterns a no-brainer, and something anyone can do, anywhere in the cloud, on-premise, or on-device.</p>

<p>Even though these little bundles of API deployment joy would be open source, there would be a significant amount of opportunity routing folks to other add-on, and premium services on top of any open source API deployment, as well as providing cloud deployment opportunities for developers–similar to the separation between WordPress.com and WordPress.org. I could see a variety of service providers emerge that could cater to the API deployment needs of various industries. Some could focus on more data specific solutions, while others could focus on content, or even more algorithmic, machine learning-focused API deployment solutions.</p>

<p>If linked data, hypermedia, gRPC, and GraphQL practitioners want to see more adoption in their chosen protocol, they should be publishing, evolving, and maintaining robust, reverse engineer-able, forkable, open source examples of the APIs they think companies, organizations, institutions, and government agencies should be deploying. People emulate what they know, and see out there. From what I can tell, many API developers are pretty lazy, and aren’t always looking to craft the perfect API, but if they had a working example, tailored for exactly their use case (or close enough), I bet they would put it to work. I track on a lot of open source API frameworks, I am going to see if I can find more examples like the GSA prototype out there, where providers aren’t just delivering an API, they are delivering a complete package including API design, deployment, management, and a functional portal to act as the front door for API operations.</p>

<p>Delivering professional API deployment templates could prove to be a pretty interesting way to develop new customers who would also be looking for other API services that target other stops along the API lifecycle like API monitoring, testing, and continuous integration and deployment. I’ll keep my eye out for open source API deployment templates that fit the professional definition I’m talking about. I will also give a little nudge to some API service providers, consulting groups, and agencies I know who are targeting the API space, and see if I can’t stimulate the creation of some professional API deployment templates for some interesting use cases.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/31/professional-api-deployment-templates/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/31/making-the-business-of-apis-more-modular-before-you-do-the-tech/">Making The Business Of APIs More Modular Before You Do The Tech</a></h3>
        <span class="post-date">31 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/containership_dark_dali.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I have been immersed in how APIs are being done in the federal government for the last week or so, looking for positive API behavior I can showcase and focus on in my storytelling. I was walking through each step of my API lifecycle, sizing up the federal government for each area I track the private sector on when it comes to APIs. I was taking a look at the areas of microservices, containerization, and serverless. You know the modularization of IT infrastructure in government? I couldn’t find much rubber meeting the road when it comes to microservices or containerization in my research, but I did see hints of modularizing the business aspects of doing APIs in the federal government.</p>

<p><a href="https://modularcontracting.18f.gov/modular-procurement/">Over at 18F you can find some interesting discussion around micro-procurement</a>, “a procurement model that breaks what would traditionally be a large, monolithic contract into several shorter-term, lower dollar amount contracts.” I feel that breaking down the business of defining, designing, deploying, managing, and even testing your APIs into small projects is an important first step for many companies, organizations, institutions, and agencies. While not all organizations will be the same, many will need to break down the business of procuring API design, deployment, and management services, before they can even getting to work breaking down the technical components of what is needing to be delivered.</p>

<p>I have been working with <a href="https://skylight.digital/">my partners at Skylight</a> on our approach to breaking down and delivering API projects, discussing the technical, business, and political aspects of doing things in as modular, bite-size chunks as we possibly can. This involves exploring the other side of the micro-procurement coin, with <a href="https://fcw.com/blogs/lectern/2017/07/kelman-microconsulting.aspx">micro-consulting</a>, providing API related services in small, modular projects. I’m exploring the delivery of <a href="http://apievangelist.com/2017/07/24/first-handful-of-lessons-using-my-google-sheet-github-approach/">API training and curriculum</a>, as well as white paper, guide, and other content-centric services using a micro-consulting approach–keeping engagements small, focused, and delivering additional services that support one or many individual API implementations.</p>

<p>Micro procurement and consulting isn’t a fit for every scenario, but I can see it working well when it comes to helping companies, organizations, institutions, and government agencies thinking about how they can break projects down into smaller chunks, which will often involve delivering services across numerous stops along the API life cycle, from design to deprecation. Designing and deploying an API under a micro project approach limits the scope of what I’m able to deliver, easily keeping each API doing one thing, and (hopefully) doing it well. Making it easier for me to deliver microservices, complete with definitions, design guides, container, deployment, management layer, documentation, testing, and other essential elements for delivering APIs.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/31/making-the-business-of-apis-more-modular-before-you-do-the-tech/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/31/you-see-duplicate-work-while-i-see-common-patterns/">You See Duplicate Work While I See Common Patterns</a></h3>
        <span class="post-date">31 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/carryload_diego_rivera1.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>Someone asked me on Twitter recently how I deal the duplicate work required to manage a large volume of OpenAPIs. All the same things you have to do when crafting the headers, parameters, responses, and schema across every OpenAPI you are crafting. My response was that I don’t see these things as repetitive or duplicate work, I see these things as common patterns across the resources I am making available. They main reason I think they seem repetitive is the tooling we are currently using needs to play catch up, and help us better apply common patterns across all our APIs–dealing with the duplicate, repetitive work for us.</p>

<p>I’m confident that open source API design tooling like <a href="http://www.apicur.io/">Apicurio</a> are going to help us better manage the common patterns we should be applying across our OpenAPIs. I’m also hopeful that <a href="https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.0.md">OpenAPI 3.0</a> contributes to reuse of parameters, schema, errors, and other common building blocks across the request and response surface of our API. I’m counting on OpenAPI + Apicurio as well as other API definition and design tooling to step up and do the hard work wen it comes to helping us manage the common patterns across our APIs, and make the reuse of common patterns across our APIs a good thing, and never a burden.</p>

<p>This reuse shouldn’t just being within any company, and we should be reusing and sharing patterns from across the space, including common web concepts and standards. The fact that you use ISO 8601 for all your dates, while employing a handful of date field names over and over across your systems isn’t repetition or duplicate work, that is consistency, and sensible reuse of common API patterns. It is the job of API design service and tooling providers to help us get over this hump, and craft notebooks, catalogs, collections, dictionaries, and stores of the common patterns we will need to be applying (over and over again) across our API definitions–OpenAPI.</p>

<p>I enjoy these regular reminders regarding how differently folks see the API space. I also enjoy the focus on the role that API specification formats and tooling will play in helping carry the load enough so folks don’t see detailed API definitions as a burden. I regularly come across OpenAPIs that you can tell are incomplete because someone was just doing the minimum amount of work they needed to deliver documentation for an API. It should be a no-brainer for API designers and developers to make sure there is always a robust, complete, and detailed OpenAPI for any existing API–while ensuring common patterns are applied at every turn.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/31/you-see-duplicate-work-while-i-see-common-patterns/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/31/balancing-domain-expertise-with-the-disruptive-power-of-upstarts-doing-apis/">Balancing Domain Expertise With The Disruptive Power Of Upstarts Who Do APIs</a></h3>
        <span class="post-date">31 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/shipping-energy-trucking.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>APIs aren’t good, or bad, nor are they neutral. APIs do the bidding of their providers, and sometimes their consumers. In my experience APIs are more often used for bad than they are ever used for good, something I try to be as vocal as I can about, while working hard to shine a light on the good that is possible. After many years of trying to help folks understand APIs, one of the biggest challenges I face involves the unrealistic rhetoric of startups. The overoptimistic vision and promises of what APIs will do, coupled with an an often limiting awareness of the challenges and complexity of industries where APIs are targeting, making for a pretty toxic, non-cooperative environment for actually getting anything done.</p>

<p>I work hard to keep APIs alive in a variety of industries that have seen multiple waves of startups trumpeting their disruption and change horns, while also often belittling and underestimating the people within the industry. <a href="http://www.dcvelocity.com/articles/20170725-capital-amnesia/">I recently came across a post recently that captures the challenge we all face when we are looking to make change within established, and often entrenched industries using APIs</a>. I feel this paragraph captures it well:</p>

<blockquote>
  <p>The new players, and the venture capital/private equity money backing them, think they are entering a world full of Luddites. Yet the brokers we’ve talked to—and we know it’s not everybody—are quite IT-oriented. In a world where visibility is paramount, they are keenly aware of technology’s role in keeping them competitive. They are investing in IT and will continue to do so as prices drop. Meanwhile, many bring vast experience in mastering the physical part of the solution that the startups can’t touch.</p>
</blockquote>

<p>It is interesting to come across this friction in the freight brokerage industry. It is something I’ve seen in industry after industry, and with each wave of startups doing APIs. In some spaces startups will find success, but in others they will find themselves stopped cold by the entrenched positions some technical groups possess. I have experience developing trucking and shipping web applications, and predict there will be endless waves of automation that impact the freight brokerage space in coming years. However, I think startups will do much better if they focus on partnering with folks already in the space, investing in the existing expertise, rather than just going with the classic disruption rhetoric we’ve seen.</p>

<p>I am fascinated by how APIs can lay the groundwork for collaboration and better access to resources, or they can also lead to the introduction of significant friction and disruption within a sector. I’m just glad there is discussion about the role funding plays when it comes to each players motivation within a particular industry, acknowledging the legacy debt owned by the entire sector, as well as the disruptive nature of upstarts when they don’t partner with, or leverage the existing domain expertise within a space. I’ll keep an eye on what is going on in the freight brokerage sector, and see if I can better understand how APIs are becoming the solution, or contributing to friction going down the road.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/31/balancing-domain-expertise-with-the-disruptive-power-of-upstarts-doing-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/27/state-of-apis-in-the-federal-government/">State of APIs In The Federal Government</a></h3>
        <span class="post-date">27 Jul 2017</span>
        <p><em>This is my talk from Washington DC with Steve Willmott of <a href="https://www.3scale.net/">3Scale</a> by <a href="https://www.redhat.com">Red Hat</a> about <a href="http://redhatapievents.com/dc">transforming enterprise IT with containers, APIs, and integration</a>, where I assess the current state of APIs in the federal government, and the opportunity in the private sector when it comes to working with government data.</em></p>

<p><strong>API Evangelist</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/kin-lane-api-evangelist-cartoon.png" width="12%" style="padding: 15px;" align="right" /></p>
<p>My name is Kin Lane. I am the API Evangelist. I have studied the technology, business, and politics of Application Programming Interfaces, or more commonly known as APIs, full time since 2010. I spend time looking through the growing number of APIs available today, as well as the evolving group of service providers selling their solutions to API providers. I take what I learn across the space and publish as over 80 independent research projects that I run on Github, covering a growing number of stops along the API lifecycle.</p>

<p>In 2011, I began studying and writing about federal government APIs. I have long had an interest in politics, and how our government works (or doesn’t work), which was in alignment with thinking about how I could take what I’ve learned about APIs and apply to the federal government. By 2013, my research and storytelling about APIs attracted the attention of folks in government, which led to an invitation to come work on open data and API projects at multiple agencies. This move took my work to new levels, opening up some interesting doors that have opened my eyes to the scope of APIs within federal government.</p>

<p><strong>Presidential Fellow</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/kin-lane-presidential-innovation-fellow.png" width="30%" style="padding: 15px;" align="right" /></p>
<p>In the summer of 2013 I was invited to be part of the round two Presidential Innovation Fellowship, and work at the Department of Veterans affairs doing web service and API inventory, as well as assist with the wider open data efforts of the Obama administration. I worked in DC until the government shutdown in October, when I decided to leave my position so that I could continue doing my work around veterans data, and continue meeting my obligations as the API Evangelist.</p>

<p>My time as a Presidential Innovation Fellow (PIF) gave me access to a number of interesting folks, doing interesting things at a variety of federal agencies, including now 18F and USDS. During my work as the API Evangelist I have spent a lot of time expanding on these relationships by helping craft API strategies, telling stories publicly on my blog, and speaking on conference calls and on-site when it makes sense. While the PIF program wasn’t a good fit for my career, the opportunity forever changed how I see government, and how I see APIs and open data making an impact at this scale.</p>

<p><strong>Adopta.Agency</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/adopta-agency-primary.png" width="15%" style="padding: 15px;" align="right" /></p>
<p>After I left DC, I began working on a project <a href="http://kinlane.com/2014/01/18/adopt-a-federal-government-dataset/">I called Adopt a Federal Agency Dataset</a>, where anyone could choose a single dataset published by a federal agency, fork and “adopt” the dataset using Github. The goal is to get civic minded folks spending time improving the quality of data, converting to another format, or even publishing a simple API using the open data. I got to work building a prototype, and talking with a variety folks about the projects viability.</p>

<p>Eventually my work paid off, and I attracted the attention of the Knight Foundation, and <a href="https://www.knightfoundation.org/grants/201551217/">received $35K USD to develop a prototype template for the project</a>. Over the course of the next year I developed a <a href="http://adopta.agency/">website for the project</a>, a <a href="http://adopta-agency.github.io/adopta-blueprint/">forkable blueprint for adopting datasets that runs 100% on Github</a>, and applied the template to a handful of open data projects including the White House budget, and Department of Education data. I have continued to support and evolve the work, regularly adopting federal agency data sets, and working to develop common schema and API definitions for use across government of all shapes and sizes.</p>

<p><strong>APIs In The Federal Government</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/fed-agencies-logos-032515.jpg" width="30%" style="padding: 15px;" align="right" /></p>
<p>Over four years of work, including my fellowship in Washington DC, I have definitely fired up my passion for understanding how government works (or doesn’t). I’ve continued to keep an eye on open data and API efforts across federal agencies, and regularly meeting and talking with folks who are doing things with APIs in government. I spent at least 2-3 hours a week, and sometimes more on playing with government APIs, and studying their approach to understand where I can help move the conversation forward.</p>

<p>I feel like I could just dedicated my career as the API Evangelist to government APIs, but I find what I learn in the private sector is extremely useful in the public sector, and vice versa. There is a symbiotic relationship between the viability of APIs in the federal government, and the viability of APIs at higher educational institutions, the enterprise, small businesses and agencies, all the way to upstarts in Silicon Valley. This relationship has kept me studying the intersection of public and private sector API operations, and tracking on, and contributing where I can to the API momentum in DC.</p>

<p><strong>96 /Developer Portals</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/gsa-prototype-city-pairs-api.png" width="30%" style="padding: 15px;" align="right" /></p>
<p>As of this week the <a href="https://raw.githubusercontent.com/18F/API-All-the-X/gh-pages/_data/developer_hubs.yml">GSA has 96 developer portals for federal agencies</a> present. A portion of these developer portals are simply data portals, a result of open data efforts in the last administration. There is another portion of them that you can tell do not have a complete strategy, and are more of a landing page for a couple of APIs, and some open data. However, there are some API developer portals present that have some high value datasets and APIs, with robust documentation, code samples, and all the other bells and whistles you’d expect of an API in the private sector.</p>

<p>Even with the inconsistencies in federal agency developer portals, all the top agencies now have an API portal presence, with a growing number of lesser agencies following their lead. This is significant. This goes beyond just these federal agencies being more transparent and machine readable in how they operate. Many of the resources being made available also have the potential to impact markets, and are powering businesses of all shapes and sizes. This momentum is setting the stage for a future where ALL federal agencies have and open data and API portal that businesses, researchers, and developers know they can find the latest data, content, and algorithms across federal agencies.</p>

<p><strong>400+ APIs</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/open-fec-portal.png" width="30%" style="padding: 15px;" align="right" /></p>
<p>As of this week the <a href="https://raw.githubusercontent.com/18F/API-All-the-X/gh-pages/_data/individual_apis.yml">GSA has over 400 APIs listed</a> across the 96 developer programs found across federal agencies. There are numerous consistency and design issues across these APIs, and many of them are simply downloads, which may have some form filter as a gatekeeper. While we don’t have any numbers to support what kind of growth we’ve seen in federal government APIs over the last five years, off the top of my head I’d say we’ve seen up to 50% growth each year in the number of APIs published by agencies.</p>

<p>In my experience APIs breed more APIs. When one group sees another group doing APIs, they often emulate what they see. Folks in the private sector have been emulating the API moves of pioneers like SalesForce, Amazon, Twitter, and Twilio for years, we are seeing this play out at the federal government level. Some agencies are emulating what they see in the private sector, but I’d wager a significant number of agencies will more likely emulate what they see out of other agencies. This is how we are going to take all of this to the next level, building on the hard work of USDS, 18F, and other individual agencies like Census, NASA, and others who are truly pushing the needle forward.</p>

<p><strong>133 Github Orgs</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/68747470733a2f2f662e636c6f75642e6769746875622e636f6d2f6173736574732f3238323735392f313333353931312f32386233656336362d333563302d313165332d386565362d3636323732623966343138362e706e67.png" width="30%" style="padding: 15px;" align="right" /></p>
<p>The almost hundred developer portals, and over 400 APIs gives me hope about APIs in the federal government, but one area that leaves me the most optimistic is the number of agencies who are operating on Github. Doing my time at the Department of Veterans of Affairs (VA), I was the one who setup the VA Github organization, which became a lifeline for every one of the API and open data projects I was working on. Github usage across these agencies goes beyond just managing code, and agencies are using it to define policy, provide documentation, solicit feedback, and much more.</p>

<p>This usage of Github sets the stage for the next wave of growth in API operations within federal governments where Github is a central actor at every stage of the API life cycle. Github adoption is the most important signal I use to understand the viability of any API effort in the private sector. The companies who are using Github effectively almost always also possess a strong position in the space, directly influenced by the way they use Github to manage code, schema, definitions, and other machine readable, reusable components of API operations. I’m predicting that the federal agencies using Github possess a similar advantage if the federal API ecosystem, in coming years.</p>

<p><strong>18F and USDS</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/18f-usds.png" width="30%" style="padding: 15px;" align="right" /></p>
<p>A significant amount of the API leadership I’m seeing across federal agencies has been stimulated by tech focused groups at the White House and GSA. <a href="https://www.usds.gov/">U.S. Digital Service (USDS)</a> and 18F are working with multiple agencies to deliver API driven solutions, and software development practices. These groups have played a critical role in bringing outside tech practices into the federal government and doing the hard work to begin applying, evolving, and hardening these practices across agencies.</p>

<p>USDS and 18F are doing great work, and while I’m not 100% up to speed on their road map, the last couple of conversations I’ve had, they are growing and expanding–looking to keep taking on new projects. I’d like to see similar models develop, but from the outside-in, beginning to shift the landscape when we think about government consulting and vendors. 18F has done an amazing job at not just rethinking the technology part of the equation, but also thinking about the business and politics of it all, something that I think should be replicated externally, creating a new realm of API opportunity outside the federal government firewall.</p>

<p><strong>Transparency In Federal Government</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-government.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>Open data and APIs provide transparency into how government works (or doesn’t work). It gives us a look into the budgets of federal agencies, and better understand what government does, manages, and the impact it has on society. I fully support the Obama’s administration mandate requiring ALL federal agencies publish their data inventory as machine readable files, as well as APIs when possible. This is how government should operate by default, ensuring that agencies are more accountable to the public on a real-time basis.</p>

<p>I’m looking to continue the trend of transparency with data and APIs across federal government–doing more of what we’ve been doing for the last 5 years. However, in coming years I’d like to elevate this to become more about observability, where things are transparent, but there is also monitoring, testing, communication, service level agreements, and other terms of service that make government more observable by the public, as well as government auditors. This should be the default state of technology in government, where each agency is able to invest in the Venn diagram of operations that can become API observability.</p>

<p><strong>Resources That Help The Private Sector</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-business-of-apis.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>Publishing data and APIs isn’t all about making the federal government more transparent. Many of the API and data resources I come across have significant commercial value as well. The healthcare industry benefits from pharmaceutical data being open as part of OpenFDA are significant. Every citizen can benefit from the web, mobile, and automobile applications developed on top of the <a href="https://ridb.recreation.gov/">recreational information database (RIDB)</a> which powers <a href="https://www.recreation.gov/">Recreation.gov</a>, and the growing ecosystem on top of the RIDB API. I can keep going, talking about APIs at Commerce, NOAA, and Census–these APIs all have significant impact on the way business works.</p>

<p>There are many ways in which the federal government has been stepping up to provide valuable data for use by companies of all shapes and sizes. Census data is baked into many systems. Labor data is getting baked into the business models of startups. Regulations.gov API allow the rules of the road to get exposed for business consideration. This open data and API stuff at the federal government level is getting pretty important when it comes to how our economy and society works. While I do have some serious concern about the current administrations leadership when it comes to APIs I feel the current momentum around opening up data and APIs can be sustained, if approached in the right away.</p>

<p><strong>Considering The API Lifecycle</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-lifecycle.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>Alright, that describes the momentum we have with APIs in the federal government. I want to step back and size up this momentum, and take a look at some of the deficiencies I’m seeing right now in federal government developer portal and API implementations. In my regular work as the API Evangelist I look at the entire API space through the lens of almost 85 stops along an API lifecycle, from definition to deprecation. I’m going to take a handful of the core stops and use them as a lens to look at the state of federal government APIs, helping me frame a discussion about how we might move this conversation forward.</p>

<p>I’m looking to see the federal government API space alongside how I look at every other API in the private sector. I’m not saying that everything that gets done by commercial API providers should be emulated by the federal government, I’m just looking to make sure both sides are learning from each other, and somewhat in sync. I’m looking to try and figure out how we can keep the momentum going that I referenced, but also make sure federal agencies are not falling behind in all the key areas commercial API providers are using to dial in their API operations.</p>

<p><strong>Definitions</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-api-definitions.png" width="14%" style="padding: 15px;" align="right" /></p>
<p>Starting with the most important stop along the API lifecycle, I’m beginning to see more common web and API definitions being applied as part of federal government API operations. I’m always on the hunt for agencies using common web standards and specifications as part of their designing, using common definitions for dates, currency, and other key elements. I’m also finding more APIs defined using OpenAPI, API Blueprint, and other commonly used API definition specifications, providing a machine readable definition of the surface area of any government API.</p>

<p>Even with the definitions in play currently, it is still a small portion of the 400+ APIs I’m seeing. Most APIs are custom definitions, without much investment in recycling and reuse. The federal government needs a heavy dose of evangelism around common definitions, and the development of templates, and schema that agencies can leverage in their projects. This is one significant area I see an opportunity for the private sector to step up, and help bring robust toolboxes full of open definitions for APIs, and underlying schema applying the leading web concepts and standards.</p>

<p><strong>Design</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-api-design-fiction.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>Along with the increased presence of common approaches to defining APIs and schema, there is an emergence of more of a design-centric approach to delivering APIs. Sadly, this is only present in a handful of leading federal government APIs. Most of them are still pretty classic IT and developer focused implementations, often reflecting the backend system where they were generated. The current API design practices reflect that of the commercial sector around 2012, where you developed then documented an API, without much consideration for design beyond just RESTful principles.</p>

<p>The last administration worked hard to put forth API design guidelines, something that has been continued by USDS and 18F, but this needs to continue in practice. <a href="https://gsa.github.io/prototype-city-pairs-api-documentation/api-docs/">You can see an example of this in action over at the GSA with their prototype City Pairs API</a>–which is a working example of a well designed API, portal, and documentation that any agency can fork, and reverse engineer their own design compliant API. A significant area of movement in the private sector that will contribute to the evolution of API design at the federal level involves the availability of <a href="http://www.apicur.io/">open source API design editors like Apicurio</a>, which is a sign of the maturing private sector API design space, which the federal government should be able to leverage across agencies.</p>

<p><strong>Deployment</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-api-deployment.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>18F gives us a regular look into how they deploy the APIs behind their projects, going into detail on the technology stack employed, and other detail about how it was done. Sadly, this isn’t a common practice across federal agencies, leaving API deployment something that happens in the dark. This practice prevents agencies from learning through each agencies deployment, keeping every API deployment in a silo. When it makes sense from a security perspective, all server side APIs should be open sourced, and available on an agencies Github organization–making all API deployments available for reverse engineering, and deployment by other agencies.</p>

<p>I have to point out again <a href="https://gsa.github.io/prototype-city-pairs-api-documentation/api-docs/">the City Pairs API prototype from the GSA</a>, which open sources the API on Github, and provides step by step instructions on how to set it up, including the entire stack of technology used to operate it. This is how all government APIs should be, and opens up a significant opportunity for the private sector to step up and provide open source APIs that federal agencies can deploy as part of their stack, opening the door to other service and consulting opportunities. The federal government will benefit from the standardization introduced by the GSA, and will continue to grow with the type of API storytelling out of the 18F, eventually bringing API deployment out of the shadows.</p>

<p><strong>Modularization</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-microservice.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>Before I begin the shift from the more technical stops along the life cycle and move into the business, politics, and more operational side I wanted to talk about modularization of APIs in the federal government. I’m talking about how APIs are designed, and deployed, considering a microservices way of doing things. I’m also talking about containerization, and ensuring the deployment of APIs are modular, scalable, and well defined. Something that makes them more deployable in any infrastructure on-premise or in the cloud.</p>

<p>Another area of modularization to consider in my evaluation of federal government APIs can be found over at 18F with their leadership in <a href="https://18f.gsa.gov/2016/11/15/modular-procurement-state-local-government/">modular procurement</a>. Identifying that the underlying technology isn’t the only thing we we need to be decoupling, and that the business and politics surrounding each API needs to be modular as well. I am seeing some talk of microservices, containerization, and serverless, mostly out of the GSA, but it will be another area we need to see growth in when it comes to API operations in the federal government. Acknowledgment that we need to be decoupling the technology, business, and politics of government resources.</p>

<p><strong>Management</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/api-management.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>API management is another area that I’ve seen some significant growth and maturity in federal agency API operations. Not only did we see one of the first open source API management solutions Umbrella emerge out of a federal agency, we have seen over 10 federal agencies adopt this as part of their API operations. We need more of this. API Management should be baked into all federal government APIs, standardizing how APIs are secured, rate limited, logged, and measured. Standardizing how all digital government resources are accessed and metered–establishing a common awareness across all federal agency APIs.</p>

<p>This layer of API operations is what is going to set the stage for the next wave of growth in not just government APIs, but also government revenue. Commercial API providers have been leverage service composition at this layer to meter and charge for access to some resources, charging different rates to the public, partners, and even amongst internal groups within a company. This approach to managing digital government resources will emerge as the standard for generating revenue from public resources similar to how timber leases, mining claims, and other physical resources are managed–except this is all being done on public servers, not public lands.</p>

<p><strong>Portal</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-portal.jpg" width="18%" style="padding: 15px;" align="right" /></p>
<p>Wrapping around any APIs you deploy, and providing a gateway to solid API management practices always begins with a proper developers portal. The almost 100 developer portals for federal agencies that now exist are due to the hard work of the GSA, direction of OMB, under the guidance of the Obama administration. A central portal is key to application developers, system integrators, and even other agencies knowing where to find data and API resources. Most of the existing API developer portals leave much to be desired, but when you look at the efforts out of CFPB, 18F, and the GSA, you get a glimpse of what is possible in government.</p>

<p>I have to point out <a href="https://gsa.github.io/prototype-city-pairs-api-documentation/api-docs/">the City Pairs API prototype from the GSA</a> yet again. This approach to providing API design, deployment, management, and portal guidance is how consistent developer portal operations will spread across all federal agencies. Each agency should be focusing on what they do best, and not worrying about having to reinvent the wheel each time they deploy a new agency portal, project, or other implementation. There should be a wealth of open source API portals available on Github for agencies to fork and employ immediately to support any API implementation.</p>

<p><strong>Documentation</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-documentation.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>I am seeing Swagger UI, Slate, and a handful of other interesting API documentation solutions as I browse through the 400+ federal government APIs. This evolution in open source documentation is partly responsible to the API definition portion of the conversation, as OpenAPI is the engine of Swagger UI, and other API documentation solutions. This approach to delivering API documentation across government APIs, and available as part of any portal template has encouraged significant reuse, and consistency  across government API implementations–something that should spread to every API implementation, at every federal agency.</p>

<p>I want to point out the OpenFDA has moved the needle when it comes to API documentation by providing more interactive documentation, including visualizations as part of their API implementations. We need to make API definition driven documentation the default practice across federal government APIs, but it is also significant that we are seeing the API documentation be evolved as part of the valuable information government agencies are making available. This is a good sign that there are healthy API documentation efforts emerging, as this type of innovation is always a by-product of healthy API ecosystems.</p>

<p><strong>Communications</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-chat.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>I’m seeing healthy communication around data, API, and developer portals from the <a href="https://usedgov.github.io/news/">Department of Education</a>, <a href="https://www.census.gov/data/developers/updates.html">Census</a>, <a href="https://18f.gsa.gov/blog/">18F</a>, <a href="https://www.usds.gov/blog">USDS</a>, and a handful of others, but for the most part communication around API operations is non-existent at the federal level. This is a problem. This makes many APIs look dead or lifeless, with very little information about what is happening beyond just the documentation. Communication with integrators and developers is an essential part of API operations, and a big reason growth in API efforts at the federal level are slower than anticipated.</p>

<p>I understand that federal agencies are constrained by rules regarding how they can communicate with the public, and operating a blog isn’t as straightforward as in the private sector. However, it is clear that it can be done. Plenty of agencies have blogs, and active social media accounts, this practice just has to be applied across API operations. I’m guessing once we see blogs, Twitter and Github accounts default across federal government agency API operations, we’ll see a significant uptick in the number of integrations and applications that are putting government resources to use in their operations.</p>

<p><strong>Road Map</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-roadmap.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>While <a href="https://tech.gsa.gov/roadmap/">you can find road maps for agencies like the one over at the GSA that include APIs</a>, communicating the road map for a federal agency API really isn’t a thing. Honestly, I don’t see much evidence of communicating what is coming down the road with federal agency API operations, what is happening currently, or what has happened in the past by providing a change log. <a href="https://developer.usajobs.gov/Guides/Change-Log">The USA Jobs API has a change log</a>, and some API related code have change logs as part of their API operations, but communicating around change in government API platforms isn’t really a common concept.</p>

<p>The absence of road maps, issues, and change logs for API operations at the federal level is a problem. I’m hopeful that the usage of Github by federal agencies might shift this reality of API operations currently, but I’m also hopeful that this could be maybe be automated for agencies as part of their API deployment, management, and portal solutions. Taking another concern off the table for API providers, while still ensuring what API consumers and integrators will need to stay in tune with federal government resources being exposed via APIs.</p>

<p><strong>Monitoring</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-heart-monitor.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>Moving on to the monitoring, testing, and other awarness aspects of API operations. Sadly, there is no evidence of API monitoring and testing at the federal government levels. I searched my news archives, and looked for evidence across the APIs I’ve been reviewing as part of this research. I am not seeing any organized approaches to monitoring API endpoints, sharing the results with the developer community, or the mention of any common API testing tooling in the space.</p>

<p>This is another critical issue regarding the state of APIs in the federal government. As I was browsing around looking for APIs I came across several portals and APIs that had gone dark, demonstrating what a problem it is that nothing is being monitored even for availability. Ideally federal agencies are monitoring the uptime and availability of APIs already, and should be moving on to more detailed, meaningful testing. The only good news on this front I found, <a href="https://www.opm.gov/developer/documentation/current-status-api/">was a single API for monitoring whether or not the government in Washington DC is operating or not</a>–at least we have that, right?</p>

<p><strong>Security</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-padlock.png" width="15%" style="padding: 15px;" align="right" /></p>
<p>Continuing down into the realm of bad news, let’s talk about what I’m seeing when it comes to API security. I am seeing a small bright spot, if we are talking about acknowledging the need for encryption in transit to be default for APIs, but API security seems to stop there. Sure we are seeing some APIs adopting API umbrella to key up APIs, but this is more API management then it is API security. All leaving me concerned about what is going on behind the scenes, as well as thinking security concerns and lack of healthy practices are probably a significant area of friction for new APIs to get out the door in the first place.</p>

<p>Security is the number one concern of agencies when you begin to talk with them about APIs. It needs to be front and center in all API conversations. I’m talking about formal strategies regarding how to security APIs, with official guidance from API leadership, and published pages sharing security practices with the community. API security begins with healthy API management practices, something I’m seeing across federal government implementations, but it is something that evolves with sensible API monitoring, and testing practices, which I’m not seeing. The absence of this awareness is a problem, and needs to become baked into all API efforts at the federal level as soon as possible.</p>

<p><strong>Reliability</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-reliability.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>My personal confidence in the reliability of government APIs is pretty low. The availability of APIs I was depending on for my work during my Presidential Innovation Fellowship during the fall shutdown in 2013 was the beginning of my concern, but the stability, longevity, support, communication, and lack of road map or change logs for APIs all have contributed to my concerns over the years. The transition from the previous administration to the current one has moved up my levels of concern significantly. I’m just not convinced some agencies will always be good stewards of the APIs they are making available.</p>

<p>API reliability is just as much a business and political challenge as it is a technical one. Staffing and budget issues will most likely contribute more to API reliability than actual server or network problems. I know that if I was building any business on top of any data or API from a federal agency I would be building in multiple layers of redundancy wherever possible, and work to always have a plan B in place in case an API goes away completely. I feel that this is an area where the private sector can step up, and not just provide vendor solutions that deliver reliable API solutions, but also help from the outside-in, providing hosting, caching, virtualization, and other approaches to ensuring there is redundancy, and always access to vital resources being made available via federal government APIs.</p>

<p><strong>Discovery</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-api-discovery.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>The GSA, and 18F has done an amazing job aggregating API resources for use by all federal agencies. This includes providing a listing of federal government <a href="https://api-all-the-x.18f.gov/pages/developer_hubs/">developer portals</a>, as well as <a href="https://api-all-the-x.18f.gov/pages/individual_apis/">all the known agency APIs</a>, in CSV, XML, JSON, and YAML formats.  I use these lists to help maintain <a href="http://federal.government.stack.network/agencies/">my own federal agency API directory</a>, and keep in tune with what is going on.</p>

<p>Another significant area of opportunity for investment in federal government APIs is a discovery engine. <a href="http://gsa.index.apievangelist.com/">I’ve worked to create APIs.json indexes for some federal government APIs including the GSA</a>, but this work is time intensive, and ideally is something that should be done by each API provider, within a government agency. You can see this in action over at Trade.gov, <a href="http://developer.trade.gov/apis.json">who has published an APIs.json for their APIs</a>, which also includes links to documentation, OpenAPI (swagger) definitions, and terms of service. Ideally, all federal agencies would have an APIs.json index in the root of their website, similar to the data.json file that already exists for federal government data inventory. It will take time, but eventually federal agencies will see the benefits in making <a href="http://apisjson.org/">their APIs discoverable using APIs.jso</a>n, making it available for inclusion in API search engines like <a href="http://apis.io/">APis.io</a>.</p>

<p><strong>Clients</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-client.png" width="20%" style="padding: 15px;" align="right" /></p>
<p>There are a wealth of SDKs, code libraries, and frameworks emerging across the Github accounts of federal agencies. <a href="https://government.github.com/">You can find a list of these government Github accounts on the government section of Github</a>. Some agencies do well at showcasing code as part of their API operations, while others you have to hunt a bit, and many who don’t offer any code at all as part of their API offerings. I’d say that client tooling and code across federal government APIs is a mess right now. There really isn’t a single discovery mechanism beyond Github, or any standard for developing and presentation across agencies, or even just across many APIs in a single federal agency.</p>

<p>The generation, development, maintenance, and integration of code for use as part of federal API integration is a pretty significant opportunity for the private sector. The APIs are publicly available, many with existing API definitions, it wouldn’t take much to develop and maintain OpenAPIs for all federal agency APIs, and regularly update SDKs, code samples, and other client tooling that put agency APIs to use. Client tooling should be another thing agency API providers shouldn’t have to worry about. They should be just focusing on maintaining an OpenAPI, and then leverage private sector tooling and services to generate client code, and put common client solutions like Postman and Restlet to use when working with federal government APIs.</p>

<p><strong>Integration</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-integration-automation.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>After client and SDK concerns I like to expand and consider wider integration solutions, and particularly the integration platform as a service (iPaaS) question. In 2017, there are open source, and software as a service solutions for integrating with APIs, and providing client tools that anyone, even non-developers can put to use. Service like <a href="https://zapier.com/">Zapier</a>, and <a href="https://github.com/DataFire">open source DataFire</a> allow for integration with existing 3rd party APIs, and custom API integration, which can be used by business users, as well as developer and IT groups.</p>

<p>You can find If This Then That (IFTTT) recipes <a href="https://ifttt.com/usagov">for working with USA.gov</a>, and <a href="https://zapier.com/blog/federal-government-terms-of-service-amendment/">guidance from Zapier for API providers to make their API terms of service government friendly</a>, but there is not a lot of other movement when it comes to the federal government making use if iPaaS solutions. While not as far along as we should be, I’d say the stage is set for federal agencies to begin thinking through their integration strategies, leveraging solutions like Zapier, because you not only get an integration solution, you get access to the experience that comes with integrating with over 750+ APIs.</p>

<p><strong>Spreadsheets</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-excel-icon.jpg" width="20%" style="padding: 15px;" align="right" /></p>
<p>Another significant area for API client integration as well as API deployment is the spreadsheet. Both Microsoft and Google Spreadsheets allow for consumption of many data, content, or algorithmic APIs right in the spreadsheet. When you consider spreadsheets across federal agencies, you see how this could significantly change how federal government APIs are put to use internally at the agencies where they are deployed, inter-agency, or even by external 3rd party researchers, partners, and integrators.</p>

<p>You can find quite a few <a href="https://catalog.data.gov/dataset?res_format=Excel">data downloads in spreadsheet format at data.gov</a>. Use the interesting <a href="https://www.eia.gov/opendata/excel/">Excel data add-in and Google Sheets add-on over at the U.S. Energy Information Administration (EIA)</a>, and see <a href="https://18f.gsa.gov/2016/05/02/from-spreadsheet-to-api-to-app-a-better-contract-forecast-tool/">the cool spreadsheet to API work over at 18F</a> to consider a handful of examples of the role spreadsheets play in the federal API game. There needs to be a significant amount of investment in spreadsheet to API and API to spreadsheet integration across the federal government in coming years, to reach beyond the technical community audience.</p>

<p><strong>Plans</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-plan.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>Now that we are seeing API management practices being adopted by federal agencies, I’d like to plant the seeds for some of the next steps for agencies when it comes to making public resources available for commercial use. API Umbrella, and <a href="https://api.data.gov/">api.data.gov</a> help manage API key usage for developers and the daily and hourly rate limits on accessing api.data.gov APIs, ensure API resources stay available, and aren’t overused by any single consumer.</p>

<p>Commercial API providers like Amazon, Google, and others often charge different rates for different API resources, while also providing a free level of access for the public, or possibly researchers. There are also many API service providers who provide advanced API management solutions for billing against API usage, charging for high levels of access, or maybe establish unlimited usage levels for a select handful of trusted consumers. It is common to have multiple plans for API access, and charging for API consumption in some cases, helping pay for infrastructure and other hard costs.</p>

<p>I get that this is a controversial topic in my circles–paying for accessing public data? In short, data is valuable, and making sure it is available, and usable creates value. It costs money to make data available as APIs is a reliable and sustainable way. It makes sense to charge for high volume of commercial access to APIs, similar to how government charges commercial operators to access resources on public lands. It may seem like a crazy notion now, but in the future, this is how government agencies will generate much needed revenue from public resources.</p>

<p><strong>Partners</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-partner.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>Continuing the API management and plans conversation, I’d like to highlight what I’d consider to be an under-realized, and under-utilized partner network of federal government API consumers. You can see what I am talking about over at <a href="http://www.opendata500.com/us/">the Open Data 500 project</a>, which is “the first comprehensive study of U.S. companies that use open government data to generate new business and develop new products and services.” This approach to quantifying the companies using federal government open data, needs to be applied to APIs, establishing a feedback loop with any company who integrates with a federal agency APIs.</p>

<p>The federal government has many partners, but is rarely ever showcasing or leveraging these partners in any meaningful way when it comes to API integration, or application development. The friction of existing laws regarding how government can work with companies causes some of this, however much of this is due to not understanding how 3rd party development works within API ecosystems at the federal level. Some efforts like the Open Data 500 have been established to help quantify how open data is being used across industries, but in my experience much more work is needed to help understand who is using federal government resources, and who might be a fit for heightened levels of engagement to strengthen partner relationships between the public and private sector.</p>

<p><strong>Legal Department</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-terms-of-use.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>The federal government has endless number of laws on the book governing how it operates, with a significant portion of these laws introduced to guide the government when it comes to using the Internet, purchasing and developing software in an Internet age. Along with the technical, and business aspects of doing APIs, federal agencies are in need of beginning the process of publishing and negotiating terms of service, privacy policy, branding guidelines, service level agreement, breach, vulnerability, and other critical guidelines out in the open, with all stakeholders (public included) present.</p>

<p>You see the beginning of this in action over at 18F in the GSA where they are drafting, sharing, and managing key aspects of developing software and managing system integrations with APIs out in the open, making the public a first class citizen in these discussions. The legal department for API operations can be conducted via Github, including partners, developers, and the public in the process. There is a precedent, and the benefits for not just making government transparency, but making technology better serve citizens exists. Agencies will learn API design, deployment, and security, privacy, and terms of service by emulating what they see–if there are no examples out there for them to follow, these legal aspects of operating government will remain behind closed doors.</p>

<p><strong>Evangelism</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/KL_InApiWeTrust-1000.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>Building on my earlier comments on the lack of communication out of existing federal agencies when it comes to data and API efforts, the advocacy and evangelism of APIs out of federal agencies is near non-existent. You see efforts from a handful of agencies trying to do more outreach and evangelism around their data and API projects, often led by 18F and USDS, but evangelism is something that just isn’t in the DNA of federal government as it currently exists.</p>

<p>Getting the word out about federal APIs is essential to establishing a vital feedback loop that government agencies will need to improve upon systems, data collection practices, and generally serving the public. Without outreach few will know these API resources exists. Without outreach agencies will never know who is using, or who might be looking to use government resources. Evangelism isn’t just about hackathons and conferences, it is also about communications, support, and other essential aspects of developing software and integrating with systems. With no outreach, all of this occurs in a silo, and behind closed doors, limiting what is possible.</p>

<p><strong>Some Momentum</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-momentum.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>There is a significant amount of momentum when it comes to data and APIs at the federal government level coming out of the last administration. With the leadership from the Obama administration we see agencies stepping up to publish their public data inventory, and increasing the number of developer portals and APIs that are available. This momentum augments existing IT Momentum to grow and expand government IT infrastructure, with API investments reflecting expansion in government doing business securely on the open web, leverage the low-cost, scalable benefits of building using existing web technology.</p>

<p>While this momentum is real, it is seriously in danger of slowing, and even reversing course in the current administration. All the leadership available regarding API implementation has gone away, leaving agencies alone in their API strategy development. Without leadership many of these API efforts will wither and die as they are. With a vacuum in leadership there is a significant opportunity for vendors, and outside leadership to step in, take ownership of open source projects, standards, and guidance, and play an important role in keeping projects active, alive, and even growing.</p>

<p><strong>Lots of Work</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-work.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>The most common advice I give agencies regarding what they can be doing on the API front is doing more of the same. Keep opening up data. Keep publishing APIs. Keep supporting and expanding your developer portals. We have lots of redundant work ahead of us to make sure all federal IT systems can securely communicate over the web to serve all constituents, and support all government workers. There are a number of areas like monitoring, testing, security, communication, and evangelism that should be invested in, but we also just need to do the hard work keeping what is already on the table active.</p>

<p>It is unrealistic to expect the federal government to do all this work alone. Which is why ALL federal agencies should be doing APIs, to open up data and internal systems to other federal agencies, vendors, partners, and the general public to help carry the load. It is perfectly acceptable for work around public data and other resources to happen out in the open via API integrations, Github, and other platforms and services already in operation. This is why APIs can’t be shuttered or allowed to go dormant. Any government project data project in operation should be externalized, API-ized, and opened up for a collaborative approach to running government.</p>

<p><strong>Private Sector Opportunity</strong></p>
<p><img src="https://s3.amazonaws.com/kinlane-productions/federal-government/state-2017/bw-opportunity.png" width="18%" style="padding: 15px;" align="right" /></p>
<p>The most important part of this state of APIs in federal government story is about what the private sector can step up and do. I mentioned several opportunities for companies, organizations, and institutions to step up and help be stewards of federal government data, APIs, and open source code. This means doing business through existing federal government procurement channels, but it also means stepping up and investing in existing federal agency open data and API efforts, engaging with agencies via Github repositories, forums, and social media.</p>

<p>You see companies like Amazon, Microsoft, Zapier and Github stepping up with government focused section of their platform, specialized products and services, and terms of service tailored for government adoption. We need more of this. We need more startups to step up, incentivize, and lead when it comes to APIs in government. Federal government APIs aren’t just about what government is doing with APIs, it is also about what the private sector does to use these APIs, and how they engage with government agencies whenever possible to hep improve the quality of APIs, and software coming out of federal agencies.</p>

<p>This reality is even more dire in a Trump administration. In the current administration any API leadership has been halted, and the two groups (USDS &amp; 18F who are leading the charge are facing friction, and seeing top talent exit back to the private sector, unable to make the impact they had envisioned when they first signed up for government service. The API hard work is continuing across the federal government, within each agency, but in coming years we need the bulk of work to be assumed by the privacy sector, letting dedicated government employees know that we have their back, are here to support, integrate, and build on their hard work, while providing valuable and constructive feedback wherever we can.</p>

<p><em>Thank you to Red Hat for bringing me out, and gathering folks at Tyson’s Corner for the interesting discussion about APIs–some interesting people showed up including government, vendors, and consulting groups.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/27/state-of-apis-in-the-federal-government/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/26/we-have-a-hostile-ceo-which-requires-a-shift-in-our-api-strategy/">We Have A Hostile CEO Which Requires A Shift In Our API Strategy</a></h3>
        <span class="post-date">26 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/raven-fence.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>As I work my way through almost one hundred federal government API developer portals, almost 500 APIs, and 133 Github accounts for federal agencies the chilling effect of the change of leadership in this country becomes clear. You can tell the momentum across hundreds of federal agency built up over the last five years is still moving, but the silence across blogs, Twitter accounts, change logs, and Github repos shows that the pace of acceleration is in jeopardy.</p>

<p>When you are browsing agency developer portals you come across phrases like this, “As part of the <a href="https://www.whitehouse.gov/sites/default/files/omb/egov/digital-government/digital-government.html">Open Government Initiative</a>, the BusinessUSA codebase is available on the BusinessUSA GitHub Open Source Repository.” With the link to the Open Government Initiative leading to a a page on the White House website that has been removed–<a href="https://obamawhitehouse.archives.gov/open/documents/open-government-directive">something you can easily find on the Obama archives</a>. I am coming across numerous examples like this of how the change in leadership has created a vacuum when it comes to API and open data leadership, at a time when we should be doubling down on sharing of data, content, and putting algorithms to work across the federal government.</p>

<p>After several days immersed in federal government developer areas it is clear we have a hostile CEO that will require us to shift in our API strategy. After six months it is clear that the current leadership has no interest transparency, observability, or even the efficiency in government that is achieved from focusing opening up data via public, but secure APIs. This doesn’t mean the end of our open data and API efforts, it just means we lose the top down leadership we’ve enjoyed for the last eight years when it came to technology in government, and efforts will have to shift to a more bottom up approach, with agencies and departments often setting their own agenda.</p>

<p>This is nothing new, and it won’t be the last time we face this working with APIs across the federal government. Even during times where we have full support of leaders we should always be on the look out for threats, either technical, business, or political. Across once active API efforts I’m regularly finding broken links to previous leadership documents and resources at the executive level. We need to make sure that we shift these resources to a more federated approach in the future, where we reference central resources, but keep a cached copy locally to allow for any future loss of leadership. This is one reason we should be emphasizing the usage of Github across agencies, which offloads the storage and maintenance of materials to each individual agency, group, or even at the project level.</p>

<p>It is easy to find yourself frustrated in the current environment being cultivated by the leadership at the moment. However, with the right planning and communication we should be able to work around, and develop API implementations that are resilient to change, whether they are technical, budgetary, or on the leadership front as we are dealing with now. Don’t give up hope. If you need someone to talk with about your project please feel free to reach out publicly or privately. There are many folks still working hard on APIs inside and outside the federal government firewall, and they need our help. If you find yourself abandoning a project, please try to make sure as much of the work is available on your agencies Github repository, including code, definitions, and any documentation. This is the best way to ensure your work will continue to live on. Thank you for your service.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/26/we-have-a-hostile-ceo-which-requires-a-shift-in-our-api-strategy/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/25/a-lack-of-communication-around-federal-government-apis/">A Lack Of Communication Around Federal Government APIs</a></h3>
        <span class="post-date">25 Jul 2017</span>
        <p><a href="https://www.census.gov/data/developers/updates.html"><img src="https://s3.amazonaws.com/kinlane-productions/census/census-api-updates.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I personally understand the challenges with communicating publicly when you work for the federal government. It is one of the top reasons I do not work in federal government anymore. It would kill me if I couldn’t blog each day without friction–it is how I create and ideate. Even with this understanding I find myself regularly frustrated with the lack of communication by owners of APIs across federal government agencies. There are numerous agencies who do successfully communicate around their APIs and open data projects, but the majority of APIs I come across have little, or no communication around their API operations.</p>

<p>Have a blog, Twitter, or Github account might seem like a nice to have, but in reality they are often the only sign that anyone is home, and an API is reliable, and make the the difference between choosing to integrate with an API, or not. A blog or Twitter account, and whats new feature box on the home page of an API developer portal can send the winning (or losing) signal that an API is actually active and alive. Developers come across a lot of APIs that are dormant or abandoned, and the presence of common communication channels (blog, Twitter, Facebook, LinkedIn, Github) are the signal we often need before we are willing to invest the time into learning another new API, or signing up for yet another developer account.</p>

<p>I know that it is possible to handle API communications in a healthy way at government agencies–<a href="https://18f.gsa.gov/blog/">18F</a>, <a href="https://www.census.gov/data/developers/updates.html">Census</a>, and others are doing it right. There is some serious storytelling friction occurring in government. I see the same illness in corporate and other institutional API platforms–geeks and IT folks aren’t always the best at getting the word out about what they are doing. However, I think there is additional friction at the government level. We’ve seen a significant increase in blogging, and social network usage usage across government agencies, we need to investigate how we can incentivize federal government API operators to get a little more chatty with their work.</p>

<p>Communication around API operations is an essential building block. Federal government isn’t immune to this. If you aren’t telling the story about why developers should be using it, and actively communicating with your integrators, you will never find the success you seek when doing APIs at your agency. You don’t have to launch a wildly active and popular blog or social media presence. You just need something. A simple blog with RSS, hosted on your Github account. A Twitter account. Something. Please. Anything. Thank you!</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/25/a-lack-of-communication-around-federal-government-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/25/api-management-across-all-government-agencies/">API Management Across All Government Agencies</a></h3>
        <span class="post-date">25 Jul 2017</span>
        <p><a href="https://api.data.gov/about/"><img src="https://s3.amazonaws.com/kinlane-productions/18f/9302707420_dbc7c2c437_o.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>This isn’t a new drum beat for me, but is one I wanted to pick it up again as part of the federal government research and speaking I’m doing this month. It is regarding the management of APIs across federal government. In short, helping agencies successfully secure, meter, analyze, and develop awareness of who is using government API resources. API management is a commodity in the private technology sector, and is something that has been gaining momentum in government circles, but we have a lot more work ahead to get things where we need them.</p>

<p>The folks over at 18F have done a great job of helping bake API management into government APIs using <a href="https://apiumbrella.io/">API Umbrella</a>, resulting in these twelve federal agencies:</p>
<ul>
    <li><a href="https://api.data.gov/docs/business-usa/">BusinessUSA.gov API</a></li>
    <li><a href="https://api.data.gov/docs/usda/">Department of Agriculture</a></li>
    <li><a href="https://api.data.gov/docs/commerce/">Department of Commerce</a></li>
    <li><a href="https://api.data.gov/docs/ed/">Department of Education</a></li>
    <li><a href="https://api.data.gov/docs/fcc/">Federal Communications Commission</a></li>
    <li><a href="https://api.data.gov/docs/fec/">Federal Election Commission</a></li>
    <li><a href="https://api.data.gov/docs/fda/">Food and Drug Administration</a></li>
    <li><a href="https://api.data.gov/docs/gsa/">General Services Administration</a></li>
    <li><a href="https://api.data.gov/docs/nasa/">National Aeronautics and Space Administration</a></li>
    <li><a href="https://api.data.gov/docs/nih/">National Institutes of Health</a></li>
    <li><a href="https://api.data.gov/docs/nrel/">National Renewable Energy Laboratory</a></li>
    <li><a href="https://api.data.gov/docs/regulations/">Regulations.gov API</a></li>
</ul>
<p>This doesn’t just mean that each of these agencies are managing their APIs. It also means that all of these agencies are managing their APIs in a consistent way, using a consistent tool. Something that is allowing these agencies to effectively manage:</p>
<ul>
  <li><a href="https://api.data.gov/docs/api-key">API Key Usage</a> - How to use your API key after signing up.</li>
  <li><a href="https://api.data.gov/docs/rate-limits">Web Service Rate Limits</a> - Daily and hourly rate limits on accessing api.data.gov APIs.</li>
  <li><a href="https://api.data.gov/docs/errors">General Web Service Errors</a> - General error codes that can be returned by any api.data.gov API.</li>
  <li><a href="https://api.data.gov/docs/https">HTTPS Usage</a> - Information about HTTPS usage on api.data.gov.</li>
</ul>

<p>I know that both 18F and USDS are working are hard on this, but this is an area we need agencies to step up in, as well as the private sector. We need any vendor doing API deployment projects for any agency to work together to make sure their agency is using a standardized approach. This means that vendors should make the investment when it comes to reaching out to the GSA, and 18F to <a href="https://api.data.gov/about/">make sure you are up to speed on what is needed to leverage the work already in motion at api.data.gov</a>.</p>

<p>Doing API management in a consistent way across ALL federal government APIs is super critical to all of this scaling as we all envision. The federal government possess a wealth of valuable data and content that can benefit the private sector. This isn’t just about making the federal government more transparent and observable, this is also about making these valuable resources available in a usable, sustainable way to the private sector–industries will be better off for it. I’m happy to see the progress these twelve agencies have made when it comes to API management, but we need to get to work helping every other agency play catch up, making it something that is baked into ALL API deployment projects by default.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/25/api-management-across-all-government-agencies/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/25/adding-vulnerability-disclosure-to-my-api-building-block-recommendations/">Adding Vulnerability Disclosure To My API Building Block Recommendations</a></h3>
        <span class="post-date">25 Jul 2017</span>
        <p><a href="https://18f.gsa.gov/vulnerability-disclosure-policy/"><img src="https://s3.amazonaws.com/kinlane-productions/18f/vulnerabilities-disclosure-policy.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I am working through the almost 100 federal government agency developer portals and the almost 500 APIs that exist across these agencies, looking for the good and bad of APIs in government at this level. One of interesting building blocks I’ve stumbled across, that I would like to shine a light on for other public and private sector API providers to consider in their own operations is <a href="https://18f.gsa.gov/vulnerability-disclosure-policy/">a vulnerability disclosure</a>.</p>

<p>I feel that 18F description of their vulnerability disclosure says it best:</p>

<blockquote>
  <p>As part of a U.S. government agency, the General Services Administration (GSA)’s Technology Transformation Service (TTS) takes seriously our responsibility to protect the public’s information, including financial and personal information, from unwarranted disclosure.</p>
</blockquote>

<blockquote>
  <p>We want security researchers to feel comfortable reporting vulnerabilities they’ve discovered, as set out in this policy, so that we can fix them and keep our information safe.</p>
</blockquote>

<blockquote>
  <p>This policy describes what systems and types of research are covered under this policy, how to send us vulnerability reports, and how long we ask security researchers to wait before publicly disclosing vulnerabilities.</p>
</blockquote>

<p>This should be default across all federal, state, county, and municipal government agencies. Hell, it should be default across all companies, organizations, and institutions. One of the reasons we have so much dysfunction in the security realm that elevates the discussion to theatrical levels with cybersecurity is that we aren’t having honest conversations about the vulnerabilities that exist. Few platforms want these conversations to occur, let alone set the tone of the conversation in such an open way. Without any guidance, and fear of retaliation, developers and analysts who find vulnerabilities will continue to hold back on what they find.</p>

<p>Vulnerability disclosure seems like something that ALL API provides should possess. There is no reason you can’t fork <a href="https://github.com/18F/vulnerability-disclosure-policy/blob/master/vulnerability-disclosure-policy.md#vulnerability-disclosure-policy">the GSA vulnerability policy</a> and share it as the official tone of the vulnerability disclosure conversation on your platform. Encouraging all API developers to understand what the tone of the conversation will look like when they stumble across a vulnerability while integrating with your API. I’m adding the concept of having a vulnerability disclosure to <a href="http://vulnerabilities.apievangelist.com/">my API vulnerability research</a>, and I am going to add GSA’s version as a tool in the API vulnerability toolbox, providing a template that other providers can put to work.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/25/adding-vulnerability-disclosure-to-my-api-building-block-recommendations/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/25/i-am-speaking-on-state-of-apis-in-federal-government-thursday-in-dc/">I Am Speaking On State Of APIs In Federal Government Thursday In DC</a></h3>
        <span class="post-date">25 Jul 2017</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/api_evangelist_site/blog/steve_and_i_apistrat_2016.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p><a href="http://redhatapievents.com/dc">I am joining my friend Steve Willmott in DC this week to talk about federal government APIs</a>. We will  be gathering at Tysons’ Biergarten between 1:30 and 5:00 PM this Thursday to talk APIs. Both Steve and I will be speaking individually, with some QA, and a happy hour afterwards as an opportunity for more discussion.</p>

<p>I am looking forward for the opportunity to hanging with my friend Steve, as the last time we’ve hung out and spoke together was APIStrat in Boston, but at APIStat we are always running a conference, and not actually focused on our views of the APIs space. So, I am eager to learn more detail about what 3Scale is up to as part of the Red Hat machine, and specifically some of the containerization, microservices, and virtualization discussions they are leading lately.</p>

<p>Anyways, I will be in DC all day Thursday. Come join the conversation. I won’t have much time in DC, so the gathering will be the best opportunity to grab a moment of my time. I’ll be talking about the state of APIs in federal government, something I’m reminded during my research and preparation for my talk is probably the most important discussion we should be having in the API space right now.</p>

<p>Looking forward to seeing you all in DC. Thanks to <a href="redhat.com">Red Hat</a> for bringing me out to DC, and making this conversation possible. I’ll see you Thursday.</p>

<p><strong>Event Details:</strong>
<strong>Date:</strong> Thursday, July 27, 2017</p>

<p><strong>Time:</strong> 1:30 p.m. – 5:00 p.m.
<strong>Registration:</strong> 1:30 – 2:00 p.m.
<strong>Presentations:</strong> 2:00 – 3:30 p.m.
<strong>Happy Hour:</strong> 3:30 – 5:00 p.m.</p>

<p><strong>Location:</strong>
Tysons’ Biergarten
8346 Leesburg Pike
Tysons, VA 22182</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/25/i-am-speaking-on-state-of-apis-in-federal-government-thursday-in-dc/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/24/federal-government-apis-in-a-trump-administration/">Federal Government APIs In A Trump Administration</a></h3>
        <span class="post-date">24 Jul 2017</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/api_evangelist_site/blog/white_house_window_propaganda_leaflets.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I haven’t written much about APIs in the federal government since the election. I’m still having conversations, and investing time into monitoring what is going on in the federal government, but honestly in the name of self-care I have to turn my head from what is going on with the current administration. It’s no secret that I’m not a Trump supporter, and honestly I have trouble not getting angry with Trump supporters when it comes to making the federal government more transparent and observable with data and APIs. The current tone the administration is taking when it comes to transparency, observability, and accountability will take us decades to recover from, making conversations about federal government APIs very difficult to have in many scenarios.</p>

<p>Luckily, I’m regularly reminded that there are MANY good people at government agencies who are doing amazing things, allowing me find more energy for thinking about APIs in federal government. I’ve been preparing for <a href="http://redhatapievents.com/dc">a talk I’m doing in DC this week with 3Scale by Red Hat</a>, which is priming the pump for a presentation I’m doing for the General Services Administration later in August. Both of these talks give me the chance to think about federal government, and invest some energy into finding the good that is going on in the federal government when it comes to APIs. It will also give me some time to take a look at what challenges exist when doing APIs at the federal level of government, with some acknowledgement of the current leadership in the White House.</p>

<p>First, I’m going to go agency by agency, taking a fresh look at anything API going on at the top level agencies, with a quick secondary look at some of the lesser known agencies. After this, I want to take a look at who is behind any API project that I’m coming across–understanding what I can about the internal groups doing APIs, any inter-agency efforts, including efforts out of <a href="https://18f.gsa.gov/">18F</a> and <a href="https://www.usds.gov/">USDS</a>. I’m looking to get a pulse for the API appetite that still exists at each agency, but also refresh the good work coming out of the forward leaning tech groups at GSA, and at the White House. From personal conversations I am having, as well as my regular monitoring of the space, I know there are still many good things still going on.</p>

<p>Second, I’m going to take another look at external forces when it comes to APIs in the federal government. I’m talking about API consumers, and companies or organizations who are building things with open data and APIs out of federal agencies. I’m also looking to better understand the vendor landscape when it comes to delivering API related projects. One of the biggest reasons APIs isn’t often seen as living up to it’s potential is because we aren’t very good at telling the stories about how the private sector is using public sector APIs, and there isn’t enough invested by federal agencies when it comes to getting the word out about their valuable resources. Many legacy rules and regulations about how the private sector and public sector can or cannot work together tends to make people in government nervous about being too vocal about this stuff–something that needs to change.</p>

<p>Third, I am sizing up the federal government in the context of <a href="http://apievangelist.com/api-lifecycle/">my API lifecycle research</a>. I am using this to drive <a href="http://redhatapievents.com/dc">my talk this Thursday in Washington DC</a>, and the one I’m doing in August with the GSA. I’m looking to start with <a href="http://definitions.apievangelist.com/">API definitions</a> and try to quantify what I’m seeing at the federal government level, then work through each stop along the API lifecycle until I get to <a href="http://deprecation.apievangelist.com/">deprecation</a>. I’m going to use this research to help quantify the “state of the union” when it comes to APIs in the federal government. I want to better understand how APIs are being designed, deployed, managed, testing, monitored, and the other critical aspects of API operations I’m tracking on in the wider API industry. As I am doing this work I will be sizing up how well the federal government is doing when it comes to each area, but also identify where they can improve, and evangelists like me might be able to reach out and help each agency.</p>

<p>Look out for more federal government API stories as I’m working through my research from the last week in this area. I need to pull together a “state of the union” presentation for Thursday, but I’m looking to refresh my advice for federal agencies regarding where they should be investing more resources, or maybe where the private sector can step in and help carry the load. <a href="http://apievangelist.com/2013/10/17/shutdown-of-government-open-data-and-apis-is-not-government-services-business-as-usual/">I’m feeling like many of my older thoughts about changing government from the outside-in are extremely relevant during a Trump administration</a>. I want to focus on the good work that is continuing across federal agencies, but I want to renew my thoughts on what the private sector should be doing as well. I feel pretty strongly that the load around operating critical federal government APIs should be shared between the public and private sector, with the percentage of the load teetering back and forth depending on the type of administration we have. We should acknowledge that some times the private sector should carry larger portion of the load, and other times the federal government should carry a larger portion of the load–with less friction as things teeter back and forth.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/24/federal-government-apis-in-a-trump-administration/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/24/the-hack-education-gates-foundation-grant-data-has-an-api/">The Hack Education Gates Foundation Grant Data Has An API</a></h3>
        <span class="post-date">24 Jul 2017</span>
        <p><a href="http://hackeducation.com/2017/07/18/personalization"><img src="https://s3.amazonaws.com/kinlane-productions/hack-education/hack-education-personalize-learning-and-the-power-of-the-gates-foundation-to-shape-education-policy.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I have been helping my partner in crime Audrey Watters (<a href="https://twitter.com/audreywatters">@audreywatters</a>) adopt my approach to managing data project(s) using Google Sheets and Github, as part of her work on ed-tech funding. She is going through many of the leading companies, and foundations behind the funding of technology used across the education sector, and doing the hard work of connecting the dots behind how technology gets funded in this critical layer of our society.</p>

<p>I want Audrey (and others), to be self-sufficient when it comes to managing their data projects, which is why I’ve engineered it to use common services (Google Sheets, Github), with any code and supporting elements as self-contained as possible–something Github excels at when it comes to managing data, content, and code. <a href="https://github.com/Hack-Education-Data">While Audrey is going to town creating spreadsheets and repos</a>, I wanted to highlight a single area of her research into the grants that the Gates Foundation are handing out. She has worked hard to normalize data across many years (1998-2017) of PDF and HTML data into a single Google Sheet, <a href="https://github.com/Hack-Education-Data/gates-foundation/tree/master/_data">then she has published as individual YAML files which live on Github</a>–making her work forkable and reusable by anyone.</p>

<p>Once published, Audrey is the first person to fork the YAML, and <a href="http://hackeducation.com/2017/07/18/personalization">put to work in her storytelling around ed-tech funding</a>, but each of her project repos also come with <a href="http://funding.hackeducation.com/gates-foundation.html">an API for her research by default</a>. Well, ok, it isn’t a full-blown searchable API, but in addition to being able to get data in YAML format, she has a JSON API for each year of the Gates Foundation grants (ie. <a href="https://hack-education-data.github.io/gates-foundation/apis/2016/">2016</a>). Increasing the surface area when it comes to collaborating and building on top of her work, which can be forked using Github, or accessed via the machine readable YAML and JSON files.</p>

<p>While she is busy creating new Google Sheets and repos for other companies, I wanted to add one more tool to her toolbox, an APIs.json index for her project APIs. <a href="https://github.com/Hack-Education-Data/gates-foundation/blob/master/_data/apis.yaml">I added an apis.yaml index of all her APIs</a>, which I also published to <a href="https://hack-education-data.github.io/gates-foundation/apis.json">the root of her project as an apis.json version</a>. Now, in addition to publishing YAML files for all the data driving her research, and enabling it all to have a JSON API, there is a single index available to quickly browse an index of machine readable feeds for all her ed-tech funding research. Did I mention, all of this on Google Sheet and Github, which both are free to use, if you leverage Github as a publicly available data project? Making it a pretty dead simple way for ANYONE to manage open data projects, and tell data-driven stories on a budget.</p>

<p>If you want to see the scope of what she is up to, head over to her <a href="https://github.com/Hack-Education-Data">Hack Education Data Github organization</a>. You can follow the storytelling side of all of this on her work at <a href="http://hackeducation.com">Hack Education</a>. What is scary about all of this, is that she is only getting started in this work. In August, we are moving to New York City where she is beginning <a href="http://spencerfellows.org/">her Spencer Fellowship in Education Reporting at Columbia</a>, where she will be focused 100% on this research. I’m looking forward to seeing what she does with <a href="https://contrafabulists-lessons.github.io/google-sheet-to-github-website/">this type of data management using Google Sheets and Github</a>, and working to support here where I can, but more importantly learning from how she takes the tools I’ve given her and evolve them to support her unique brand of data-driven storytelling in the education space.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/24/the-hack-education-gates-foundation-grant-data-has-an-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/24/finding-things-i-want-to-write-about-when-apis-are-dumb/">Finding Things I Want To Write About When APIs Are Dumb</a></h3>
        <span class="post-date">24 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/training/gargoyle_light_dali.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>You ever wake up some days, and find yourself not caring about APIs, or much else in the realm of technology? No? Well, I do. Regularly. I find myself in this headspace on this fine Monday morning, and without a weeks worth of stories scheduled, it is a very bad place to be as the API Evangelist. Part of this problem is me–I am a pain in my ass. However, a another portion of it is just about staying motivated, engaged, and producing compelling (ha) content on a regular basis for the blog, and other projects I’m working on.</p>

<p>There are almost a hundred stories in my notebook and all of them seem really, really dumb to me this morning. I can’t seem to muster up the energy to take any of them and turn into even a three paragraph API blah blah blah story. It’s just words right? I should be able to do it. I churn out meaningless API words all the time, non-stop for the last seven years! I should be able to do it today. What is wrong with you man? C’mon, you should be able to just turn it on, and the words will flow. Not today. Like many days before I am going to need to trick myself into turning on the faucet.</p>

<p>The best place to start (for me) when I have lost my writing mojo, is to find a project I truly care about 100%. This is why I work on <a href="http://org.open.referral.adopta.agency/">the human services API project</a>, and look for ways that I can help my partner in crime Audrey Watters (@audreywatters) with <a href="http://hackeducation.com">her Hack Education work</a>, as she is always focused on the most critical area we face when it comes to our use of technology–education. Understanding how technology is helping, or hurting us when it comes to educating every human on earth is serious business, and something that might just help pull me from my writing funk. Let’s give it a shot.</p>

<p><a href="http://hackeducation.com/2017/07/18/personalization">Audrey is working on some pretty interesting ed-tech funding research</a> which <a href="http://funding.hackeducation.com/gates-foundation.html">uses my Google Sheet to Github approach to publishing data</a>–this is way more interesting than the other commercial API blah blah blah in my notebook. I will write about that. I’m already feeling like I’m on my way towards finding some writing mojo with this warm up post. Crafting a whiney warmup fluff piece like this helps me identify the things I actually care about, and maybe after writing about an important topic, demonstrating the good that I see in APIs will help me churn out the rest of my work this week, and keep the API Evangelist gears a turning–I have too my writing ahead to not have any mojo. If you have ever wondered how it is I’m able to churn out so much content, hopefully this post provides a little insight into how the sausage is made.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/24/finding-things-i-want-to-write-about-when-apis-are-dumb/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/24/first-handful-of-lessons-using-my-google-sheet-github-approach/">First Handful Of Lessons Using My Google Sheet Github Approach</a></h3>
        <span class="post-date">24 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/lessons/google-sheet-to-github.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>With my recent shift to using Google Sheets as my data backend for my research, and my continued usage of Github as my data project publishing platform, I started pushing out some new API related lessons. I wanted to begin formalizing my schema and process for this new approach to delivering lessons with some simple topics, so I got to work taking my 101, and history of APIs work, and converting them into a multi-step lesson.</p>

<p>Some of my initial 101 API lessons are:</p>

<ul>
  <li><strong>API 101</strong> (<a href="http://101.apievangelist.com/">Website</a>) (<a href="https://github.com/api-evangelist/101">Github Repo</a>) (<a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vRZ5VwkOard0nnwu8N_C-XMjAgOAElAMBrs7HHMKPtoGApmrau9yHPxVcNdiFfzLX6y7gKrPn12j6pr/pubhtml">Google Sheet</a>) - Just a general overview of what is API, targeting average user.</li>
  <li><strong>API Provider 101</strong> (<a href="http://101.consumer.apievangelist.com/">Website</a>) (<a href="https://github.com/api-evangelist/101-provider">Github Repo</a>) (<a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vRZ5VwkOard0nnwu8N_C-XMjAgOAElAMBrs7HHMKPtoGApmrau9yHPxVcNdiFfzLX6y7gKrPn12j6pr/pubhtml">Google Sheet</a>) - Working to evolve an opening pitch to would be API providers.</li>
  <li><strong>API Consumer 101</strong> (<a href="http://101.provider.apievangelist.com/">Website</a>) (<a href="https://github.com/api-evangelist/101-consumer">Github Repo</a>) (<a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vRZ5VwkOard0nnwu8N_C-XMjAgOAElAMBrs7HHMKPtoGApmrau9yHPxVcNdiFfzLX6y7gKrPn12j6pr/pubhtml">Google Sheet</a>) - Working to get better at providing information for API consumers.</li>
  <li><strong>The History of APIs</strong> (<a href="http://history.apievangelist.com/">Website</a>) (<a href="https://github.com/api-evangelist/history/tree/gh-pages">Github Repo</a>) (<a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vRZ5VwkOard0nnwu8N_C-XMjAgOAElAMBrs7HHMKPtoGApmrau9yHPxVcNdiFfzLX6y7gKrPn12j6pr/pubhtml">Google Sheet</a>) - Continuing to expand on my history of APIs story.</li>
</ul>

<p>I will keep working those 101 lessons. Editing, polishing, expanding, and as I found out with this revision–removing some elements of APIs that are fading away. While my 101 stories are always working to reach as wide as possible, my wider research is always based in two sides of the API coin, with information about providing APis, while also keep my API consumer hat on, and thinking about the needs of developers and integrators.</p>

<p>Now that I have the 101 lessons under way I wanted to focus on my API life cycle research, and work on creating a set of high level lessons for each of <a href="http://apievangelist.com/api-lifecycle/">the 80+ stops I track on along a modern API life cycle</a>. So I got to work on the lesson for API definitions, which I think is the most important stop along any API life cycle–one that actually crosses with every other line.</p>

<ul>
  <li><strong>Definitions</strong> (<a href="http://definitions.lesson.apievangelist.com/">Website</a>) (<a href="https://github.com/api-evangelist-api-provider-lessons/definitions">Github Repo</a>) (https://docs.google.com/spreadsheets/d/13WXRAA30QMzKXRu-dH8gr-UrAQlLLDAD9pBAmwUPIS4/edit#gid=0)</li>
</ul>

<p>After kicking off a lesson for my API life cycle that speaks to API providers, I wanted to shift gears at look at things from the API consumer side of things, and kick off a lesson for what I consider to be one of the more important APIs today–Twitter.</p>

<ul>
  <li><strong>Twitter API</strong> (<a href="http://twitter.lesson.apievangelist.com/">Website</a>) (<a href="https://github.com/api-evangelist-api-consumer-lessons/twitter">Github Repo</a>) (<a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vTgmzyXYB3CdPdvSL8xy9Eyg7lJ-Z0zCjuyktpwHo4Pdj1x_Rod_sxCl2WCQ27aw5WrgcAk-T28hzXE/pubhtml">Google Sheet</a>)</li>
</ul>

<p>Like my life cycle research I will continue creating lessons for each area of my API Stack research, where I am studying the approaches of specific API platforms, and the industries they are serving. Next I will be doing Facebook, Instagram, Reddit, and other APIs that are having a significant impact on our world. I’m looking to create lessons for all the top APIs that have a big brand recognition, and leverage them to help onboard a new wave of API curious folks.</p>

<p>My API industry research all lives as separate data driven Github repositories, using Google Sheets as the central data store. I edit all the stories published across these sites using Prose.io, but the data behind all my research live in a series of spreadsheets. This model has been extended to my API lessons, and I’ll be shifting my storytelling to leverage more of a structured approach in the future.  To help onboard folks with the concept I’ve also created a lesson, about how you create data-driven projects like this:</p>

<ul>
  <li><strong>Google Sheets To Github Website</strong> (<a href="https://contrafabulists-lessons.github.io/google-sheet-to-github-website/">Website</a>) (<a href="https://github.com/contrafabulists-lessons/google-sheet-to-github-website/">Github Repo</a>) (<a href="https://docs.google.com/spreadsheets/d/e/2PACX-1vSJoniCTFaaQuB7vB6mVkq6PMzQpQqxNomkKWcCpnZOsOwszGTbaiiLUP06wjsqDcSIueQgKsoVsyzT/pubhtml">Google Sheet</a>) - Walking through how you can use Google Sheets, and a Github Pages site to manage data driven websites.</li>
</ul>

<p>All of these lessons are works in progress. It is why they run on Github, so that I can incrementally evolve them. An essential part of this is getting feedback from folks on what they’d like to learn. I’m happy to open up and collaborate around any of these lessons using Google Sheets or Github–you just let me know which one is more your jam. I am collaborating with my partner in crime Audrey Watters (@audreywatters) using this format, and I’m finding it to be a great way to not just manage my world, but also create and manage new worlds with other people.</p>

<p>While each of the lessons use the same schema, structure, and process, I’m reserving the right to publish the lessons in different ways, experimenting with different variations in the layout. You’ll notice the Twitter and Google Sheets to Github Website lessons have a Github issues associated with each step, as I’m looking to stimulate conversations about what makes good (or bad) curriculum when it comes to learning about APIs and the platforms I’m building on. When it comes to my API lifecycle and stack work I am a little more opinionated and not looking for as much feedback at such a granular level, but because each lesson does living on Github, folks are still welcome to edit, and share their thoughts.</p>

<p>I have hundreds of lessons that I want to develop. The backlog is overwhelming. Now that I have the schema, base process, and first few stories published, I can just add to my daily workload and publish new stories, and evolve existing ones as I have time. If there are any lessons you’d like to see, either at the 101, provider, or consumer level let me know–feel free to hit me up through any channel. I’m going to be doing these lessons for my clients, either publishing them privately or publicly to Github repositories, and developing API life cycle curriculum in this way. I am also going to develop a paid version of the lesson, which will perform alongside my API industry guides, as simple, yet rich walk throughs of specific API industry concepts–for a small fee, to support what I do. Ok, lots of work ahead, but I’m super stoked to have these first few lessons out the door, even if there is a lot of polishing still to be done.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/24/first-handful-of-lessons-using-my-google-sheet-github-approach/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/21/structured-threat-information-expression-language/">Structured Threat Information Expression (STIX)</a></h3>
        <span class="post-date">21 Jul 2017</span>
        <p><a href="https://oasis-open.github.io/cti-documentation/"><img src="https://s3.amazonaws.com/kinlane-productions/stix/stix-logo.png" align="right" width="35%" style="padding: 15px;" /></a></p>
<p><a href="http://apievangelist.com/2017/07/10/opportunity-to-develop-a-threat-intelligence-apis-json/">I wrote about the opportunity around developing an aggregate threat information API</a>, and got some interest in both creating, as well as investing in some of the resulting products and services that would be derived from this security API work. As part of the feedback and interest on that post, I was pointed in the direction of the <a href="https://oasis-open.github.io/cti-documentation/">Structured Threat Information Expression (STIX)</a>, as one possible schema for definining and sharing the information I’m talking about. Here is a quick summary of STIX is from the website:</p>

<blockquote>
  <p>Structured Threat Information Expression (STIX™) is a language for describing cyber threat information in a standardized and structured manner to enable the exchange of cyber threat intelligence (CTI). STIX characterizes an extensive set of CTI to include indicators of adversary activity, as well as contextual information characterizing cyber adversary motivations, capabilities, and activities and best courses of action for defense and mitigation.</p>
</blockquote>

<p>I haven’t dug into STIX too much, so I’m not making recomendations on the value it brings to the table yet, but I want to make sure we take a look at any existing work that was already on the, and make sure we aren’t reinventing the wheel with any part of an aggregated threat information API. At first glance STIX looks like a pretty damn good start for a potential API schema, that speaks a common language, and is seeing adoption with other existing threat information storage and sharing providers.</p>

<p>I am adding STIX to my research into threat information sharing, and wider <a href="http://security.apievangelist.com/">API security research</a>. <a href="https://apievangelist.com/2017/06/21/i-am-working-with-elastic-beam-to-help-define-api-security/">I am currently diving deeper into API security thanks to investment from Elastic Beam, and I will be publishing a guide, as well as an API security white paper</a> as part of the work. I’m going to try and provide some intelligence to a group of folks who expressed interest in developing an aggregate threat information sharing API. I’m hoping to better flesh out my thoughts on how API security and threat information sharing might feed into an overall API industry ranking scoring system that would help us understand which APIs are secure and observable enough to warrant usage, or not.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/21/structured-threat-information-expression-language/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/21/requiring-all-platform-partners-use-the-API-so-there-is-an-application-defined/">Requiring ALL Platform Partners Use The API So There Is A Registered Application</a></h3>
        <span class="post-date">21 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/google/google-apps-connected.png" align="right" width="40%" style="padding: 15px;" /></p>
<p><a href="http://kinlane.com/2017/07/16/opting-in-out-to-sharing-our-data-through-partnerships/">I wrote a story about Twitter allowing users to check or uncheck a box regarding sharing data with select Twitter partners</a>. While I am happy to see this move from Twitter, I feel the concept of information sharing being simply being a checkbox is unacceptable. I wanted to make sure I praised Twitter in my last post, but I’d like to expand upon what I’d like to see from Twitter, as well as ALL other platforms that I depend on in my personal and professional life.</p>

<p>There is no reason that EVERY platform we depend on couldn’t require ALL partners to use their API, resulting in every single application of our data be registered as an official OAuth application. The technology is out there, and there is no reason it can’t be the default mode for operations. There just hasn’t been the need amongst platform providers, as as no significant demand from platform users. Even if you don’t get full access to delete and adjust the details of the integration and partnership, I’d still like to see companies, share as many details as they possibly can regarding any partner sharing relationships that involve my data.</p>

<p>OAuth is not the answer to all of the problems on this front, but it is the best solution we have right now, and we need to have more talk about how we can make it is more intuitive, informative, and usable by the average end-users, as well as 3rd party developers, and platform operators. API plus OAuth is the lowest cost, widely adopted, standards based approach to establishing a pipeline for ALL data, content, and algorithms operate within that gives a platform the access and control they desire, while opening up access to 3rd party integrators and application developers, and most importantly, it gives a voice to end-users–we just need to continue discussing how we can keep amplifying this voice.</p>

<p>To the folks who will DM, email, and Tweet at me after this story. I know it’s unrealistic and the platforms will never do business like this, but it is a future we could work towards. I want EVERY online service that I depend on to have an API. I want all of them to provide OAuth infrastructure to govern identify and access management for personally identifiable information. I want ALL platform partners to be required to use a platforms API, and register an application for any user who they are accessing data on behalf. I want all internal platform projects to also be registered as an application in my OAuth management area. Crazy talk? Well, Google does it for (most of) their internal applications, why can’t others? Platform apps, partner apps, and 3rd party apps all side by side.</p>

<p>The fact that this post will be viewed as crazy talk by most who work in the technology space demonstrates the imbalance that exists. The technology exists for doing this. Doing this would improve privacy and security. The only reason we do not do it is because the platforms, their partners and ivnestors are too worried about being this observable across operations. There is no reason why APIs plus OAuth application can’t be universal across ALL platforms online, with ALL partners being required to access personally identifiable information through an API, with end-uses at least involved in the conversaiton, if not given full control over whether or not personally identifiable information is shared, or not.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/21/requiring-all-platform-partners-use-the-API-so-there-is-an-application-defined/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/21/misconceptions-about-what-openapi-is-still-slowing-conversations/">Misconceptions About What OpenAPI Is(nt) Still Slowing Conversations</a></h3>
        <span class="post-date">21 Jul 2017</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/api_evangelist_site/blog/desert_dragon_light_dali.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’ve been pushing forward conversations around <a href="http://org.open.referral.adopta.agency/">my Human Services Data API (HSDA) work</a> lately, and hitting some friction with folks around the finer technical details of the API. I feel the friction around these API conversations could be streamlined with OpenAPI, but with most folks completely unaware of what OpenAPI is and does, there is friction. Then for the handful of folks who do know what OpenAPI is and does, I’m seeing the common misconceptions about what they think it is slowing the conversation.</p>

<p>Let’s start with the folks who are unaware of what OpenAPI is. I am seeing two main ways that human services data vendors and implementations have conversations about what they need: 1) documentation, and 2) code. The last wave of HSDA feedback was very much about receiving a PDF or Word documentation about what is expected of an application and an API behind it. The next wave of conversations I’m having are very much here are some code implementations to demonstrate what someone is looking to articulate. Both very expensive waves of articulating and sharing what is needed for the future, or to develop a shared understanding. My goal throughout these conversations is to help folks understand that there are other more efficient, low costs ways to articulate and share what is needed–OpenAPI.</p>

<p>Beyond the folks who are not OpenAPI aware, the second group of folks who see OpenAPI as a documentation tool, or code generation tool. Interestingly enough a vantage point that is not very far evolved beyond the first group. Once you know what you have, you document it using OpenAPI, or you generate some code samples from it. Relinquishing OpenAPI to a very downstream tool, something you bring in after all the decisions are made. I had someone say to me, that OpenAPI is great, but we need a way to be having a conversation about each specific API request, the details of the that request, with a tangible response to that request–which I responded, “that is OpenAPI”. Further showing that I have a significant amount of OpenAPI education ahead of me, before we can efficiently use it within these conversations about moving the industry specification forward. ;-(</p>

<p>The reasons OpenAPI (fka Swagger) began as documentation, then code generation, then exploded as a mocking, and API design solution was the natural progression of things. I feel like this progression reflects how people are also learning about the API design life cycle, and in turn the OpenAPI specification itself. This is why the name change from Swagger to OpenAPI was so damaging in my opinion, as it is further confusing, and setting back these conversation for many folks. No use living in the past though! I am just going to continue doing the hard work of helping folks understand what OpenAPI is, and how it can help facilitate conversations about what an API is, what an API should do, and how it can be delivering value for humans–before any code is actually written, helping make sure everyone is on the same page before moving to far down the road.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/21/misconceptions-about-what-openapi-is-still-slowing-conversations/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/20/charles-proxy-generated-har-to-openapi-using-api-transformer/">Charles Proxy Generated HAR To OpenAPI Using API Transformer</a></h3>
        <span class="post-date">20 Jul 2017</span>
        <p><a href="https://twitter.com/jpmonette/status/885545611906428928"><img src="https://s3.amazonaws.com/kinlane-productions/charles-to-openapi/har-conversion.png" align="right" width="25%" style="padding: 15px;" /></a></p>
<p>I was responding to Jean-Philippe M. (@jpmonette) tweet regarding <a href="https://apievangelist.com/2015/06/21/parsing-charles-proxy-exports-to-generate-swagger-definitions-while-also-linking-them-to-each-path/">whether or not I had moved forward my auto generation of OpenAPIs from traffic captured by Charles Proxy</a>. It is one of many features of my internal systems I have not gotten around to finishing, but thankfully he actually answered his own question, and found a better solution than even I had–using my friends over at API Transformer.</p>

<p>I had been exploring ways for speeding up the process of generating OpenAPI specs for the APIs that I’m reviewing, something that becomes very tedious when working with large APIs, as well as just profiling the sheer number of APIs I am looking profile as part of my work. I haven’t been profiling many APIs lately, but the approach Jean-Philippe M. came up is petty damn easy, leaving me feeling pretty silly that I hadn’t connected the dots myself.</p>

<p>Here is what you do. Fire up Charles Proxy:</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/charles-to-openapi/charles-session.png" align="center" width="80%" style="padding: 15px;" /></p>

<p>Then open up Postman, and make any API calls. Of course you could also proxy mobile application or website API calls through your Charles Proxy, but Postman is a great way to for a majority of the APIs I depend on.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/charles-to-openapi/postman-apis-how.png" align="center" width="80%" style="padding: 15px;" /></p>

<p>After you’ve made the calls to all the APIs you are looking to generate an OpenAPI for, save your Charles Proxy session as a .har file, which is the last option on the dropdown menu available while saving. Then you head over <a href="https://apimatic.io/transformer">to API Transformer</a> and upload your .har file, and select OpenAPI (Swagger) 2.0 as the output–push convert.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/charles-to-openapi/api-transformer-convert.png" align="center" width="80%" style="padding: 15px;" /></p>

<p><a href="https://apimatic.io/transformer">API Transformer will then push a fresh OpenAPI to your desktop</a>, or allow you to publish via a portal, and generate an SDK using <a href="https://apimatic.io">APIMATIC</a>. Automated (mostly) generation of OpenAPI definitions from API traffic you generate through your browser, Postman, Restlet Client, mobile application, or other tooling.</p>

<p>I have abandoned my internal systems, except for my stack of APIs, and depending mostly on 3rd party services like Charles Proxy, Postman, and API Transformer. So I won’t be moving forward the custom solution I had developed. However, there still might be benefit of automatically saving .har files to my Dropbox sync folder, then using the Dropbox API, and API Transformer API to automate the conversation of .har files to OpenAPI, and write them back to the appropriate Dropbox folder.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/20/charles-proxy-generated-har-to-openapi-using-api-transformer/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/20/100k-view-of-bot-space-from-the-api-evangelist-perspective/">100K View Of Bot Space From The API Evangelist Perspective</a></h3>
        <span class="post-date">20 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bots-satellites.jpg" align="right" width="40%" style="padding: 15px" /></p>
<p>I had a friend ask me for my thoughts on bots. It is a space I tend to rant about frequently, but isn’t an area I’m moving forward <a href="http://bots.apievangelist.com/">any meaningful research</a> in, but it does seem to keep coming up and refuses to ever go way. I think bots are a great example of yet another thing that us technologists get all worked up about and think is the future, but in reality, while there will only be a handful of viable use cases, and bots will cause more harm, than they ever will do any good, or fully enjoy a satisfactory mainstream adoption.</p>

<p>First, bots aren’t new. Second, bots are just automation. Sure, there will be some useful automation implementations, but more often than not, bots will wreak havoc and cause unnecessary noise. Conveniently though, no matter what happens, there will be money to be made deploying and defending against each wave of bot investment. Making bots is pretty representative of how technology is approached in today’s online environment. Lot’s of tech. Lot’s of investment. Regular waves. Not a lot of good sense.</p>

<p><strong>Top Bot Platforms</strong><br />
Ok, where can you deploy and find bots today? These are the dominant platforms where I am seeing bots emerge:</p>

<ul>
  <li>Twitter - Building bots on the public social media platform using their API.</li>
  <li>Facebook - Building Facebook messenger bots to unleash on the Facebook Graph.</li>
  <li>Slack - Building more business and productivity focused bots on Slack.</li>
</ul>

<p>There are other platforms like Telegram, and <a href="http://bots.apievangelist.com/2017/07/05/a-bot-that-does-useful-things-for-me/">folks developing interesting Github bots</a>, but these three platforms dominate the conversation when it comes to bots in 2017. Each platform brings it’s own tone when it comes to what bots are capable of doing, and who is developing the bots. Another important thing to note across these platforms is that Slack is really the only one working to own the bot conversation on their platform, while on Facebook and Twitter allow the developer community to own the conversation about exactly what are bots.</p>

<p><strong>Conversational Interfaces</strong><br />
When it comes to bots, and automation, I’m always left thinking more broadly about other conversational interfaces and Siri, or more specifically Amazon Alexa. The Amazon Alexa platform operates on a similar level to Slack when it comes to providing developers with a framework, and tooling to define and deliver conversational interfaces. Voice just happens to be the interface for Amazon, where the chat and messaging window is the interface for Slack, as well as Twitter and Facebook. Alexa is a bot, consuming API resources alongside the other popular definitions of what is a bot on messaging and social channels–expanding the surface area for how bots are deployed and engaged with in 2017.</p>

<p><strong>Bots And APIs</strong><br />
To me, bots are just another client application for APIs. In early days APIs were about syndicating content on the web, then they were used to deliver resources to mobile applications, and now they are delivering content, data, and increasingly algorithms to devices, conversational interfaces, signage, automobiles, home appliances, and on and on. When any user asks a bot a question, the bot is the making one or many API calls to get the sports statistic, news and weather report, or maybe the purchase of a product. There will be many useful scenarios in which APIs will be able to deliver critical resources to conversational interfaces, but like many other client implementations, there will be many, many bad examples along the way.</p>

<p><strong>Algorithmic Shift</strong><br />
In 2017, the API space is shifting gears from primarily data and content based APIs, to a more algorithmic focus. Artificial intelligence, machine learning, deep learning, cognitive, and other algorithmically fueled interfaces are emerging, wrapped in APIs, intent on delivering “smart” resources to the web, mobile, and conversational interfaces. We will continue to see an overwhelming amount of discussion at the intersection of bots, API, and AI in coming years, with very little actual results delivered–regardless, there will be lots of money to be made by a few, along the way. Algorithms will play a central role in ensuring the “intelligence” behind bots stay a black box, and sufficiently pass as at least magic, if not entirely passed off as comparable to human intelligence.</p>

<p><strong>Where Will The Bot Money Be?</strong><br />
When it comes to making money with bots, there will only be a couple value creation centers. First, the platforms where bots operate will do well (most of them)–I am not sure they all will generate revenue directly from bots, but they will ensure bots are driving value that is in alignment platform revenue goals. Next, defensive bot solutions will generate sufficient amounts of revenue identifying and protecting businesses, institutions, and government agencies from the bot threat. Beyond that, venture capital folks will also do well investing in both the bot disruption, and bot defensive layers of the conversation–although VCs who aren’t directly involved with bot investment, will continue to be duped by fake users, customers, and other bot generated valuations. Leaving bot blemishes on their portfolios.</p>

<p><strong>Who Will Lose With Bots?</strong><br />
Ultimately it is the rest of us who will come out with on the losing side of these “conversations”. Our already very noisy worlds will get even noisier, with more bot chatter in the channels we currently depend on daily. The number of humans we engage with on a daily basis will decrease, and the number of frustrating “conversation” we find ourselves stuck in will increase. Everything fake will continue inflate, and find new ways to morph, duping many of us in new and exciting ways. Markets will be noisy, emotional, and always artificially inflated. Elections will continue be just an an outright bot assault on voters, leaving us exhausted, numb, and pretty moldable by those who have the biggest bot arsenals.</p>

<p><strong>Some Final Thoughts On Bots</strong><br />
I am continuing to see interesting bots emerge on Twitter, Facebook, Slack, and other channels I depend on like Github. I have no doubts that bots and conversational solutions will continue to grow, evolve, and result in a viable ecosystem of users, service providers, and investors. However, I predict it will be very difficult for bots to ever reach an acceptable mainstream status. As we’ve seen in every important conversation we are having online today, some of most badly behaved amongst us always seem to dominate any online conversation. Why is this? Bots. We will see this play out in almost every business sector.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/20/100k-view-of-bot-space-from-the-api-evangelist-perspective/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/20/managing-platform-terms-of-service-in-a-site-policy-repository/">Managing Platform Terms of Service In A Site Policy Repository</a></h3>
        <span class="post-date">20 Jul 2017</span>
        <p><a href="https://github.com/blog/2393-open-sourcing-our-site-policies-and-new-changes-to-our-terms-of-service"><img src="https://s3.amazonaws.com/kinlane-productions/github/github-site-policy.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p><a href="https://github.com/blog/2393-open-sourcing-our-site-policies-and-new-changes-to-our-terms-of-service">Github is releasing an update to their platform Terms of Service and Corporate Terms of Service</a>. Guess what platform their are using to manage the evolution, and release of their terms of service? Github of course! They are soliciting feedback, along with clarifications and improvements to their terms of service, with an emphasis on helping making things more readable! #nice</p>

<p>Github has provided a deadline for everyone to submit comments by the end of the month, then they’ll spend about a week going through the comments before making any changes. It provides a pretty useful way for any platform to manage their terms of service in a way that gives the community a voice, and provides some observability into the process for everyone else who might not feel confident enough to chime in on the process. This can go a long way towards building trust with the community, even if they don’t directly participate in the process.</p>

<p>Managing terms of service using Github makes sense for all providers, not just Github. It provides an open, transparent, and participatory way to move forward one of the most important documents that is governing API consumption. It is logical that the drafting, publishing, and evolution of platform terms be done out in the open, where the community can watch and participate. Pushing forward the design of the legal document in sync with the design, deployment, management, SDKs and other aspects of API operations. Bringing the legal side of things out of the shadows, and making it part of the conversation within the community.</p>

<p>Eventually, I’d like to see the terms of service, privacy policies, service level agreements, and other legal documents that govern API operations managed and available on Github like this. It gives the wider API community the chance to play a more significant role in hammering out the legal side of API operations, ensuring this are easier to follow and understand, and maybe even standardized across APIs. Who knows, maybe some day terms of service, privacy policies, and service level agreements will all be available in plain language, as well as machine readable YAML, shifting how the API contract will scale.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/20/managing-platform-terms-of-service-in-a-site-policy-repository/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/20/the-plivo-support-portal-knowledge-base/">The Plivo Support Portal And Knowledge Base</a></h3>
        <span class="post-date">20 Jul 2017</span>
        <p><a href="https://support.plivo.com"><img src="https://s3.amazonaws.com/kinlane-productions/plivo/plivo-support-knowledge-base.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I’m always watching out for how existing API providers are shifting up their support strategies in their communities as part of my work. This means staying into tune with their communications, which includes processing their email newsletters and developer updates. Staying aware of what is actually working, and what is not working, based upon active API service providers who are finding ways to make it all work.</p>

<p>Plivo opted out to phase out direct emails at the end of the month, and pushing developers to use <a href="https://support.plivo.com">the Plivo support portal</a>, and the ticketing system. The support portal provides a knowledge base which provides a base of self-service support before any developer actually uses the support ticketing system to:</p>

<ul>
  <li>Create, manage, respond to and check the status of your support ticket(s)</li>
  <li>Select improved ticket categories for more efficient ticket routing and faster resolution</li>
  <li>Receive resolution suggestions from our knowledge base before you submit a ticket to help decrease resolution time</li>
</ul>

<p>Email only support isn’t always the most optimal way of handling support, and using a ticketing system definitely provides a nice trail to follow for both sides of the conversations. The central ticketing system also provides a nice source of content to feed into the self-service support knowledge base, keeping self-service support in sync with direct support activity.</p>

<p>I’m going to continue to track on which API providers offer a ticketing solution, as well as a knowledge base. I’m feeling like these are what I’m going to recommend to new API providers as what I consider to be default support building blocks that EVERY API platform should be starting with, covering the self-service and direct support requirements of a platform. I’m going to start pushing 1-3 support solutions like ZenDesk, also giving API providers some options when it comes to quickly delivering adequate support for their platforms.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/20/the-plivo-support-portal-knowledge-base/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/19/more-investment-in-api-security/">More Investment In API Security</a></h3>
        <span class="post-date">19 Jul 2017</span>
        <p><a href="https://www.elasticbeam.com/"><img src="https://s3.amazonaws.com/kinlane-productions/elastic-beam/elasticbeam-security.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I’m getting some investment from <a href="https://www.elasticbeam.com/">ElasticBeam</a> to turn up the volume on my <a href="http://security.apievangelist.com/">API security research</a>, so I will be telling more stories on the subject, and publishing an industry guide, as well as a white paper in coming weeks. I want my API security to become a first class area of my API research, along side definitions, design, deployment, management, monitoring, testing, and performance.</p>

<p><a href="https://www.owasp.org/index.php/OWASP_API_Security_Project">Much of my API security research is built on top of OWASP’s hard work</a>, but honestly I haven’t gotten very far along in it. I’ve managed to curated a handful of companies who I’ve come across in my research, but haven’t had time to dive in deeper, or fully process all the news I’ve curated there. It takes time to stay in tune with what companies are up to, and I’m thankful for ElasticBeam’s investment to help me pay the bills while I’m heads down doing this work.</p>

<p>I am hoping that my API security research will also help encourage you to invest more into API security. As I do with my other partners, I will find ways of weaving ElasticBeam into the conversation, but my stories, guides, and white papers will be about the wider space–which Elastic Beam fits in. I’m hoping they’ll <a href="http://apis.how/8nlsropidv">compliment Runscope as my partner when it comes to monitoring, testing, and performance</a> (see how I did that, I worked Runscope in too), adding the security dimension to these critical layers of operating a reliable API.</p>

<p>One thing that attracted me to conversations with ElasticBeam was that they were developing a solution that could augment existing API management solutions like 3Scale and Amazon Web Services. I’ll have a talk with the team about integrating with <a href="http://apis.how/zflfesymzk">Tyk</a>, <a href="http://apis.how/bgdteovduo">DreamFactory</a>, and <a href="http://apis.how/5ytnitnakm">Restlet</a>–my other partners. Damn I’m good. I got them all in here! Seriously though, I’m thankful for these partners investing in what I do, and helping me tell more stories on the blog, and produce more guides and papers.</p>

<p>I feel like 3Scale has long represented what I’ve been doing over seven years–a focus on API management. Restlet, DreamFactory, and Tyk represent the maturing and evolution of this layer. While Runscope really reflects the awareness that has been generated at the API management layer, but evolving to serve not just API providers, but also API consumers. I feel like ElasticBeam reflects the next critical piece of the puzzle, moving the API security conversation beyond the authentication and rate limiting of API management, or limiting the known threats, and making it about identifying the unknown threats our API infrastructure faces today.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/19/more-investment-in-api-security/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/19/the-most-imortant-aspect-of-the-api-discussions-is-leaning-to-think-outside-our-boxes/">The Most Important Aspect Of The API Discussion Is Learning To Think Outside Our Boxes</a></h3>
        <span class="post-date">19 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-box.png" width="25%" align="right" style="padding: 10px;" /></p>
<p>There are many good things to come out of doing APIs properly. Unfortunately there are also many bad things that can come out of doing APIs badly, or with misaligned expectations. It is easy to focus on the direct benefits of doing APIs like making data resources available to partners, or maybe developing a mobile application. I prefer looking for the more indirect benefits, which are more human, more than they are ever technical.</p>

<p>As I work with different groups on a variety of API definitions and strategies, one very significant part of the process I see, is people being forced to think outside their box. APIs are all about engaging around data, content, and algorithms on the web, with 3rd parties that operate outside your box. You are forced to lookup, and outward a bit. Not everyone I engage with is fully equipped to do this, for a variety of reasons, but overall the API process does make folks just a little more critical than they do with even their websites.</p>

<p>The web has come with a number of affordances. Those same affordances aren’t always present in API discussions forcing folks to have more conversations around why we are doing APIs (an answer shouldn’t always be yes), and discussing the finer details not just storing your data, and managing your schema, but doing in a way that will play nicely with other external systems. You may be doing things one way internally, and it might even be working for you, but it is something that can only get better with each outside partner, or consumer you are exposed to along your journey. Even with all of the internal politics I encounter in my API conversations, the API process always leaves me enjoying almost any outcome.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/19/the-most-imortant-aspect-of-the-api-discussions-is-leaning-to-think-outside-our-boxes/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/19/does-your-platform-have-an-integrations-page/">Does Your Platform Have An Integrations Page?</a></h3>
        <span class="post-date">19 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/airtable/airtable-integrations-page.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’m continuing to come across more dedicated integration pages for the API platforms I’m test driving, and keeping an eye on. <a href="https://airtable.com/integrations">This time it is out of spreadsheet and database hybrid AirTable, that allows you to easily deploy an API complete with a portal, with a pretty robust integrations page for their platform</a>. Airtable’s dedicated integrations page is made easier since they use Zapier, which helps them aggregate over 750+ APIs for possible integration.</p>

<p>Airtable is pretty slick all by itself, but once you start wiring it up to some of the other API driven platforms we depend on, <a href="https://apievangelist.com/2017/07/11/each-airtable-datastore-comes-with-complete-api-and-developer-portal/">it becomes a pretty powerful tool for data aggregation, and then publishing as an API</a>. I don’t understand why a Zapier-driven API integrations page isn’t default for every API platform out there. API consumption today isn’t just about deploying web or mobile applications, it is about moving data and content around the web–making sure it is where we need it, when we need it.</p>

<p>I’m playing with different variations of the API integrations page lately. <a href="https://apievangelist.com/2017/07/12/a-zapier-advocate-and-dedicated-api-resources-page-for-your-company/">I’m exploring the idea of how I can encourage some higher education folks I know, and government open data folks I know to be Zapier advocates within their organizations, and publish a static integrations page, showing the integrations solutions available around the platforms they depend on</a>. Dedicated integration pages help API developers understand the potential of any API, and they help non-developers also understand the potential, but in a way they can easily put into action to solve problems in their world. I’m going to keep beating the API integration page drum, and <a href="https://apievangelist.com/2017/04/26/zapier-was-pretty-savvy-in-their-approach-to-launching-their-partner-api/">now that Zapier has their partner API</a> you will also hear me talking about Zapier a lot more.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/19/does-your-platform-have-an-integrations-page/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/19/containerized-microservices-monitoring-driving-api-infrastructure-visualizations/">Containerized Microservices Monitoring Driving API Infrastructure Visualizations</a></h3>
        <span class="post-date">19 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/netsil/1-P8w_-2-oCz0QfV6OENawJQ.png" align="right" width="40%" style="padding: 15px" /></p>
<p>While I track on what is going on with visualizations generated from data, I haven’t seen much when it comes to API driven visualizations, or specifically visualization about API infrastructure, that is new and interesting. <a href="https://blog.netsil.com/kubernetes-monitoring-needs-maps-6ef673d840c7">This week I came across an interesting example in a post from Netsil about mapping microservices so that you can monitor them</a>. They are a pretty basic visualization of each database, API, and DNS element for your stack, but it does provide solid example of visualizing not just the deployment of database and API resources, but also DNS, and other protocols in your stack.</p>

<p>Netsil microservices visualization is focused on monitoring, but I can see this type of visualization also being applied to design, deployment, management, logging, testing, and any other stop along the API lifecycle. I can see API lifecycle visualization tooling like this becoming more common place, and play more of a role in making API infrastructure more observable. Visualizations are an important of the storytelling around API operations that moves things from just IT and dev team monitoring, making it more observable by all stakeholders.</p>

<p>I’m glad to see service providers moving the needle with helping visualize API infrastructure. I’d like to see more embeddable solutions deployed to Github emerge as part of API life cycle monitoring. I’d like to see what full life cycle solutions are possible when it comes to my partners like deployment visualizations from <a href="http://apis.how/zflfesymzk">Tyk</a> and <a href="http://apis.how/bgdteovduo">Dreamfactory APIs</a>, and <a href="https://s3.amazonaws.com/kinlane-productions/partners/3scale-red-hat-logo.png">management visualizations with 3Scale APIs</a>, and monitoring and testing visualizations using <a href="http://apis.how/8nlsropidv">Runscope</a>. I’ll play around with pulling data from these provides, and publishing to Github as YAML, which I can then easily make available as JSON or CSV for use in some basic visualizations.</p>

<p>If you think about it, thee really should be a wealth of open source dashboard visualizations that could be embedded on any public or private Github repository, for every API service provider out there. API providers should be able to easily map out their API infrastructure, using any of the API service providers they are using already using to operate their APIs. Think of some of the embeddable API status pages we see out there already, and what Netsil is offering for mapping out infrastructure, but something for ever stop along the API life cycle, helping deliver visualizations of API infrastructure no matter which stop you find yourself at.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/19/containerized-microservices-monitoring-driving-api-infrastructure-visualizations/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/18/one-api-development-partner-every-api-provider-should-have/">One API Development Partner Every API Provider Should Have</a></h3>
        <span class="post-date">18 Jul 2017</span>
        <p><a href="https://zapier.com/engineering/zapier-issues/"><img src="https://s3.amazonaws.com/kinlane-productions/zapier/4b21d50900beffcc0bcfa2c09bcc7bfe.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>Yet another reason to be making sure Zapier is part of your API operations–issue management. Zapier is now providing an important window into how people are integrating with your API(s)–now any public API connected to Zapier can see filtered, categorized feedback from their users with <a href="https://zapier.com/engineering/zapier-issues/">Zapier Issues</a>, and use that information to improve upon their APIs and integrations. This is the biggest movement I’ve seen in <a href="http://issues.apievangelist.com/">my API issues research</a> since I first started doing it on April of 2016.</p>

<p><a href="https://zapier.com/engineering/zapier-issues/">Zapier Issues</a> doesn’t just provide you with a look at the issues that arise within API integrations (the bad news), it also provides you with a feedback look where you can engage with Zapier users who have integrated with your API, and hear feature requests (the good news), and other road map influencing suggestions. Zapier sees, “thousands of app combinations and complex workflows from more than 1.5 million people—and we want to give you more insight into how your best customers use your app on Zapier.”</p>

<p>It is another pretty big reason that ALL API providers should be baking Zapier into their platforms. Not only will you be opening up API consumption to the average business user, you can now get feedback from them, and leverage the wisdom Zapier has acquired integrating with over 750 APIs. As an API provider you should be jumping at this opportunity to get this type of feedback on your API resources. Helping you make sure your APIs more usable, stable, reliable, and providing the solutions that actual business users are needing to solve the problems they encounter in their daily lives.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/18/one-api-development-partner-every-api-provider-should-have/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/18/specialized-collections-of-machine-learning-apis-could-be-interesting/">Specialized Collections Of Machine Learning APIs Could Be Interesting</a></h3>
        <span class="post-date">18 Jul 2017</span>
        <p><a href="https://algorithmia.com/enterprise"><img src="https://s3.amazonaws.com/kinlane-productions/algorithmia/algorithmia-enterprise.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I was learning more <a href="https://algorithmia.com/enterprise">about CODEX, from Algorithmia, their enterprise platform for deploying machine learning API collections on premise or in the cloud</a>. Algorithmia is taking the platform in which their algorithmic marketplace is deployed on and making it so you can deploy it anywhere. I feel like this is where the algorithmic-centered API deployment is heading, potentially creating some very interesting, and hopefully specialized collections of machine learning APIs.</p>

<p>I talked about how the economics of what Algorithmia is doing interests me. I see the potential when it comes to supporting machine learning APIs that service an image or video processing pipeline–something I’ve enjoyed thinking about with my drone prototype. Drone is just one example of how specialized collections of machine learning APIs could become pretty valuable when they are deployed exactly where they are needed, either on-premise or in any of the top cloud platforms.</p>

<p>Machine learning marketplaces operated by the cloud giants will ultimately do fine because of their scale, but I think where the best action will be at is delivering curated, specialized machine learning models, tailored to exactly what people need, right where they need them–no searching necessary. I think recent moves by Google to put TensorFlow on mobile phones, and Apple making similar moves show signs of a future where our machine learning APIs are portable, operating on-premise, on-device, and on-network.</p>

<p>I see Algorithmia having two significant advantages right now. 1) they can deploy their marketplace anywhere, and 2) they have the economics, as well as the scaling of it figured out. Allowing for specialized collections of machine learning APIs to have the metering, and revenue generation engines built into them. Imagine a future where you can deploy and machine learning and algorithmic API stack within any company or institution, or the factory floor in an industrial setting, and out in the field in an agricultural or mining situation–processing environmental data, images, or video.</p>

<p>Exploring the possibilities with real world use cases of machine learning is something I enjoy doing. I’m thinking I will expand on my drone prototype and brainstorm other interesting use cases beyond just my drone video. Thinking about how I can develop prototype machine learning API collections, that could be used for a variety my content, data, image, or video side-projects. I think when it comes to machine learning I’m more interested in specialty collections over the general machine learning hype I”m seeing peddled in the mainstream right now.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/18/specialized-collections-of-machine-learning-apis-could-be-interesting/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/18/diagramming-the-components-of-api-observability/">Diagramming The Components Of API Observability</a></h3>
        <span class="post-date">18 Jul 2017</span>
        <p><a href="https://apievangelist.com/2014/03/17/politics-of-apis/">I created a diagram of the politics of APIs sometime ago that has really held true for me</a>, and is something I’ve continue to reference as part of my storytelling. I wanted to do a similar thing to help me evolve my notion of <a href="https://apievangelist.com/2016/10/25/thinking-about-an-api-observability-stack/">API observability</a>. Like the politics of APIs, observability overlaps many areas of <a href="http://apievangelist.com/api-lifecycle/">my API life cycle research</a>. Also like the politics of APIs, observability involves many technical, business, and legal aspects of operating a platform online today.</p>

<p>Here is my first draft of a Venn diagram beginning to articulate what I see as the components of API observability:</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/observable/api-observability-venn.png" width="75%" style="padding: 15px;" align="center" /></p>

<p>The majority of the API observability conversation in the API space currently centers around logging, monitoring, and performance–driven by internal motivations, but done in a way that is very public. I’m looking to push forward the notion of API observability to transcend the technical, and address the other operational, industry, and even regulatory concerns that will help bring observability to everyone’s attention.</p>

<p>I do not think we should always be doing API, AI, ML and the other tech buzzwords out there if we do not have to–saying no to technology can be done. In the other cases where the answer is yes, we should be doing API, AI, and ML in an observable way. This is my core philosophy. The data, content, algorithms, and networks we are exposing using APIs, and using across web, mobile, device, and network applications should be observable by internal groups, as well as partners, and public stakeholders as it makes sense. There will be industry, community, and regulatory benefits for sectors that see observability as a positive thing, and go beyond just the technical side of observability, and work to be more observable in all the areas I’ve highlight above.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/18/diagramming-the-components-of-api-observability/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/18/http-status-codes-are-an-essential-part-of-api-design-and-deployment/">HTTP Status Codes Are An Essential Part Of API Design And Deployment</a></h3>
        <span class="post-date">18 Jul 2017</span>
        <p><a href="https://www.runscope.com/"><img src="https://s3.amazonaws.com/kinlane-productions/api-evangelist/runscope/runscope-200-ok.jpeg" alt="" align="right" width="40%" /></a>
It takes a lot of work provide a reliable API that people can depend on. Something your consumers can trust, and will provide them with consistent, stable, meaningful, and expected behavior. There are a lot of affordances built into the web, allowing us humans to get around, and make sense of the ocean of information on the web today. These affordances aren’t always present with APIs, and we need to communicate with our consumers through the design of our API at every turn.</p>

<p>One area I see IT and developer groups often overlook when it comes to API design and deployment are <a href="https://en.wikipedia.org/wiki/List_of_HTTP_status_codes">HTTP Status Codes</a>. That standardized list of meaningful responses that come back with every web and API request:</p>

<ul>
  <li><strong>1xx Informational</strong> - An informational response indicates that the request was received and understood. It is issued on a provisional basis while request processing continues.</li>
  <li><strong>2xx Success</strong> - This class of status codes indicates the action requested by the client was received, understood, accepted, and processed successfully.</li>
  <li><strong>3xx Redirection</strong> - This class of status code indicates the client must take additional action to complete the request. Many of these status codes are used in URL redirection.</li>
  <li><strong>4xx Client errors</strong> - This class of status code is intended for situations in which the client seems to have errored.</li>
  <li><strong>5xx Server error</strong> - The server failed to fulfill an apparently valid request.</li>
</ul>

<p>Without HTTP Status codes, application won’t every really know if their API request was successful or not, and even if an application can tell there was a failure, it will never understand why. HTTP Status Codes are fundamental to the web working with browsers, and apis working with applications. HTTP Status Codes should never be left on the API development workbench, and API providers should always go beyond just 200 and 500 for every API implementation. Without them, NO API platform will ever scale, and support any number of external integrations and applications.</p>

<p>The most important example I have of the importance of HTTP Status Codes I have in my API developers toolbox is when <a href="http://apievangelist.com/2012/06/02/tracking-federal-agencies-progress-on-api-deployment/">I was working to assist federal government agencies in becoming compliant with the White House’s order for all federal agencies to publish a machine readable index of their public data inventory of their agency website</a>. As agencies got to work publishing JSON and XML (an API) of their data inventory, I got to work building an application that would monitor their progress, indexing the available inventory, and providing a dashboard the the GSA and OMB could use to follow their progress (or lack of).</p>

<p>I would monitor the dashboard in real time, but weekly I would also go through many of the top level cabinet agencies, and some of the more prominent sub agencies, and see if there was a page available in my browser. There were numerous agencies who I found had published their machine readable public data inventory, but had returned a variety of HTTP status codes other than 200-resulting in my monitoring application to consider the agency not compliant. <a href="http://kinlane.com/2013/11/06/knowing-your-http-status-codes-in-federal-government/">I wrote several stories about HTTP Status Codes</a>, in which the GSA, and White House groups circulated with agencies, but ultimately I’d say this stumbling block was one of the main reasons that cause this federated public data API project to stumble early on, and never gain proper momentum–a HUGE loss to an open and more observable federal government. ;-(</p>

<p>HTTP Status Codes aren’t just a nice to have thing when it comes to APIs, they are essential. Without HTTP  Status Codes each application will deliver unreliable results, and aggregate or federated solutions that are looking to consume many APIs will become much more difficult and costly to develop. Make sure you prioritize HTTP Status Codes as part of your API design and deployment process. At the very least make sure all five layers of HTTP Status Codes are present in your release. You can always get more precise and meaningful with specific series HTTP status codes later on, but ALL APIs should be employing all five layers of HTTP Status Codes by default, to prevent friction and instability in every application that builds on top of your APIs.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/18/http-status-codes-are-an-essential-part-of-api-design-and-deployment/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/17/writing-api-stories-that-speak-to-my-audience-but-also-influences-their-view-of-technology/">Writing API Stories That Speak To But Also Influences Their View Of Technology</a></h3>
        <span class="post-date">17 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-storytelling.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I know that some of my friends who follow API Evangelist shake their heads when I talk about API business models, partner programs, and many of the business sides of API operations. Much of my work will have an almost delusional attraction towards the concept of an API. Heavily doused in a belief in technology as a solution. This isn’t accidental. This is API Evangelist. A persona I have developed to help me make a living, and help influence where we go (or don’t go) with technology.</p>

<p>I am delusional enough to think I can influence change in how the world uses technology. I’m borderline megalomaniac, but there really is not sufficient ego to get me quite all the way there. While still very, very, very minor, I feel I have influenced where technology has flowed over my seven years as the API Evangelist. Even if it just slowing the speed (seconds) at which the machines turn on us, and kills us all. If nothing else, I know there are few folks out there who I have touched, and shaped how they see, use, and allow technology in their lives (cause they told me so).</p>

<p>Through my storytelling on API Evangelist, I am always looking for the next convert–even if it takes years and hundreds of stories. A significant portion of this outreach involves telling stories that reach my intended audience–usually startups, business, institutional, and government agency workers and influencers. To reach them I need to tell stories that speak to them, and feed their current goals around finding success in their startup, or their role within businesses, institutions, and government agencies. With this in mind, I am always trying to bend my stories in their direction, talking about topics that they’ll care about, and tune into.</p>

<p>Once I have their attention, I will work on them in other ways. I’ll help them think about their business model, but also help them understand transparency and communication when it comes to executing this model. I will help them understand the best practices for managing an API using open source solutions like Tyk or Dreamfactory, and the leading approaches to using Runscope for monitoring and testing, while also encouraging them to me more <a href="https://apievangelist.com/2016/10/25/thinking-about-an-api-observability-stack/">observable</a> with these practices. Making sure companies tell stories about what they are doing, and how they are doing it all–the good and bad.</p>

<p>I’m always working to build bridges to folks who might not see this whole API thing like I do. I’d  say that many of these bridges will never get fully walked across by my target audience, but when someone does, and my stories influence the way they see or use technology even a little bit–mission accomplished. I’m constantly testing new ways or reaching out, speaking in the language of my target audience (without selling out), using trendy terms like microservices, devops, and serverless, but this isn’t just about following the latest fad. It is meant to capture your attention, build some trust, and then when it matters I can share some information about what really matters in all of this–in hopes of influencing how you see technology, and how it can be used a little more sensibly, securely, or maybe not even at all. ;-)</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/17/writing-api-stories-that-speak-to-my-audience-but-also-influences-their-view-of-technology/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/17/bot-observability-for-every-platform/">Bot Observability For Every Platform</a></h3>
        <span class="post-date">17 Jul 2017</span>
        <p><a href="http://bots.apievangelist.com/"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-bot-showcase.png" align="right" width="45%" style="padding: 15px" /></a></p>
<p><a href="http://bots.apievangelist.com/">I lightly keep an eye on the world of bots, as APIs are used to create them</a>. In my work I see a lot of noise about bots usually in two main camps: 1) pro-bot - bots are the future, and 2) anti-bot - they are one of the biggest threats we face on the web. This is a magical marketing creating formula, which allows you to sell products to both sides of the equation, making money off of bot creation, as well as bot identification and defense–it is beautiful (if you live by disruption).</p>

<p>From my vantage point, I’m wondering why platforms do not provide more bot observability as a part of platform operations. There shouldn’t be services that tell us which accounts are bots, the platform should tell us by default, which users are real and which are automated (you know you know). Platforms should embrace automation and providing services and tooling to assist in their operation, which includes actual definitions of what is acceptable, and what unacceptable bot behavior. Then actually policing this behavior, and being observable in your actions around bot management and enforcement.</p>

<p>It feels like this is just another layer of technology that is being bastardized by the money that flow around technology so easily. Investment in lots of silly useless bots. Investment in bot armies that inflate customer numbers, advertising, and other ways of generating attention (to get investment), and generate revenue. It feels like Slack is the only leading bot platform that has fully embraced the bot conversation. Facebook and Twitter lightly reference the possibilities, and have made slight motions when it comes to managing the realities of bots, but when you Google “Twitter Bots” or “Facebook Bots”, neither of them dominate the conversation around what is happening–which very telling around how they view the world of bots.</p>

<p>Slack has <a href="https://openreferral.slack.com/apps/category/At0MQP5BEF-bots">a formal bots directory</a>, and has <a href="https://api.slack.com/bot-users">defined the notion of a bot user</a>, separating them from users–setting an example for bot developers to disclose who is bot, and who is not. They talk about <a href="https://medium.com/slack-developer-blog/hard-questions-about-bot-ethics-4f80797e34f0">bot ethics</a>, and <a href="https://medium.com/slack-developer-blog/the-bot-rulebook-a442d9fb21cb">rules for building bots</a>, and <a href="https://slackhq.com/a-beginner-s-guide-to-your-first-bot-97e5b0b7843d">do a lot of storytelling about their vision for bots</a>. Providing a pretty strong start towards getting a handle on the explosion of bots on their platform–taking the bull by the horns, owning the conversation, and setting the tone.</p>

<p>I’d say that Slack has a clearer business model for bots–not that people are actually going to pay for your bot (they aren’t), but a model is present. You can some smell of revenue strategies on Facebook, but it just feels like all roads lead to Facebook, and advertising partners there. I’d say Twitter has no notion of a botlike business model for developers. This doesn’t mean that Facebook and Twitter bots don’t generate revenue for folks targeting Facebook and Twitter, or play a role in influencing how money flows when it comes to eyeballs and clicks. Indirectly, Twitter and Facebook bots are making folks lots of money, it is just that platforms have chosen not to observable when it comes their bot practices and ecosystems.</p>

<p>Platform observability makes sense for not just platform, and bot developers, as Slack demonstrates it makes sense for end-users. Incentivizing bots generating value, instead of mayhem. I’m guessing advertising-driven Facebook and Twitter have embraced the value of mayhem–with advertising being the framework for generating their revenue. Slack has more of a product, with customers they want to make happy. With Facebook and Twitter the end-users are the product, so the bot game plays to a different tune.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/17/bot-observability-for-every-platform/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/17/making-all-sub-resources-available-within-the-core-set-of-human-service-apis/">Making All Sub-Resources Available Within The Core Set Of Human Service APIs</a></h3>
        <span class="post-date">17 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/open-referral/hsds-organization.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I had recently taken <a href="http://openreferral.readthedocs.io/en/latest/reference/">the Human Services Data Specification (HSDS)</a> and exposed it as a set of API paths that provide access to about 95% of the schema, which we are calling the <a href="https://openreferral.github.io/api-specification/definition/">Human Services Data API (HSDA)</a>. When you make a call to the /organizations/ path, you receive an array collection of organizations that each match the HSDA organization schema. The same applies when you make a call to the /locations, /contacts, and /services, opening up access to the entire schema–minus three objects I pushed off until future releases.</p>

<p>After the core set of API paths /organization, /service, /location, /contact, there are a set of sub-resources available across those as it makes sense–including /phone, /programs, /physical_address, /postal_address, /regular_schedule, /holiday_schedule, /funding, /eligibility, /service_area, /required_document, /payment_accepted, /language, /accessiblity_for_disabilities, and /service_at_location_id. I took the HSDA schema, and published API paths for each sub-resource so that it exactly returned, and accepted HSDA compliant schema–making all aspects of the schema accessible via an API, with POST and PUT requests accepting compliant schema, and GET returning compliant schema.</p>

<p>One of the “stoppers” we received from several folks in the HSDS community during the feedback cycle going from version 1.0 of the API to version 1.1, was that the design was overly complex, and that it would serve any of the human services use cases on the table currently, unless you could get at all the sub resources directly with each core API path, eliminating the need for making additional call(s) to each sub-resource. You could get at everything about an /organization, /service, /location, and /contact in a single API URL.</p>

<p>Currently the core four API paths accept and return the following schema:</p>

<p><strong>Organization</strong></p>
<table border="1" class="docutils">
<colgroup>
<col width="17%" />
<col width="17%" />
<col width="50%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Field Name</th>
<th class="head">Type (Format)</th>
<th class="head">Description</th>
<th class="head">Required?</th>
<th class="head">Unique?</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>id</td>
<td>string (uuid)</td>
<td>Each organization must have a unique identifier.</td>
<td>True</td>
<td>True</td>
</tr>
<tr class="row-odd"><td>name</td>
<td>string</td>
<td>The official or public name of the organization.</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-even"><td>alternate_name</td>
<td>string</td>
<td>Alternative or commonly used name for the organization.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>description</td>
<td>string</td>
<td>A brief summary about the organization. It can contain markup such as HTML or Markdown.</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-even"><td>email</td>
<td>string (email)</td>
<td>The contact e-mail address for the organization.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>url</td>
<td>string (url)</td>
<td>The URL (website address) of the organization.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>tax_status</td>
<td>string</td>
<td>Government assigned tax designation for for tax-exempt organizations.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>tax_id</td>
<td>string</td>
<td>A government issued identifier used for the purpose of tax administration.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>year_incorporated</td>
<td>date (%Y)</td>
<td>The year in which the organization was legally formed.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>legal_status</td>
<td>string</td>
<td>The legal status defines the conditions that an organization is operating under; e.g. non-profit, private corporation or a government organization.</td>
<td>False</td>
<td>False</td>
</tr>
</tbody>
</table>

<p><strong>Service</strong></p>
<table border="1" class="docutils">
<colgroup>
<col width="17%" />
<col width="17%" />
<col width="50%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Field Name</th>
<th class="head">Type (Format)</th>
<th class="head">Description</th>
<th class="head">Required?</th>
<th class="head">Unique?</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>id</td>
<td>string</td>
<td>Each service must have a unique identifier.</td>
<td>True</td>
<td>True</td>
</tr>
<tr class="row-odd"><td>organization_id</td>
<td>string</td>
<td>The identifier of the organization that provides this service.</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-even"><td>program_id</td>
<td>string</td>
<td>The identifier of the program this service is delivered under.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>name</td>
<td>string</td>
<td>The official or public name of the service.</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-even"><td>alternate_name</td>
<td>string</td>
<td>Alternative or commonly used name for a service.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>description</td>
<td>string</td>
<td>A description of the service.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>url</td>
<td>string (url)</td>
<td>URL of the service</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>email</td>
<td>string (email)</td>
<td>Email address for the service</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>status</td>
<td>string</td>
<td>The current status of the service.</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>interpretation_services</td>
<td>string</td>
<td>A description of any interpretation services available for accessing this service.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>application_process</td>
<td>string</td>
<td>The steps needed to access the service.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>wait_time</td>
<td>string</td>
<td>Time a client may expect to wait before receiving a service.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>fees</td>
<td>string</td>
<td>Details of any charges for service users to access this service.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>accreditations</td>
<td>string</td>
<td>Details of any accreditations. Accreditation is the formal evaluation of an organization or program against best practice standards set by an accrediting organization.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>licenses</td>
<td>string</td>
<td>An organization may have a license issued by a government entity to operate legally. A list of any such licenses can be provided here.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>taxonomy_ids</td>
<td>string</td>
<td>(Deprecated) A comma separated list of identifiers from the taxonomy table. This field is deprecated in favour of using the service_taxonomy table.</td>
<td>False</td>
<td>False</td>
</tr>
</tbody>
</table>

<p><strong>Location</strong></p>
<table border="1" class="docutils">
<colgroup>
<col width="17%" />
<col width="17%" />
<col width="50%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Field Name</th>
<th class="head">Type (Format)</th>
<th class="head">Description</th>
<th class="head">Required?</th>
<th class="head">Unique?</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>id</td>
<td>string</td>
<td>Each location must have a unique identifier</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>organization_id</td>
<td>string</td>
<td>Each location entry should be linked to a single organization. This is the organization that is responsible for maintaining information about this location. The identifier of the organization should be given here. Details of the services the organisation delivers at this location should be provided in the services_at_location table.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>name</td>
<td>string</td>
<td>The name of the location</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>alternate_name</td>
<td>string</td>
<td>An alternative name for the location</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>description</td>
<td>string</td>
<td>A description of this location.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>transportation</td>
<td>string</td>
<td>A description of the access to public or private transportation to and from the location.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>latitude</td>
<td>number</td>
<td>Y coordinate of location expressed in decimal degrees in WGS84 datum.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>longitude</td>
<td>number</td>
<td>X coordinate of location expressed in decimal degrees in WGS84 datum.</td>
<td>False</td>
<td>False</td>
</tr>
</tbody>
</table>

<p><strong>Contact</strong></p>
<table border="1" class="docutils">
<colgroup>
<col width="17%" />
<col width="17%" />
<col width="50%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Field Name</th>
<th class="head">Type (Format)</th>
<th class="head">Description</th>
<th class="head">Required?</th>
<th class="head">Unique?</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>id</td>
<td>string</td>
<td>Each contact must have a unique identifier</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>organization_id</td>
<td>string</td>
<td>The identifier of the organization for which this is a contact</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>service_id</td>
<td>string</td>
<td>The identifier of the service for which this is a contact</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>service_at_location_id</td>
<td>string</td>
<td>The identifier of the &#8216;service at location&#8217; table entry, when this contact is specific to a service in a particular location.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>name</td>
<td>string</td>
<td>The name of the person</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>title</td>
<td>string</td>
<td>The job title of the person</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>department</td>
<td>string</td>
<td>The department that the person is part of</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>email</td>
<td>string (email)</td>
<td>The email address of the person</td>
<td>False</td>
<td>False</td>
</tr>
</tbody>
</table>

<p>To ensure that all sub-resource area available as part of each of the requests and responses for all core API paths, we are going to have to evolve the HSDS schema to be:</p>

<p><strong>Organization</strong></p>
<table border="1" class="docutils">
<colgroup>
<col width="17%" />
<col width="17%" />
<col width="50%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Field Name</th>
<th class="head">Type (Format)</th>
<th class="head">Description</th>
<th class="head">Required?</th>
<th class="head">Unique?</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>id</td>
<td>string (uuid)</td>
<td>Each organization must have a unique identifier.</td>
<td>True</td>
<td>True</td>
</tr>
<tr class="row-odd"><td>name</td>
<td>string</td>
<td>The official or public name of the organization.</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-even"><td>alternate_name</td>
<td>string</td>
<td>Alternative or commonly used name for the organization.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>description</td>
<td>string</td>
<td>A brief summary about the organization. It can contain markup such as HTML or Markdown.</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-even"><td>email</td>
<td>string (email)</td>
<td>The contact e-mail address for the organization.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>url</td>
<td>string (url)</td>
<td>The URL (website address) of the organization.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>tax_status</td>
<td>string</td>
<td>Government assigned tax designation for for tax-exempt organizations.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>tax_id</td>
<td>string</td>
<td>A government issued identifier used for the purpose of tax administration.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>year_incorporated</td>
<td>date (%Y)</td>
<td>The year in which the organization was legally formed.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>legal_status</td>
<td>string</td>
<td>The legal status defines the conditions that an organization is operating under; e.g. non-profit, private corporation or a government organization.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>services</td>
<td>array</td>
<td>Returns a collection of services for each organization</td>
<td>False</td>
<td>False</td>
</tr>  
<tr class="row-odd"><td>locations</td>
<td>array</td>
<td>Returns a collection of locations for each organization</td>
<td>False</td>
<td>False</td>
</tr>  
<tr class="row-odd"><td>contacts</td>
<td>array</td>
<td>Returns a collection of contacts for each organization</td>
<td>False</td>
<td>False</td>
</tr>    
<tr class="row-odd"><td>phones</td>
<td>array</td>
<td>Returns a collection of phones for each organization</td>
<td>False</td>
<td>False</td>
</tr>   
<tr class="row-odd"><td>programs</td>
<td>array</td>
<td>Returns a collection of programs for each organization</td>
<td>False</td>
<td>False</td>
</tr>   
<tr class="row-odd"><td>fundings</td>
<td>array</td>
<td>Returns a collection of fundings for each organization</td>
<td>False</td>
<td>False</td>
</tr>   
</tbody>
</table>

<p><strong>Service</strong></p>
<table border="1" class="docutils">
<colgroup>
<col width="17%" />
<col width="17%" />
<col width="50%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Field Name</th>
<th class="head">Type (Format)</th>
<th class="head">Description</th>
<th class="head">Required?</th>
<th class="head">Unique?</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>id</td>
<td>string</td>
<td>Each service must have a unique identifier.</td>
<td>True</td>
<td>True</td>
</tr>
<tr class="row-odd"><td>organization_id</td>
<td>string</td>
<td>The identifier of the organization that provides this service.</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-even"><td>program_id</td>
<td>string</td>
<td>The identifier of the program this service is delivered under.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>name</td>
<td>string</td>
<td>The official or public name of the service.</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-even"><td>alternate_name</td>
<td>string</td>
<td>Alternative or commonly used name for a service.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>description</td>
<td>string</td>
<td>A description of the service.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>url</td>
<td>string (url)</td>
<td>URL of the service</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>email</td>
<td>string (email)</td>
<td>Email address for the service</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>status</td>
<td>string</td>
<td>The current status of the service.</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>interpretation_services</td>
<td>string</td>
<td>A description of any interpretation services available for accessing this service.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>application_process</td>
<td>string</td>
<td>The steps needed to access the service.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>wait_time</td>
<td>string</td>
<td>Time a client may expect to wait before receiving a service.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>fees</td>
<td>string</td>
<td>Details of any charges for service users to access this service.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>accreditations</td>
<td>string</td>
<td>Details of any accreditations. Accreditation is the formal evaluation of an organization or program against best practice standards set by an accrediting organization.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>licenses</td>
<td>string</td>
<td>An organization may have a license issued by a government entity to operate legally. A list of any such licenses can be provided here.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>taxonomy_ids</td>
<td>string</td>
<td>(Deprecated) A comma separated list of identifiers from the taxonomy table. This field is deprecated in favour of using the service_taxonomy table.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>contacts</td>
<td>array</td>
<td>Returns a collection of contacts for each service.</td>
<td>False</td>
<td>False</td>
</tr>   
<tr class="row-odd"><td>phones</td>
<td>array</td>
<td>Returns a collection of phones for each service.</td>
<td>False</td>
<td>False</td>
</tr>   
<tr class="row-odd"><td>regular_schedules</td>
<td>array</td>
<td>Returns a collection of regular schedules for each service.</td>
<td>False</td>
<td>False</td>
</tr>   
<tr class="row-odd"><td>holiday_schedules</td>
<td>array</td>
<td>Returns a collection of holiday schedules for each service.</td>
<td>False</td>
<td>False</td>
</tr>    
<tr class="row-odd"><td>fundings</td>
<td>array</td>
<td>Returns a collection of fundings for each service.</td>
<td>False</td>
<td>False</td>
</tr>   
<tr class="row-odd"><td>eligibilities</td>
<td>array</td>
<td>Returns a collection of eligibilities for each service.</td>
<td>False</td>
<td>False</td>
</tr>  
<tr class="row-odd"><td>service_areas</td>
<td>array</td>
<td>Returns a collection of service areas for each service.</td>
<td>False</td>
<td>False</td>
</tr>  
<tr class="row-odd"><td>required_documents</td>
<td>array</td>
<td>Returns a collection of required documents for each service.</td>
<td>False</td>
<td>False</td>
</tr>   
<tr class="row-odd"><td>payments_accepted</td>
<td>array</td>
<td>Returns a collection of payments accepted for each service.</td>
<td>False</td>
<td>False</td>
</tr>   
<tr class="row-odd"><td>languages</td>
<td>array</td>
<td>Returns a collection of languages for each service.</td>
<td>False</td>
<td>False</td>
</tr>   
</tbody>
</table>

<p><strong>Location</strong></p>
<table border="1" class="docutils">
<colgroup>
<col width="17%" />
<col width="17%" />
<col width="50%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Field Name</th>
<th class="head">Type (Format)</th>
<th class="head">Description</th>
<th class="head">Required?</th>
<th class="head">Unique?</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>id</td>
<td>string</td>
<td>Each location must have a unique identifier</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>organization_id</td>
<td>string</td>
<td>Each location entry should be linked to a single organization. This is the organization that is responsible for maintaining information about this location. The identifier of the organization should be given here. Details of the services the organisation delivers at this location should be provided in the services_at_location table.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>name</td>
<td>string</td>
<td>The name of the location</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>alternate_name</td>
<td>string</td>
<td>An alternative name for the location</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>description</td>
<td>string</td>
<td>A description of this location.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>transportation</td>
<td>string</td>
<td>A description of the access to public or private transportation to and from the location.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>latitude</td>
<td>number</td>
<td>Y coordinate of location expressed in decimal degrees in WGS84 datum.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>longitude</td>
<td>number</td>
<td>X coordinate of location expressed in decimal degrees in WGS84 datum.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>phones</td>
<td>array</td>
<td>Returns a collection of phones for each location.</td>
<td>False</td>
<td>False</td>
</tr>  
<tr class="row-odd"><td>physical_addresses</td>
<td>array</td>
<td>Returns a collection of physical addresses for each location.</td>
<td>False</td>
<td>False</td>
</tr>    
<tr class="row-odd"><td>postal_addresses</td>
<td>array</td>
<td>Returns a collection of postal addresses for each location.</td>
<td>False</td>
<td>False</td>
</tr>   
<tr class="row-odd"><td>regular_schedules</td>
<td>array</td>
<td>Returns a collection of regular schedules for each location.</td>
<td>False</td>
<td>False</td>
</tr>   
<tr class="row-odd"><td>holiday_schedules</td>
<td>array</td>
<td>Returns a collection of holiday schedules for each location.</td>
<td>False</td>
<td>False</td>
</tr>  
<tr class="row-odd"><td>languages</td>
<td>array</td>
<td>Returns a collection of languages for each location.</td>
<td>False</td>
<td>False</td>
</tr>    
<tr class="row-odd"><td>accessiblity_for_disabilities</td>
<td>array</td>
<td>Returns a collection of accessiblity_for_disabilities for each location.</td>
<td>False</td>
<td>False</td>
</tr>    
</tbody>
</table>

<p><strong>Contact</strong></p>
<table border="1" class="docutils">
<colgroup>
<col width="17%" />
<col width="17%" />
<col width="50%" />
<col width="8%" />
<col width="8%" />
</colgroup>
<thead valign="bottom">
<tr class="row-odd"><th class="head">Field Name</th>
<th class="head">Type (Format)</th>
<th class="head">Description</th>
<th class="head">Required?</th>
<th class="head">Unique?</th>
</tr>
</thead>
<tbody valign="top">
<tr class="row-even"><td>id</td>
<td>string</td>
<td>Each contact must have a unique identifier</td>
<td>True</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>organization_id</td>
<td>string</td>
<td>The identifier of the organization for which this is a contact</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>service_id</td>
<td>string</td>
<td>The identifier of the service for which this is a contact</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>service_at_location_id</td>
<td>string</td>
<td>The identifier of the &#8216;service at location&#8217; table entry, when this contact is specific to a service in a particular location.</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>name</td>
<td>string</td>
<td>The name of the person</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>title</td>
<td>string</td>
<td>The job title of the person</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-even"><td>department</td>
<td>string</td>
<td>The department that the person is part of</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>email</td>
<td>string (email)</td>
<td>The email address of the person</td>
<td>False</td>
<td>False</td>
</tr>
<tr class="row-odd"><td>phones</td>
<td>array</td>
<td>Returns a collection of phones for each contact</td>
<td>False</td>
<td>False</td>
</tr>  
</tbody>
</table>

<p>Once we add all relevant sub-resources added as arrays to the HSDS schema, we can allow API consumers to POST and PUT, or GET using as little, or as much of the schema using the path, header, or parameter. Allowing for reading and writing HSDS at the granular level, or everything at once using a single path.</p>

<p>Next we need to consider this updated schema as part of the 1.2 release of HSDS. If we update HSDA to allow for filtering the schema across /organization, /service, /location, and /contact, and return each sub-resource as part of the API request or response, it will be out of sync with HSDS. Ideally, both HSDS, and HSDA move forward in sync with each version. I’m curious why this expanded schema became such an issue once we got to the API phase–it seems like it should have been part of v1.1 of HSDS, making the schema drive the API instead of the other way around.</p>

<p>I’m guessing that these concerns about schema don’t come into focus until we start talking about access to the schema, and data. <a href="http://apievangelist.com/2017/07/13/quantifying-the-difference-between-human-services-data-specification-hsds-and-its-api/">Making the separation and relationships between HSDS and HSDA all the more important</a>, providing a framework to move the schema forward in a way that is rooted in how it will actually be accessed. Which is why we do APIs…</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/17/making-all-sub-resources-available-within-the-core-set-of-human-service-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/16/learning-more-about-amazon-alexas-approach-to-apis-and-skills-development/">Learning More About Amazon Alexas Approach to APIs And Skills Development</a></h3>
        <span class="post-date">16 Jul 2017</span>
        <p><a href="https://developer.amazon.com/alexa"><img src="https://s3.amazonaws.com/kinlane-productions/amazon/alexa/avs_getting_started_1.png" align="right" width="40%" style="padding 15px" /></a></p>
<p>I have had <a href="https://developer.amazon.com/alexa">Amazon Alexa</a> in my cross hairs for some time now. I regularly digest stories about what Amazon is up to with Alexa, but haven’t had the time to think deeply about voice enablement, and their approach to developing what they call “skills”. I’m not 100% convinced voice enablement is the future of human compute interfaces, but I do see the role they can play in some situations, for some people. Plus, all the actions involved with Alexa and it’s ecosystem are all driven using APIs, which will almost always make me perk up, and pay a little more attention–I have a serious problem.</p>

<p>The Amazon Alexa platform centers around two specific areas of development:</p>

<ul>
  <li><strong>Alexa Voice Service</strong> - The actual voice enablement, and baking Alex voice into applications, devices, your home, car, and other physical objects in our world.</li>
  <li><strong>Alexa Skills Kit</strong> - The things that you can say to your Alex that will tigger specific actions, which make calls to APIs, and return something useful (or not).</li>
</ul>

<p>Its all about baking Alex Voice Service baked into as many devices you possibly can, and develop the catalog of skills that the voice enabled application can put to use. Ok. Well, my next question(s) are 1) what is a skill, and 2) what can you actually do with skills? Amazon provides some resources to help with the basics:</p>

<ul>
  <li><a href="https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/understanding-the-different-types-of-skills"><strong>Understanding the Different Types of Skills</strong></a></li>
  <li><a href="https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/understanding-how-users-interact-with-skills"><strong>Understanding How Users Interact with Skills</strong></a></li>
  <li><a href="https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/requirements-to-build-a-skill"><strong>Requirements to Build a Skill</strong></a></li>
</ul>

<p>Ok, helps me grasp what is their definition of a skill a little bit, and how it delivers their view of a voice enabled user interaction. Next, what can a skill really do? Or, what types of “skills” does Amazon want you to build? They start with the lofty perspective of “anything”, or “<a href="https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/overviews/understanding-custom-skills">custom  skills</a>”, to hook us technologists who like to think about at this level and fill in the gaps with our magical technological skills–a fundamental building block of API culture.</p>

<ul>
  <li>Look up information from a web service</li>
  <li>Integrate with a web service to order something (order a car from Uber, order a pizza from Domino’s Pizza)</li>
  <li>Interactive games</li>
</ul>

<p>This is API Evangelism 101. You start with everything and anything is possible, and work your way down from there. After lighting the imagination with custom skills, the focus in on a couple of specific types of actions, serving very specific purposes:</p>

<ul>
  <li><a href="https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/overviews/understanding-the-smart-home-skill-api">Smart Home Skill API</a>
    <ul>
      <li>Turn lights on and off</li>
      <li>Change the brightness of dimmable lights</li>
      <li>Change the color or color temperature of a tunable light</li>
      <li>Change the temperature on a thermostat</li>
      <li>Query a lock to see if it is currently locked</li>
      <li>Ask for a smart home camera feed</li>
    </ul>
  </li>
  <li><a href="https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/building-smart-home-skills-for-entertainment-devices">Entertainment Device Control in the Smart Home Skill API</a>
    <ul>
      <li>Change the volume</li>
      <li>Change the channel</li>
      <li>Pause, rewind or fast forward music or video content</li>
    </ul>
  </li>
  <li><a href="https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/understanding-the-video-skill-api">Video Skills API</a>
    <ul>
      <li>Play a movie</li>
      <li>Find a TV show</li>
      <li>Change a channel</li>
      <li>Pause, rewind, or fast forward video content</li>
    </ul>
  </li>
  <li><a href="https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/understanding-the-flash-briefing-skill-api">Flash Briefing Skill API</a></li>
</ul>

<p>Now the concept of a skill comes into focus a little more for me. Amazon really wants to encourage developers to develop features that deliver in a home environment, entertaining us. Alexa performance and entertainment skills. So far I have just pulled references from Amazon Alexa’s documentation, if you want to see what has been developed by the community, you can head over <a href="https://www.amazon.com/b?node=13727921011">to the Alex skills catalog</a>. I also recommend <a href="https://github.com/dale3h/alexa-skills-list">checking out a pretty robust 3rd party skills list</a>, which gives a view of skills from the outside-in.</p>

<p>Alexa Voice Service and Skills Kit are the two core services, but when you browse the documentation, you see there is a 3rd area given just as much prominence–the <a href="https://developer.amazon.com/alexa-fund">Alexa Fund</a>, which “provides up to $100 million in investments to fuel voice technology innovation”. This is an important aspect of the skills development conversation, an opportunity to get funding to support the creation of skills. While Slack doesn’t use the word skills, <a href="https://slack.com/developers/fund">they also have a fund for investing in conversational interfaces</a> (messaging, chat, and bot). One thing to note here, <a href="https://developer.amazon.com/public/solutions/alexa/rewards-for-skill-developers">Amazon has additional rewards program where developers can earn rewards when developing specially game skills for the Alexa platform</a>–providing another glimpse into their strategy.</p>

<p>I am writing this post to support our Contrafabulist podcast, but I’m also doing it to feed my wider voice research as the API Evangelist. So I have to highlight some of the common building blocks of the Alexa approach to API management, helping me better understand Amazon’s approach to this set of API resources.</p>

<ul>
  <li><strong>Glossary</strong> - https://developer.amazon.com/public/solutions/alexa/alexa-skills-kit/docs/alexa-skills-kit-glossary</li>
  <li><strong>Forum</strong> - http://forums.developer.amazon.com/forums/category.jspa?categoryID=60</li>
  <li><strong>FAQs</strong> - https://developer.amazon.com/public/solutions/alexa/alexa-voice-service/docs/alexa-voice-service-developer-preview</li>
  <li><strong>Blog</strong> - https://developer.amazon.com/blogs/alexa/tag/AVS</li>
  <li><strong>Terms of Service</strong> - https://developer.amazon.com/public/solutions/alexa/alexa-voice-service/support/terms-and-agreements</li>
</ul>

<p>Beyond the common building blocks for operating their developer portal, supporting their APIs, they have some interesting design elements available for developers. Helping direct Alex developers to develop skills and voice-enabled applications that fit in with their objectives:</p>

<ul>
  <li><strong>Designing for AVS</strong> - https://developer.amazon.com/public/solutions/alexa/alexa-voice-service/content/designing-for-the-alexa-voice-service</li>
  <li><strong>Functional Design Guide</strong> - https://developer.amazon.com/public/solutions/alexa/alexa-voice-service/content/alexa-voice-service-functional-design-guide</li>
  <li><strong>UX Design Guidelines</strong> - https://developer.amazon.com/public/solutions/alexa/alexa-voice-service/content/alexa-voice-service-ux-design-guidelines</li>
  <li><strong>Marketing Brand Guidelines</strong> - https://developer.amazon.com/public/solutions/alexa/alexa-voice-service/content/marketing-brand-guidelines</li>
</ul>

<p>These design elements tell an interesting story regarding how Amazon is aligning the concept of skills development with their voice enabled API strategy. It also provides an interesting approach to design guides that other API providers might want to consider. After the design guides, Amazon provides some interesting code and hardware to help developers, providing starter kits to get going developing skills as well as actual physical voice integration:</p>

<ul>
  <li><strong>Projects and Sample Code</strong> - https://github.com/alexa/alexa-avs-sample-app</li>
  <li><strong>API and Reference</strong> - https://developer.amazon.com/public/solutions/alexa/alexa-voice-service/content/avs-api-overview</li>
  <li><strong>Development Kits for AVS</strong> - https://developer.amazon.com/dev-kits</li>
  <li><strong>Development Kits (Hardware)</strong> - https://developer.amazon.com/alexa-voice-service/dev-kits</li>
</ul>

<p>There are three different stories going on here for me which I think are relevant to how we think about, and approach our usage of technology. I’m really interested in Amazon Alexa because of:</p>

<ul>
  <li><strong>Voice Enablement</strong> - How do you enable voice applications using APIs? What role will voice play in the wider conversational API landscape?</li>
  <li><strong>Skills Concept</strong> - I am fascinated by the concept of a skill and how it applies to not just voice enablement, and conversational interface, but also representing a unit of compute or a transaction that occurs in API land (serverless, microservices, containers, etc.)</li>
  <li><strong>API Management</strong> - How do you manage a set of APIs that serve conversational interfaces and voice-enablement? What is different than regular API management, and what can others learn from their approach?</li>
</ul>

<p>As I said in the opening, I’m not 100% convinced that voice interfaces will be the future for everyone. I depend on the intimacy that exists between my fingers and the keyboard to make the magic happen each day–which might be a little noisy, but it doesn’t involve me rambling on, talking to a device. Even though I’m not into voice interfaces, and much of a talker, I am interested in the motivations behind, and the approach that Amazon is taking with their conversational interfaces. It is something i’m comparing with my research into Twitter, Slack, as well as Facebook. These companies are investing a lot into their ecosystems–you can see the signs of it over at Amazon with <a href="https://www.amazon.jobs/en/teams/alexa-skills?base_query=&amp;loc_query=&amp;job_count=10&amp;result_limit=10&amp;sort=relevant&amp;team_category%5B%5D=alexa-skills&amp;cache">the 137 job openings for their Alex team</a>.</p>

<p>Audrey and I are going to talk about Amazon Alexa on our Contrafabulists podcast this week, so you can tune in to get more thoughts of mine regarding Alexa, and voice enablement APIs. I’ll probably continue the exploration of my thoughts about this approach to interfaces, and particularly the development of “skills”. Which I think has an interesting overlap with APIs, and the modularization we are seeing as a result of compute (ie. containers, microservices, devops), as well as the impact APIs are having on labor (ie. Uber, Mechanical Turk, Task Rabbit). I feel like there is a lot more going on here than just developing fun skills for having conversations with Alexa in your home.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/16/learning-more-about-amazon-alexas-approach-to-apis-and-skills-development/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/13/quantifying-the-difference-between-human-services-data-specification-hsds-and-its-api/">Quantifying The Difference Between Human Services Data Specification (HSDS) And Its API</a></h3>
        <span class="post-date">13 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/api-evangelist/openapi-spec/openapi-spec-icon.png" align="right" width="25%" style="padding: 15px;" /></p>
<p>To help quantify the move from version 1.0 to 1.1 of the Human Services Data API (HSDA) definition I took the existing Ohana API and created an OpenAPI definition to describe what was present in version 1.0 of the HSDA. Then I took <a href="http://openreferral.readthedocs.io/en/latest/reference/#organization">version 1.1 of the Human Services Data Specification (HSDS)</a> and made sure as much of HSDS was returned as part of API responses, as well as allowing adding, updating, and deleting across the schema.</p>

<p>During the vendor API review portion of our process I took the documentation for four of the vendors APIs and created OpenAPI for each of them. I then laid all the vendor OpenAPIs alongside <a href="https://openreferral.github.io/api-specification/definition/">the current draft I had of the HSDA definition</a>. I then consider each path, the parameters, body, and responses for inclusion as part of the HSDA definition. This allowed me to consider the existing vendor API implementations that are already serving human service implementations.</p>

<p>OpenAPI plays a central role in defining what is, what might be, while opening up a forum for having a conversation about the specific detail of the HSDS/A definition. I’m using OpenAPI to establish a definition of what both HSDS and HSDA are. It will be the contract that gets hammered out as part of the Open Referral governance process, so you will see me use it regularly to articulate specific aspects of what is going. With this in mind, I’d like to use a distilled OpenAPI, articulated just a single API path for GET /organizations.</p>

<script src="https://gist.github.com/kinlane/b44af277c9ad8948e82215ee31a7e195.js"></script>

<p>I won’t go into to much detail on the OpenAPI, <a href="https://github.com/OAI/OpenAPI-Specification">I recommend learning more about the specification on the GitHub repository</a>, and at <a href="https://www.openapis.org/">the OpenAPI Initiative (OAI)</a>. What I’d like to articulate for this story is to help quantity the separation and connection between the Human Services Data Specification (HSDA), and the Human Services Data API (HSDA), using this single OpenAPI, describing a single HSDA path–organizations.</p>

<p>When you take the schemes: located at line 7, and combine it with host: at line 5, basePath: at line 6, and the path for /organizations/ at line 12 you get http://api.open.referral.adopta.agency/organizations/, which when you load in a browser will give you a JSON listing of many organizations. Line 13-29 describes how to make an API request, and line 30-36 describes what you can expect as a response.</p>

<p>Lines 1-38 is HSDA, and 42-71 is HSDS. Line 36 is the link between HSDA, and HSDS, providing a reference that binds the API request, with the API response. HSDS is the valid schema being returned–HSDA is not the schema, it is the surface of the API that lets you send, and in this case received valid HSDS. This is a line that we honestly haven’t had the level of detail, or even the acronyms before now to even articulate HSDS/A at this level. So don’t worry if what I said doesn’t quite make sense–it will come. ;-)</p>

<p>The first reason I’m writing this story is to help myself better articulate the difference between HSDS, and HSDA, and the relationship between them. The second portion is to help other folks participating in the HSDS/A governance conversation see the separate layers between the schema and API, but also understand how they work together. I’d say a third portion is about helping folks understand the value of using OpenAPI to facilitate these types of conversations.</p>

<p>It might take a couple times reading this post and/or having a conversation directly with me about OpenAPI, but there is nothing in this OpenAPI definition that any engaged users can learn to work with–developers, and non-developers. I’ll keep producing simple lessons like this to better articulate aspects of the HSDS/A contract using OpenAPI. I’d love to hear any feedback on how I can better articulate the great work we are doing around the Human Services Data Specification and it’s API.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/13/quantifying-the-difference-between-human-services-data-specification-hsds-and-its-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/13/moving-the-human-services-api-specification-from-version-11-to-12/">Moving The Human Services API Specification From Version 1.1 to 1.2</a></h3>
        <span class="post-date">13 Jul 2017</span>
        <p><a href="http://developer.open.referral.adopta.agency/documentation/"><img src="https://s3.amazonaws.com/kinlane-productions/open-referral/hsda-organizations-documentation.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I am preparing for the recurring governance meeting for the Open Referral Human Services Data API standard–which I’m the technical lead for. I need to load up every detail of my Human Services Data API work into my brain, and writing stories is how I do this. I need to understand where the definition is with v1.1, and encourage discussion around a variety of topics when it comes to version 1.2.</p>

<h3 id="constraints-from-version-10-to-v11">Constraints From Version 1.0 To v1.1</h3>
<p>I wasn’t able to move as fast as I’d like from 1.0 to 1.1, resulting in me leaving out a number of features. The primary motivation to make sure as much of the version 1.1 of Human Services Data Specification (HSDS) was covered as possible–something I ended up doing horizontally with new API paths, over loading up the core paths of /organizations, /locations, and /services. There were too many discussion on the table regarding the scope and filtering of data, and schema for these core paths. Something which led to a discussion, about /search–resulting in me pushing off API design discussions on how to expand vertically at the core API path level to future versions.</p>

<p>There were just too many decisions to make at the API request and response level for me to make a decision in all the areas–warranting more discussion. Additionally, there were other API design discussion regarding operational, validation, and more utility APIs to discuss for inclusion in future versions expanding the scope and filtering discussions to the API path, and now API project level. In preparation for our regular governance meeting I wanted to run through all of the open API design issues, as well as additional projects the community needs to be thinking about.</p>

<h2 id="api-design">API Design</h2>
<p>As part of my Human Services Data API (HSDA) work we have opened up a pretty wide API design conversation regarding where the API definition could (should) be going. I’ve tried to capture the conversations going on across the Slack, and Google Group using GitHub issues for the HSDA GitHub repository. I will be focusing in on 16 of these issues for the current community discussions.</p>

<h3 id="versioning"><a href="https://github.com/openreferral/api-specification/issues/8">Versioning</a></h3>
<p>We are moving forward the version of the API specification from 1.0 to 1.1. This version describes the API definition, to help quantify the compliance of any single API implementation. This is not guidance regarding how API providers should version their API–each implementation can articulate their compliance using an OpenAPI definition, or just in operation by being compliant. I purposely dodged providing versioning guidance of specific API implementations–until I could open up discussion around this subject.</p>

<p>If you need a primer on API versioning I recommend <a href="https://www.troyhunt.com/your-api-versioning-is-wrong-which-is/">Troy Hunt’s piece</a> which helps highlight:</p>

<ul>
  <li>URL: We put the API version into the URL: https://example.com/api/v1.1/organizations/</li>
  <li>Custom request header: Using a header such as “api-version: 1.1”</li>
  <li>Accept header: Using the accept header to specify the version “Accept: application/vnd.hsda.v1.1+json - which relates to content negotiation discussions.</li>
  <li>No Versioning - We do not offer any versioning guidance and let each API implementation decide for themselves with no version being a perfectly acceptable answer.</li>
</ul>

<p>API versioning discussions are always hot topics, and there is no perfect answer. If we are to offer API versioning guidance for HSDA compliant API providers I recommend putting it in the URL, not because it is the right answer, but it is the right answer for this community. It is easy to implement, and easy to understand. Although I’m not 100% convinced we should be offering guidance at all.</p>

<p>I would like to open it up to the community, and get more feedback from vendors, and implementors. I’m curious what folks prefer when they are building applications. This decision was one that was wrapped up with potential content negotiation, hypermedia, and schema scope discussions to make without more discussion.</p>

<h3 id="paths"><a href="https://github.com/openreferral/api-specification/issues/27">Paths</a></h3>
<p>The API definition provides some basic guidance for HSDA implementations when it comes to naming API paths, providing a core set or resources, as well as sub-resources. There are a number of other API designs waiting in the wings to be hammered out, making more discussion around this relevant. How do we name additional API paths? Do we keep evolving a single stack of resources (expanding horizontally), or do we start grouping them and evolve using more sub-resources (expanding vertically)?</p>

<p>Right now, we are just sticking with a core set of paths for /contacts, /locations, /organizations, and /services, with /search somewhat of an outlier, or I guess wrapper. We have moved forward with sub-resource guidance, but should standard API design guidance when it comes to crafting new paths, as well as sub-paths, including the actions discussion below. This will be an ongoing discussion when it comes to API design across future versions, making it an evergreen thread that will just keep growing as the HSDA definition matures.</p>

<h3 id="verbs"><a href="https://github.com/openreferral/api-specification/issues/26">Verbs</a></h3>
<p>HTTP verbs usage was another aspect of the evolution of the HSDA specification from v1.0 to v1.1–the new specification uses its verbs. Making sure POST, PUT, and DELETE were used across all core resources, as well as sub-resources, making the entire schema open for reading and writing at all levels. This further expanded the surface of the API definition, making it manageable at all levels.</p>

<p>Beyond this expansion we need to open up the discussion regarding OPTIONS, and PATCH. Is there a need to provide partial updates using PATCH, and providing guidance on using OPTION for providing requirements associated with a resource, and the capabilities of the server behind the API. Also we should be having honest conversations about which verbs are available for sub-resources, especially when it comes to taking specific actions using HSDA paths. There is a lot more to discuss when it comes to HTTP verb usage across the HSDA specification.</p>

<h3 id="actions"><a href="https://github.com/openreferral/api-specification/issues/24">Actions</a></h3>
<p>I want to prepare for the future when we have more actions to be taken, and talk about how we approach API design in the service of taking action against resources. Right now HTTP verbs are taking care of the CRUD features for all resources and sub-resources. While I don’t have any current actions in the queue to discus, we may want to consider this as part of the schema scope and filtering discussion–allowing API consumers to request partial, and complete representations of API resources using action paths. For example: /organization/simple, or /organizations/complete.</p>

<p>As the HSDA specification matures this question will come up more and more, as vendors, and implementations require more specialized actions to be taken against resources. Ideally, we are keeping resources very resource oriented, but from experience I know this isn’t always the case. Sometimes it becomes more intuitive for API developers to take action with simple, descriptive API paths, than adding more complexity with parameters, headers, and other aspects of the APIs design. I will leave this conversation open to help guide future versions, as well as the schema scope and filtering discussions.</p>

<h3 id="parameters"><a href="https://github.com/openreferral/api-specification/issues/24">Parameters</a></h3>
<p>Currently the numbers parameters in use for any single endpoint is pretty minimal. The core resources allow for querying, and sorting, but as of version 1.1, parameters are still pretty well-defined and minimal. The only path that has an extensive set of parameters is /search, which possesses category, email, keyword, language, lat_lng, location, org_name, page, per_page, radius, service_area, and status. I’d like to to continue the discussion about which parameters should be added to other paths, as well as used to help filter the schema, and other aspects of the API design conversation.</p>

<p>I’d like to open up the parameter discussion across all HSDA paths, but I’d also like to establish a way to regularly quantify how many paths are available, as well as how loaded they are with default values, and enumerators. I’d like to feed this into overall API design guidance, helping keep API paths reflecting a microservices approach to delivering APIs. Helping ensure HSDA services do one thing, and do it well, with the right amount of control over the surface area of the request and response of each API path.</p>

<h3 id="headers"><a href="https://github.com/openreferral/api-specification/issues/5">Headers</a></h3>
<p>Augmenting the parameter discussion I want to make sure headers are an equal part of the discussion. They have the potential to play a role across several of these API design questions from versioning to schema filtering. They also will continue to emerge in authentication, management, security, and even sorting and content negotiation discussions.</p>

<p>It is common for there to be a lack of literacy in developer circles when it comes to HTTP headers. A significant portion of the discussion around header usage should always be whether of not we want to invest in HTTP literacy amongst implementors, and their developer communities, over leveraging other non-header approaches to API design. HTTP Headers are an important building block of the web that developers should understand, but educating developers around their use can be time intensive and costly when it comes to guidance.</p>

<h3 id="body"><a href="https://github.com/openreferral/api-specification/issues/25">Body</a></h3>
<p>There is an open discussion around how the body will be used across HSDA compliant implementations. Currently the body is default for POST and PUT, aka add and update. This body usage has been extended across all core resources, as well as sub-resource, requiring the complete, or sub resource representation to be part of each POST or PUT request.</p>

<p>There is no plan for any other APIs that will deviate from this approach, but we should keep this thread open to make sure we think about when the usage of the body is appropriate and when it might not be. We need to make sure that developers are able to effectively use the body, alongside headers, as well as parameters to get the desired results they are looking for.</p>

<h3 id="data-scope--filtering"><a href="https://github.com/openreferral/api-specification/issues/22">Data Scope / Filtering</a></h3>
<p>Currently the only filtering beyond pagination that is available is the query parameter available on /contact, /organizations, /locations, and /services resources. After that search is where the heaviest data scope and filtering can be filtered and defined. We need to discuss the future of this. Should the core resources have similar capabilities to /search, or should /search be a first class citizen with the majority of the filtering capabilities?</p>

<p>There needs to be more discussion around how data will be available bia default, and how it will be filtered as part of each API request. Will search be carrying most of the load, or will each core resource be given some control when it comes to filtering data. Whatever the approach it needs to be standardized across all existing paths, as well as applied to new API designs, keeping data filtering consistent across all HSDA designs. As this comes into focus I will be making sure there is a guide that provides guidance when it comes to data filtering practices in play.</p>

<h3 id="schema-scope--filtering"><a href="https://github.com/openreferral/api-specification/issues/21">Schema Scope / Filtering</a></h3>
<p>This is one of the top issues being discussed as part of the migration from v1.1 to v1.2, regarding how to not just filter data that is returned as part of API responses, but how do you filter what schema gets returned as part of the response. When it came to v1.0 to v1.1 I didn’t want to shift the response structure so that I can reduce any breaking changes for existing Ohana implementations, and open up with the community regarding the best approach for allowing schema filtering.</p>

<p>My current recommendation when it comes to the filtering of how much or how little of the schema to return with each request is to allow for schema templates to be defined and named, then enable API consumers to specify which template they’d like returned. This should be specified through either through a <a href="https://apievangelist.com/2017/05/24/considering-http-prefer-header-instead-of-field-filtering-for-my-api/">prefer header</a>, as part of the path structure as an action, or possibly through a parameter–all would accept the name of a schema template they desire (ie. simple, complete, etc.).</p>

<p>This approach to enabling schema templating could be applied at the GET, and could be also applied to POST or PUT requests. I personally recommend using a <a href="https://apievangelist.com/2017/05/24/considering-http-prefer-header-instead-of-field-filtering-for-my-api/">prefer header</a>, but I also emphasize the ease of use, and ease of defining the usage as part of documentation, and the OpenAPI definition–which it might make sense to allow for schema enablement as pat of the path name as an action. I’ll leave it to the community to ultimately decide, as with the rest of this API design and project list, I’m just looking to provide guidance, and direction, built on the feedback of the community.</p>

<h3 id="path-scope--filtering"><a href="https://github.com/openreferral/api-specification/issues/38">Path Scope / Filtering</a></h3>
<p>Next up in the scope and filtering discussion is regarding how we define, group, and present all available API paths included in the HSDA specification. With the current specification I see three distinct groups of API paths emerging: 1) core resources (/contacts, /organizations, /locations, /services), and 2) sub resources (/physical-address, /postal-address, /phones, and more), then the more utility aspects of meta data, taxonomy, and eventually webhooks.</p>

<p>When a new user lands on the API documentation, they should see the core resources, and not be burdened with the cognitive load associated sub resources or the more utility aspects of HSDA consumption. However, once ready more advanced API paths are available. The grouping and filtering of the API paths can be defined as part of the OpenAPI definitions for the API(s), as well as the APIs.json index for the site. This path grouping will allow for API consumers to limit scope and filter which API paths are available in the documentation, and possibly with SDKs, testing, and other aspects of integration.</p>

<p>There are additional API projects on the table that might warrant the addition of new API groups, beyond core resources, sub resources, and utility paths. The approval, feedback, and messaging discussions might require their own group, allowing them to be separated in documentation, code, testing, and other areas–reducing the load for new users, while expanding the opportunities for more advanced consumers. Eventually there might be a one to one connection between API path groups, and the API projects in the queue, allowing for different groups of APIs to be moved forward at different rates, and involve different groups of API consumers and vendors in the process.</p>

<h3 id="project-scope--filtering"><a href="https://github.com/openreferral/api-specification/issues/40">Project Scope / Filtering</a></h3>
<p>Adding the fourth dimension to this scope / filtering discussion, I’m proposing we discuss how projects are defined and isolated, which can allow them to move forward at different rates, and be reflected in documentation, code, and other resources–allowing for filtering by consumers. This will drive the path filtering described above, but apply beyond just the API, and influencing documentation, SDKs, testing, monitoring, validation, and other aspects of API operations.</p>

<p>With this tier I am looking to decouple API projects from one another, and from the core specification. I want the core HSDS/A specification to stay focused on doing one thing well, but I’d like to establish a clear way to move forward complimentary groups of API definitions, and supporting tooling independently of the core specification. As we prepare to begin the journey from version 1.1 to 1.2, there are a number of significant projects on the table, and we need a way to isolate and decouple each additional API project in the same we we do with individual API resources–keeping them clearly defined, focused on specific problem set, and a buffet of resources that the community can choose where they’d like to participate.</p>

<h3 id="pagination"><a href="https://github.com/openreferral/api-specification/issues/10">Pagination</a></h3>
<p>This is the discussion around how results will be paginated, allowing for efficient or complete requests to be requested, and navigate through large volumes of human services data. We need to be discussing how we will evolve the current approach to using page= and per_page= to articulate pagination. This approach is a common, well understood way to allow developers to paginate, but we need to keep discussion open as we answer some of the other API design questions on the table.</p>

<p>The pagination topic overlaps with the hypermedia and response structure discussion. Eventually we may offer pagination as part of a response envelope, or relational links provided as part of the response when using JSON API, HAL, or other media type. Right now we will leave pagination as it is, but we should be thinking about how it will evolve alongside all other API design conversations in this list.</p>

<h3 id="sorting"><a href="https://github.com/openreferral/api-specification/issues/12">Sorting</a></h3>
<p>According to the current Ohana API implementation, which is the HSDA v1.0 definition, the guidance for sorting availability is as follows:</p>

<blockquote>
  <p>Except for location-based and keyword-based searches, results are sorted by location id in ascending order. Location-based searches (those that use the lat_lng or location parameter) are sorted by distance, with the ones closest to the search query appearing first. keyword searches are sorted by relevance since they perform a full-text search in various fields across various tables.</p>
</blockquote>

<p>This guidance follows the API definition from version 1.0 to 1.2, but for future versions we should be considering providing further guidance regarding sorting of results. I’d like to get more feedback from the community on how they are providing data sorting capabilities for API consumes, or even as part of web and mobile applications.</p>

<h3 id="response-structure"><a href="https://github.com/openreferral/api-specification/issues/6">Response Structure</a></h3>
<p>Right now the API responses for HSDA are pretty flat, like the schema. As part of the move from version 1.1 to 1.2 we need to be expanding on them, allowing for sub-resources to be included. This conversation will be heavily influenced by the schema filtering conversation above, as well as potentially the hypermedia and content negotiation discussions below. If we are gong to expand on the the schema being returned with API response we should be discussing all possible changes to the schema at once.</p>

<p>This conversation is meant to bring together the API schema filtering, hypermedia, and content negotiation conversations into a single discussion regarding the overall structure of a response, by default, as well as through filtering at the path, parameter, or header levels. I’d like to see  HSDA responses expand to accommodate sub resources, but also the relationships between resources, as well as assisting with pagination, sorting, and other aspects of data, schema, and path filtering. I am looking to make sure the expansion of the response structure be more inclusive beyond just talk of sub resource access.</p>

<h3 id="hypermedia"><a href="https://github.com/openreferral/api-specification/issues/7">Hypermedia</a></h3>
<p>I really want to see a hypermedia fork in the HSDA definition, allowing more advanced users to negotiate and hypermedia version of the specification, instead of the more simpler, or even advanced default versions of the API. I recommend the adoption of HAL, Siren, or JSON API, as an alternate edition of an HSDA implementation. This expansion of the design of the HSDA specification would not impact the current version, but would allow for another dimension of API consumption and integration.</p>

<p>The relationships between human services data, and the semantic nature of the data really begs for a hypermedia solution. It would allow more meaningful API responses, and defining of relationships between resources, and emphasis of the taxonomy. I will be encouraging a separate, but complimentary version of HSDA that uses one of the leading hypermedia media types. I’d like to ensure there is community awareness of the potential of this approach, and support for investing in this as part of the HSDA design strategy.</p>

<h3 id="status-codes"><a href="https://github.com/openreferral/api-specification/issues/3">Status Codes</a></h3>
<p>One of the areas of design around version 1.1 of the HSDA specification that was put off until future versions is guidance when it comes to API response status and error codes. Right now the OpenAPI definition for version 1.1 of the HSDA specification only suggests a 200 successful response, returning a reference to the appropriate HSDS schema. A project needs to be started that would provider further guidance for 300, 400, and 500 series status codes, as well as error responses.</p>

<p>Each HSDA path should provide guidance on all relevant HTTP Status Codes, but should also provide guidance regarding the error object schema returned as part of every possible API response. Helping standardize how errors are communicated, and provide further guidance on how to help API consumers navigate a solution. Currently there is no guidance when it comes to HTTP responses and errors, something that should be considered in version 1.2 or 1.3, depending on available resources.</p>

<h3 id="content-negotiation"><a href="https://github.com/openreferral/api-specification/issues/39">Content Negotiation</a></h3>
<p>Augmenting other conversations around schema filtering, API response structure, and hypermedia, I want to make sure content negotiation stays part of the conversation. This aspect of API design will significantly impact API integration, and the evolution of the API specification. I want to make sure vendors, and other key actors are aware of it as an option, and can participate in the conversation regarding the different content types.</p>

<p>This conversation should begin with making CSV and HTML representations of the data available as part of the API response structure alongside the current JSON representations. API consumers should have the option to get raw HTML, CSV, and JSON through content negotiation–with JSON remaining as the default. Then the conversation should evolve to consider HSDA specific content type designation, as well as implementation of a leading hypermedia media type like JSON API, HAL, or Siren.</p>

<p>Content negotiation plays an important role in versioning the HSDA specification, as well as providing different dimensions for dealing with more complex integrations, as well as other aspects of operations like pagination, sorting, access to sub resources, other actions and even data, schema, and path filtering. Like headers, the mainstream developer community tends to not all be aware of content negotiation, but the benefits of adopting far outweigh the overhead involved with bringing developers up to speed.</p>

<p>That concludes the list of API design conversations that are occurring as part of the move from version 1.0 to 1.1, and will set the stage for the move towards 1.2, and beyond. It is a lot to consider, but it is a manageable amount for the community to think about as part of the version 1.1 feedback cycle. Allowing us to make a community informed decision regarding what should be focused on with each release–delivering what matters to the community.</p>

<h2 id="api-projects">API Projects</h2>
<p>As the version 1.0 to 1.1 migration occurred several projects were identified, or suggested for consideration. I want to make sure all these projects are on the table as part of the evolution of HSDA, beyond just the current API design discussion occurring. These are the projects we added to the specification that are moving forward but will have varying degrees of impact on the core API definition.</p>

<h3 id="taxonomy"><a href="https://github.com/openreferral/api-specification/issues/19">Taxonomy</a></h3>
<p>There are two objects included in version 1.1 of the Human Services Data Specification (HSDS) that deal with taxonomy, the service_taxonomy object, and the core taxonomy object. I purposely left these aspects of the schema out of version 1.1 of HSDA. I wanted to see more discussion regarding taxonomy before we included in the specification. This is one of the first areas that influenced the above discussions regarding path scope and filtering, as well as project scope and filtering.</p>

<p>I’d like to see taxonomy exist as a separate set of paths, as a separate project, and out of the core specification. In addition to further discussion about what is HSDA taxonomy, I’d like to see  more consideration regarding what exactly is acceptable levels of HSDA compliant taxonomy. Ideally, the definition allows for multiple taxonomy, and possibly even a direct relationship between the available content types and a taxonomy, allowing for a more meaningful API response.</p>

<p>I will leave open a Github issue to discuss taxonomy, and either move forward as entirely separate schema, or inclusion in version 1.2, 1.3 of the core HSDA definition. One aspect of this delay is to ensure that my awareness of available taxonomies is up to snuff to help provide guidance. I’m just not aware of everything out there, as well as an intimacy the leading taxonomies in use–I need to hear more from vendors and implementors on this subject before I feel confident in making any decision.</p>

<h3 id="metadata"><a href="https://github.com/openreferral/api-specification/issues/28">Metadata</a></h3>
<p>The metadata, and the meta_table_description objects v1.1 of HSDA were two elements I also left out of version 1.1 of HSDA. I felt like there should be more discussion around API management, logging, and other aspects of API operations that feed into this area, before we settled in on an API design to satisfy the HSDA metadata conversation. I’d like to hear more from human services implementors regarding what metadata they desire before we connect the existing schema to the API.</p>

<p>The metadata conversation overlaps with the approval and feedback project. There are aspects of logging and meta data collection and storage that will contribute to the transactional nature of any approval and feedback solution. There is also conversation going on regarding privacy concerns around API access to HSDS data, and logging, auditing that occurs at the metadata level. This thread covers these conversations, and is looking to establish a separate group of API paths, and separate project to drive documentation, and other aspects of API operations.</p>

<h3 id="approval--feedback"><a href="https://github.com/openreferral/api-specification/issues/34">Approval &amp; Feedback</a></h3>
<p>One of the projects that came up recently was about working to define the layer that allows developers to add, update, and delete data via the API. Eventually through the HSDA specification we to encourage 3rd party developers, and external stakeholders to help curate and maintain critical human services data within a community, through trusted partners.</p>

<p>HSDA allows for the reading and writing of organizations, locations, and services for any given area. I am looking to provide guidance on how API implementors can allow for POST, PUT, PATCH, and DELETE on their API, but require approval before any changing transaction is actually executed. Requiring the approval of an internal system administrator to ultimately give the thumbs up or thumbs down regarding whether or not the change will actually occur.</p>

<p>A process which immediately begs for the ability to have multiple administrators or even possibly involving external actors. How can we allow organizations to have a vote in approving changes to their data? How can multiple data stewards be notified of a change, and given the ability to approve or disprove, logging every step along the way? Allowing any change to be approved, reviewed, audited, and even rolled back. Making public data management a community affair, with observability and transparency built in by default.</p>

<p>I am doing research into different approaches to tackling this, ranging from community approaches like Wikipedia, to publish and subscribe, and other events or webhook models. I am looking for technological solutions to opening up approval to the API request and response structure, with accompanying API and webhook surface area for managing all aspects of the approval of any API changes. If you know of any interesting solutions to this problem I’d love to hear more, so that I can include in my research, future storytelling, and ultimately the specification for the Open Referral Human Services Data Specification and API.</p>

<h3 id="universal-unique-ids"><a href="https://github.com/openreferral/api-specification/issues/35">Universal Unique IDs</a></h3>
<p>How will we allow for a universal unique ID system for all organizations, locations, and services, providing some provenance on the origin of the record. There is a solid conversation started about how to approach a universal ID system to live alongside, or directly as part of the core HSDA specification–depending on how we decide to approach project scope. Ideally, a universal ID system isn’t pat of being compliant, but could add a healthy layer of certification for some leading providers.</p>

<p>More research needs to be done regarding how universal IDs are handled in other industries. An exhaustive search needs to be conducted regarding any existing standards and guidance that can help direct this discussion. This approach to handling identifiers will have a significant impact on individual API implementations, as well as the overall HSDA definition. More importantly, it will set the stage for future HSDA aggregation and federation, allowing HSDA implementations to work together more seamlessly, and better serve end-uses.</p>

<h3 id="messaging"><a href="https://github.com/openreferral/api-specification/issues/37">Messaging</a></h3>
<p>I separated this project out of the approval and feedback project. I am suggesting that we isolate the messaging guidance for APIs, setting a standard for how you communicate within a single implementation as well across implementations. There are a number of messaging API standards and best practices available out there, as well as existing messaging APIs that are already in use by human services practitioners, including social channels like Facebook and Twitter, but also private channels like Slack.</p>

<p>HSDA compliant messaging channels should live as a separate project, and set of API path specifications. It should augment the core HSDA definition, overlaying with existing contact information, but it should also be dovetailed with new projects like approval and feedback system. More research needs to be conducted on existing messaging API standards, and leading channels that existing human services implementations and their software vendors are already using.</p>

<h3 id="webhooks"><a href="https://github.com/openreferral/api-specification/issues/35">Webhooks</a></h3>
<p>I want to begin separate project for handling an important aspect of any API operations, and not just being their to receive requests, but can also push information externally, and respond to scheduled, or event driven aspects of API operations. Webhooks will play a role in the approval and feedback system, as well as the metadata, and messaging projects–eventually touching all aspects of the core HSDA resources, and separate projects.</p>

<p>Alongside the approval and feedback, universal id, and messaging projects, webhooks will set the stage for the future of HSDA, where individual city and regional implementations can work together, share information, federate and share responsibility in updates and changes. Webhooks will be how each separate implementation will work in concert, making the deliver of human services more real time, and orchestrated across providers, achieving API the vision of Open Referral founder Greg Bloom.</p>

<h2 id="what-is-next">What Is Next?</h2>
<p>We have a lot on the table to discuss currently. We need to settle some pretty important API design discussions that will continue to have an impact on API operations for a long time. I want to help push forward the conversation around these API design discussions, and get these API projects moving forward in tandem. I need more input from the vendors, and the community around some of the pressing discussions, and then I’m confident we can settle in on what the final version 1.1 of the API specification should be, and what work we want to tackle as part of 1.2 and beyond. I’m feeling like with a little discussion we can find a path forward to reach 1.2 in the fall of 2017.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/13/moving-the-human-services-api-specification-from-version-11-to-12/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/12/challenges-when-aggregating-data-from-across-the-years/">Challenges When Aggregating Data Published Across Many Years</a></h3>
        <span class="post-date">12 Jul 2017</span>
        <p><a href="http://funding.hackeducation.com/"><img src="https://s3.amazonaws.com/kinlane-productions/hack-education/ed-tech-investment-research.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p><a href="http://funding.hackeducation.com/">My partner in crime is working on a large data aggregation project regarding ed-tech funding</a>. She is publishing data to Google Sheets, and I’m helping her develop Jekyll templates she can fork and expand using Github when it comes to publishing and telling stories around this data across her network of sites. Like API Evangelist, <a href="https://github.com/hackeducation/">Hack Education runs as a network of Github repositories</a>, with a common template across them–we call the overlap between API Evangelist, <a href="http://contrafabulists.com/">Contrafabulists</a>.</p>

<p>One of the smaller projects she is working on as part of her ed-tech funding research involves pulling the grants made by the Gates Foundation since the 1990s. Similar to my story a couple weeks ago about my friend David Kernohan, <a href="https://apievangelist.com/2017/06/28/i-have-two-interesting-apis-and-i-am-not-a-developer-what-do-i-do/">where he was wanting to pull data from multiple sources</a>, and aggregate into a single, workable project. Audrey is looking to pull data from a single source, but because the data spans almost 20 years–it ends up being a lot like aggregating data from across multiple sources.</p>

<p>A couple of the challenges she is facing trying to gather the data, and aggregate as a common dataset are:</p>

<ul>
  <li><strong>PDF</strong> - The enemy of any open data advocate is the PDF, and a portion of her research data data is only available in PDF format which translates into a good deal of manual work.</li>
  <li><strong>Search</strong> - Other portions of the data is available via the web, but obfuscated behind search forms requiring many different searches to occur, with paginated results to navigate.</li>
  <li><strong>Scraping</strong> - The lack of APIs, CSV, XML, and other machine readable results raises the bar when it comes to aggregating and normalizing data across many years, making scraping a consideration, but because of PDFs, and obfuscated HTML pages behind a search, even scraping will have a significant costs.</li>
  <li><strong>Format</strong> - Even once you’ve aggregated data from across the many sources, there is a challenge with it being in different formats. Some years are broken down by topic, while others are geographically based. All of this requires a significant amount of overhead to normalize and bring into focus.</li>
  <li><strong>Manual</strong> - Ultimately Audrey has a lot of work ahead of her, manually pulling PDFs and performing searches, then copying and pasting data locally. Then she’ll have to roll up her sleeves to normalize all the data she has aggregated into a single, coherent vision of where the foundation has put its money.</li>
</ul>

<p>Data research takes time, and is tedious, mind numbing work. I encounter many projects like hers where I have to make a decision between scraping or manually aggregating and normalizing data–each project will have it’s own pros and cons. I wish I could help, but it sounds like it will end up being a significant amount of manual labor to establish a coherent set of data in Google Sheets. Once, she is done though, she has all the tools in place to publish as YAML to Github, and get to work telling stories around the data across her work using Jekyll and Liquid. I’m also helping her make sure she has a JSON representation of each of her data projects, allowing others to build on top of her hard work.</p>

<p>I wish all companies, organizations, institutions, and agencies would think about how they publish their data publicly. It’s easy to think that data stewards will have ill intentions when it comes to publishing data in a variety of formats like they do, but more likely it is just a change of stewardship when it comes to managing and publishing the data. Different folks will have different visions of what sharing data on the web needs to look like, and have different tools available to them, and without a clear strategy you’ll end up with a mosaic of published data over the years. Which is why I’m telling her story. I am hoping to possibly influence one or two data stewards, or would-be data stewards when it comes to the importance of pausing for a moment and thinking through your strategy for standardizing how you store and publish your data online.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/12/challenges-when-aggregating-data-from-across-the-years/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/12/20k-40k-60k-and-80k-foot-levels-of-industry-api-design-guidance/">20K, 40K, 60K, and 80K Foot Levels Of Industry API Design Guidance</a></h3>
        <span class="post-date">12 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/definition-of-high-altitude.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p><a href="http://org.open.referral.adopta.agency/">I am moving my Human Services Data API (HSDA) work</a> forward and one of the top items on the list to consider as part of the move from version 1.1 to 1.2 is all around the scope of the API design portion of the standard. We are at a phase where the API design still very much reflects the Human Services Data Specification (HSDS)–basically a very CRUD (Create, Read, Update and Delete) API. With version 1.2 I need to begin considering the needs of API consumers a little more, looking to vendors and real world practitioners to help understand what the next version(s) of the API definition will/should contain.</p>

<p>The most prominent discussion in the move from version 1.1 to 1.2 centers around scope of API design at four distinct levels of this work, where we are looking to move forward a variety of API design concerns for a large group of API consumers:</p>

<ul>
  <li><a href="https://github.com/openreferral/api-specification/issues/22">Data Scope / Filtering</a> - Discussions around how to filter data, allowing API consumers to search across the contents of any HSDA implementation, getting exactly the data they need, no more, no less.</li>
  <li><a href="https://github.com/openreferral/api-specification/issues/21">Schema Scope / Filtering</a> - Considering the design of simple, standard, or full schema responses that can specified using a prefer header, parameter, or path levels.</li>
  <li><a href="https://github.com/openreferral/api-specification/issues/38">Path Scope / Filtering</a> - How are API paths going to be group and organized, allowing a large surface area to be shared via documentation (APIs.json) in a way that new API consumers can start simple, advanced users can get what they need, and serving as many needs in between as we can.</li>
  <li><a href="https://github.com/openreferral/api-specification/issues/40">Project Scope / Filtering</a> - Adding the fourth dimension to this scope / filtering discussion, I’m proposing we discuss how projects are defined and isolated, which can allow them to move forward at different rates, and be reflected in documentation, code, and other resources–allowing for filtering by consumers, as  well as prioritization by vendors involved in the API design discussion.</li>
</ul>

<p>In short I have a large number of desires put on the table by vendors and practitioners. There is a mix of desire to load up as much functionality and API design guidance as we can at the single path level–meaning /organizations, /locations, and /services will allow you to get as much, or as little, as you desire. In this same conversation I have to defend the interests of newcomers, allowing them to easily learn about, and get what they need from these three distinct API paths, without loading them up with too much functionality. While also defend the long tail of needs for mobile, voice, and other leading edge application developers.</p>

<p>I’m looking at this discussion in these four dimensions, but I am trying to apply a horizontal or vertical approach in all four dimensions. Meaning, do I access or filter the amount of data I receive vertically with parameters or headers at the single API path level, or do I access and filter the amount of data I want horizontally across many separate API paths. The same logic applies to the API schema level with accessing vertically at the same path, by adding many subpaths, or do we approach horizontally with new paths. This continues at the API path, and project levels–how do discuss, develop, evolve, and allow API consumers to filter and only get at the API paths and projects they need–limiting the scope for new users, but meeting the demand of vendors, implementors, analysts, and other power consumers.</p>

<p>Aight, ok. Phew. I just needed to get that out. Not 100% sure it makes sense, but it is a framework I’m running with for a group conversation I’m having tomorrow–so we’ll see how it goes. One significant difference in this API design process from others is that it is not about any single API implementation. It is focused on moving forward a single API definition, or in the case of this discussion, many little API definition discussions, with lots of overlap, and under a single umbrella–to support thousands of API implementations. I’ll be publishing another piece shortly which zooms out to the 100K level of my HSDA work, where I’d consider this to be a little 20K (data), 40K (schema), 60K (path), and 80K (project) levels. I just needed to get a handle on this piece–if you actually read this far in the post, you are pretty geeky, and probably need a hobby (HSDA is mine).</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/12/20k-40k-60k-and-80k-foot-levels-of-industry-api-design-guidance/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/12/providing-solid-examples-that-api-consumers-can-learn-from-like-slack-app-blueprints/">Providing Solid Examples That API Consumers Can Learn From Like Slack App Blueprints</a></h3>
        <span class="post-date">12 Jul 2017</span>
        <p><a href="https://api.slack.com/best-practices/blueprints"><img src="https://s3.amazonaws.com/kinlane-productions/slack/slack-app-blueprints.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>People often learn through example. Before I’d ever consider myself a software engineer, I’d consider myself a reverse software engineer. 93% of what I know has been extracted from the work of others. Even with 7% being of my own creation, it is always heavily influenced by the work of others. People emulate what they know, what they see, and use. This is why as an API provider you should be showcasing best practices, positive examples, and healthy blueprints of what API consumers could (should) be doing.</p>

<p><a href="https://api.slack.com/best-practices/blueprints">You can see this in action with Slack’s best practice blueprints page</a>, where they provide six blueprints of applications that API consumers should be learning from. Slack doesn’t just provide a title, description and image of example applications, it is truly a blueprint–providing diagrams, links to documentation, code samples, and other essential knowledge you will need to successfully develop an application on Slack. Providing six solid examples that anyone can reverse engineer to understand how Slack application development could (should) work.</p>

<p>Slack app blueprints is just one component of <a href="https://api.slack.com/slack-apps">a pretty sophisticated getting started section offered as part of the Slack API ecosystem</a>. I am adding application blueprint as a building block to my <a href="http://getting-started.apievangelist.com/">getting started API research</a>, and adding it as a dimension to my <a href="http://documentation.apievangelist.com/">API documentation</a> &amp; <a href="http://sdk.apievangelist.com/">SDK research</a>–the overlap in these areas seem like it should be strong to me. Coming across Slack app blueprints, and writing this story has reminded me that I also need to write another piece on the Slack ecosystem, and generate an outline of all the building blocks they are using in their API ecosystem, and create an updated blueprint for successful API operations that other API providers can emulate.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/12/providing-solid-examples-that-api-consumers-can-learn-from-like-slack-app-blueprints/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/12/a-zapier-advocate-and-dedicated-api-resources-page-for-your-company/">A Zapier Advocate And Dedicated API Resources Page For Your Company</a></h3>
        <span class="post-date">12 Jul 2017</span>
        <p><a href="ttps://zapier.com"><img src="https://s3.amazonaws.com/kinlane-productions/zapier/zapier-icons.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>I am spending time going through some of the most relevant APIs I know of online today, working to create some 101 training materials for average folks to take advantage of. I’m looking through these APIs: Twitter, Google Sheets, Github, Flickr, Instagram, Facebook, YouTube, Slack, Dropbox, Paypal, Weather Underground, Spotify, Google Maps, Reddit, Pinterest, NY Times, Twilio, Stripe, SendGrid, Algolia, Keen, Census, Yelp, Walgreens. I feel they are some of the most useful solutions in the average business person who is API curious.</p>

<p>With these new lessons I’m trying to continue my work evangelizing APIs amongst the normals, helping them understand what APIs are, and what is possible when you put them to work. Once I introduce folks to each API I’m left with the challenge of how do I actually onboard them with each API when they aren’t actually a programmer. The number one way I’m helping alleviate this problem is by including <a href="https://zapier.com">Zapier</a> examples with each of my API lessons, helping folks understand that they can quickly get up and running with each API using the Zapier integration platform as a service (iPaaS). I will be including one or more Zapier examples along with each of my API 101 lessons, helping normal folk put what they’ve learned about APIs to use–hopefully making each lesson a little more sticky.</p>

<p>One of the primary targets for my lessons is the average worker at small, medium, and enterprise businesses, trying to help them understand that APIs aren’t just for developers, and that they can be putting APIs to use in their world. I tried to pick a handful of APIs that are relevant and useful in their daily lives, and helping them become aware of useful Zapier recipes they can adopt in their daily work. I’m looking to encourage users to become more API-literate, and begin connecting and orchestrating using APIs in their daily work. I’m hoping that eventually they will become confident enough by leverage APIs using Zapier that they will eventually become an advocate within their companies and organizations.</p>

<p>In my opinion, each company could really use a Zapier advocate. To help incentivize this behavior I’m  going to show folks how they can become an advocate for APIs and Zapier at their company, and provide them with some templates for how they can publish API training material on a page dedicated to Zapier within the company firewewall, or on some sort of company portal that the rest of the company has access to. Similar to how <a href="http://apievangelist.com/2014/03/13/api-management-adding-reciprocity-building-blocks/">I’ve been advocating API providers to publish an integration page in their developer portals</a>, I’m looking to also encourage business users to publish a similar page of useful Zaps involving API that are relevant to their company–allowing other folks at a company to learn, explore, and implement useful recipes that can help them be more successful  in their work.</p>

<p>A significant portion of my work as API Evangelist is dedicated to pushing forward the conversation around APIs, telling stories about the leading and bleeding edge of APIs, but I’m trying to not forget my roots, and my original mission to help non-developers understand the API potential. I feel that a wealth of <a href="http://101.apievangelist.com">API 101 materials</a>, combined with examples of Zapier advocacy and storytelling, and pages dedicated to sharing Zapier recipes (Zaps) will help go a long ways to help encourage adoption amongst business users. My first API 101 lessons are rolling off the assembly line, and the next step is to create an example page where these lessons can be published, including other resources and recipes for using Zapier to exercise each lessons learned. If you would like to learn how to become a <a href="https://zapier.com">Zapier</a> advocate at your company please drop me a line, I’m looking for a few beta users to help me push forward this work in a meaningful way.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/12/a-zapier-advocate-and-dedicated-api-resources-page-for-your-company/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/11/each-airtable-datastore-comes-with-complete-api-and-developer-portal/">Each Airtable Datastore Comes With Complete API and Developer Portal</a></h3>
        <span class="post-date">11 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/airtable/airtable-api-for-or2.png" width="40%" style="padding: 15px;" align="right" /></p>
<p>I see a lot of tools come across my desk each week, and I have to be honest I don’t alway fully get what they are and what they do. There are many reasons why I overlook interesting applications, but the most common reason is because I’m too busy and do not have the time to fully play with a solution. One application I’ve been keeping an eye on as part of my work is <a href="https://airtable.com">Airtable</a>, which I have to be honest, I didn’t get what they were doing, or really I just didn’t notice because I was too busy.</p>

<p>Airtable is part spreadsheet, part database, that operates as a simple, easy to use web application, which with a push of a button, you can publish an API from. You don’t just get an API by default with each Airtable, you get a pretty robust developer portal for your API complete with  good looking API documentation. Allowing you to go from an Airtable (spreadsheet / database) to API and documentation–no coding necessary. Trust me. Try it out, anyone can create an Airtable and publish an API that any developer can visit and quickly understand what is going on.</p>

<p>As a developer, API deployment still feels like it can be a lot of work. Then, once I take off my programmers hat, and put on my business user hat, I see that there are some very easy to use solutions like <a href="https://airtable.com">Airtable</a> available to me. Knowing how to code is almost slowing me down when it comes API deployment. Sure, the APIs that Airtable publishes aren’t the perfectly designed, artisanally crafted API I make with my bare hands, but they work just as well as mine. Most importantly, they get business done. No coding necessary. Something that anyone can do without the burden of programming.</p>

<p><a href="https://airtable.com">Airtable provides me another solution</a> that I can recommend that my readers and clients should consider using when managing their data, which will also allow them to easily deploy an API for developers to build applications against.<a href="https://airtable.com/integrations">I also notice that Airtable has a whole API integration part of their platform, which allows you to integrate your Airtables into other APIs</a>–something I will have to write about separately in a future post. I just wanted to make sure and take the time to properly add Airtable to my research, and write a story about them so that they are in my brain, available for recall when people are asking me for easy to use solutions that will help them deploy an API.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/11/each-airtable-datastore-comes-with-complete-api-and-developer-portal/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/11/when-you-publish-a-google-sheet-to-the-web-it-als-becomes-an-api/">When You Publish A Google Sheet To The Web It Also Becomes An API</a></h3>
        <span class="post-date">11 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/google-sheets/google-sheets-icon.jpg" align="right" width="30%" style="padding: 15px;" /></p>
<p>When you take any Google Sheet and choose to publish it to the web, you immediately get an API. Well, you get the HTML representation of the spreadsheet (shared with the web), and if you know the right way to ask, you also can get the JSON representation of the spreadsheet–which gives you an interface you can program against in any application.</p>

<p>Articles I curate, the companies, institutions, organizations, government agencies, and everything else I track on lives in Google Sheets that are published to the web in this way. When you are viewing any Google Sheet in your browser you are viewing it using a URL like:</p>

<p>https://docs.google.com/spreadsheets/d/[sheet_id]/edit</p>

<p>Of course, [sheet_id] is replaced with the actual id for your sheet, but the URL demonstrates what you will see. Once you publish your Google sheet to the web you are given a slight variation on that url:</p>

<p>https://docs.google.com/spreadsheets/d/[sheet_id]/pubhtml</p>

<p>This is the URL you will share with the public, allowing them to view the data you have in your spreadsheet in their browsers. In order to get at a JSON representation of the data you just need to learn the right way to craft the URL using the same sheet id:</p>

<p>https://spreadsheets.google.com/feeds/list/[sheet_id]/default/public/values?alt=json</p>

<p>Ok, one thing I have to come clean on is that the JSON available for each Google sheet is not the most intuitive JSON you will come across, but once you learn what is going on you can easily consume the data within a spreadsheet using any programming languages. Personally, I use a <a href="https://github.com/jsoma/tabletop">JavaScript library called tabletop.js</a> that quickly helps you make sense of a spreadsheet and get to work using the data in any (JavaScript) application.</p>

<p>The fastest, lowest cost way to deploy an API is to put some data in a Google Sheet, and hit publish to the web. Ok, its not a full blown API, it’s just JSON available at a public URL, but it does provide an interface you can program against when developing an application. I take all the data I have in spreadsheets and publish to Github as YAML, and then make static APIs available using that YAML in XML, CSV, JSON, Atom, or any other format that I need. Taking the load of Google, creating a cached version at any point in time that runs on Github, in a versioned repository that anyone can fork, or integrate into any workflow.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/11/when-you-publish-a-google-sheet-to-the-web-it-als-becomes-an-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/11/either-you-own-the-conversation-around-your-apis-or-someone-else-will/">Either You Own The Conversation Around Your APIs Or Someone Else Will</a></h3>
        <span class="post-date">11 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/rogue/tinder-api-google-search.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I was looking at how many of the top mobile applications in the iTunes story actually had a public API presence, and was finding it very telling what came up in the Google search results for each company when I searched [company name] + API. It tells a lot about how a company sees the world, when they don’t have a public API presence, but they have a very public mobile application that uses APIs.</p>

<p>An example of this is with Tinder, where the top listings are all Github rogue API repositories, when you Google “Tinder API”. Tinder doesn’t own the conversation when it comes to their own APIs. While the Tinder APIs are public, and well documented, Tinder prefers acting like they are private–they aren’t. Pinterest uses SSL pinning, but <a href="https://ritcsec.wordpress.com/2016/12/11/bypassing-certificate-pinning-on-tinder/">there is even a good amount of information out there at how to get around that</a>, making the mapping out and documenting of Tinder APIs a pretty doable thing.</p>

<p>Honestly, I don’t care about Tinder’s APIs. They are just an easy example to point a finger at and use as a poster child. I don’t even expect them to have fully public APIs that any developer could use without permission. Sure, lock that shit down, but provide a sandbox, and make sure every application gets approval before they can more access to live data. Make sure that you own the API conversation by having a developers portal, and provide information regarding what it takes to get access, and maybe some day actually become an approved partner.</p>

<p>I’m not saying that every company should have freely available public APIs. I’m saying every company should own the public conversation around their APIs, no matter what their strategy for developing applications around a platform’s APIs. Have a presence. Own the conversation. Have a door for application developers to walk, even if there is a waiting room. Not all applications will be competing with your own web, mobile, device, or network applications. Some will be about enabling data portability for you users, or maybe provide useful access aggregate data for use in visualizations–you never know what folks will be bringing to the table, why keep the door closed?</p>

<p>I understand. You may not be all team API like I am, but you are using APIs to drive your mobile experience. I just don’t get why you wouldn’t want to own the conversation around these APIs. You are leaving so much on the table. If your mobile app is finding success, people will want access to the goodness going on behind it–<a href="http://apievangelist.com/2011/02/08/instagram-launches-api/">a rogue API is what kickstarted the Instagram API in the early days</a>. It is pretty easy to reverse engineer any mobile application, and map out the surface area of the API behind, as well as the authentication in play. Either you own the conversation around your API, or someone will step up and do it for you in todays online world.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/11/either-you-own-the-conversation-around-your-apis-or-someone-else-will/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/11/locking-down-drones-and-iot-devices-by-manufacturers/">Locking Down Drones And IoT Devices By Manufacturers</a></h3>
        <span class="post-date">11 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/drones/drone-rock-outdoors.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>I have been following stories about, as well as personally experiencing DJI restricting where their drones can fly, going beyond just warning you about restricted areas and actually locking down or restricting your drone capabilities. <a href="https://motherboard.vice.com/en_us/article/3knkgn/dji-is-locking-down-its-drones-against-a-growing-army-of-diy-hackers">So it was interesting to also read a post in Motherboard about the company also locking down drones to prevent against hacking, modifying, and tweaking your DJI drones as you wish</a>. Drones for me are a poster child for the entire Internet of Things (IoT), and I think DJI’s approach is a sign of what is to come for all Internet connected devices.</p>

<p>In coming years, there will be a lot that the IoT community can learn from the drone space. From the technical to regulatory, drones will be pushing forward conversations about our networks, cameras, security, privacy, surveillance, and corporate and government control over us, and our devices. Drones stimulate some interesting emotions within people associated with the industry, but more importantly people who know nothing about drones, and will be weighing in on regulation at the municipal, all the way up to the federal and international levels.</p>

<p>I thought it was interesting when DJI began enforcing the recommendations I get in the dashboard for my drones, and requiring that I update my drones, RC controller, and mobile applications to reduce their liability regarding what I an actually doing with my devices. However, locking down drones so people can’t modify, augment, or fix their own drones is a whole other layer to this discussion that isn’t just about stopping ISIS from strapping bombs to their drones, it is also about maintaining sovereignty over their creations, and limiting what we can do as owners when it comes to fixing our devices. <a href="https://www.theguardian.com/environment/2017/mar/06/nebraska-farmers-right-to-repair-john-deere-apple">We already see the right to fix conversation bubble up in the John Deere ecosystem</a>, but it is something we will continue to see showing up in IoT ecosystems across many different business sectors.</p>

<p><a href="http://kinlane.com/2017/01/26/the-digital-things-that-happen-in-the-privacy-of-our-homes/">The bold entry into our homes and lives that IoT device manufacturers are making amazes me</a>, but what amazes me even more is how consumers allow this to happen with little resistance. This is another outcome from the drone sector I believe we’ll see more of, is drone operators standing up to defend their right to fix, as well as push back own data, content, an algorithmic ownership over what is produced using devices. Sadly consumers do not understand the value of their data, but hobbyists and commercial operators of drones, and hopefully other devices, do see the value of it, and will begin to shift the balance when it comes to who is profiting off the data our devices are generating.</p>

<p>There will be many technical, business, and political lessons to be learned from the drone space in coming years. I’m strangely thankful that my <a href="http://dronerecovery.org">Drone Recovery project</a> happened, because before that summer I really was not interested in drones, but now I’m not just interested, I own three drones, and have an active interest in understanding what manufacturers like DJI are doing. I’m feel that what DJI is doing with their platform will set a precedent (good and bad) for other IoT operators to follow–something I’ll be keeping a close eye on.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/11/locking-down-drones-and-iot-devices-by-manufacturers/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/10/github-serverless/">Github Serverless</a></h3>
        <span class="post-date">10 Jul 2017</span>
        <p><a href="https://octodex.github.com/daftpunktocat-thomas"><img src="https://octodex.github.com/images/daftpunktocat-thomas.gif" align="right" width="30%" style="padding: 15px;" /></a></p>
<p>I run the entire front-end of my online presence using Github. <a href="http://apievangelist.com/api-lifecycle/">All my API Evangelist research lives as open repositories on Github, with the website running Jekyll, hosted on Github Pages</a>. My front-end is all HTML, JavaScript, and CSS, that leverages YAML data, and displayed using <a href="https://shopify.github.io/liquid/">Liquid</a>. It provides me a nice way to offload the public side of my operations to Github.</p>

<p>I am increasingly doing this with all of my data, by publishing it as YAML, and rendering a dynamic (static) API representation in JSON–all done with the same approach I’m using to publish my website(s). <a href="http://developer.apievangelist.com/">You can get at all of the data I use across my API research in a single API Evangelist developer portal</a>, which just aggregates all of the JSON APIs I’ve published across my network almost 100 Github repositories, and supporting sites.</p>

<p>Another thing I’m experimenting with is publishing simple JavaScript functions to individual pages within Github repositories. These scripts do a range of things from pulling items I’ve curated from <a href="https://developer.feedly.com/">the Feedly API</a>, fresh data from Google Sheets that I am using as data stores, and a variety of other jobs across my network of research sites, data projects, and API tooling. Some of these scripts I’m running manually, while others I run on a variety of schedules using <a href="https://www.easycron.com">EasyCron</a>.</p>

<p>The approach definitely has some significant limitations, but I find that I’m able to get quite a bit done with JavaScript by pulling data from external APIs and other feeds, and using each Github repo as storage, and the Github API as the read/write layer for this storage. I do not store any API keys, tokens, or other secrets in the Github repositories, I’m passing them all in via the URL, which isn’t the most secure, and could in theory be abducted in transit even though I’m using SSL–something I’d like to improve upon by passing a single token to unlock a private store. I have access to external systems via APIs, storage and compute via Github, and I can control everything through variety of functional JavaScripts I maintain using Github, and keep indexed using APIs.json.</p>

<p>It is becoming a kind of poor man’s serverless. I’m going to keep polishing my approach. Get better with my responses, and my approach to reading and writing data schema to the data storage folder in each of my repositories, which can then be read statically using JSON APIs I’ve pushed from this data, using Liquid. It is a pretty scrappy approach to serverless, but done in a way that takes the servers out of the equation for me, offloading the front-end and back-end work for my network of sites to Github. I am not sure where I’m going with this. Sometimes I get better results from a more straightforward API implementation on my Amazon infrastructure, but I am finding some interesting use cases, and seeing another side-effect I am enjoying–it is making my serverless infrastructure forkable and usable by others.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/10/github-serverless/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/10/having-the-right-commications-pipeline-for-your-api-platform/">Having The Right Communications Pipeline For Your API Platform</a></h3>
        <span class="post-date">10 Jul 2017</span>
        <p><a href="https://matthewreinbold.com/2017/07/04/SiteUpdate/"><img src="https://s3.amazonaws.com/kinlane-productions/matts-blog.png" align="right" width="40%" style="padding: 15px;" /></a></p>
<p>My friend Matthew Reinbold, formerly of Vox Pop, and now the Lead for the Capital One API Center of Excellence, as well as the maintainer of <a href="http://webapi.events/">web API events</a> has shifted <a href="https://matthewreinbold.com/2017/07/04/SiteUpdate/">his blogging platform to use Github, using Jekyll</a>. Ok, yawn, why is this news? Someone is shifting the underlying platform for their blog. Well, first Matt is one of the leading API practitioners in the space, who is also a storyteller. Second, his approach highlights a set of tools that other API providers should be considering for their API communications pipeline.</p>

<p>Matt is using a pretty potent formula for his communications platform in my opinion, with a handful of essential ingredients:</p>

<ul>
  <li><strong><a href="https://github.com">Github</a></strong> - Using a Github repository as the open source folder for your website.</li>
  <li><strong><a href="https://pages.github.com/">Github Pages</a></strong> - Using Github Pages to publish the front-end for your website.</li>
  <li><strong><a href="https://jekyllrb.com/">Jekyll</a></strong> - The content management system that sits in the folder for your website.</li>
  <li><strong><a href="https://www.cloudflare.com/">CloudFlare</a></strong> - The DNS and SSL front-end for your website, complete with analytics.</li>
  <li><strong><a href="https://www.hover.com/">Hover</a></strong> - The registrar for the domain which you offload DNS management to CloudFlare.</li>
</ul>

<p>Matt is taking advantage of the benefits of static website development, which some of the benefits are, as Matt describes:</p>

<ul>
  <li><strong>SPEED</strong> - There’s no processing server side; posts have already been reduced to the essential atomic units of the web: HTML, Javascript, and CSS. There’s something poetic to me about that.</li>
  <li><strong>Security</strong> - While not so much an issue with my own coded CMS, I lived in constant fear of missing a zero-day Wordpress exploit patch and finding myself, along with clients, compromised. Reducing the number of moving parts significantly decreases the places where something might go wrong.</li>
  <li><strong>Hosting</strong> - Rather than having to find, research, and deploy to increasingly rare ColdFusion hosts (or port to another language), I can post my content to anywhere that supports HTTP/JS/CSS. hosting. This becomes very compelling given that Github Pages, one option, is free.</li>
</ul>

<p>This is the cheapest and quickest way for your API to get a blog stood up, and get publishing stories about the value your API is bringing to the table. This approach isn’t just limited to your developer portal or engineering team blog, this could be for partners, or any API related project that you are running. I publish a static Jekyll blog <a href="http://apievangelist.com/api-lifecycle/">for each area of my research</a>, and I try to always have one for each of my data, or API tooling project–telling the story of each project that is independent from the API Evangelist blog, providing a static log of everything that has happened.</p>

<p>Github isn’t just a pipeline for code, it can be a pipeline for your communications and storytelling. It also can be a pipeline for your documentation, how-to-guides, and other resources. I’m happy to see Matt putting it to be use. Another thing I like about his post, other than him mentioning me ;-), is that he also mentioned the benefits of this approach over using Medium, which is something I’ve been advising API providers against for some time. In my opinion you are better off publishing your blog like Matt has, and then syndicating to Medium if you want. <a href="https://matthewreinbold.com/2017/07/04/SiteUpdate/">There is a lot more detail available on Matt’s story behind his new blog strategy</a>, I recommend heading over and learning from what he’s done.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/10/having-the-right-commications-pipeline-for-your-api-platform/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/10/being-first-with-any-technology-trend-is-hard/">Being First With Any Technology Trend Is Hard</a></h3>
        <span class="post-date">10 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/iron-io/Iron-io-Platform_Diagram_V3-05.png" align="right" width="40%" style="padding: 15px;" /></p>
<p><a href="https://www.iron.io/ich-bin-ein-evangelist/">I first wrote about Iron.io back in 2012</a>. The are an API-first company, and they were the first serverless platform. I’ve known the team since they first reached out back in 2011, and I consider them one of my poster children for why there is more to all of this than just the technology. Iron.io gets the technology side of API deployment, and they saw the need for enabling developers to go serverless, running small scalable scripts in the cloud, and offloading the backend worries to someone who knows what they are doing.</p>

<p>Iron.io is what I’d consider to be a pretty balanced startup, slowly growing, and taking sensible amounts of funding they needed to grow their business. The primary area I would say that Iron.io has fallen short is when it comes to storytelling about what they are up to, and generally playing the role of a shiny startup everyone should pay attention to. They are great storytellers, but unfortunately the frequency and amplification of their stories has fallen short, allowing other strong players to fill the void–opening the door for Amazon to take the lion share of the conversation when it comes to serverless. Demonstrating that you can rock the technology side of things, but if you don’t also rock the storytelling and more theatrical side of things, there is a good chance you can come in second.</p>

<p>Storytelling is key to all of this. I always love the folks who push back on me saying that nobody cares about these stories, the markets only care about successful strong companies–when it reality, IT IS ALL ABOUT STORYTELLING! Amazon’s platform machine is good at storytelling. Not just their serverless group, but the entire platform. They blog, tweet, publish press releases, whisper in reporter ears, buy entire newspapers, publish science fiction patents, conduct road shows, and flagship conferences. Each AWS platform team can tap into this, participate, and benefit from the momentum, helping them dominate the conversation around their particular technical niche.</p>

<p>Being first with any technology trend will always be hard, but it will be even harder if you do not consistently tell stories about what you are doing, and what those who are using your platform are doing with it. Iron.io has been rocking it for five years now, and <a href="https://medium.com/travis-on-development/what-is-serverless-computing-and-why-is-it-important-3278b4ffe814">are continuing to define what serverless is all about</a>, they just need to turn up the volume a little bit, and keep doing what they are doing. I’ll own a portion of this story, as I probably didn’t do my share to tell more stories about what they are up to, which would have helped amplify their work over the years–something I’m working to correct with a little storytelling here on API Evangelist.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/10/being-first-with-any-technology-trend-is-hard/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/10/opportunity-to-develop-a-threat-intelligence-apis-json/">Opportunity To Develop A Threat Intelligence Aggregation API</a></h3>
        <span class="post-date">10 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/facing-cannon_copper_circuit.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>I came across <a href="https://github.com/hslatman/awesome-threat-intelligence">this valuable list of threat intelligence resources</a> and think that the section on information sources should be aggregated and provided as a single threat intelligence API. When I come across valuable information repos like this my first impulse is to go through them, standardize and upload as JSON and YAML to Github, making all of this data forkable, and available via an API.</p>

<p>Of course if I responded to every impulse like this I would never get any of my normal work done, and actually pay my bills. A second option for me is to put things out there publicly in hopes that a) someone will pay me to do the work, or b) someone else who has more time, and the rent paid will tackle the work. With this in mind, this list of sources should be standardized, and publish to Github and as an API:</p>

<ul>
  <li><a href="http://s3.amazonaws.com/alexa-static/top-1m.csv.zip" target="_blank">Alexa Top 1 Million sites</a> - Probable Whitelist of the top 1 Million sites from Amazon(Alexa).</li>
  <li><a href="https://docs.google.com/spreadsheets/u/1/d/1H9_xaxQHpWaa4O_Son4Gx0YOIzlcBWMsdvePFX68EKU/pubhtml" target="_blank">APT Groups and Operations</a> - A spreadsheet containing information and intelligence about APT groups, operations and tactics.</li>
  <li><a href="https://www.autoshun.org/" target="_blank">AutoShun</a> - A public service offering at most 2000 malicious IPs and some more resources.</li>
  <li><a href="https://www.circl.lu/projects/bgpranking/" target="_blank">BGP Ranking</a> - Ranking of ASNs having the most malicious content.</li>
  <li><a href="https://intel.malwaretech.com/" target="_blank">Botnet Tracker</a> - Tracks several active botnets.</li>
  <li><a href="http://danger.rulez.sk/projects/bruteforceblocker/" target="_blank">BruteForceBlocker</a> - BruteForceBlocker is a perl script that monitors a server’s sshd logs and identifies brute force attacks, which it then uses to automatically configure firewall blocking rules and submit those IPs back to the project site, <a href="http://danger.rulez.sk/projects/bruteforceblocker/blist.php">http://danger.rulez.sk/projects/bruteforceblocker/blist.php</a>.</li>
  <li><a href="http://osint.bambenekconsulting.com/feeds/c2-ipmasterlist.txt" target="_blank">C&amp;C Tracker</a> - A feed of known, active and non-sinkholed C&amp;C IP addresses, from Bambenek Consulting.</li>
  <li><a href="http://cinsscore.com/list/ci-badguys.txt" target="_blank">CI Army List</a> - A subset of the commercial <a href="http://cinsscore.com/">CINS Score</a> list, focused on poorly rated IPs that are not currently present on other threatlists.</li>
  <li><a href="http://s3-us-west-1.amazonaws.com/umbrella-static/index.html" target="_blank">Cisco Umbrella</a> - Probable Whitelist of the top 1 million sites resolved by Cisco Umbrella (was OpenDNS).</li>
  <li><a href="https://intel.criticalstack.com/" target="_blank">Critical Stack Intel</a> - The free threat intelligence parsed and aggregated by Critical Stack is ready for use in any Bro production system. You can specify which feeds you trust and want to ingest.</li>
  <li><a href="https://www.c1fapp.com/" target="_blank">C1fApp</a> - C1fApp is a threat feed aggregation application, providing a single feed, both Open Source and private. Provides statistics dashboard, open API for search and is been running for a few years now. Searches are on historical data.</li>
  <li><a href="https://www.cymon.io/" target="_blank">Cymon</a> - Cymon is an aggregator of indicators from multiple sources with history, so you have a single interface to multiple threat feeds. It also provides an API to search a database along with a pretty web interface.</li>
  <li><a href="https://intel.deepviz.com/recap_network.php" target="_blank">Deepviz Threat Intel</a> - Deepviz offers a sandbox for analyzing malware and has an API available with threat intelligence harvested from the sandbox.</li>
  <li><a href="http://rules.emergingthreats.net/fwrules/" target="_blank">Emerging Threats Firewall Rules</a> - A collection of rules for several types of firewalls, including iptables, PF and PIX.</li>
  <li><a href="http://rules.emergingthreats.net/blockrules/" target="_blank">Emerging Threats IDS Rules</a> - A collection of Snort and Suricata <i>rules</i> files that can be used for alerting or blocking.</li>
  <li><a href="https://exonerator.torproject.org/" target="_blank">ExoneraTor</a> - The ExoneraTor service maintains a database of IP addresses that have been part of the Tor network.  It answers the question whether there was a Tor relay running on a given IP address on a given date.</li>
  <li><a href="http://www.exploitalert.com/" target="_blank">Exploitalert</a> - Listing of latest exploits released.</li>
  <li><a href="https://feodotracker.abuse.ch/" target="_blank">ZeuS Tracker</a> - The Feodo Tracker <a href="https://www.abuse.ch/" target="_blank">abuse.ch</a> tracks the Feodo trojan.</li>
  <li><a href="http://iplists.firehol.org/" target="_blank">FireHOL IP Lists</a> - 400+ publicly available IP Feeds analysed to document their evolution, geo-map, age of IPs, retention policy, overlaps. The site focuses on cyber crime (attacks, abuse, malware).</li>
  <li><a href="https://fraudguard.io/" target="_blank">FraudGuard</a> - FraudGuard is a service designed to provide an easy way to validate usage by continuously collecting and analyzing real-time internet traffic.</li>
  <li><a href="http://hailataxii.com/" target="_blank">Hail a TAXII</a> - Hail a TAXII.com is a repository of Open Source Cyber Threat Intelligence feeds in STIX format. They offer several feeds, including some that are listed here already in a different format, like the Emerging Threats rules and PhishTank feeds.</li>
  <li><a href="https://www.iblocklist.com/lists" target="_blank">I-Blocklist</a> - I-Blocklist maintains several types of lists containing IP addresses belonging to various categories. Some of these main categories include countries, ISPs and organizations. Other lists include web attacks, TOR, spyware and proxies. Many are free to use, and available in various formats.</li>
  <li><a href="https://majestic.com/reports/majestic-million" target="_blank">Majestic Million</a> - Probable Whitelist of the top 1 million web sites, as ranked by Majestic. Sites are ordered by the number of referring subnets. More about the ranking can be found on their <a href="https://blog.majestic.com/development/majestic-million-csv-daily/" target="_blank">blog</a>.</li>
  <li><a href="http://www.malshare.com/" target="_blank">MalShare.com</a> - The MalShare Project is a public malware repository that provides researchers free access to samples.</li>
  <li><a href="http://www.malwaredomains.com/" target="_blank">MalwareDomains.com</a> -   The DNS-BH project creates and maintains a listing of domains that are known to be used to propagate malware and spyware. These can be used for detection as well as prevention (sinkholing DNS requests).</li>
  <li><a href="https://www.metadefender.com/threat-intelligence-feeds" target="_blank">Metadefender.com</a> -   Metadefender Cloud Threat Intelligence Feeds contains top new malware hash signatures, including MD5, SHA1, and SHA256. These new malicious hashes have been spotted by Metadefender Cloud within the last 24 hours. The feeds are updated daily with newly detected and reported malware to provide actionable and timely threat intelligence.</li>
  <li><a href="https://services.normshield.com" target="_blank">NormShield Services</a> -   NormShield Services provide thousands of domain information (including whois information) that potential phishing attacks may come from. Breach and blacklist services also available. There is free sign up for public services for continuous monitoring.</li>
  <li><a href="http://www.openbl.org/lists.html" target="_blank">OpenBL.org</a> -   A feed of IP addresses found to be attempting brute-force logins on services such as SSH, FTP, IMAP and phpMyAdmin and other web applications.</li>
  <li><a href="https://openphish.com/phishing_feeds.html" target="_blank">OpenPhish Feeds</a> - OpenPhish receives URLs from multiple streams and analyzes them using its proprietary phishing detection algorithms. There are free and commercial offerings available.</li>
  <li><a href="https://www.phishtank.com/developer_info.php" target="_blank">PhishTank</a> - PhishTank delivers a list of suspected phishing URLs. Their data comes from human reports, but they also ingest external feeds where possible. It’s a free service, but registering for an API key is sometimes necessary.</li>
  <li><a href="http://ransomwaretracker.abuse.ch/" target="_blank">Ransomware Tracker</a> - The Ransomware Tracker by <a href="https://www.abuse.ch/" target="_blank">abuse.ch</a> tracks and monitors the status of domain names, IP addresses and URLs that are associated with Ransomware, such as Botnet C&amp;mp;C servers, distribution sites and payment sites.</li>
  <li><a href="https://isc.sans.edu/suspicious_domains.html" target="_blank">SANS ICS Suspicious Domains</a> - The Suspicious Domains Threat Lists by <a href="https://isc.sans.edu/suspicious_domains.html" target="_blank">SANS ICS</a> tracks suspicious domains. It offers 3 lists categorized as either <a href="https://isc.sans.edu/feeds/suspiciousdomains_High.txt" target="_blank">high</a>, <a href="https://isc.sans.edu/feeds/suspiciousdomains_Medium.txt" target="_blank">medium</a> or <a href="https://isc.sans.edu/feeds/suspiciousdomains_Low.txt" target="_blank">low</a> sensitivity, where the high sensitivity list has fewer false positives, whereas the low sensitivty list with more false positives. There is also an <a href="https://isc.sans.edu/feeds/suspiciousdomains_whitelist_approved.txt" target="_blank">approved whitelist</a> of domains. Finally, there is a suggested <a href="https://isc.sans.edu/block.txt" target="_blank">IP blocklist</a> from <a href="https://dshield.org">DShield</a>.</li>
  <li><a href="https://github.com/Neo23x0/signature-base" target="_blank">signature-base</a> - A database of signatures used in other tools by Neo23x0.</li>
  <li><a href="https://www.spamhaus.org/" target="_blank">The Spamhaus project</a> - The Spamhaus Project contains multiple threatlists associated with spam and malware activity.</li>
  <li><a href="https://sslbl.abuse.ch/" target="_blank">SSL Blacklist</a> -   SSL Blacklist (SSLBL) is a project maintained by abuse.ch. The goal is to provide a list of “bad” SSL certificates identified by abuse.ch to be associated with malware or botnet activities. SSLBL relies on SHA1 fingerprints of malicious SSL certificates and offers various blacklists</li>
  <li><a href="https://statvoo.com/dl/top-1million-sites.csv.zip" target="_blank">Statvoo Top 1 Million Sites</a> -   Probable Whitelist of the top 1 million web sites, as ranked by Statvoo.</li>
  <li><a href="https://strongarm.io" target="_blank">Strongarm, by Percipient Networks</a> - Strongarm is a DNS blackhole that takes action on indicators of compromise by blocking malware command and control. Strongarm aggregates free indicator feeds, integrates with commercial feeds, utilizes Percipient’s IOC feeds, and operates DNS resolvers and APIs for you to use to protect your network and business. Strongarm is free for personal use.</li>
  <li><a href="http://www.talosintelligence.com/aspis/" target="_blank">Talos Aspis</a> - Project Aspis is a closed collaboration between Talos and hosting providers to identify and deter major threat actors. Talos shares its expertise, resources, and capabilities including network and system forensics, reverse engineering, and threat intelligence at no cost to the provider.</li>
  <li><a href="http://www.threatglass.com/" target="_blank">Threatglass</a> - An online tool for sharing, browsing and analyzing web-based malware. Threatglass allows users to graphically browse website infections by viewing screenshots of the stages of infection, as well as by analyzing network characteristics such as host relationships and packet captures.</li>
  <li><a href="https://www.threatminer.org/" target="_blank">ThreatMiner</a> - ThreatMiner has been created to free analysts from data collection and to provide them a portal on which they can carry out their tasks, from reading reports to pivoting and data enrichment. The emphasis of ThreatMiner isn’t just about indicators of compromise (IoC) but also to provide analysts with contextual information related to the IoC they are looking at.</li>
  <li><a href="https://virusshare.com/" target="_blank">VirusShare</a> - VirusShare.com is a repository of malware samples to provide security researchers, incident responders, forensic analysts, and the morbidly curious access to samples of malicious code. Access to the site is granted via invitation only.</li>
  <li><a href="https://github.com/Yara-Rules/rules" target="_blank">Yara-Rules</a> -   An open source repository with different Yara signatures that are compiled, classified and kept as up to date as possible.</li>
  <li><a href="https://zeustracker.abuse.ch/" target="_blank">ZeuS Tracker</a> -   The ZeuS Tracker by <a href="https://www.abuse.ch/" target="_blank">abuse.ch</a> tracks ZeuS Command &amp; Control servers (hosts) around the world and provides you a domain- and a IP-blocklist.</li>
</ul>

<p>Ideally, <a href="https://github.com/hslatman/awesome-threat-intelligence">each source on this list</a> would be publishing a forkable version of their data on Github and/or deploying a simple web API, but alas it isn’t the world we live in. Part of the process to standardardize and normalize the threat intelligence from all of these source would be to reach out to each provider, and take their temperature regarding working together to improve the data source by itself, as well as part of an aggregated set of data and API sources.</p>

<p>Similar to what I’m trying to do across many of the top business sectors being impacted by APIs, we need to to work aggregating all the existing sources of threat intelligence, and begin identifying a common schema that any new player could adopt. We need an open data schema, API definition, as well as suite of open source server and client tooling to emerge, if we are going to stay ahead of the cybersecurity storm that has engulfed us, and will continue to surround us until we work together to push it back.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/10/opportunity-to-develop-a-threat-intelligence-apis-json/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/07/when-json-schema-is-seen-as-power/">When JSON Schema Is Seen As Power</a></h3>
        <span class="post-date">07 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/power-lines-empty-space_sunday.jpg" align="right" width="40%" style="padding: 15px;" /></p>
<p>In a 30 year career as a database professional I’ve seen some extraordinary ways in which owning and controlling data is associated with power. Those who have the data leverage it against those who do not have it. Losing control means losing power, so people do whatever they can to stay in control, protecting the spreadsheets and databases at all costs. After 30 years of seeing this play out over and over again, I thought I’d seen it all, but sadly in an API era I’m just seeing new incarnations of data being wielded by those in power.</p>

<p>I recently came across an example where a company was holding back a series of JSON schema for a variety of public datasets, and standards in use as part of some government systems. From what I can tell company had been brought in to handle the systems and open data work a few years back, and with each version of the software and schema they slowly began to maintain tighter control over the schema, while they were also being mandated to be more open with the data–shifting from being controlling over the data, to being controlling of the schema.</p>

<p>They see the ability to be able to validate data, API requests and responses as something only a handful of people should be able to do. If you have the ability to validate, and say, “yes that data or API is compliant”, you are now in a position of power. This groups was mandated to be open with the data, allowing it flow freely between open source and proprietary systems, keeping in sync with laws and regulations, but they had found another way to remain as gatekeeper–I think this is what some folks call innovation, and thinking out of the box.</p>

<p>In my world, it is just another example of how power will always find ways to keep data from flowing, no matter how it learns to be perceived as playing nicely in an open data and API world. Many companies are still playing by the old rules and just hoarding, locking, up and controlling data–refusing to play along in the API game. However it is fascinating to see how power can shape shift and find new ways to protect its interest in this new landscape. After 30 years of doing this I am not surprised, but I do have to call it out when I see it because, well it is not right. Be open and share your schema, and let everyone be able to validate that data is what it should be.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/07/when-json-schema-is-seen-as-power/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/07/06/the-essential-api-elements-in-my-world/">The Essential API Elements In My World</a></h3>
        <span class="post-date">06 Jul 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/periodic-elements.png" align="right" width="40%" style="padding: 15px;" /></p>
<p>In 2017 there seems to be an API for just about everything. You can make products available via an API, messing, images, videos, and any of the digital bits that make up our lives. I still get excited by some new APIs, but APIs have to have real usage, and deliver real value before I’ll get too worked up about them. I’m regularly looking down the <a href="http://apievangelist.com/2017/01/09/the-api-driven-marketplace-that-is-my-digital-self/">list of my digital bits</a> thinking about which are the most important to me, which ones I’ll keep around, and the services I’ll adopt to help me define and manage these bits.</p>

<p>This process has got me thinking really deeply about what I’d consider to be the three most important types of APIs in my life:</p>

<ul>
  <li><strong>Compute</strong> - In my world compute is all about AWS EC2 instances, but when I think about it, Github really handles the majority of the compute for my front-end, but EC2 is the scalable compute for the backend of my world that is driving my APIs.</li>
  <li><strong>Storage</strong> - Primarily storage is all about Amazon S3, but I also depend on Dropbox, Google Drive, and I also put Github into the storage bucket because I store quite a bit of JSON, YAML, and other data there.</li>
  <li><strong>DNS</strong> - apievangelist.com and kinlane.com are very important domains in my world–they are how I make my living, and share my stories. CloudFlare is how I manage this frontline of my world, making DNS an extremely important element in my world.</li>
</ul>

<p>I leverage compute, storage, and DNS APIs regularly throughout each day–making them very important APIs in my existence. However, these are also the essential ingredients of my APIs as well. I consume these APIs, but I also deploy my APIs with these three elements. Each API has a compute and storage layer, with DNS as the naming, addressing, and discovery for these valuable resources in my world. This makes these three aspects of operating online, the three most essential elements in my world–even beyond images, messaging, video, and other elements that are ubiquitous across my digital presence.</p>

<p>It is interesting for me to think about the importance of these elements in my world, as storage and compute were the first two APIs that turned on the light bulb in my head when it came to the importance of web APIs. When Amazon launched Amazon S3 and Amazon EC2, that is when I knew APIs were going to be bigger than Flickr or Twitter. You could deploy global infrastructure with APIs–you could deploy APIs with APIs! <a href="http://apievangelist.com/2017/01/09/the-api-driven-marketplace-that-is-my-digital-self/">I really enjoy thinking deeply about all my digital bits</a>, and the role APIs are playing–regularly reassessing the value of API-driven resources in my world. It helps me think through what is important, and what isn’t–showing the 98% of all of this tech doesn’t matter, but there is a 2% that does make an actual difference in my digital existence.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/07/06/the-essential-api-elements-in-my-world/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  

	<table width="100%" border="1" style="background-color:#FFF; border: 0px #FFF;">
		<tr style="background-color:#FFF; border: 0px #FFF;">
			<td align="left">
				<a href="/blog/page13" class="button"><< Prev</a></li>
			</td>
			<td></td>
			<td align="right">
				<a href="/blog/page15" class="button">Next >></a>
			</td>
		</tr>
	</table>

  </div>
</section>

              
<footer>
  <hr>
  <div class="features">
    
    
      
      <article>
        <div class="content">
          <p align="center"><a href="https://www.getpostman.com/" target="_blank"><img src="https://apievangelist.com/images/postman-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
        </div>
      </article>
      
    
      
      <article>
        <div class="content">
          <p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://apievangelist.com/images/tyk-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
        </div>
      </article>
      
    
  </div>
  <hr>
  <p align="center">
    relevant work:
    <a href="http://apievangelist.com">apievangelist.com</a> |
    <a href="http://adopta.agency">adopta.agency</a>
  </p>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Homepage</a></li>
    <li><a href="http://101.apievangelist.com/">101</a></li>
    <li><a href="/blog/">Blog</a></li>
    <li><a href="http://history.apievangelist.com/">History of APIs</a></li>
    <li><a href="/#api-lifecycle">API Lifecycle</a></li>
    <li><a href="/search/">Search</a></li>
    <li><a href="/newsletters/">Newsletters</a></li>
    <li><a href="/images/">Images</a></li>
    <li><a href="/archive/">Archive</a></li>
  </ul>
</nav>

              <section>
  <div class="mini-posts">
    <header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
    
    
      
        <article style="display: inline;">
          <a href="https://www.getpostman.com/" class="image"><img src="https://apievangelist.com/images/postman-logo.png" alt="" width="50%" style="padding: 15px; border: 1px solid #000;" /></a>
        </article>
      
    
      
        <article style="display: inline;">
          <a href="https://tyk.io/" class="image"><img src="https://apievangelist.com/images/tyk-logo.png" alt="" width="50%" style="padding: 15px; border: 1px solid #000;" /></a>
        </article>
      
    
  </div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
