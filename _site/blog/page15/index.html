<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
	<a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions2.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
	<ul class="icons">
		<li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
		<li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
		<li><a href="https://www.linkedin.com/company/api-evangelist/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
		<li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
	</ul>
</header>

    	        <section>
	<div class="content">

		<h3>The API Evangelist Blog</h3>

	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-stretching.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/13/keeping-api-communications-in-shape-with-workbench-blogging/">Keeping API Communications In Shape With Workbench Blogging</a></h3>
			<p><em>13 Oct 2016</em></p>
			<p>I consider about 75% of the content I create on my network of sites to be workbench blogging--where I tell the story of what I am working on each day. You can see this approach in action with my friend&nbsp;Guillaume Laforge (@glaforge) over at&nbsp;Google, with his post on&nbsp;a day in the life of a Developer Advocate for Google Cloud Platform. Guillaume is workbench blogging, pulling back the curtain on API operations a little bit, while also keeping API communications flowing. This type of blogging isn't about any specific API, feature, or products and services. Workbench blogging really isn't about people learning any particular thing, they are more about pulling back the curtain, humanizing API operations, generating a little SEO, while also keeping the communication pipes open. The more you write, the easier it is to craft valuable stories. Not everyone will be reading these stories, but they are great for collecting your thoughts&nbsp;and even communicate internally with other stakeholders. You know how many times I've used my blog to recall what I worked on last week, last month, and last year? I know this type of storytelling isn't for everyone, but if you want to be able to create quality content you need practice. Writing regularly is much easier when you write regularly. It helps to have several different areas to write in, allowing you to avoid writer's block by shifting gears to a new topic or just blogging about what was accomplished that day. People always ask how I am able to generate so much content for the blog(s), and staying in shape with workbench blogging is how I do it.I consider about 75% of the content I create on my network of sites to be workbench blogging--where I tell the story of what I am working on each day. You can see this approach in action with my friend&nbsp;Guillaume Laforge (@glaforge) over at&nbsp;Google, with his post on a day in the life of...[<a href="/2016/10/13/keeping-api-communications-in-shape-with-workbench-blogging/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-drone-signal.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/13/drones-and-other-devices-having-their-own-software-defined-networks/">Drones And Other Devices Having Their Own Software Defined Networks</a></h3>
			<p><em>13 Oct 2016</em></p>
			<p>I was learning about&nbsp;Verizon starting to sell wireless data plans for drones in the Wall Street Journal, as part of my research on what could be a drone API stack. As an Internet of Things (IoT)&nbsp;concept, I find drones fascinating because they can have so many dimensions of APIs at play. The drone, its camera, battery, and other components can have APIs. They can publish video to Youtube, Facebook via APIs. They can receive real-time updates about weather, fire, infrastructure, and other changing events via APIs. This post is about thinking through about network API layer when it comes to drones. If we are going to be rolling out networks for specific devices like drones, automobiles, and other IoT devices, it seems like every object should have an API, including the underlying network itself. As I wade through the different companies in my network API research and learn about the myriad of ways web APIs are being exposed at the network level, I am left thinking about the programmability of the network in service of specific devices like this, and what this will mean for net neutrality. Let's take that to another level. What happens when drones can be programmed to fly around and either be the network, or program the network? This is what captivates&nbsp;me about drones and APIs. It all breaks me free from thinking about APIs from just a provider and consumer dimensions&nbsp;and blurs all the lines. I know that Verizon is just reselling its existing network to a new type of device ($$$), but I think the potential with rolling out networks for IoT devices will be in making sure it is also programmable, along with every object that is connected. All of this opens up some really interesting security concerns&nbsp;when you have so many dimensions of the physical and virtual world possessing APIs. Drones with APIs, flying around providing and consuming APIs, redefining its own network with an API--it is...[<a href="/2016/10/13/drones-and-other-devices-having-their-own-software-defined-networks/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-hourglass.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/12/we-understimated-the-time-it-would-take-for-hypermedia-to-be-absorbed/">We Understimated The Time It Would Take For Hypermedia To Be Absorbed</a></h3>
			<p><em>12 Oct 2016</em></p>
			<p>
While I still see a steady uptick in the number of hypermedia APIs out there in the wild, as well as conversations around the different media types that are available, I think we severely underestimated the time it would take for the average API developer to absorb and accept the concept. When you are immersed in any of the leading formats, from HAL to Siren, and you have the aha moment about why hypermedia makes sense, it can be easy to think everyone will see the future like we do. When in reality, I just don't think people are always seeking the wisdom in the same way, they are often just looking to get the job done.
It takes a lot of work to become hypermedia literate. An investment, not everyone can afford. While I am seeing more APIs employ hypermedia I have not seen an increase in the tooling and definitions we need to help developers speed up their understanding--providing examples they can reverse engineer. Siren is my hypermedia format of choice&nbsp;and I found that the&nbsp;TV and streaming video API platform Wurl&nbsp;gave me a strong example to reverse engineer&nbsp;and learn from in my own journey.
Maybe not everyone learns like I do, but I can't help but feel like people need more common examples to learn from. I will spend some time going through the hypermedia APIs I've included in my research, and generalize some of the design patterns that are present and publish them as simple examples on Github. I need to refresh my own hypermedia skills, as I dive back into my subway map API design, which uses Siren as an enabler of the journey. It will also give me some good stories here on the blog--hypermedia is always an evergreen driver of users, as they Google for things in their hypermedia journey.
[<a href="/2016/10/12/we-understimated-the-time-it-would-take-for-hypermedia-to-be-absorbed/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/tool/white-house-seal.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/12/the-white-house-wants-our-thoughts-on-data-portability/">The White House Wants Our Thoughts On Data Portability</a></h3>
			<p><em>12 Oct 2016</em></p>
			<p>
The White House is looking for our thoughts on data portability. While it is the U.S. federal government asking for our thoughts, something that could apply to our tax returns, veterans records, or student loan information. They seem to be most interested in what data portability means to us as consumers, or via many online services today--as the product.
Here is what they are looking for:
The Office of Science and Technology Policy (OSTP) is interested in understanding the benefits and drawbacks of increased data portability as well as potential policy avenues to achieve greater data portability. The views of the American people, including stakeholders such as consumers, academic and industry researchers, and private companies, are important to inform an understanding of these questions.
They want our input in 5000 words or less by&nbsp;November 23, 2016. I am going to gather my own thoughts on data portability, and publish to API Evangelist, as well as submit via their RFI form. This is an extremely important topic&nbsp;and one I'm glad to see the White House picking up, and fingers crossed, will be moving forward in a consumer-centered way.
While I feel government open data is a critical piece of the puzzle, I think data literacy amongst consumer is just as critical. Data is what is fueling the tech sector, and in most scenarios, it is our personal data. The more control we have over this data, the better off we will be. The more opportunities we have to store locally, and migrate where we need to, the more data literate we will all be.
Anyways, I'll save my thoughts for my own response for the White House data portability RFI. I'd love to hear your thoughts on the subject, if you want to post to your blog, or share with me via email.
[<a href="/2016/10/12/the-white-house-wants-our-thoughts-on-data-portability/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/acti_e77_10mp_od_dome_with_1027471.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/12/some-key-iot-security-considerations/">Some Key IoT Security Considerations</a></h3>
			<p><em>12 Oct 2016</em></p>
			<p>I am continuing to learn from folks studying the recent DDOS attack on Krebs on Security. While not a straightforward API story, it overlaps with the API world in several ways, from the technical aspects of how the IoT devices were hacked and enlisted in the bot army, to how the hack has been analyzed online, and the sharing of machine-readable details of the attack. There were some interesting nuggets from the attack analysis I wanted to include in my wider Internet of Things (IoT) research. When it came to the security lapses in the surveillance cameras, printers, and other devices used as part of the DDOS, there were several key areas in play: Username / Password - Default passwords for devices aren't changed, or the settings of unique passwords are not enforced. This makes it pretty easy to get right into the most common devices, after learning the default configuration. Universal Plug and Play (UPnP) - Increased usage of UPnP for the discovery and default opening up of networks to support device communication and monitoring. Often doing so without much consideration for security, and the bigger picture, serving the specific needs of a company and a single device. Telnet / SSH - Even if one changes the password on the device&rsquo;s Web interface, the same default credentials may still allow remote users to log in to the device using telnet and/or SSH--providing a separate doorway for manufacturers, developers, and hackers to gain control over a device. Firmware Upgrades - A device may have had a patch released, but the actual firmware for a device has not actually been updated. The ease of installation, discover and access to the Internet, allows devices to be more easily installed by average folks unaware of wider security concerns, and the fact that devices&nbsp;will need to be updated regularly. While none of these areas speak directly to APIs, they definitely speak to some of the similar ways in which...[<a href="/2016/10/12/some-key-iot-security-considerations/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/google_developer_agency_program.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/12/do-you-have-agencies-ready-to-develop-with-your-api/">Do You Have Agencies Ready To Develop With Your API?</a></h3>
			<p><em>12 Oct 2016</em></p>
			<p>
I was doing some research on how API providers are providing certification of their developers. I want to better understand how leading API providers are developing curriculum for certifying that developers have the skills needed to integrate with their API, and how API providers are also showcasing these certified developers.
As I was looking through various API programs I came across Google's Developers Agency Program, where they certify companies as competent to work with Google's APIs when developing Android applications. Google provides an eBook to help provide agencies with the resources they need to get certified, the details on how to get certified, as well as a page showcasing the agencies that have been certified. I came across the program through a press release from one of the agencies, tooting their horn about being a Google certified agency.

Google's agency certification got me thinking about how this can apply to the average API provider. While API providers might not have the resources to provide an agency certification program like Google does, it might make sense to at least establish relationships with a handful of agencies, and showcase them as part of API operations. Having some pre-certified, ready to go, agency level development resources that can tackle API integrations seems like a good idea for any size API provider.
I've had conversations with agencies before, discussing which APIs they put to use, and they tend to be either commerce related, or social integrations with Facebook, Twitter, and Instagram. This is something I'd like to explore further. I would also like to continue looking for examples of how API providers certify and showcase development resources, whether they are individual, or agency focused like Google is doing. I'm thinking that investment in API expertise at the agency level is one of the areas that will need a lot more attention as the world of APIs continues to expand and evolve.
[<a href="/2016/10/12/do-you-have-agencies-ready-to-develop-with-your-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/base_drone_screenshot.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/12/apis-driving-augmented-reality-for-drones/">APIs Driving Augmented Reality For Drones</a></h3>
			<p><em>12 Oct 2016</em></p>
			<p>Now that I have API Evangelist fired back up I am spending more time with my drones, working to understand the role APIs can play in the booming industry. I have been studying how companies like Airmap are working to be API brokers for the drone industry, and how government open data and APIs are being injected into the personal and commercial drone experience. I want to continue this exploration, and learn more about how companies are injecting data, content, and other algorithmic resources into this experience, and brainstorm some other approaches to augmenting the world of drones with API resources. As part of my regular monitoring of the space, I am tuned into many of the examples of how drones are being used. Many of them are pretty unrealistic, but some of them have real world usage, like&nbsp;disaster relief. Think about the potential when you being to feed in vital data about local infrastructure pulled from city open data, or maybe household data from census data. This is where augmenting the drone experience begins to get interesting for me, going beyond just telling me where I can or cannot fly. I fly a DJI Phantom 3 Professional drone (I am eyeballing that Mavic), and my drone experience is primarily through the DJI Go iPad application, making API integration a pretty straightforward concept. My flight plan, and the latitude, longitude, elevation, and direction is shared via APIs in real-time (if there is an Internet connection), and I can share live-stream of the video to Youtube and Facebook via APIs. I also get regular updates about the flight condition, warnings of airports, forest fires, and other activity via the app--this is just the beginning.&nbsp; I want to explore what is next for APIs, based on what I am seeing emerge across the drone sector. I'm taking a series of screenshots of the DJI Go app and turn it into a transparent augmented reality template that I...[<a href="/2016/10/12/apis-driving-augmented-reality-for-drones/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_10_at_8.08.25_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/11/some-api-embeddable-best-practices-out-of-yelp/">Some API Embeddable Best Practices Out Of Yelp</a></h3>
			<p><em>11 Oct 2016</em></p>
			<p>Yelp has shared some of the wisdom behind how they design, deploy, and operate their embeddable reviews. I like it when leading API providers share the story behind their tooling like this. This type of storytelling generates SEO for their API, educates their API consumers, and provides educational resources for other API providers (and content for analysts like me). So, what makes for a good embeddable widget, according to Yelp? a minimal amount of HTML code&nbsp; a consistent &amp; responsive design stay up-to-date with contextual information load fast &amp; gracefully handle traffic spikes record accurate analytics Yelp shares a little bit about the technical approach to achieving their definition of a good embeddable widget, which "are served as Yelp pages within iframes, adhere to the Yelp Styleguide, and show the most up-to-date review": iframe embeds allow for a simpler widgets.js iframe embeds make development easier iframe embeds can take advantage of HTML caching I've heard about lightening the load for your JavaScript, and the ease of embeddability before, from other providers. I hadn't thought about the cache-ability of using iframes before. It makes sense, allowing the user's browser to carry more of the load. They close up the post with some more interesting insight into the architectural decisions behind their embeddable(s): The embed HTML snippet consists of unstyled and empty elements so that the HTML snippet is minimal and durable. We use a controller loaded via script tag to create and load iframes. The controller consists of pure JavaScript and doesn&rsquo;t use any libraries. The controller communicates with the iframes using postMessage. We use a Google Analytics iframe served by Yelp to handle sending events for multiple review widget iframes on a single page. Resources served by Yelp, such as the controller and the iframes are either cached or served via CDN. The use of Google Analytics is another interesting aspect that I hadn't considered. It gets me thinking about what other examples are there...[<a href="/2016/10/11/some-api-embeddable-best-practices-out-of-yelp/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/zapier_push_icon.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/11/ipaas-in-your-browser-with-push-by-zapier/">iPaaS In Your Browser With Push By Zapier</a></h3>
			<p><em>11 Oct 2016</em></p>
			<p>Zapier is up to more good things with the launch of Push by Zapier, allowing you to trigger API driven events from your browser. The new Chrome browser extension lets anyone, even non-developers to trigger the functionality of over 700 apps from the browser toolbar--further expanding the definition of how APIs can be put to work. Allowing users to trigger API functionality from the browser adds an empowering dimension to the API conversation for non-developers. It allows the average user to access the features of any SaaS platform with an API, in their default environment--the browser. It allows the user to define, and queue up the API driven events that matter to them, where they operate the most. Zapier gave non-developers access to orchestrate the integration between the platforms they depend on, and Push by Zapier gives them even more granular level control over this world.&nbsp; Some of the API driven events Zapier highlights in their release are: Add Tasks to Your To-Do List Send an Email or SMS to a Specific Person Crunch Numbers and Calculate Payments Copy Data from an Unsupported Site or App and Add It to a Zapier Workflow Create Invoices with a Click Send Documents to be Signed Create a New Project or Folder Impersonate a Slack Bot Look Up Data and Use It in a Workflow Get Details from Your CRM Delivered Anywhere Translate Text Send Sales and Onboarding Emails to Potential or New Customers Set a Reminder for Yourself Create a Templated Document Push a Button to Build a Report or Pull Statistics All very business-centric, API-driven functionality. That is what I like about what Zapier enables. They provide meaningful API-driven functionality for the average business user, and Push just gives them another way to define what this functionality is, and when it gets executed. This transcends&nbsp;just the features and functionality that each API platform offers, it also is about end-users being API literate, and aware that they...[<a href="/2016/10/11/ipaas-in-your-browser-with-push-by-zapier/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_10_at_10.23.45_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/11/i-am-stuck-on-the-datadog-integration-page/">I Am Stuck On The Datadog Integration Page</a></h3>
			<p><em>11 Oct 2016</em></p>
			<p>
I wrote about having an integrations page for your API service the other day, and as I'm continuing to study the approach of other providers I find myself stuck on&nbsp;b DataDog's integration page.&nbsp;Datadog provides the monitoring layer across many of the top&nbsp;service providers in the space, making for a pretty stellar list of what solutions are being put to use across the sector.&nbsp;
The Datadog integration page has been open in my browser for the last week, as I make way through each provider. Some of them I'm very familiar with, but others are entirely new to me. Integration pages like this show me what is possible with a service provider like Datadog, but also provides me an opportunity to learn about new services that I can put to use in my own operations, and what the cool kids are using.
When you click on the detail for each of the potential integrations you get more information about what is possible, as well as a configuration file in YAML, and what metrics are made available when monitoring is activated. Datadog even groups their integrations by a tag, something they don't expose via the user interface very well, but is something I'll include in my suggestions for crafting an integration page for any other API.&nbsp;
An integration page is definitely a building block I will be suggesting to other API providers, and I am also a big fan of sharing configuration, and other integration details like Datadog does. There are infinite learning opportunities available on this type of API integration pages, for analysts like me, for API providers, as well as service providers who sell to API providers.&nbsp;These are the types of common API building blocks that I feel contribute in a positive way to the tech sector, reflecting&nbsp;what APIs do best--enabling API integration, and API literacy all in the same motion.
[<a href="/2016/10/11/i-am-stuck-on-the-datadog-integration-page/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-cybersecurity-2.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/11/expanding-on-the-3rd-party-analysis-of-security-threats/">Expanding On The 3rd Party Analysis Of Security Threats</a></h3>
			<p><em>11 Oct 2016</em></p>
			<p>I was learning from the Splunk's analysis of the Mirai Botnet, which was behind the massive attack against Krebs on Security, implemented via common Internet of Things devices like security cameras, and printers. I've been reading several of these types of security event analysis, which is something I think is extremely important in helping the industry deal with the increasing number of security events that are occurring across the online landscape.&nbsp; The sharing of log files from compromised systems in this way is super important. We need as many eyes as we can get on these attacks, helping analyze what happened, and maybe possibly who was behind it. Of course, there&nbsp;are some scenarios where you might want to be cautious in opening up this data to the general public, but using common approaches to API deployment and management, this can be managed sensibly--while also adding another logging layer to the conversation, keeping track of who joins participates in the analysis.&nbsp; At a minimum, the DNS, application, and server logs should be made available via Github, leveraging it's Git core, as well as the Github API as part of the evaluation and analysis of the attack information. Ideally, key aspects of the data, attack vector, and other elements should also be added to some sort of shared API infrastructure for continued community security threat analysis. In addition to the growing number of attacks, and analysis by leading analysts like Splunk, I'm also seeing increased discussion around the sharing of threat data in a standardized way--APIs can act as a distributed engine for this operation. I learn a lot from the analysis&nbsp;that occurs on security events like this. I know that other security analysts learn from this as well. With digital security being such a critical issue, right along with environmental events like hurricanes, or health care concerns like the Zika virus, I'm suggesting that APIs be employed in a standardized way. We should have a common...[<a href="/2016/10/11/expanding-on-the-3rd-party-analysis-of-security-threats/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-embeddable.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/11/api-embeddables-with-skills-and-intent/">API Embeddables With Skills and Intent</a></h3>
			<p><em>11 Oct 2016</em></p>
			<p>I am seeing some renewed interest and discussion around API driven embeddable(s)--an area of my API research that has been going on for years, focusing on buttons, badges, and widgets, but is something that I'm seeing continued investment in from API providers lately. To help fuel the innovation that is already occurring, I figured I'd contribute with my API thoughts extracted from across the bot and voice API landscape. As I monitor the bot community growing out of the Slack platform, the voice API integration emerging from the Alexa development community, and read news about Google's latest push into the space, I'm thinking about how APIs are being used to define the intents, skills, and actions that are&nbsp;driving these bot and voice implementations. I am also processing this intersection with the latest release of Push by Zapier. All of this about delivering the meaningful API responses, to where the end users desire--in their browser, their chat, or voice enablement in the business and home. While processing the wisdom shared by Yelp about their deployment of embeddable reviews, I'm thinking about how these embeddable JavaScript widgets can be used to further allow users to quantify the intent, discover the skills, and achieve the action they are looking for. How can API providers, and the savvy API developers make valuable API resources accessible to users on their terms, and in the client they desire. For example, I might need to know my availability next Thursday while talking to my Amazon Echo, engaging in a Slack conversation, or possibly filling out a form on my corporate network--in all these scenarios I will need API access to the calendar(s) that I depend on, in the way that is required in each unique situation. I am not always the biggest fan of voice and bot enabled scenarios, but I do think they provide us with some interesting constraints&nbsp;on API design. I'm hoping that some of these constraints can further be...[<a href="/2016/10/11/api-embeddables-with-skills-and-intent/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-frankenstein-beast.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/10/the-internet-of-things-shows-us-how-regulatory-beasts-are-created/">The Internet of Things Shows Us How Regulatory Beasts Are Created</a></h3>
			<p><em>10 Oct 2016</em></p>
			<p>I am watching the world of Internet of Things (IoT) unfold, not because I'm a big fan of it, but more because I'm concerned that it is happening, and often worried that much of it is happening without any focus on security, and privacy. As I look at this week's stories in my API IoT bucket I can't help but think that IoT is a live demonstration of how the regulatory beasts, that we love to hate on in America, are created. It starts with a bunch of fast moving, greedy, corner cutting capitalists who are innovating and all that shit. These are not always the first wave of movers in a space, but usually the second and third waves of opportunists with one thing in mind--making some money. These are the companies that are so focused on revenue and profits they ignore things like security, and they see the data generated being key to their success, and concepts around privacy often do not even exist--it's the new oil motha fuckkers! As the number of security and privacy events increase, things like the unprecedented attack on Krebs on Security, the calls for a fix will only&nbsp;grow. Eventually, these calls for help are heard by the government, if they are negatively impacting enough well to do white folk, and the government steps up to figure out what to do. Often times, these investigative forces aren't fully up to speed on the area they are investigating, but with the resources they have, they'll usually inflict some regulatory and legal response. If there are any existing companies with a strong lobbying presence, the immediate response will be significantly watered down, making it more of a nuisance than anything else. This is when the market voice begins its complaining about the government overstepping its responsibilities, and stepping in to throw a wet blanket on business. The government is bad. Regulation is bad. Then we repeat, rinse, and go about...[<a href="/2016/10/10/the-internet-of-things-shows-us-how-regulatory-beasts-are-created/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-car-mechanic.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/10/opportunity-for-someone-to-help-organize-auto-industry-data/">Opportunity For Someone To Help Organize Auto Industry Data</a></h3>
			<p><em>10 Oct 2016</em></p>
			<p>
There is a lot of data coming out of the automobile industry. I was just reading about Udacity open sources an additional 183GB of driving data and the global public registry of electric vehicle charging locations with 42K+ listings, providing us with two examples from the wild. I'm seeing an increasing number of these stories about institutions, government agencies, and the private sector making automobile related data available in this way--pointing to a pretty big opportunity when it comes to aggregating this valuable "data exhaust" (pun intended) in a coherent&nbsp;way.
Whether its self-driving, electric, car share, rental or otherwise, the modern automobile is generating a lot of data. There are some significant ways in which the automobile industry is being expanded upon, and the need to understand, and become more aware using data are immense. Making API access for this public, and private data will be increasingly important.&nbsp;
There is a number of different interests producing this data, and they aren't always immediately thinking about sharing, and reuse of the data, let alone making sure there is standardized APIs and data schema at play. Opening up a pretty big opportunity for someone to focus on aggregating all these emerging automobile datasets, make available via a unified API, and helping define a common set of API definitions and schema for accessing this valuable data exhaust from the industry.
I do not think automobile industry data aggregation is the next VC fundable idea, but I to think that with some hard work, and the slow build of some expertise in this fast moving area, an individual, or small group of folks could do very well. I know from dabbling in this area that the auto industry, the department of transportation, and the aftermarket product and service providers don't always see eye to eye, and a neutral, 3rd party aggregator&nbsp;and evangelist has the potential to make a significant impact.
[<a href="/2016/10/10/opportunity-for-someone-to-help-organize-auto-industry-data/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/google_improve_your_api_experience.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/10/google-shares-insight-on-how-to-improve-upon-the-api-experience/">Google Shares Insight On How To Improve Upon The API Experience</a></h3>
			<p><em>10 Oct 2016</em></p>
			<p>We all like it when the API providers we depend on make using their APIs easier to put to work. I also like it when API providers also share the story behind how they are making their APIs easier to use because it gives me material for a story, but more importantly it provides examples that other API providers can consider as part of their own operations. Google recently shared some of the improvements they have made to help make our API experience better--here are some of the key takeaways: Faster, more flexible key generation - Making this step simpler, by reducing the old multi-step process with a single click. Streamlined getting started flow -&nbsp;Introduced an in-flow credential set up procedure directly embedded within the developer documentation. An API Dashboard - To easily view usage and quotas, so you can view all the APIs you&rsquo;re using along with usage, error and latency data. If you spend any time-consuming APIs you know that these areas represent the common friction many of us API developers experience regularly. It is nice to see Google addressing these areas of friction, as well as sharing their story with the rest of us, providing us all a reminder of how we can cut off these sharp corners in our own operations. These areas represent what I'd say are the two biggest pain points with getting up and going using an API, and the API dashboard represents the biggest pain point we face once we are up and running--where do we stand with our API consumption, within the rate limits provided by the platform. If you use a modern API management platform you probably have a dashboard solution in place, but for API providers who have hand-rolled their own solution, this continues to be a big problem area. While some of the historical Google API experiences have left us API consumers desiring more (Google Translate, Google+, Web Search), they have over 100...[<a href="/2016/10/10/google-shares-insight-on-how-to-improve-upon-the-api-experience/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-danger-shaky.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/10/embrace-extend-and-exterminate-in-the-world-of-apis/">Embrace, Extend, and Exterminate In The World Of APIs</a></h3>
			<p><em>10 Oct 2016</em></p>
			<p>I am regularly reminded in my world as the API Evangelist that things are rarely ever what they seem on the surface. Meaning that what a company actually does, and what a company says it does are rarely in sync. This is one of the reasons I like APIs, is they often give a more honest look at what a company does (or does not do), potentially cutting through the bullshit of marketing. It would be nice if companies were straight up about their intentions, and relied upon building better products, offering more valuable services, but many companies prefer being aggressive, misleading their customers, and in some cases an entire industry. I'm reminded of this fact once again while reading a post on software backward compatibility, undocumented APIs and importance of history, which provided a stark example of it in action from the past: &ldquo;Embrace, extend, and extinguish&ldquo;,[1]&nbsp;also known as &ldquo;Embrace, extend, and exterminate&ldquo;,[2]&nbsp;is a phrase that the&nbsp;U.S. Department of Justice&nbsp;found[3]&nbsp;that was used internally by&nbsp;Microsoft[4]&nbsp;to describe its strategy for entering product categories involving widely used standards, extending those standards with&nbsp;proprietary&nbsp;capabilities, and then using those differences to disadvantage its competitors. This behavior is one of the contributing factors to why the most recent generation(s) of developers are so adverse to standards&nbsp;and is behavior that exists within current open API and open source efforts. From experience, I would emphasize that the more a company feels the need to say they are open source, or open API, the more likely they are indulging in this type of behavior. It is like,&nbsp;some sort of subconscious response, like the dishonest person needing to state that they are being honest, or that you need to believe them--we are open, trust us. I am not writing this post as some attempt to remind us that Microsoft is bad--this isn't at all about Microsoft. It is simply to remind us that this behavior has existed in the past, and it exists right now....[<a href="/2016/10/10/embrace-extend-and-exterminate-in-the-world-of-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/ct3y_zqviaaggk5.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/07/the-anatomy-of-api-call-failure/">The Anatomy Of API Call Failure</a></h3>
			<p><em>07 Oct 2016</em></p>
			<p>I have been&nbsp;spending time thinking about how we can build in fault tolerance, and change resiliency into our API SDKs, and client code. I want to better understand what is necessary to develop the best possible integrations as possible. While doing my regular monitoring this week I came across a Tweet from @Runscope, with a pretty interesting image on this subject crafted by @realm, a mobile platform for sync.

There is a wealth of building blocks here to apply at the client and SDK level, helping us achieve more fault tolerance, and make our applications, systems, and device integrations more change resilient. I wanted to break them out, providing a bulleted list I could include in my research:


Is the API Online?
Did the server receive the request?
Was URL request successful?
Did the request timeout?
Was there a server error?
Was JSON receive successfully?
Was JSON malformed?
Was there an unexpected response?
Were we&nbsp;able to map to JSON successfully?
Is the JSON valid?
Does local model match server model?

There are some valuable nuggets present in this diagram. It should be crafted into some sort of algorithmic template that developers can apply when developing their API integrations, as well as for API providers when developing the SDK and client solutions they make available to their API communities. I'm taking note so that next time I spend some cycles on my API SDK research I can help solidify my own definition.
This is a very micro look at fault-tolerance when it comes to API integration, and I'm continuing to look for other examples of change resiliency at this layer. Meaning, is there a plan B for the API call? Is there revenue ceiling considerations? Or other more non-technical, business and political considerations that should be baked into the code as well. Helping us all think more deeply around how we encourage change resiliency across&nbsp;the API community.
[<a href="/2016/10/07/the-anatomy-of-api-call-failure/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-check-black-round.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/07/regulatory-api-monitoring-for-validating-algorithmic-assertions/">Regulatory API Monitoring For Validating Algorithmic Assertions</a></h3>
			<p><em>07 Oct 2016</em></p>
			<p>
As I was learning about behavior driven development (BDD) and test driven development (TDD) this week, I quickly found myself applying this way of thought to my existing API regulation, and algorithmic transparency research. BDD and TDD are both used by API developers to ensure APIs are doing what they are supposed to, in development, QA, and production environments. There is no reason that this line of thought can't be elevated beyond just development groups to other business units, up to a wider industry level, or possibly employed by regulators to validate data or algorithmic solutions.
I am not a huge fan of government regulation, but I am a fan of algorithms doing what is being promised, and APIs plus BDD and TDD testing is one way that we can accomplish this. Similar to how the federal government is working together to define OAuth scopes which help&nbsp;sets the bar for how a user&nbsp;data is accessed, BDD assertion templates can be defined, shared, and validated within regulated industries.
Right now we are just focused at the very local level when it comes to API assertions. With time I'm hoping an API assertion template format will emerge (maybe already something out there), and I'm hoping that we evolve ways for allowing the average business user to be part of defining and validating API assertions. I know my friends over at Restlet are working towards this with their DHC client solution, which provides testing solutions.&nbsp;
BDD, TDD, and API assertions still very much exist in the technical environments where APIs are born&nbsp;and managed. I'm hoping to help define the space, identify opportunities for establishing common patterns&nbsp;while encouraging more reuse of leading patterns. Like other layers of the API economy, I am hoping that API assertions will expand beyond just the technical, and enjoy use amongst business groups, including industry leaders, and government regulators when it applies.
[<a href="/2016/10/07/regulatory-api-monitoring-for-validating-algorithmic-assertions/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/usdf_logo.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/07/harmonizing-api-definitions-across-government-with-the-u-s-data-federation/">Harmonizing API Definitions Across Government With The U.S. Data Federation</a></h3>
			<p><em>07 Oct 2016</em></p>
			<p>Sharing of API definitions is critical to any industry or public sector where APIs are being put to work. If the API sector is going to scale effectively, it needs to be reusing common patterns, something that many API and open data providers have not been that great at historically. While this is critical in any business sector, there is no single area where this needs to happen more urgently than within the public sector. I have spent years trying wade through the volumes of open data that comes out of government, and even spent a period of time doing this in DC for the White House. The lack of open API definition formats like OpenAPISpec, API Blueprint, APIs.json, and JSON Schema across government is a passion of mine, so I'm very pleased to the&nbsp;new US Data Federation project coming out of the General Services Administration (GSA). "The U.S. Data Federation supports data interoperability and harmonization across Federal, state, and local government agencies by highlighting common data formats, API specifications, and metadata vocabularies." The&nbsp;U.S. Data Federation has focused in on some of the existing patterns that exist in service of the public sector, including seven existing initiatives: Building &amp; Land Development Specification National Information Exchange Model Open Referral Open311 Project Open Data Schema.org The Voting Information Project I am a big supporter of Open Referral, Open311, Project Open data, and Schema.org. I will step up and get more familiar&nbsp;with the building &amp; land development specification, national information exchange model, and the voting information projects. The US Data Federal project echoes the work I've been doing with Environmental Protection Agency (EPA) Envirofacts Data Service API, Department of Labor APIs, FAFSA API, and my general Adopta.Agency efforts. Defining the current inventory of government APIs and open data using OpenAPI Spec, and indexing the with APIs.json&nbsp;is how we do the hard work of identifying the common patterns that are already in place&nbsp;and being used by agencies on the...[<a href="/2016/10/07/harmonizing-api-definitions-across-government-with-the-u-s-data-federation/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_06_at_12.30.23_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/07/hacking-on-amazon-alexa-with-aws-lambda-and-apis-at-apistrat/">Hacking on Amazon Alexa with AWS Lambda and APIs At @APIStrat</a></h3>
			<p><em>07 Oct 2016</em></p>
			<p>
I'm neck deep in studying how Amazon is operating their Alexa platform, so I'm pretty excited about the chance to listen&nbsp;and learn from the Alexa team at APIStrat in Boston. Even if you aren't building voice-enabled applications, the approach to developing, managing, and evangelizing the Alexa platform provides a wealth of best practices that we should all strive to emulate in our own operations.
Rob McCauley (@RobMcCauley) from the Amazon Alexa team is doing a workshop, as well as a keynote at @APIStrat in Boston next month. This is relevant to what is going on in the wider space because voice-enablement is a fast-moving layer when it comes to delivering API resources, helping define what is being dubbed as the conversational interface movement, while also providing the best practices for a modern API strategy that I mentioned above.
There are a number of things that the Alexa team does which have captured my attention, including their approach to developing skills, their investment ($$) into their developers, and their overall communication strategy. I'm working on profiling all of this as part of what I call a blueprint reports, where I map out the approach of the Alexa team in a way that other API providers can put to work in their own operations.
I'm thinking I will have to wait until after @APIStrat to finish my blueprint report, as I'd like to attend the Alexa workshop, hear his keynote, and possibly even talk to him personally about their approach, in the hallway. I hope to see you there, and hear you share your story, even if you aren't on the stage at APIStrat, the hallways tend to be a great place to listen to the story of leaders from across the space,&nbsp;as well as share your own--no matter how big or small you might be.
Make sure you get registered for APIStrat before it is sold out, and I'll see you there!
[<a href="/2016/10/07/hacking-on-amazon-alexa-with-aws-lambda-and-apis-at-apistrat/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/aws_answers_splat_1.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/07/amazon-launches-their-own-qa-solution-called-aws-answers/">Amazon Launches Their Own QA Solution Called AWS Answers</a></h3>
			<p><em>07 Oct 2016</em></p>
			<p>
Amazon launched their own questions and answers site called simply called AWS Answers. Amazon is definitely in a class of their own, but I thought the move reflects illnesses in the wider QA space&nbsp;and an approach that smaller API providers might want to consider for their operations.
Quora doesn't have an API, so why would we use as a QA solution for the API space? I don't care how much network they have. While Stack Overflow is a wealth of API related questions and answers, the environment has been found to be pretty toxic for some API providers. Making hand rolling your own QA site a more interesting option.
AWS answers is&nbsp;a pretty basic implementation&nbsp;but also has a wealth of valuable content. it wouldn't take much to handroll your own FAQ or wider answers solution&nbsp;within your API developer portal. I can understand why AWS would do their own, to help ensure their users are able to find the answers they need, without leaving the AWS platform. It depends on the type of platform you are operating, but keeping QA local might make more sense than using 3rd party solutions--allowing for more precise control over the answers your customers receive.
As I work to expand my API portal definition beyond just the minimum version, I'm adding a FAQ solution to the stack, and now I'm going to consider adding a separate answers solution modeled after AWS Answers. While I think platforms like Stack Overflow and Quora will continue to do well, I'm more interested in supporting API providers to roll their own solution, maybe even provide an API, and allow for more interoperability, and control over their own resources.
[<a href="/2016/10/07/amazon-launches-their-own-qa-solution-called-aws-answers/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/14500451_10157473672265368_6769931295752399398_o.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/06/your-southwest-airlines-flight-has-an-api/">Your Southwest Airlines Flight Has An API</a></h3>
			<p><em>06 Oct 2016</em></p>
			<p>A friend of mine messaged me this photo of the Southwest Airlines flight API on Facebook the other day. After doing a little homework I found that every flight has this available on the planes local network. There is a pretty interesting write up on it from Roger Parks&nbsp;if you care to learn more. Looking through the response it has all the information you need for your flight update screen. It might seem scary for folks like us poking around the network on airplanes looking for things like this, but this is just the nature of the Internet and something any network operator should consider as normal. The API is available at&nbsp;getconnected.southwestwifi.com/current.json when you are on the planes local network, and I'd consult Roger's post if you want more details about how to sniff it out using your browser. Anytime I am on a guest network on a plane or in a hotel, I enjoy turning on my Charles Proxy to log a list of all the domains and IP address in use. This is a good way to learn about how people are architecting their networks, and delivering their resources to web, mobile, and device users. The problem with this activity is that sometimes you can discover things that you shouldn't. A line that I worry about a lot. I feel pretty strongly that if companies are using public DNS, or opening up their private network to the public, they should be aware that this is going to happen. I hope that someday this type of behavior is embraced by companies, institutions, and government agencies. Not everyone will have good intentions like I do, but network operators should know this will happen, and make the those of us where white hats welcome, so that we will report insecure infrstructure,&nbsp;and help&nbsp;keep things locked down--before the bad guys get in. Thanks to my friend Jason for pinging me with this. From reading up on it, it...[<a href="/2016/10/06/your-southwest-airlines-flight-has-an-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/cloudflare_dns_api_inline_1.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/06/providing-inline-api-documentation-within-your-saas-user-interface/">Providing Inline API Documentation Within Your SaaS User Interface</a></h3>
			<p><em>06 Oct 2016</em></p>
			<p>The common approach to discovering that a SaaS provider has an API is through a single, external link in the footer of a website, simply labeled API or developers. Whenever I can I'm on the lookout for evolutionary approaches to making users aware of an API, and I just found a good one over at CloudFlare. When you are logged into CloudFlare managing your DNS, right below the area for adding, editing, and deleting DNS records you are given some extra options, including expandable access to your API--down in the right-hand corner, between Advanced and Help. Once you click on the API option, you are given a listing of DNS record related API endpoints, allowing me to bake the same functionality available to me in the CloudFlare UI, into my own systems and application. A summary, path, and verb is provided for each relevant API, with a link to the full API documentation. I really like this approach. It is a great way to make APIs more accessible to the muggles (thanks @CaseySoftware). It is also a great way to think about connecting UI functionality to the (hopefully) API behind. Imagine if every UI element had an API link in the corner to see the API behind, and a link to its documentation . You could even display the request and response bodies for the API call made by the UI, allowing people to easily reverse engineer what an API does.&nbsp; I have suggested this approach at several events, and to other API technologists who felt it was a bad idea, as the user doesn't want to be bothered by the details of why something does what it does, they just want it to be done. I disagree. I strongly believe that this is an extension of old school beliefs by the IT wizards, that the muggles aren't smart enough, and IT should have all the power (one ring and all that). Seriously, though. There...[<a href="/2016/10/06/providing-inline-api-documentation-within-your-saas-user-interface/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/google_auditing.gif" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/06/an-auditing-api-for-checking-in-on-api-client-activity/">An Auditing API For Checking In On API Client Activity</a></h3>
			<p><em>06 Oct 2016</em></p>
			<p>Google just released a mobile audit solution for their Google Apps Unlimited users&nbsp;looking to monitor activity across iOS and Android devices. At first look, the concept didn't strike me as anything I should write about, but once I got to thinking about how the concept applies beyond mobile to IoT, and the potentially for external 3rd party auditing of API and endpoint consumption--it stood out as a pattern I'd like to have in the filing cabinet for future reference. Using the Google Admin SDK Reports&nbsp;API you can access mobile audit information by users, device, or by auditing&nbsp;event. API responses include details about the device including model, serial numbers, user emails, and any other element that included as part of device inventory. This model seems like it could easily be adapted to IoT devices, bot and voice clients. One aspect that stood out for me as a pattern I'd like to see emulated elsewhere, is the ability to verify that all of your deployed devices are running the latest security updates. After the recent IoT launched DDOS attack on Krebs on Security, I would suggest that the security camera industry needs to consider implementing an audit API, with the ability to check for camera device security updates. Another area that caught my attention was their mention that "mobile administrators have been asking for is a way to take proactive actions on devices without requiring manual intervention." Meaning you could automate certain events, turning off, or limiting access to specific API resources. When you open this up to IoT devices, I can envision many benefits depending on the type of device in play. There are two dimensions of this story for me. That you can have these audit events apply to potentially any client that is consuming API resources, as well as the fact that you can access this data in real time, or on a scheduled basis via an API. With a little webhook action involved,...[<a href="/2016/10/06/an-auditing-api-for-checking-in-on-api-client-activity/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-check3.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/06/adding-behaviordriven-development-assertions-to-my-api-research/">Adding Behavior-Driven Development Assertions To My API Research</a></h3>
			<p><em>06 Oct 2016</em></p>
			<p>I was going through Chai, a behavior, and test driven assertion library, and spending some time learning about behavior driven development, or BDD, as it applies to APIs today. This is one of the topics I've read about and listened to talks from people I look up to, but just haven't had the time to invest too many cycles in learning more. As I do with other interesting, and applicable areas, I'm going to add as a research area, which will force me to bump it up in priority. In short, BDD is how you test to make sure an API is doing what is expected of it. It is how the smart API providers are testing their APIs, during development, and production to make sure they are delivering on their contract. Doing what I do, I started going through the leading approaches to BDD with APIs, and came up with these solutions: Chai -&nbsp;A BDD / TDD assertion library for&nbsp;node&nbsp;and the browser that can be delightfully paired with any javascript testing framework. Jasmine - A&nbsp;behavior-driven development framework for testing JavaScript code. It does not depend on any other JavaScript frameworks.&nbsp; Mocha -&nbsp;Mocha is a feature-rich JavaScript test framework running on&nbsp;Node.js&nbsp;and in the browser, making asynchronous testing&nbsp;simple&nbsp;and&nbsp;fun. Nightwatch.js -&nbsp;Nightwatch.js is an easy to use Node.js based End-to-End (E2E) testing solution for browser based apps and websites.&nbsp; Fluent Assertions -&nbsp;Fluent Assertions is a set of .NET extension methods that allow you to more naturally specify the expected outcome of a TDD or BDD-style test. Vows -&nbsp;Asynchronous behaviour driven development for Node. Unexpectd -&nbsp;The extensible BDD assertion toolkit If you know of any that I'm missing, please let me know. I will establish a research project, add them to it, and get to work monitoring what they are up to, and better track on the finer aspects of BDD. As I was searching on the topic I also came across these references that I think are worth...[<a href="/2016/10/06/adding-behaviordriven-development-assertions-to-my-api-research/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-puzzle-piece-gear.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/06/a-machine-readable-jekyll-jig-for-each-area-of-my-api-research/">A Machine Readable Jekyll Jig For Each Area Of My API Research</a></h3>
			<p><em>06 Oct 2016</em></p>
			<p>I have over 70 areas of research occurring right now as part of my API lifecycle work--these are areas that I feel directly impact how APIs are provided and consumed today. Each of these areas lives as a Github repository, using Github Pages as the front-end of the research.&nbsp; I use Github for managing my research because of its capabilities for managing not just code, but also machine readable data formats like JSON, CSV, and YAML. I'm not just trying to understand each area of the API lifecycle, I am working to actually map it out in a machine readable way.&nbsp; This process takes a lot of effort, and is always work in progress. To help me manage the workload I rely on Github, the Github API, and Github Pages. On top of this&nbsp;Github base, I leverage the data and content capabilities of Jekyll when you run it on Github Pages (or any other Jekyll enabled server or cloud service).&nbsp; Each of my research areas begins with me curating news from across space, then I profile companies and individuals who are doing interesting things with APIs, and the services, tooling, and APIs they are developing. I process all of this information on a weekly basis and publish to each of my research projects as its YAML core.&nbsp; An example of this can be seen with my API monitoring research (the most up to date) with the following machine-readable&nbsp;components: Master APIs.json&nbsp; - Each project has a YAML APIs.json&nbsp;(I know, I know) core, with a dynamically generated JSON version in the root of site, providing an index of all the companies and APIs included in this research.&nbsp; Individual APIs.json - In addition to the central project APIs.JSON file, there are individual APIs.JSON files for each company included in my research, which includes OpenAPI Specs for all APIs I have indexed. Blog Atom - There is an atom feed for the blog, showing any posts I write on...[<a href="/2016/10/06/a-machine-readable-jekyll-jig-for-each-area-of-my-api-research/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-wordpress.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/05/where-is-the-wordpress-for-apis/">Where Is The WordPress For APIs?</a></h3>
			<p><em>05 Oct 2016</em></p>
			<p>
I feel like I have said this before, but probably is something that is worth refreshing--where is the WordPress for APIs? First, I know WordPress has an API, that isn't what I'm talking about. Second, I know WordPress is not our best foot forward when it comes to the web. What I am talking about is a ready to go API deployment solutions in a variety of areas, that are as easy to deploy and manage as WordPress.
There is a reason WordPress is as popular as it is. I do not run WordPress for any of my infrastructure, but I do help others setup&nbsp;and operate their own WordPress installs from time to time. I get why people like it. I personally think its a nightmare in there, when you start having to make it do things as a programmer, but I fully grasp why others dig it, and willing to support that whenever I can.
I want the same type of enabling solution for APIs. If you want a link API -- here you go. If you want a product API -- download over here. There should be a wealth of open source solutions that you can just download, unzip, upload, and go through the wizard. You get the API&nbsp;and a simple management interface. I would get to work building one in PHP / MySQL just to piss all the real programmers off, but I have too many projects on my plate already.
If you want to develop the WordPress of APIs for the community and make it push-button deployment via Heroku, AWS, Google, or Azure, please let me know and I'm happy to help amplify. ;-)
[<a href="/2016/10/05/where-is-the-wordpress-for-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-api-evolution.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/05/the-web-evolved-under-different-environment-than-web-apis-are/">The Web Evolved Under Different Environment Than Web APIs Are</a></h3>
			<p><em>05 Oct 2016</em></p>
			<p>I get the argument from hypermedia and linked data practitioners that we need to model our web API behavior on the web. It makes sense, and I agree that we need to be baking hypermedia into our API design practices. What I have trouble with is the fact that the web is a cornerstone that we should be modeling&nbsp;it after. I do not know what web y'all use every day, but the one I use, and harvest regularly is quite often is a pretty broken thing. It just feels like we overlooking so much to support this one story. I'm not saying that hypermedia principles don't apply because the web is shit, I'm just saying maybe it isn't as convincing of an anchor to build a story that currently web APIs are shit. I understand that you want to sell your case, and trust me...I want you to sell your case, but using this argument just does not pencil out for me. There is another aspect of this that I find difficult. That the web was developed and took root in a very different environment than web APIs are. We had more time and space to be more thoughtful about the web, and I do not think we have that luxury with web APIs. The stakes are higher, the competition is greater, and the incentives for doing it thoughtfully really do not exist in the startup environment that has taken hold. We can't be condemning API designers and architects for serving their current master (or can we?).&nbsp; While I will keep using core web concepts and specs to help guide my views on designing, defining, and deploying my web APIs, I'm going to explore other ways to articulate why we should be putting them to use. I'm going to also be considering the success or failure of these elements based on the shortcomings of the web, and web APIs,&nbsp;while I work to better polish the...[<a href="/2016/10/05/the-web-evolved-under-different-environment-than-web-apis-are/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/undefined/bw-api-engine.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/05/github-as-the-api-life-cycle-engine/">Github As The API Life Cycle Engine</a></h3>
			<p><em>05 Oct 2016</em></p>
			<p>
I am playing around with some new features from the SDK generation as a service provider APIMATIC, including the ability to deploy my SDKs to Github. This is just many of the ways Github, and more importantly Git is being used as what I'd consider as an engine in the API economy. Deploying your SDKs is nothing new, but when your autogenerating SDKs from API definitions, deploying to Github and then using that to drive deployment, virtualization, containers, serverless, documentation, testing, and other stops along the API life cycle--it is pretty significant.
Increasingly we are publishing API definitions to Github, the server side code that serves up an API, the Docker image for deploying and scaling our APIs, the documentation that tell us what an API does, the tests that validate our continuous integration, as well as the clients and SDKs. I've been long advocating for use of Github as part of API operations, but with the growth in the number of APIs we are designing, deploying, and managing--Github definitely seems like the progressive way forward for API operations.
I will keep tracking on which service providers allow for importing from Github, as well as publishing to Github--whether its definitions, server images, configuration, or code. As these features continue to become available in these companies APIs I predict we will see the pace of continuous integration and API orchestration dramatically pick up. As we are more easily able to automate the importing and exporting of essential definitions, configurations, and the code that makes our businesses and organizations function.
[<a href="/2016/10/05/github-as-the-api-life-cycle-engine/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/apimatic_dx_kits.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/05/evolving-the-api-sdk-with-apimatic-dx-kits/">Evolving The API SDK With APIMATIC DX Kits</a></h3>
			<p><em>05 Oct 2016</em></p>
			<p>I've been a big supporter of APIMATIC since they started, so I'm happy to see them continuing to evolve their approach to delivering SDKs&nbsp;using machine readable API definitions. I got a walkthrough of their new DX Kits the other day, something that feels like an evolutionary step for SDKs, and contributing to API providers making onboarding and integration as frictionless as possible for developers. Let's walk through what APIMATIC already does, then I'll talk more about some of the evolutionary steps they are taking when auto-generating SDKs. It helps to see the big picture of where APIMATIC fits into the larger API lifecycle to assist you in getting beyond any notion of them simply being just an SDK generation service. API DefinitionsWhat makes APIMATIC such an important service, in my opinion, is that they just don't speak using modern API definition formats, they speak in all of the API definition formats, allowing anyone to generate SDKs from the specification of your choice:&nbsp; API Blueprint Swagger 1.0 - 1.2 Swagger 2.0 JSON Swagger 2.0 YAML WADL - W3C 2009 Google Discovery RAML 0.8 I/O Docs - Mashery HAR 1.2 Postman Collection APIMATIC Format As any serious API service provider should do be doing, APIMATIC then opened up their API definition transformation solution as a standalone service and API. This allows this type ofAPI &nbsp;transformations to occur&nbsp;and be baked in, at every stop along a modern API lifecycle,&nbsp;by anyone. API DesignBeing so API definition driven focused, APIMATIC needed a practical way to manage API definitions, and allow their customers to add, edit, delete, and manipulate the definitions that would be driving the SDK auto generation process. APIMATIC provides one of the best API design interfaces I've found across the API service providers that I monitor, allowing customers to manage: Endpoints Models Test Cases Errors Because APIMATIC is so heavily invested in having a complete API definition, one that it will result in a successful SDK, they've had...[<a href="/2016/10/05/evolving-the-api-sdk-with-apimatic-dx-kits/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_04_at_11.24.39_am.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/05/considering-a-web-api-ecosystem-through-featurebased-reuse/">Considering A Web API Ecosystem Through Feature-Based Reuse</a></h3>
			<p><em>05 Oct 2016</em></p>
			<p>I recently carved out some time to read&nbsp;A Web API ecosystem through feature-based reuse by Ruben Verborgh (@RubenVerborgh) and Michel Dumontier. It is a lengthy, very academic proposal on how we can address the fact that "the current Web API landscape does not scale well: every API requires its own hardcoded clients in an unusually short-lived, tightly coupled relationship of highly subjective quality." I highly recommend reading their proposal, as there are a lot of very useful patterns and suggestions in there that you can put to use in your operations. The paper centers around the notion that the web has succeeded because we were able to better consider interface reuse, and were able to identify the most effective patterns using analytics, and pointing out that there really is no equivalent to web analytics for measuring an APIs effectiveness.&nbsp; In order to evolve Web API design from an art into a discipline with measurable outcomes, we propose an ecosystem of reusable interaction patterns similar to those on the human Web, and a task-driven method of measuring those. To help address these challenges in the world of web APIs, Verborgh and Dumontier propose that we work to build web interfaces, similar to what we do with the web, employing a bottom-up to composing reusable features such as full-text search, auto-complete, file uploads,&nbsp;etc.--in order to unlock the benefits of bottom-up interfaces, they propose 5 interface design principles: Web APIs consist of features that implement a common interface Web APIs partition their interface to maximize feature reuse. Web API responses advertise the presence of each relevant feature Each feature describes its own invocation and functionality. The impact a feature on a Web API should be measured across implementations. They provide us with a pretty well thought out vision involving implementations and frameworks, and the sharing of documentation, while universally applying metrics for being able to identify the successful patterns. It provides us with a compelling, "feature-based method to...[<a href="/2016/10/05/considering-a-web-api-ecosystem-through-featurebased-reuse/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_01_at_12.43.12_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/04/please-share-your-openapi-specs-so-i-can-use-across-the-api-life-cycle/">Please Share Your OpenAPI Specs So I Can Use Across The API Life Cycle</a></h3>
			<p><em>04 Oct 2016</em></p>
			<p>I was profiling the New Relic API, and while I was pleased to find OpenAPI Specs behind their explorer, I was less than pleased to have to reverse engineer their docs to get at their API definitions. It is pretty easy to open up my Google Chrome Developer Tools and grab the URLs for each OpenAPI Spec, but you know what would be easier? If you just provided me a link to them in your documentation!

Your API definitions aren't just driving the API documentation on your website. They are being used across the API life cycle. I am using them fire up and playing with your API in Postman, generating SDKs using APIMATIC, or creating a development sandbox so I do not have to develop against your&nbsp;live environment. Please do not hide your API definitions, bring them out of the shadow of your API documentation and give me a link I can click on--one click access to a machine-readable definition of the value your API delivers.
I'm sure my regular readers are getting sick of hearing about this, but the reality of my readers is that they are a diverse, and busy group of folks and will most likely not read every post on this important subject. If you have read a previous post on this subject from me, and are reading this latest one, and still do not have API definitions&nbsp;or prominent links--then shame on you for not making your API more accessible and usable...because isn't that what this is all about?
[<a href="/2016/10/04/please-share-your-openapi-specs-so-i-can-use-across-the-api-life-cycle/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-waiter.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/04/making-data-serve-humans-through-api-design/">Making Data Serve Humans Through API Design</a></h3>
			<p><em>04 Oct 2016</em></p>
			<p>APIs can help make technology better serve us humans&nbsp;when you execute them thoughtfully. This is one of the main reasons I kicked off API Evangelist in 2010. I know that many of my technologist friends like to dismiss me in this area, but this is more about their refusal to give up the power they possess&nbsp;than it is ever about APIs. I have been working professionally with databases since the 1980s, and have seen the many ways in which data and power go together, and how technology is used as smoke and mirrors as opposed to serving human beings. One of the ways people keep data for themselves is to make it seem big, complicated, and only something a specific group of people (white men with beards (wizards)) can possibly&nbsp;make work. There is a great excerpt from a story by Sara M. Watson (@smwat), called Data is the New &ldquo;___&rdquo;&nbsp;that sums up this for me: The dominant industrial metaphors for data do not privilege the position of the individual. Instead, they take power away from the person to which the data refers and give it to those who have the tools to analyze and interpret data. Data then becomes obscured, specialized, and distanced. We need a new framing of a personal, embodied relationship to data. Embodied metaphors have the potential to bring big data back down to a human scale and ground data in lived experience, which in turn, will help to advance the public&rsquo;s investment, interpretation, and understanding of our relationship to our data. DATA IS A MIRROR portrays data as something to reflect on and as a technology for seeing ourselves as others see us. But, like mirrors, data can be distorted, and can drive dysmorphic thought. This is API for me. The desire to invest, interpret, and understand our relationship to our data is API design. This is why I believe in the potential of APIs, even if the reality of it...[<a href="/2016/10/04/making-data-serve-humans-through-api-design/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/aws_coding_analytics.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/04/increased-analytics-at-the-api-client-and-sdk-level/">Increased Analytics At The API Client And SDK Level</a></h3>
			<p><em>04 Oct 2016</em></p>
			<p>I am seeing more examples of analytics at the API client and SDK level, providing more access to what is going on at this layer of the API stack. I'm seeing API providers build them into the analytics they provider for API consumers, and more analytic services from providers for the web, mobile, and device endpoints. Many companies are selling these features in the name of awareness, but in most cases, I'm guessing it is about adding another point of data generation which can then be monetized (IoT is a gold rush!). As I do, I wanted to step back from this movement&nbsp;and look at it from many different dimensions, broken down into two distinct buckets: Positive(s) More information - More data than can be analyzed More awareness - We will have visibility across integrations. Real-time insights - Data can be gathered on real time basis. More revenue - There will be more revenue opportunities&nbsp;here. More personalization - We can personalize the experience for each client. Fault Tolerance - There are opportunities for building in API fault tolerance. Negative(s) More information - If it isn't used it can become a liability. More latency - This layer slows down the primary objective. More code complexity - Introduces added complexity for devs. More security consideration - We just created a new exploit opportunity. More privacy concerns - There are new privacy concerns facing end-users. More regulatory concerns - In some industries, it will be under scrutiny. I can understand why we want to increase the analysis and awareness at this level of the API stack. I'm a big fan of building in resiliency in our clients &amp; SDKs, but I think we have to weigh the positive and negatives before jumping in. Sometimes I think we are too willing to introduce unnecessary&nbsp;code, data gathering, and potentially opening up security and privacy holes chasing new ways we can make money. I'm guessing it will come down to each...[<a href="/2016/10/04/increased-analytics-at-the-api-client-and-sdk-level/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_03_at_10.44.34_am.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/04/an-integrations-page-for-your-api-solution/">An Integrations Page For Your API Solution</a></h3>
			<p><em>04 Oct 2016</em></p>
			<p>
A new way that I am discovering the new tech services that the cool kids are using is from the dedicated integrations pages of API service providers I track on. Showcasing the services your platform integrates with is a great way of educating consumers about what the possibilities are when it comes to your tools and services. It is also a great way for analysts like me to connect the dots around which services are most important to the average user.
API service providers like DataDog, OpsClarity,&nbsp;and Pingometer&nbsp;are providing dedicated integration pages showcasing the other 3rd party platforms they integrate with. Alpha API dogs like Runscope also have integration APIs, allowing you to get a list of integrations your team depends on (perfect for another story). I'm just getting going tracking on tracking the existence of these integration pages, but each time I have come across one lately I find myself stopping and looking through each of the services included.
Directly, API integrations provide a great way to inform customers about which of the other services they use can be integrated with this platform, potentially adding to the number of reasons why they might choose to go with a service. Indirectly, API integration pages provide a great way to inform the sector about which API driven platforms are important to service providers, and their customers. After I get a number of these integration pages bookmarked as part of my research, I will work on other stories showcasing the various approaches I find.

[<a href="/2016/10/04/an-integrations-page-for-your-api-solution/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/amazon_alexa_echo_dot_tap_4011.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/04/amazon-alexa-as-an-example-when-it-comes-to-api-communications/">Amazon Alexa As An Example When It Comes To API Communications</a></h3>
			<p><em>04 Oct 2016</em></p>
			<p>
I'm always looking for specific API providers to showcase as examples we can follow when crafting different portions of our API strategies. The Amazon Alexa team is doing a pretty kick ass job at blogging, and owning the conversation when it comes to developing conversational interfaces, so I thought I'd highlight them as an example to follow when planning the communications portion of your strategy.
Take a look at the #Alexa tag for the AWS blog. They have a regular stream of storytelling coming out of the platform. Its a mix of talking about the tech of the platform, and showcasing what it can do. What really captured my attention for this story is there regular showcasing of the interesting solutions developers are building on top of the platform. Many platform blogs I read are a one trick pony, just talking about their service, and I think the AWS Alexa team has found a compelling blend.
Ok, AWS probably has just a few more resources than your API team, but trust me, one person can do a lot when they are really engaged. I produce at least five posts a day (ok, they are ranty and weird), and always work to keep thing as diverse as possible, and not about my products or services (fact I don't have any probably helps as well). I do not recommend you using API Evangelist as a model for your platform blogging. I do recommend you use Amazon Alex as a model for how you can create a compelling API platform communication experience.
[<a href="/2016/10/04/amazon-alexa-as-an-example-when-it-comes-to-api-communications/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/whitepapers/definitions/api-evangelist-api-definitions-guide-may-2016-cover.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/03/the-different-reasons-behind-why-we-craft-api-definitions/">The Different Reasons Behind Why We Craft API Definitions</a></h3>
			<p><em>03 Oct 2016</em></p>
			<p>
I wrote a post about the emails I get from folks telling me the API definitions contained within my API stack research, something that has helped me better see why it is I do API definitions. I go through APIs and craft OpenAPI Specs for them because it helps me understand the value each company offers, while also helping me discover interesting APIs&nbsp;and the healthy practices behind them.
The reason I create API definitions and organize them into collections is all about discovery. While some of the APIs I will be putting to use, most of them just help me&nbsp;better understand the world of APIs&nbsp;and the value and the intent behind the companies who are doing the most interesting things in the space.
I would love it if all my API definitions were 100% certified, and included complete information about the request, response, and security models, but just having the surface area defined makes me happy. My intention is to try and provide as complete of a definition as possible, but the primary stop along the API lifecycle I'm looking to serve is discovery, with other ones like design, mocking, deployment, testing, SDKs, and others following after that.
Maybe if we can all better understand the different reasons behind why&nbsp;we all craft and maintain API definitions we can better leverage Github to help make more of them complete. For now, I'll keep working on my definitions, and if you want to contribute head over to the Github repo for my work, and share any of your own definitions, or submit an issue about which APIs you'd like to see included.
[<a href="/2016/10/03/the-different-reasons-behind-why-we-craft-api-definitions/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/bw_synthetic_api.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/03/running-synthetic-data-and-content-through-your-apis/">Running Synthetic Data And Content Through Your APIs</a></h3>
			<p><em>03 Oct 2016</em></p>
			<p>
I was profiling the New Relic API and came across their Synthetics service,which is a testing and monitoring solution that lets you&nbsp;"send calls to your APIs to make sure each output and system response are successfully returned from multiple locations around the world"--pretty straight forward monitoring stuff. The name is what caught my attention, and got me thinking the data and content that we run through our APIs.
Virtualization feels like it defines the levers and gears our API-driven systems, and synthetics feels like it speaks to the data and content that flows through&nbsp;flows through these systems. It feels like everything in the API stack should be able to be virtualized, and sandboxes, including the data and content, which is the lifeblood--allowing us to test&nbsp;and monitor everything.
It also seems like another reason we'd want to share our data schemas, as well as employ common ones like schema.org, so that others can create synthetic data and content sets for variety of scenarios--then API providers could put these sets to work in testing and monitoring their operations. A sort of synthetic data and content marketplace for the growing world of API testing and monitoring.
I see that New Relic has the name Synthetics trademarked, so I'll have to play around with variations to describe the data and the content portion of my API virtualization research. I'll use virtualization to describe gears of the engine, and something along the lines of synthetic data and content to describe everything that we run through it. I am just looking for ways to better describe the different approaches I am seeing, and tell more stories about API virtualization, and sandboxing in ways that resonate with folks.
[<a href="/2016/10/03/running-synthetic-data-and-content-through-your-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_10_02_at_11.04.26_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/03/apis-can-give-an-honest-view-of-what-a-company-does/">APIs Can Give An Honest View Of What A Company Does</a></h3>
			<p><em>03 Oct 2016</em></p>
			<p>
One of the reasons I enjoy profiling APIs is that they give an honest view of what a company does, absent of all the marketing fluff, and the promises that I see from each wave of startups. If designed right, APIs can provide a very functional, distilled down representation of data, content, and algorithmic resources of any company. Some APIs can be very fluffy and verbose, but the good ones are simple, concise, and straight to the point.
As I'm profiling the APIs for the companies included in my API monitoring research, what&nbsp;API Science, Apica, API Metrics, BMC Software, DataDog, New Relic, and Runscope offer quickly become pretty clear. A simple list of valuable resources you can put to use when monitoring your APIs. Crafting an OpenAPI Spec allows me to define each of these companies APIs, and easily articulate what it is that they do--minus all the bullshit that often comes with the businesses side of all of this.&nbsp;
I feel like the detail I include for each company in an APIs.json&nbsp;file provides a nice view of the intent behind an API, while the details I put into the OpenAPI Spec provide insight into whether or not a company actually has any value behind this intent. It can be frustrating to wade through the amount of information some providers feel they need to publish as API documentation, but it all becomes worth it once I have the distilled down OpenAPI Spec, giving an honest view of what each company does.

[<a href="/2016/10/03/apis-can-give-an-honest-view-of-what-a-company-does/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-service-level-agreements.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/03/a-service-level-agreement-api-for-api-service-providers/">A Service Level Agreement API For API Service Providers</a></h3>
			<p><em>03 Oct 2016</em></p>
			<p>
I am spending some time profiling the companies who are part of my API monitoring research, specifically learning about the APIs they offer as part of their solutions. I do this work so that I can better understand what API monitoring service providers are offering, but also for the discoveries I make along the way--this is how I keep API Evangelist populated with stories.&nbsp;
An interesting API I came across during this work was from the Site24X7 monitoring service, specifically their service level agreement (SLA) API. An API for adding, managing, and reporting against SLA's that you establish as part of the monitoring of your APIs. Providing a pretty interesting API pattern that seems like it should be part of the default API management stack for all API providers.
This would allow API providers to manage SLA's for their operations, but also potentially expose this layer for each consumer of the API, letting them understand SLA"s that are in place, and whether or not they have been met--in a way that could be seamlessly integrated with existing systems. An API for SLA management for API providers seems like it could also be a standalone operation as well, helping broker this layer of the API economy, and provide a rating system for how well API providers are holding up their end of the API contract.
[<a href="/2016/10/03/a-service-level-agreement-api-for-api-service-providers/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/security__datadog.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/10/03/a-dedicated-security-page-for-your-api-portal/">A Dedicated Security Page For Your API Portal</a></h3>
			<p><em>03 Oct 2016</em></p>
			<p>
One area I am keeping an eye on while profiling APIs, and API service providers, are any security-related practices that I can add to my research. While looking through DataDog I came across their pretty thorough security page, providing some interesting building blocks that I will add to my API security research. This is all I do as the API Evangelist--aggregate the best practices of existing providers, and shine a light on what they are up to.&nbsp;
On their security page, DataDog provides details on physical and corporate security, information about data in transit, at rest, as well as retention, including personally identifiable information (PII), and details surrounding customer data access. They also provide details of their monitoring agent and how it operates, as well as how they patch, employ SSO, and require their staff to undergo security awareness training. The important part of this is that they encourage you to disclose any security issues you find--critical for providers to encourage this.
Transparency when it comes to security practice is an important tool in our API security toolbox. It is important that API providers share their security practices like DataDog does, helping build trust, and demonstrate competency when it comes to operations. I'm working on an API security page template for my default API portal, and DataDog's approach provides me with some good elements I can add to my template.
[<a href="/2016/10/03/a-dedicated-security-page-for-your-api-portal/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-algorithmic-transparency-2.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/30/you-can039t-say-ai-benefits-outweigh-risk-without-some-algorithmic-transparency/">You Can&#039;t Say AI Benefits Outweigh Risk Without Some Algorithmic Transparency</a></h3>
			<p><em>30 Sep 2016</em></p>
			<p>
I am increasingly hearing the phrase, "the benefits outweigh&nbsp;the risks" applied when talking about AI, machine learning, and the increasing number of algorithmic decisions that are being made in all parts of our digital world. This seems&nbsp;to be the new default of AI and machine learning advocates looking to tip the scales in favor of their technology, over the human side of the discussion.
This can be found used in discussions about AI used in self-driving cars, all the way to policing algorithms making decisions on the street or in a court of law. I'm not opposed to this argument if it is truly the case, but it seems something you can claim without providing the data behind this decision, and simply relying on your lack of faith in humans being able to consistently making decisions.
This is why I wrote about the important of data sharing in industries where algorithms are making an impact, and I am an advocate for providing API access for journalists, analysts, and regulators to actually follow-up with claims that are being made. Allowing 3rd parties to actual weight the pros and cons, and make a collective, more fair and balanced determination of whether or not the benefits truly do outweigh&nbsp;the risk.
I'm not saying that folks who make these claims are being dishonest, but in my experience, in the API space most folks blindly believe in tech and their algorithms, and seem to have almost no faith in humans, and are more than happy to make false claims in the service of the algorithm. This is why I have to say that you can't ever tell me the benefits outweigh the risk without some algorithmic transparency involved--it just won't mean anything to me.
[<a href="/2016/09/30/you-can039t-say-ai-benefits-outweigh-risk-without-some-algorithmic-transparency/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-theatre.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/30/if-you-have-an-api-for-your-platform-you-are-a-stage-for-cybersecurity-theater/">If You Have An API For Your Platform You Are A Stage For Cybersecurity.Theater</a></h3>
			<p><em>30 Sep 2016</em></p>
			<p>
Adding to the many reasons you would want, or not want APIs these days, is the escalating cyber war playing out on the web around the world. APIs aren't playing a role in the cyber security realm in the way you'd think, allowing the bad guys, or even the good guys to get into systems, but they are how these actors are spreading information&nbsp;or disinformation about their cyber activities.&nbsp;
Increasingly Facebook, Twitter, Instagram, Reddit, and other API driven platforms are being used to broadcast, engage, and study the fast growing world of&nbsp;cyber security. Whether it is the Israeli Defense Force, U.S. Cyber Command, or a 15-year-old hacker in your basement, they are using these API driven channels to broadcast their message, as well as monitor the message of their adversaries, with us analysts&nbsp;following up behind trying to make sense of it all--using the same channels.&nbsp;
Moving forward if you have a platform with an API, you will have a stage for the Cybersecurity.Theater to play out. Actors will use you to tell their story, to communicate, syndicate images, publish their videos, and make their payments. This will scare the shit out of many of you, but for others, it will be an opportunity to sell popcorn&nbsp;and other concession items.&nbsp;
Since "securing cyberspace is a 24/7 responsibility&nbsp;(United States army Cyber Command and Second Army), it will need a 24/7, API driven theater to perform in.
[<a href="/2016/09/30/if-you-have-an-api-for-your-platform-you-are-a-stage-for-cybersecurity-theater/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-conversational-interfaces.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/30/defining-a-conversational-layer-on-top-of-apis/">Defining A Conversational Layer On Top Of APIs</a></h3>
			<p><em>30 Sep 2016</em></p>
			<p>As I am exploring, and writing about&nbsp;Meya's&nbsp;Bot Flow Markup Language (BFML), &nbsp;I came across the announcement from Google about their acquisition of API.AI, titled "Making Conversational Interfaces Easier to Build". I feel like this description reflects what I was writing about "Beyond Mobile: API Ready For iPaaS, Voice, and Bots", and sounds better to me than saying voice, bot, or integration workflow. Whether its skills for voice enablement, intents and flows for bot interactions, or triggers, actions, and integrations with iPaaS, I'm guessing we are going to need a way to define, and convey meaning through this growing conversation we'll be having using API resources. With OpenAPI Spec and API Blueprint we finally have adequate ways to describe where our data, content, and algorithmic resources reside, and a little bit about what they do, but it feels like we need a similar way of defining the conversational layer on top. I see the beginning of this present in&nbsp;Meya's&nbsp;Bot Flow Markup Language (BFML), which is a YAML definition description a flow, made of components that can each make an API call, all in the services of what they consider "intent". &nbsp;I"ll have to see how other bot providers are defining this layer, as well as learn more about how Alexa is defining the conversational layer for their skills. All of this smells like we need some Hydra injected into the conversation, but I need to do more research before I start evangelizing anything. The whole Slackbot thing is interesting to me from a technical point of view--not so much from a business side of things. Twitter bots I find intriguing because they are public, and can wreak havoc, or be very creative. Alexa is interesting from both a technical&nbsp;and business perspective for me. But, helping define a conversational layer on top of the world of APIs is intriguing to me, mostly because it continues building on top of what I consider to be one of the...[<a href="/2016/09/30/defining-a-conversational-layer-on-top-of-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/bw_sdk_expanding.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/30/api-sdks-getting-more-specialized/">API SDKs Getting More Specialized</a></h3>
			<p><em>30 Sep 2016</em></p>
			<p>I have been doing a lot of thinking about the client and SDK areas of my research lately, considering how these areas overlap with the world of bots, as well as with voice, and iPaaS. I'm thinking about the hand-crafted, autogenerated, and even API client as a service like Postman, and Paw. I'm thinking about how APIs are being put to work, across not just web and mobile, but also systems to system integration, and&nbsp;the skills in voice platforms like Alexa, and the intents in bot platforms like Meya. I'm considering how APIs can deliver the skills needed for the next generation of apps beyond just a mobile phone. I kicked off my SDK research over a year ago, where I track on the approaches of leading platforms who are offering up code samples, libraries, and SDKs in a variety of programming languages. While conducting my research, I've been seeing the definition of what is an SDK slowly expand and get more specialized, with most of the expansion in these areas: Mobile Development Kit - Providing code resources to help developers integrate their iPhone, Android, Windows, and other mobile&nbsp;applications with APIs. Platform Development Kits - Provide code resources for using APIs in other specific platforms like WordPress, SalesForce, and others. In addition to mobile, and specific platform solutions, I am seeing API providers stepping up and providing iPaaS options, like ClearBit is doing with their Zapier solutions. As part of this brainstorm exercise, I feel like I should also add a layer dedicated to delivering via iPaaS: Integration Platform as a Service Development Kits - Delivering code resources for use in iPaaS services like Zapier, allowing for simpler system to system integration across many different platforms, with some having a specific industry focus. Next, if I scroll down the home page of API Evangelist, I can easily spot 11 other areas of my research that stand out as either areas I'm seeing SDK movement&nbsp;or an...[<a href="/2016/09/30/api-sdks-getting-more-specialized/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-plan-b.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/30/a-plan-b-api-switch/">A Plan B API Switch</a></h3>
			<p><em>30 Sep 2016</em></p>
			<p>I've had an idea for a bot-related&nbsp;service I call "plan b", which would act as a secondary action for any sort of bot request / response to an API. When developers are providing common bot responses like looking up a business address, sports statistic or stock quote, it could be exposed to suggestions for a "plan b". When a request is made, it can travel via its regular path, but it would also be included in a queue&nbsp;where other 3rd party developers could provide plan b suggestions, either free or paid. When a user is engaging with the bot and didn't like the primary response, they could click on the "plan b" option, opening up alternative responses. In theory, the user could cycle through each "plan b" suggestion until they find a suitable response.&nbsp; Since I don't have any startup aspirations&nbsp;I enjoy&nbsp;working through these ideas on my blog as part of my wider research, I found myself thinking my Plan B bot&nbsp;idea as I was learning about Meya's&nbsp;Bot Flow Markup Language, and in the context of how we can build in resiliency into API client code. The concept of a plan B seems extremely relevant to this discussion, and worth consideration beyond just bots, into voice, iPaaS, and other clients being put to work on top of APIs. In the context of fault and change resistance, it seems like we'd have a "plan b" layer in our SDKs to deal with when an API goes away temporarily, or even permanently. I know I do not have ANY plan b in place for any of my API integrations, either directly in the SDK, or in my business strategy--I am guessing this is the case for most API integrations. It seems like responding to status codes etc could be considered fault-tolerance (micro), where a plan b option would be in the change resistance category (macro). I had pictured "plan b" being some sort of hypermedia layer that...[<a href="/2016/09/30/a-plan-b-api-switch/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/schemahub_thanks.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/29/schemahub039s-usage-of-github-to-launch-their-api-service-is-a-nice-approach/">SchemaHub&#039;s Usage Of Github To Launch Their API Service Is A Nice Approach</a></h3>
			<p><em>29 Sep 2016</em></p>
			<p>I'm looking through a new API definition focused service provider called SchemaHub today, and I found their approach to using Github as a base of operations was interesting and provided a nice blueprint for other API server providers to follow. I'm continually amazed at the myriad of ways that Github can be put to use in the world of APIs, which is one of the things I love about it. As a base for SchemaHub, they created a Github Org, and made their first repository the website for the service, hosted on Github Pages. In my opinion, this is how all API services should begin, as a repo, under an organization on Github--leveraging the social coding platform as a base for their operations. SchemaHub is taking advantage of Github for hosting their API definition focused project--free, version controlled, static website hosting for schemahub.io.&nbsp; As I was looking through their site, learning about what they are doing I noticed a subscription button at the bottom of the page, asking me to subscribe, and they'll notify me when things are ready. Once I clicked on the button, I was taken for a Github OAuth dance, which now makes SchemaHub not just a Github repo for the site, it is an actual Github Application that I've authenticated with using my Github account. They only have access to my profile and email, but is the types of provider to developer connection I like to see in the API world. Once I authorize and connect I am taken to a thank you page back on their website, letting me know I will be contacted shortly with any updates about the service. Oh, and I'm offered a Twitter account as well, allowing me to stay in tune with what they are up to--providing a pretty complete picture for how new API services can operate.&nbsp; SchemaHub's approach reflects what I'm talking about when I say that Github should offer an Oauth service,...[<a href="/2016/09/29/schemahub039s-usage-of-github-to-launch-their-api-service-is-a-nice-approach/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/1_lfoybsgdnspy0i24b3dubg.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/29/flow-abstraction-and-intent-layer-on-top-of-apis-to-feed-the-bots/">Flow Abstraction And Intent Layer On Top Of APIs To Feed The Bots</a></h3>
			<p><em>29 Sep 2016</em></p>
			<p>I was reading an interesting post on developing bots from Meya, a bot platform provider, which I think describes the abstraction layer between what we are calling&nbsp;bots, and what we know as APIs. I have been trying to come up with a simple way of quantifying the point where bots and APIs work together,&nbsp;and Meya's approach to flow and intent provides me with a nice scaffolding. The flow step of their bot design rationale&nbsp;provides a nice way to think about how bots will work, breaking out each step of the bot interaction in plain English.&nbsp;They use a YAML format they call Bot Flow Markup lLnguage, or BFML, to describe the flows, comparing BFML to HTML, with this definition: HTML is spatial, and BFML is temporal. HTML determines&nbsp;where&nbsp;UI exists, and BFML determines&nbsp;when&nbsp;UI exists. The second part of their bot design rationale involves&nbsp;Intents, providing this additional definition: If BFML is like HTML, then intents are like URLs. According to Meya, "intents can be keywords, regular expressions, and natural language models as you get more sophisticated". This seems to be where the&nbsp;more human aspect of what is getting done here is defined, mapping each intent to a specific flow, which can execute one or many steps to potentially satisfy the intent. The third step is components, which is where the direct API connection comes clear. If you look at their example, in the component they are simply making a call to the Chuck Norris joke API, returning the results as part of the flow. Each part of the flow calls its targeted component, and each component can make a GET, POST, PUT, PATCH, or DELETE to an API that provides the data, content, or algorithm behind the component. This provides me with a beginning scaffolding to think about how bot platforms are constructing the API abstraction layer behind bot activity. I will be going through other bot platforms to understand each individual napproach. Bots to me are just...[<a href="/2016/09/29/flow-abstraction-and-intent-layer-on-top-of-apis-to-feed-the-bots/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/twitter_widgets_js_depoly_arch_v2.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/29/code-resiliency-lessons-in-how-twitter-deploys-their-embeddables/">Code Resiliency Lessons In How Twitter Deploys Their Embeddables</a></h3>
			<p><em>29 Sep 2016</em></p>
			<p>I am learning about how Twitter deploys their widgets. Extracting some insight for my research around how we can build change resiliency into our client code. As I'm doing my regular monitoring of the API space I am trying to keep an eye out for any examples from leading providers of how there are investing in client code being more change resilient. This Twitter blog post provides me with three concepts I wanted to&nbsp;add to my research: Reversibility:&nbsp;&lsquo;Rollback first, debug later&rsquo; is our motto. Rollback should be fast, easy, and simple. Ideally, it&rsquo;s a giant red button that can get our heart rates down. Incremental release:&nbsp;All code has bugs and deploys have an uncanny way of surfacing them. That&rsquo;s why we wanted the ability to release new code in phases. Visibility:&nbsp;We need to have graphs to show how both versions of widgets.js are doing at all times. We also need the ability to drill down by country, browser type, and widget type. These graphs should be real time so we can quickly tell how a deploy is going and take action as necessary. These are change elements that seem like they need consideration as we craft our web, mobile, device, visualization, bot, voice, and other types of API clients. These three elements should be present in the code, anywhere I'm making an API call. Being able to reverse how I'm interacting with an API, the incremental release of new API paths or changes to existing APIs, and having an analytics&nbsp;layer can contribute to helping us deal with change. I think I am going to get started with an analytics layer for my own client code. Start thinking about logging the calls I'm making to any API I depend on. I have this in place for the server side of the APIs that I manage&nbsp;but do not have any sort of logging at the client level. Not only do I not have any plan for change...[<a href="/2016/09/29/code-resiliency-lessons-in-how-twitter-deploys-their-embeddables/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/announcing_the_clearbit__zapier_integration.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/29/beyond-mobile-api-ready-for-ipaas-voice-and-bots/">Beyond Mobile: API Ready For iPaaS, Voice, and Bots</a></h3>
			<p><em>29 Sep 2016</em></p>
			<p>I enjoy being able to switch gears between all the different areas of my API research. It helps me find the interesting areas of&nbsp;overlap and potentially synchronicity&nbsp;in how APIs are being put to work. After thinking about the API abstraction layer present in Meya's&nbsp;bot platform, I was reading about Clearbit's&nbsp;iPaaS integration layer with Zapier. Zaps are just like the components employed by Meya, and Clearbit walks us through delivering intended workflows with the valuable APIs they provide, executed Zapier's iPaaS service. Whether its skills for voice, intents for bots, or triggers for iPaaS, an API is delivering the data, content, or algorithmic response required for these interactions. I've been pushing for API providers to be iPaaS ready, working with providers like Zapier for some time. I predict you'll hear find me showcasing examples of API providers sharing their voice and bot integration solutions, just like with Clearbit has with their iPaaS solutions, in the future. I would say that even before API providers think about the Internet of Things, they should be thinking more deeply about iPaaS, voice, and bots. Not that all these areas will be relevant, or valuable to your API operations, but they should be considered. If you have the resources, they might provide you with some interesting ways to make your API more accessible to non-developers--as Clearbit opens their blog post&nbsp;opening. When it comes to skills, intents, and iPaaS workflows, I am thinking we are going to have to&nbsp;be more willing to share our definitions (broken record), &nbsp;like we see Meya&nbsp;doing with their Bot&nbsp;Flow&nbsp;Markup&nbsp;Language (BFML) in YAML. I will have to do some more digging to see how Amazon is working to make Alexa Skills more shareable and reusable, as well as take another look edition of the Zapier API to understand what is possible--I took a look at it back in the spring, but will need a refresher.&nbsp; While the world of voice and bots API integration seems to be...[<a href="/2016/09/29/beyond-mobile-api-ready-for-ipaas-voice-and-bots/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/tensorflow.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/29/an-opportunity-for-a-restful-api-layer-on-top-of-new-tensorflow-models/">An Opportunity For A RESTful API Layer On Top Of New TensorFlow Models</a></h3>
			<p><em>29 Sep 2016</em></p>
			<p>
I was looking the open source models available for execution via the machine learning&nbsp;platform TensorFlow, and couldn't help but think there is a pretty big opportunity for a web API layer on top of it. After a little Googling, I see there is someone asking on Stack Overflow, Google Groups, and a student project to tackle the need. Maybe there are some other projects out there already in the works, but I couldn't find anything with 10 minutes of Googling (mad skills).
Google has twelve pretty compelling machine learning models available on Github:

autoencoder&nbsp;-- various autoencoders
inception&nbsp;-- deep convolutional networks for computer vision
namignizer&nbsp;-- recognize and generate names
neural_gpu&nbsp;-- highly parallel neural computer
privacy&nbsp;-- privacy-preserving student models from multiple teachers
resnet&nbsp;-- deep and wide residual networks
slim&nbsp;-- image classification models in TF-Slim
swivel&nbsp;-- the Swivel algorithm for generating word embeddings
syntaxnet&nbsp;-- neural models of natural language syntax
textsum&nbsp;-- sequence-to-sequence with attention model for text summarization.
transformer&nbsp;-- spatial transformer network, which allows the spatial manipulation of data within the network
im2txt&nbsp;-- image-to-text neural network for image captioning.

That would make a pretty stellar machine learning API stack, with a simple, intuitive, RESTl wrapper. Once done it seems like there would also be a pretty big opportunity for containerized deployment of these machine learning APIs, on a wholesale basis. I'm still not sure how the whole open source code to commercial API implementation model will work, but I'm sure there is some money to made in there somewhere--at least when it comes to implementation and support.
I will add to the list of open source software I'd like to see have an accompanying web API, as well as containerized, or even serverless implementation. It makes me happy that Google is helping commoditize&nbsp;machine learning by open sourcing their tools, but I'd also like to see them further simplified and polished for consumption by a wider developer, or even non-developer audience, using web APIs.
[<a href="/2016/09/29/an-opportunity-for-a-restful-api-layer-on-top-of-new-tensorflow-models/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-localhost.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/28/we-focus-on-interacting-with-the-api-developer-community-where-they-live/">We Focus On Interacting With The API Developer Community Where They Live</a></h3>
			<p><em>28 Sep 2016</em></p>
			<p>Another story I harvested fro&nbsp;a story&nbsp;by Gordon Wintrob (@gwintrob) about how Twilio's distributed team solves developer&nbsp;evangelism, was about how they invest in having a distributed team, providing an on the ground presence in the top cities they are looking to reach. I know this isn't something all API providers can afford, but I still think it was still an important approach worth noting. Like with many other aspects of Twilio's approach, they are pretty genuine about why they invest in a distributed API evangelism team: We also focus on interacting with the developer community where we actually live. We don&rsquo;t think it&rsquo;s valuable to parachute into a tech community, do an event, and then leave. We need to participate in that community and make a real impact.&nbsp; I wish there was a way that smaller API providers could deliver like this. I wish we all had the resources of Twilio, but in reality, most API providers won't even be able to "parachute into a tech community", let alone have a dedicated presence there. I've seen several attempts like this fail before, so I am hesitant to say it, but I can't help but think there is an opportunity for evangelists in certain cities. There isn't any startup potential here (let me make that clear), but I think there is an opportunity for developer advocates, evangelists, and would-be evangelists to band together, network, and offer up services to API providers. All you'd have to do is take the page from the Twilio&nbsp;playbook&nbsp;and execute in a decentralized way--where multiple evangelists could work together as a co-op. The trick is to bring together evangelists who actually give a shit about the space--something that would be very difficult to accomplish. Anyways, just some more thoughts from my API notebook, inspired by Gordon's post. If nothing else, Twilio's approach should help guide other larger API providers, showing how important it is to invest in developers, in-person at the local level....[<a href="/2016/09/28/we-focus-on-interacting-with-the-api-developer-community-where-they-live/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-change-2.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/28/thinking-about-how-i-can-build-change-resilience-into-my-api-integrations/">Thinking About How I Can Build Change Resilience Into My API Integrations</a></h3>
			<p><em>28 Sep 2016</em></p>
			<p>After I wrote a piece on guidance from the USGS around writing fault-resistant code when putting their API to use, my friend Darrel Miller expanding on this by suggesting I include "change resilience" as part of the definition.&nbsp; @kinlane I would like to see that guidance expanded to include writing change resilient client code. &mdash; Darrel Miller (@darrel_miller) September 9, 2016 It is something that has sat in my notebook for a couple weeks, and keeps floating up as a concept I'd like to explore further. I have some initial thoughts on what this means&nbsp;but is something that I need to write about before I grasp better. Hopefully, it will bring more suggestions about what change resilient code means to other people. Ok, so off the top of my head, what elements would I consider when thinking about producing change resilient client code: Status Codes - Making sure clients read, and pay attention to HTTP status codes used by API providers. Hypermedia - Links are fragile, and avoiding baking them into clients makes a whole lotta sense.&nbsp; Plan B API - Have a backup API identified, that can be used when the A API provider goes away. Circuit Breaker - Build in a circuit breaker into code that responds to specific status codes and events. Now that I'm exploring, I have to ask, who's responsibility is it to build change resilience into the clients? Provider or consumer? Seems like there is a healthy responsibility on both parties? IDK. I guess we should just all be honest about how fragile the API space is, and providers should be honest with consumers when it comes to thinking about change resiliency, but ultimately API consumers have to begin to thinking more deeply&nbsp;and investing more when it comes planning for change--not just freaking out when it happens. I have to admit that the code I have written as part of my API monitoring system, which integrates with over 30...[<a href="/2016/09/28/thinking-about-how-i-can-build-change-resilience-into-my-api-integrations/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/amazon_alexa_blue.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/28/the-bot-platform-that-operates-like-alexa-will-win/">The Bot Platform That Operates Like Alexa Will Win</a></h3>
			<p><em>28 Sep 2016</em></p>
			<p>
I'm going through Amazon's approach to their Alexa voice services, and it is making me think how bot platforms out there should be following their lead when it comes crafting their own playbook. I see voice and bots in the same way that I see web and mobile--they are just one possible integration channel for APIs. They each have their own nuances of course, but as I'm going through Amazon's approach, there are quite a few lessons on how to do it correctly here--that apply to bots.&nbsp;
Amazon's approach to investment in developers on the Alexa platform and their approach to skills development&nbsp;should be replicated across the bot space. I know Slack has an investment fund, but I don't see the skills development portion present in their ecosystem. Maybe it's in there, but it's not as prominent as Amazon's approach. Someday, I envision galleries of specific voice and bot skills like we have application galleries today--the usefulness and modularity of these skills will be central to each provider's success (or failure).
I had profiled Slack's approach before I left for the summer, something I will need to update as it stands today. I will keep working on profiling Amazon's approach to Alexa, and put together both as potential playbook(s). I would like to eventually be able to lay them side by side and craft a common definition that could be applied in both vthe oice API, as well the bot API sector. I need to spend more time looking at the bot landscape, but currently I'm feeling like any bot platform that can emulate Amazon's approach is going to win at this game--like Amazon is doing with voice.
[<a href="/2016/09/28/the-bot-platform-that-operates-like-alexa-will-win/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/plcopen_opcfoundation_opc_ua_diagram.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/28/learning-about-opc-the-interoperability-standard-for-industrial-automation/">Learning About OPC, The Interoperability Standard For Industrial Automation</a></h3>
			<p><em>28 Sep 2016</em></p>
			<p>I am spending a portion of my time each week learning about how APIs are being applied at the industrial level. An example of this can be found over at Opto 22, with their approach to using REST across their Programmable Automation Controllers (PAC). As I do with other industries I spend my time looking through the approaches&nbsp;of API pioneers in the space, which leads me to other contributing factors to why web APIs are being used to change how things are done in any industry. For now, my industrial API research is a pretty big umbrella, encompassing&nbsp;oil &amp; gas, manufacturing, and often moving into other areas I'm already tracking agriculture and energy. This approach allows me to identify companies who are leading the charge (like Opto 22), as well as specifications, tools, and other elements that are contributing to the evolution of APIs in each area--in this case, its broadly industrial usage of web APIs. In my researching of industrial APIs I have come across the&nbsp;OPC format which was originally known as the Object Linking and Embedding for Process Control, which is defined as: OPC is the interoperability standard for the secure and reliable exchange of data in the industrial automation space and in other industries&nbsp;The OPC standard is a series of specifications developed by industry vendors, end-users and software developers. These specifications define the interface between Clients and Servers, as well as Servers and Servers, including access to real-time data, monitoring of alarms and events, access to historical data and other applications. I'm still getting going with the world of industrial automation, but I am looking through the OPC Unified Architecture to see where I can find any common definitions and schemas that could apply to industrial API design. I don't have any sense of how open these standards bodies are with their specifications, and I don't want to end up like Carl Malumud, but I do want to help identify and encourage...[<a href="/2016/09/28/learning-about-opc-the-interoperability-standard-for-industrial-automation/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_09_27_at_9.27.34_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/28/every-government-agency-should-have-an-faq-api-like-the-dol/">Every Government Agency Should Have An FAQ API Like The DOL</a></h3>
			<p><em>28 Sep 2016</em></p>
			<p>
I wrote about my feelings that all government agencies should have a forms API like the Department of Labor (DOL), and I wanted to separately showcase their FAQ API, and say same thing--ALL government agencies should have a frequently asked question (FAQ) API. Think about the websites and mobile applications that would benefit from ALL government agencies at the federal, state, and local level having frequently asked questions available in this way--it would be huge.&nbsp;
In a perfect world, like any good API provider, government agencies should also use their FAQ API to run their website(s), mobile, and internal systems--this way the results are always fresh, up to date, and answering the relevant questions (hopefully). I get folks in government questioning the opening up of sensitive information via APIs, but making FAQs available in a machine readable way, via the web, just makes sense in a digital world.
Like the forms API, I will be looking across other government agencies for any FAQ APIs. I will be crafting an OpenAPI Spec for the DOL FAQ API (man that is a lot of acronyms). I will take any other FAQ APIs that I find and consider any additional parameters, and definitions I might want to include in a common FAQ API definition for government agencies. This is another area that should have not just a common open API and underlying schemas defined, but also a wealth of server and client side code--so any government agency can immediately put it to work in any environment.
[<a href="/2016/09/28/every-government-agency-should-have-an-faq-api-like-the-dol/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-email.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/27/thanks-for-reaching-out-about-your-api/">Thanks For Reaching Out About Your API</a></h3>
			<p><em>27 Sep 2016</em></p>
			<p>I get a number of folks emailing me about their API and API-focused&nbsp;services. When I have the bandwidth I spend time in my inbox and respond to these emails. To help me do this a little more efficiently (I'm not always very quick about it), I'm formalizing some snippets I can use in my response(s). I want to thank them for reaching out, while also helping them understand my approach to successfully operating API Evangelist. Here is one basic email I crafted today, in response to a pretty slick API provider that I will be writing about shortly: Hi There, I received your email. Thanks for the kind words.&nbsp;Appreciate you introducing me to your [API / API related service]. I'm going to have to pass on the posting of the [guest post, infographic, white paper, case study, etc] to apievangelist.com, but I'm happy to keep an eye on what you are up to as part of my regular work. I visited your site and see that you have a blog (with feed), Twitter, and a Github account. These are the channels I&rsquo;ll be keeping an eye on, and when you post a&nbsp;blog post or press release, Tweet something out, or I see a Github repo or commit of interest, I'll definitely include in my research, and craft a story for the blog. I have also added your company, blog, feed, twitter, and Github accounts to my monitoring system. Keep on doing interesting things with APIs and I'll make sure it becomes part of my storytelling in the space. So far, and reviewing your web site and developer, your API efforts [looked pretty polished / could use some work / is not very modern] I&rsquo;ll keep digging around and publish anything interesting that I find. Thanks! Kin Lane@kinlane This is a basic template I will use moving forward. I'll tweak it some for each response, but ultimately I am trying to keep thing consistent with folks...[<a href="/2016/09/27/thanks-for-reaching-out-about-your-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/new_personal_access_token.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/27/github-needs-client-oauth-proxy-for-more-complete-clientside-apps-on-pages/">Github Needs Client OAuth Proxy For More Complete Client-Side Apps On Pages</a></h3>
			<p><em>27 Sep 2016</em></p>
			<p>I'm building what I am calling "micro tools", that run 100% on Github. To push my work forward I developed a base template I can use for deploying apps that run 100% on Github, using Github Pages, the Github API, and Github OAuth as the engine. As a next step I wanted to develop a simple YAML editor that run on Github, allowing me to edit the YAML core of each tool, that is stored in the _data folder for each Jekyll site I host on Github Pages. The key to all of this working securely is Github personal access tokens, which every Github user has in their accounts under settings. I have employed this approach to running apps on Github Pages before using OAuth.io as the broker, something that works very well, and I highly recommend it. I have also run using my own Github OAuth proxy, where I had server side code that would do the OAuth dance for me, when authenticating via these apps. The problem is I want them to run 100% on Github, and be forkable by anyone, leaving personal access tokens as my only option. What would really rock, is if Github provide us with a solution to client-side authentication via the Github API. We can already accomplish the hole thing, we just need Github to offer the same functionality that OAuth.io -- heck I recommend you just buy them and implement. An increasing number of API providers are managing their API operations on Github. From API portal, to documentation and SDKS--they are using Github and Github Pages to take care of business. So having Github OAuth, plus authentication via other providers would be a huge benefit. Additionally, it would open up Github Pages to be more than just static project pages--they could become little mini apps, or micro tools as I call them. Forking one of my micro tools, then finding your personal access tokens is not that...[<a href="/2016/09/27/github-needs-client-oauth-proxy-for-more-complete-clientside-apps-on-pages/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_09_26_at_4.59.29_pm.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/27/every-government-agency-should-have-a-forms-api-like-dol-does/">Every Government Agency Should Have A Forms API Like DOL Does</a></h3>
			<p><em>27 Sep 2016</em></p>
			<p>
I was taking another look at the API efforts out of the Department of Labor (DOL), to help refresh my awareness of what they are serving up, and I came across the DOL Forms API. The API does what it says, providing access on " the most frequently requested Department of Labor forms", which seems like to me should be the&nbsp;default for ALL government agencies.
The API returns some valuable details about each agency from including OMB number, URL, file extension, file size, and other meta information like a description, tags, and revision. I know that many in the API community would like all forms to be APIs, but I would be happy if we just started by making the concept of a forms API default across all government agencies first.
Before I dig into this individual API, I'm thinking that I will craft an OpenAPI Spec for the DOL Forms API, and see if there are any other form APIs available across US federal agencies that I should be considering. With a little work maybe I can merge them into a single open API definition that any government agency can follow, when thinking about which APIs they should be making available.
[<a href="/2016/09/27/every-government-agency-should-have-a-forms-api-like-dol-does/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-design-empathy.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/27/api-design-is-not-requirement-for-all-devs-but-a-little-empathy-should-be/">API Design Is Not Requirement For All Devs But A Little Empathy Should Be</a></h3>
			<p><em>27 Sep 2016</em></p>
			<p>My friend Matthew Reinbold wrote a great post on his blog asking "what if developers aren't meant to do API design"? I think he is touching on an important aspect of why DevOps might not work everywhere in the same expected ways. We all have our strengths, and we all have our weaknesses, and I agree with him that maybe we are asking too much of our developers--API design might not be their strength.&nbsp; As the owner of a small business operated by one person (me), DevOps is hard. I cannot do everything myself, and require a variety of services to help me out, but I still hit areas where I'm deficient like graphic design and editing. I'm getting better at editing, but my graphic design skills never seem to evolve at all. I would like to think I can do everything, but I can't, and if I had a job at a large organization, and was expected to learn every piece of a modern stack--I'm not sure I could do it. Bringing it back to the API design, though, the question of who does API design still needs to be answered. It can't just be left behind because a developer of an API doesn't have the chops. Ideally, a company could afford to hire an API designer and architect to come in and work their magic, but I know in&nbsp;reality this isn't going to happen within many organizations--so what can be done? API Design In Our Tools &amp; Services - Developers use language and platform specific&nbsp;IDE functionality as a crutch, we need the same enablement for API design in leading API design tools and services. This is why projects like the API design stylebook are so important&nbsp;because they begin to provide us with the definitions that are needed to drive the advancements. Sharing Of Common API Patterns - I feel like such a nag on this subject, but we need to share the common...[<a href="/2016/09/27/api-design-is-not-requirement-for-all-devs-but-a-little-empathy-should-be/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-access-cloud.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/27/api-access-to-your-account-by-default-but-requires-permission-to-see-others/">API Access To Your Account By Default But Requires Permission To See Others</a></h3>
			<p><em>27 Sep 2016</em></p>
			<p>I wrote about SoundCloud beginning to require approval before developers get access to any API resources&nbsp;yesterday, a concept that I want to keep exploring. I'm going to be going through the APIs track on, looking for different variations of this, but before I did this I wanted to explore a couple of approaches I already had rattling around in my head. What if, when you first sign up for API access you only get access to your own data, and content? You couldn't get access to any other users until you were approved. It seems like something that would incentivize developers to publish data and content, build their profiles out, which is good for the platform right? It will also protect other end-users from malicious activity by random developers who are just looking to wreak havoc in support of their own objectives and do not care about the platform--like we saw with Soundcloud. A good example of how this could be applied is evident in&nbsp;the post yesterday by Kris Shaffer on Medium, who was looking to get his content out of the platform. I use the Medium API to syndicate blog posts to Medium&nbsp;(POSSEE), but there is no read API allowing me to pull my content out--I agree with Kris, this is a problem. What if Medium opened up API access, allowing us platform users to get at our own content, but then required approval of any app before there ever is access to other users content? Some food for thought. I hear a lot of platforms say they don't do APIs because they don't want to end up with the same problems as Twitter. I think this is the result of some legacy views about public APIs that should just go away. Not all APIs are created equal, and I feel that APIs shouldn't always be just about applications, and often times are just a lifeline for platform users, helping us end-users better manage...[<a href="/2016/09/27/api-access-to-your-account-by-default-but-requires-permission-to-see-others/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/developer_dol_gov__united_states_department_of_labor_developer_portal.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/26/taking-another-look-at-the-department-of-labor-api-efforts/">Taking Another Look At The Department Of Labor API Efforts</a></h3>
			<p><em>26 Sep 2016</em></p>
			<p>Someone asked me about the current state of the Department of Labors (DOL) API efforts the other day, and since I hadn't actually taken a look in a few months I wanted to spend some time in there seeing what they have going on. There is no better way to get a feel for what a government agency is up to than going through their API efforts--the DOL is pretty ahead of the game in this area. The vibe when you land on developer.dol.gov (which is a great subdomain) is nice. It is clean&nbsp;and has all the links that I am looking for, providing access to their APIs, as well as supporting code, while allowing you to ask questions and report bugs. One thing I think is interesting in their approach is that they efficiently use Github in support of their code, apply Stack Exchange in support of asking questions, and employ Github issues for reporting bugs. I understand that government agencies do not always have the resources necessary to support their API efforts, so this approach seems sensible to me when providing the minimum viable support for an API. The Department of Labor provides a number of APIs which provide access to some key economic data, broken down into four separate categories Health &amp; Safety&nbsp; Injuries And Illness - Rate and number of work-related injuries, illnesses, and fatal injuries. Gulf Oil Spill - Sample results for chemicals, noise, and heat stress index measurements from monitoring workers engaged in the oil spill cleanup in the Gulf of Mexico for exposure to hazardous chemicals and conditions. DOL OSHA Compliance - Fatalities and catastrophes resulting in the hospitalization of three or more workers. MSHA Employment Production - Annual summation of employee hours and coal production reported by mine operators. Mine Violation - Violations issued from 1/1/1978 to 1/1/2000 as a result of MSHA inspections. Fatal Occupational Injuries - Nonfatal and fatal data for the nation and for...[<a href="/2016/09/26/taking-another-look-at-the-department-of-labor-api-efforts/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-share-stack.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/26/my-api-definitions-are-incomplete-but-you-do-not-want-to-contribute/">My API Definitions Are Incomplete But You Do Not Want To Contribute</a></h3>
			<p><em>26 Sep 2016</em></p>
			<p>I am perpetually working to publish all of my API definitions my API Stack Github repository, with the front available as the API Stack. I regularly push the latest copies of all of my OpenAPI Specs to these Github repos when I have time, but my OpenAPI Specs are far from complete, and are&nbsp;something I'm always working to make as complete as a I can, and certify when possible. My primary objective around defining APIs using OpenAPI Spec is all about API discovery, and understanding the request and response surface area for many of the publicly available APIs out there today. Secondarily I'm interested in actually putting these API definitions to use across the API lifecycle in design, deployment, management, testing, virtualization, and client tools and services. It takes a lot of work to certify an API definition as complete--a process best executed by the API owner. In the absence of this, I am trying to fill in the gap(s) where I can, but as a one-person operation, I only&nbsp;have so much bandwidth. I have had people ask me if I'd focus on specific APIs they are interested in having, and even have some folks pay me for this work, but what really surprises me is the number of emails, DMs, and Github requests I get from folks telling me about how incomplete they are, and the low number of folks who ever contribute back by sharing their definitions with me. People really enjoy emailing me to tell me my API definitions are incomplete, but they rarely&nbsp;fork&nbsp;and contribute back any API definitions to my work--even though I regularly ask people if they could (at the very least submit an issue with URL to where it exists on your server). I feel like this is the general tone of the space that has been set by VC investment and unhealthy views of intellectual property defined by companies like Oracle and Google. People are perfectly happy building...[<a href="/2016/09/26/my-api-definitions-are-incomplete-but-you-do-not-want-to-contribute/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/bw_algorithmic_sovereignty.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/26/helping-validate-data-and-algorithmic-sovereignty-at-the-api-layer/">Helping Validate Data And Algorithmic Sovereignty At The API Layer</a></h3>
			<p><em>26 Sep 2016</em></p>
			<p>
I have showcased&nbsp;examples of API providers allowing you to deploy your API into various regions around the world like Algolia does, but it is a topic that I think will keep gaining traction as data, content, or algorithmic sovereignty continues to be a privacy, security, or regulatory concern. As the Internet continues to evolve, people and companies are only going to continue being concerned&nbsp;and dare I say become nationalistic about where they digital worlds exist and operate.
When I Google "data sovereignty" I get:
Data sovereignty is the concept that information which has been converted and stored in binary digital form is subject to the laws of the country in which it is located.
Where data is stored tends to dominate conversations historically, but I think that increasingly algorithmic sovereignty will become a concern as well. With more platforms employing algorithms at almost every level of operation, at some point where they are operating is going to come into play (ie. the Facebook news feed algorithm might have to have different considerations in the US than it does in EU).
The DNS layer is the first place to start when validating where data is being stored, or an algorithm is being executed, and which rules apply. After that I'd guess that we will need the API layer to help us also broker this, ensuring that web, mobile, and device based clients can articulate&nbsp;where data should be stored, and which algorithms&nbsp;or variations of algorithms can be applied.&nbsp;
With the growth in regional specific cloud storage and compute solutions this doesn't seem like it will be too difficult to implement, but I'm sure will take a significant amount of time to standardize how API providers actually get it done. I'll keep looking for examples of it in the wild, and contribute to how we can better validate and ensure data and algorithmic sovereignty at the API layer.
[<a href="/2016/09/26/helping-validate-data-and-algorithmic-sovereignty-at-the-api-layer/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/backstage_blog__api_sign_up_changes__soundcloud_developers.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/26/doing-away-with-selfservice-api-access-without-approval-like-soundcloud/">Doing Away With Self-Service API Access Without Approval Like SoundCloud</a></h3>
			<p><em>26 Sep 2016</em></p>
			<p>
SoundCloud recently made changes to the signup process for their API and are now requiring approval before any 3rd party developer can get an API key and access the&nbsp;API. While I encourage API providers to be as open and transparent with their API portal, documentation, and other resources, I honestly can't criticize&nbsp;API providers for locking down APIs and requiring approval--especially when 3rd party developers can be so badly behaved.&nbsp;
Modern API management solutions allow for API providers to decide how open they want to be with their APIs, and while there are many benefits for having an open presence for an API portal, documentation, and other resources, I predict that many API providers will require approval before you get full access to resources in the future. Especially if it impacts the end user experience in a negative way like it was on SoundCloud.
I support SoundCloud's decision because&nbsp;they kept their overall operations public and because&nbsp;they shared the decision so transparently on their blog. This type of transparent, communicative approach is important to helping set the tone for the API community&nbsp;and prevent any backlash from developers. There really is no reason API providers have to be 100% open with the data, content, and algorithms &nbsp;thatthey are providing access to, and I think SoundCloud's approach provides a basic model that other providers can consider when they are thinking about exactly how "open" they want to be.
[<a href="/2016/09/26/doing-away-with-selfservice-api-access-without-approval-like-soundcloud/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/dreamfactory_logo__google_search.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/26/api-service-providers-please-have-a-logos-page-like-dreamfactory/">API Service Providers Please Have A Logos Page Like dreamfactory</a></h3>
			<p><em>26 Sep 2016</em></p>
			<p>
I was adding dreamfactory as one of my sponsors&nbsp;today. I have them in my in my API monitoring system already, so I have a logo for them, but whenever there is a significant event involving one of the companies I keep an eye on in the API space, I tend to make sure I have an update to date version of their logo. In the case of adding them as a sponsor I definitely want the latest logo--so I did what I always do, I Googled "dreamfactory logo".
A while back&nbsp;I wrote about how companies who ask me to update their logo on my site(s) almost never have a dedicated logo page--which might have helped make sure I have had the right logo in the first place. &nbsp;So when I Googled the&nbsp;dreamfactory&nbsp;logo", I was pleased to find tthe dreamfactory logo page as the first result, with a wealth of logos for me to select from.
I can't imagine why logo and branding pages are default for all companies, but I'll focus on making sure API service providers, as well as API providers, invest in this area (by writing about whenever I can). When you open up your data, content, and algorithms to 3rd party developers it just makes sense to have logo assets easy available, allowing them to provide attribution. It also makes sense to have a logo and branding so that journalists, bloggers, and analysts like me can get the most up to date assets as possible, for&nbsp;use in our storytelling.
[<a href="/2016/09/26/api-service-providers-please-have-a-logos-page-like-dreamfactory/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-information-sharing.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/23/the-sharing-of-data-via-apis-will-be-key-to-viability-of-every-industry/">The Sharing Of Data Via APIs Will Be Key To Viability Of Every Industry</a></h3>
			<p><em>23 Sep 2016</em></p>
			<p>
As I'm processing some guidelines around the importance of sharing data in the cybersecurity.theater a story on NPR came on the radio about the importance of data sharing when it comes to the emerging self-driving car marketing. I do API Evangelist, not to encourage everyone to do APIs and be the next Twitter, it is to help everyone understand the important of sharing machine-readable&nbsp;information in a secure and accessible way, to make all industries healthier, secure, and more viable environments for digital transformation (man I hate that phrase).
APIs are about sharing information in machine readable formats like YAML, JSON, and XML. Modern approaches to API deployment and management makes this information available in a self-service, secure way, ensuring those who should have access do, in a 24/7, always-on environment. Sharing cybersecurity threat information, and sharing of real-world and laboratory data around self-driving cars fall into this area, and APIs should be a&nbsp;default for anyone playing in these industries.
We are seeing API efforts emerge for expediting Zika virus research, by sharing machine-readable data that other researchers can put to work in their own efforts, without reinventing the wheel and duplicating work within many separate silos--we need more of this in other industries. If you need a primer&nbsp;on how APIs can be put to work in your industry feel free to reach out to me. APIs are not the next vendor solution, and if someone is telling you this, you should run the other direction. APIs are about sensibly, and securely sharing critical information by default across your organization, and industry, using low-cost web technology--not buying the next product or service.
[<a href="/2016/09/23/the-sharing-of-data-via-apis-will-be-key-to-viability-of-every-industry/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/sunlight_foundation_transparency.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/23/learning-from-the-sunlight-foundation-situation-and-baking-transparency-into-projects/">Learning From The Sunlight Foundation Situation And Baking Transparency Into Projects</a></h3>
			<p><em>23 Sep 2016</em></p>
			<p>As I work through the APIs, and Github repositories of soon to be gone Sunlight Foundation, I wanted to take some more time to help open data and API efforts realize the important of real-time transparency and openness of their projects--specifically how Github can help contribute to this. I'm super stoked at the number of projects on Sunlight Lab's Github account, but after identifying the actual gaps between what is there and what is available in their APIs, I want to emphasize the importance of doing our work out in the open on Github when working on these types of projects. In short, it is really&nbsp;difficult to package up any project once the hammer comes down, and a company or individuals are moving on. You'll never be as thorough with sharing the data, code, and the story behind as if you did in real-time, while&nbsp;in the moment. Even if the lights aren't being shut off, it is extremely difficult to remember all the details after the fact--which is why I am an extreme advocate for being transparent by default throughout the life of any open data and API projects in the service of government transparency. Many technologists see sharing your work as it happens as extra work, but in my experience, it is actually the opposite, even before you come up against circumstances where you have to recreate work after the fact. Technologists also tend to view Github as purely for managing open source code, when in reality it can be used for much, much more. After reviewing the Sunlight Foundation's Github repo, here are few area that come to mind: Data - Publishing raw JSON and YAML data, as part of regular updates, including backups in native database format (ie. MySQL, etc.) Server Code - Using Github to manage all server side code for APIs, with regular commits as the code evolves and changes. Scraping Code - Making sure all libraries and code used...[<a href="/2016/09/23/learning-from-the-sunlight-foundation-situation-and-baking-transparency-into-projects/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/sunlight_foundation.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/23/identifying-the-important-work-from-the-sunlightfoundation-i-would-like-to-see-live-on/">Identifying The Important Work From The @SunlightFoundation I Would Like To See Live On</a></h3>
			<p><em>23 Sep 2016</em></p>
			<p>I am saddened to hear the news of the Sunlight Foundation dimming the lights on their important work around government transparency. They have provided me a constant spotlight on government activity, and provide a model for me when it comes to opening up government data, and providing APIs that can make a difference. Having helped run non-profit organizations, working to make social change, I know this can be a very difficult thing to keep above water. I have already reached out to the Sunlight foundation staff letting them know I'm here to help with any API related projects, and happy to fund their existence until I can find a suitable, caring adoption situations for them. First up, I wanted to make sure and go through their APIs, and make sure there is a current OpenAPI Specification snapshot for each one, in case they get shuttered: Capital Words API - The Capitol Words API is an API allowing access to the word frequency count data powering the Capitol Words project. Congress API v3 - A live JSON API for the people and work of Congress. Information on legislators, districts, committees, bills, votes, as well as real-time notice of hearings,&nbsp; floor activity and upcoming bills. Open States API - Information on the legislators and activities of all 50 state legislatures, Washington, D.C. and Puerto Rico. Political Party Time API - Provides access to the underlying, raw data that the Sunlight Foundation creates based on fundraising invitations collected in Party Time. As we enter information on new invitations, the database updates automatically. Real-Time Federal Campaign Finance API - A JSON and CSV API that delivers up-to-the-minute campaign finance information on federal candidates, committees, PACs and other groups that file electronically with the Federal Election Commission. Summary information for Senate candidate committees, which file on paper, is available as soon as they have been digitized by the FEC. Next, I wanted to go through their Github repositories and identify...[<a href="/2016/09/23/identifying-the-important-work-from-the-sunlightfoundation-i-would-like-to-see-live-on/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/mr_wilson.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/23/how-to-discover-no-name-and-description-twitter-accounts-for-folks-in-the-enterprise/">How To Discover No Name And Description Twitter Accounts For Folks In The Enterprise</a></h3>
			<p><em>23 Sep 2016</em></p>
			<p>
I am always fascinated by the online fence sitting persona that is the enterprise tech industry employee. I know many them are there, but few ever retweet my work, respond to my posts via comments, or other channels. Usually, I only know that many of them are there from the occasional&nbsp;like on one of my stories, but I'm slowly developing other ways to build lists of Twitter users from behind the enterprise fence.
One of the best ways to root them out is to write positive stories about their company, products, group, or their flagship clients. When you do this many of the no names, no description Twitter accounts for the enterprise fence sitters will come out of the woodworks. They can't help themselves it seems, when someone writes positively about what they are doing, as many are so starved for genuine praise, they will often retweet a story revealing themselves.&nbsp;
I do not believe that everyone should be as open and transparent as I am online. Especially if you have a boss breathing down your neck. Though I do like to know that you are there and that I occasionally write things you like. I would also enjoy more signals from you about what is important to you out in the space, and which topics impact your success in the enterprise trenches. You are always welcome to drop me a line via email if you don't feel comfortable talking out loud, or DM now that I have triangulated&nbsp;your Twitter account and followed you.
[<a href="/2016/09/23/how-to-discover-no-name-and-description-twitter-accounts-for-folks-in-the-enterprise/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-storytelling.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/23/decoupling-the-solution-provided-from-the-product-in-your-storytelling/">Decoupling The Solution Provided From The Product In Your Storytelling</a></h3>
			<p><em>23 Sep 2016</em></p>
			<p>
I come across a number of really useful stories about APIs in my regular monitoring of the space that can't seem to separate the solution their product delivers from the product itself. I get that you want people to know that your product does the really useful thing that you are telling the story about, but I want to help you understand that they are most likely turning people off to the solution by tightly coupling the solution story with your product and company.&nbsp;
This type of storytelling is more sales than it is evangelism. It shows you don't really have a good product in my opinion. If you can't talk endless about what your product accomplishes without mentioning the product name or the company behind, you probably don't have much of a thing in the first place. However, I'm guessing in many cases you just do not have the storytelling experience, both reading, and writing, to understand the difference, and that is why I want to help you reach more people.
I know your boss is telling you to sell, sell, sell, and that you need to make your "numbers". The reality though is people are being sold, sold, sold to all the time, and they really need actual solutions for problems they face and they appreciate the companies who focus on solutions, not yet just another vendor solution to be bombarded with. If you are going to take the time to craft content for your blog, Medium, or other popular channels, then take the time to thoughtfully disconnect your solution from the product--if you do it well, people will know it is you, and find the product or company behind, when they are ready to implement your solution.
[<a href="/2016/09/23/decoupling-the-solution-provided-from-the-product-in-your-storytelling/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/the_github_graphql_api__github_engineering.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/22/the-why-behind-the-github-graphql-api/">The Why Behind The Github GraphQL API</a></h3>
			<p><em>22 Sep 2016</em></p>
			<p>I wrote a skeptical piece the other day about GraphQL, which I followed up with another post saying I would keep an open mind. I've added GraphQL to my regular monitoring of the space, but I don't have its own research area yet, but if the conversation keeps expanding I will. A recent expansion in the GraphQL conversation for me was Github releasing the GitHub GraphQL API. In the release blog&nbsp;post, from Github they provide exactly what I'm looking for in the GraphQL conversation--the reasons why they&nbsp;chose to start supporting GraphQL. In their post Github&nbsp;describes some of the challenges API consumers were having with the existing API, which led them down the GraphQL path: sometimes required two or three separate calls to assemble a complete view of a resource responses simultaneously sent too many data and didn&rsquo;t include data that consumers needed They also talk about some of what they wanted to accomplish: wanted to identify the OAuth scopes required for each endpoint wanted to be smarter about how our resources were paginated wanted assurances of type-safety for user-supplied parameters wanted to generate documentation from our code wanted to generate clients Github says they "studied a variety of API specifications built to make some of this easier, but we found that none of the standards totally matched our requirements" and felt that "GraphQL represents a massive leap forward for API development. Type safety, introspection, generated documentation and predictable responses benefit both the maintainers and consumers of our platform". Some interesting points to consider, as I work to understand the benefits GraphQL brings to the table. I'm still processing the entire story behind their decision to go GraphQL, and will share any more thoughts I'm having in future blog posts. With this major release from Github, I am now keeping an eye out for other providers who are headed in this direction. Hopefully, they will be as transparent about their reasons why as Github has--this kind...[<a href="/2016/09/22/the-why-behind-the-github-graphql-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/medium_black.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/22/syndicating-api-evangelist-posts-to-medium-using-their-api/">Syndicating API Evangelist Posts To Medium Using Their API</a></h3>
			<p><em>22 Sep 2016</em></p>
			<p>
Now that I have API Evangelist up to regular levels of operation after a summer break, I'm working to expand where I publish my content, and next up on the list is Medium. Like many other popular destinations I refuse to completely depend on Medium for my blogging presence, but I recognize the network effects, and I'm more than happy to syndicate my work there.&nbsp;
To help me manage the publishing of my stories to Medium I wired up the Medium API into my API monitoring and publishing platform. I use the Github API to publish blog posts to API Evangelist, Kin Lane, and API.Report, and it is pretty easy to add a layer that will publish select stories to Medium as well. All I have to do is tag posts in a certain way, and my "scheduler" and the Medium API does the rest.
I will be evaluating which of my stories go up to Medium on an individual basis. I'm not wanting everything to go there, but would like to open up some of my work for discussion on the platform. While I already share my API Evangelists posts to LinkedIn and Facebook, I will also be syndicating select stories using LinkedIn Publishing and Facebook Instant Articles&nbsp;next. I will only be publishing my content to platforms that bring value, but more importantly have APIs so I can retain as much control over my work from a central location within my domain.
You can find everything published under @KinLane over a Medium, something I might expand upon with specific publications in the near future, but for now, I'll keep it all under my user account.
[<a href="/2016/09/22/syndicating-api-evangelist-posts-to-medium-using-their-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-brand.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/22/providing-branding-and-attribution-assets-with-each-api-response/">Providing Branding And Attribution Assets With Each API Response</a></h3>
			<p><em>22 Sep 2016</em></p>
			<p>I am tracking on the approaches of API providers who have branding&nbsp;world together when it comes to platform operations. I'm always surprised at how few API providers actually have anything regarding branding in place, especially when it seems like loss of&nbsp;brand control, attribution, and other concerns seem to be at the top of everyone's list. I was hooking up the Medium API to my API monitoring and publishing system, syndicating select stories of mine to the platform and found myself thinking about how important an API branding strategy is (should be) to content platforms like them. Medium doesn't let you pull posts via the API (yet), but if it did, I would make sure branding and attribution was&nbsp;default. Few API providers have their API brand strategy together, let alone provide easy to understand&nbsp;and find assets to support the strategy. It seems like to me that if you are concerned about brand control, or just want to really extend your brand across all websites and mobile applications where your API resources are put to use, you would want to bake branding and attribution into the API response itself, as well as a robust branding area of the developer portal. I'm going to explore concepts around branding and attribution as a default layer of API access. Everything from thinking about hypermedia approaches to providing link relations, to maybe including link relations in the header like Github does with pagination, but using branding and attribution focused link relations. I would like to be able to provide light footprint options that may not require changing up the JSON response, or add an entirely new media type. When Medium does open up /GET for posts on the platform, I'd be stoked if there were branding and attribution elements present, driven by settings in my account. I'm not under the delusion that every developer who makes a call to an API will respect branding guidelines, but if it is front and...[<a href="/2016/09/22/providing-branding-and-attribution-assets-with-each-api-response/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/how_to_make_a_twitter_bot_with_google_spreadsheets_version_0_4_zach_whalen.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/22/google-spreadsheets-as-an-engine-for-api-goodness/">Google Spreadsheets As An Engine For API Goodness</a></h3>
			<p><em>22 Sep 2016</em></p>
			<p>
I was watching my partner in crime Audrey Watters (@audreywatters) build the weaponized edu Twitter bot using a Google Spreadsheet as an engine. Something she learned from Zach Whalen, a professor at University of Mary Washington. Audrey is not a programmer, but she has become extremely&nbsp;proficient at building these little bots, and using the Twitter API--demonstrating the potential of Google Sheets as an engine for an API-driven bot solutions, or in this case bot mayhem.
Zach's approach is extremely well defined--you will have to copy and go through the spreadsheet yourself to see. Everything you need to get the job done is there, from step by step instructions, to storing your API tokens, and planting the seeds for your bot intelligence. This is the kind of API stuff I'm always talking about when I say that API shouldn't just be for developers--all it takes is having no fear of APIs, and well laid out blueprints like Zach has provided.
It is an approach I'd like to explore more as I have time. I'm not a big fan of the spreadsheet but I fully get its role amongst muggle society. Spreadsheets keep me fascinated because of the many dimensions of API they possess. Spreadsheets can provide APIs, consume APIs, and as Zach's approach to bot development demonstrates, they can be a pretty serious engine for driving API goodness.
[<a href="/2016/09/22/google-spreadsheets-as-an-engine-for-api-goodness/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/bw_boost_incentives.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/22/api-branding-embeddables-that-can-boost-my-api-rate-limits/">API Branding Embeddables That Can Boost My API Rate Limits</a></h3>
			<p><em>22 Sep 2016</em></p>
			<p>I'm expanding on my API branding research, putting some thought into how we might be able to include branding and attribution in API responses. Next, I'd like to brainstorm ways to incentivize both API providers, as well as API consumers to employ sensible branding practices. You'd think API providers would be all over this stuff, but for some reason, they seem to need as much encouragement, and structure as API consumers do--this is why I'm wanting to explore how I can drive both sides. First, why do I care about branding when it comes to APIs? Well, the more successful companies are with their APIs, the more their companies brand can be not just protected, but enhanced--the more APIs are seen in a positive light, rather than the threat to brand control that is often cast on them. And, the more APIs we have, the more access to valuable data and content for use in web, and mobile applications. While there are many nuances to API branding, it often centers around making text, link, and image assets available&nbsp;for developers to use wherever they put API driven data, content, and algorithms to use. APIs have many different approaches to branding requirements and enforcement, but few actually provide rich assets and tooling to support a coherent branding strategy. All it takes is a handful of logos, some JavaScript APIs, and guidance for developers, and branding can significantly extend the reach of any brand--not hurt it as many perceive an API will do. The benefits of branding to API provider are clear for me, but I'd like to explore what we can do to incentivize API consumers. What if, with all the tracking of where branding and attribution are deployed (aka API brand reach), we tracked each domain or subdomain, as well as each impression of text, logos, and other assets? &nbsp;What if network reach and brand exposure could buy me API credits, and raise my API rate...[<a href="/2016/09/22/api-branding-embeddables-that-can-boost-my-api-rate-limits/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-api-a.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/21/what-i-mean-when-i-say-api/">What I Mean When I Say API</a></h3>
			<p><em>21 Sep 2016</em></p>
			<p>
People love to tell me the limitations of my usage of the acronym API. They like to point out they were around before the web, that they are used in hardware, or are not an API unless it is REST. There are endless waves of dudes who like to tell me what I mean when I say API. To help counter-balance&nbsp;each wave I like to regularly&nbsp;evolve, and share what I mean when I say API--not what people might interpret I mean.
When I say API, I am talking about exposing data, content, or algorithm as an interface for programmatic use in other applications via web technology. Application in "Application Programming Interface" means any "application" to me, not just a software application. Consider visualizations, image rendering, bots, devices, or any other way that web technology is being applied in 2016.
I do not mean REST when I say API. I do not mean exclusively dynamic&nbsp;APIs--it could simply be a JSON &nbsp;data store &nbsp;made available via a Github repo. If machine-readable data, content, and algorithms are being accessed using web technology for use in any application, in a programmatic way--I'm calling it API. You may have your own interpretations, and be bringing your own API baggage along for the ride, but this is what I'm talking about when you hear me say API.
[<a href="/2016/09/21/what-i-mean-when-i-say-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/psa_group_for_developers_api_for_connected_cars_peugeot_citroen_and_ds_automobiles.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/21/the-psa-peugeot-citrons-apis/">The PSA Peugeot Citrons APIs</a></h3>
			<p><em>21 Sep 2016</em></p>
			<p>I was turned on to the API program out of&nbsp;Groupe PSA, &nbsp;the French multinational manufacturer of automobiles and motorcycles sold under the Peugeot, Citro&euml;n and DS Automobiles brands from a friend online the other day. Rarely do I just generally showcase an API provider, but I think their approach is simple, clean, and a nice start for a major automobile brand, and worthwhile to take note of.&nbsp; Companies of all shapes are doing APIs, but very few have the awareness to make their API program public, and accessible to the general public. I think the&nbsp;PSA Peugeot Citro&euml;n&rsquo;s APIs are a pretty interesting set of resources for making available to car owners, and worth talking about: Telemetry - Request data from the car: average speed, location, instantaneous consumption, engine speed, etc. Maintenance Alerting - Request data from the various events or notifications that can be detected by the car: time before maintenance, fired alerts, etc. Correlation - These APIs lets your application evaluate your driving style with others. Making vehicle data, events and notifications makes sense to me when it comes to vehicles and APIs, and is something that seems like it should just be default&nbsp;mode for all automobile manufacturers. The correlation API seems in a different category, and elevated to more of an innovate class, beyond just the usual car activity. I'm not a huge car guy, but I know people who are, and being able to size up against the competition or a specific community, could become a little addictive. I've added the&nbsp;PSA Peugeot Citro&euml;n&rsquo;s APIs to my monitoring of the space, and will keep an eye on what they are up to. I'm already following other user manufacturers&nbsp;like Ford and GM who have API efforts, as well as Japanese manufactuers like Honda. I may have to stop, and take roll call in the world of automobiles and see who have official public API develop efforts, and put some pressure on those who do...[<a href="/2016/09/21/the-psa-peugeot-citrons-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-api-a.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/21/be-part-of-your-community-do-not-just-sell-to-it/">Be Part Of Your Community, Do Not Just Sell To It</a></h3>
			<p><em>21 Sep 2016</em></p>
			<p>A recent story&nbsp;from Gordon Wintrob (@gwintrob) about how Twilio's distributed team solves developerevangelism&nbsp;has given me a variety of seeds for stories on API Evangelist this week. I love that in 2016, even after an IPO, I am still writing positive things about Twilio and showcasing them as an example for other API providers to emulate. Twilio just gets APIs, and they deeply understand how to effectively build a community of passionate developers, demonstrated by this statement from Gordon's story on developing credibility: How do you have technical credibility? You have to really be part of your programming community. Each of us is a member of our community, not marketing or trying to sell to it. It sounds so simple. Yet is something so many companies struggle with. An API community is often seen as something external, and often times even the API is seen as something external, and this is where most API efforts fail. I know, you are saying that not all companies can be API-first, where the API is the core product focus like Twilio--it doesn't matter. Not being able to integrate with your developer community, is more about your company culture, than it is about APIs. Another area my audience will critique me is around sales--you have to do sales to make money! Yes, and even Twilio has a sales team to come in at the right time. This is about building technical credibility with your developer community, by truly being part of it--if you are always trying to sell to them, there will always be an us and them vibe, and you will never truly be part of your own community. As an API provider, I always recommend that you get out there and use other APIs, to experience the pain of being an API consumer. Using Twilio, and participating in the Twilio community should be the 101 edition of this. Where all API providers spend a couple of months using...[<a href="/2016/09/21/be-part-of-your-community-do-not-just-sell-to-it/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/devicon_all_programming_languages_and_development_tools_related_icons_font.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/21/a-new-api-programming-language-sdk-icon-set/">A New API Programming Language SDK Icon Set</a></h3>
			<p><em>21 Sep 2016</em></p>
			<p>
I was working on a&nbsp;forkable definition of my API portal&nbsp;and I wanted to evolve the icons that I usually use as part of my API storytelling. I primarily use the Noun Project API, to associate simple black and white icons which represent&nbsp;the stories I tell, companies I showcase, and topics I cover. One area I find the Noun Project deficient&nbsp;is when it comes to icons for specific technologies, so while working on my project I wanted to find a new source. I fired up the Googles and got to work.
I quickly came across Devicon,&nbsp;a set of icons representing programming languages, designing &amp; development tools which you can use as a font or with&nbsp;SVG code. At the Github repo for the project, it says they have&nbsp;78 icons with over 200 versions total. I used a set of the icons to display API SDKs on my API portal prototype, allowing anyone who forks to turn on and off which programming languages they offer SDKs for.
Being pretty graphically challenged, as you can tell by my logo, I'm a big fan of projects like Devicon--especially when they make it so simple, and so good looking, all at the same time. If you are needing icons for your API portal I recommend taking a look at what they are doing. They have images for all the technology that cool kids are using these days, and they seem open to crafting more if you find something missing.
[<a href="/2016/09/21/a-new-api-programming-language-sdk-icon-set/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-unstable.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/20/why-would-you-build-a-business-on-apis-they-are-unreliable/">Why Would You Build A Business On APIs? They Are Unreliable!</a></h3>
			<p><em>20 Sep 2016</em></p>
			<p>People love to tell me how unreliable APIs are, while also echoing this sentiment across the tech blogosphere. I always find it challenging to reconcile how the entrenreurs&nbsp;who spread these tales choose to put the blame on the technology, and not the companies behind the technology, or more appropriately the investment behind the companies. APIs are just a reflection of what is going on already within a company, and are good, nor bad--they are just a tool that can be implemented well, or not so well. I was taking some time this last week to work on my API monitoring system, which I call Laneworks. In addition to having my own API stack, I depend on a variety of other APIs to operate my business. As I was kicking the tires, poking around the code for some of my most valuable integrations I found myself thinking about the stability and reliability of APIs, and how stable some APIs have been for me. Since 2011 I have stored ALL heavy objects (images, video, audio) used in my API monitoring and research on S3. I have NEVER had to update the code. Since 2012 I have used Pinboard as the core of my API curation system, aggregating links I favorited on Twitter, and added using my browser bookmarklet--again I have NEVER updated the code that drives this. Since 2013 all of my public websites run on Github using Github Pages, employing the Github API to publish blog posts, and all other content and data used in my research. The Amazon S3, Pinboard, and Github APIs make my business work. Three suppliers who have been working without a problem for 5, 4, and 3 years. The only thing I have had to do is pay my bill, and keep my API keys rotated, and the reliable API vendors to the rest. Storing images, video, and audio, curated the news and other stories I share with you&nbsp;and publish the...[<a href="/2016/09/20/why-would-you-build-a-business-on-apis-they-are-unreliable/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-standard.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/20/standards-evangelism/">Standards Evangelism</a></h3>
			<p><em>20 Sep 2016</em></p>
			<p>
As the API Evangelist, I spend a lot of time thinking about evangelism (*your mind is&nbsp;blown*). TFrom what I'm seeing, the world of technology evangelism has been expanding, where database, container, and other types of platforms are borrowing the approaches proven by API pioneers like Amazon and Twilio. As I'm doing work with Erik Wilde (@dret) around his Webconcepts.info work, and reading an article about industrial automation standards, I'm left thinking about how important evangelism is going to be for standards and specifications.
Standards are super important, so I have to be frank--the community tends to suck at evangelizing itself, in an accessible way that reflects the success established in the API world. I'm super thankful for folks like Erik Wilde, Mike Amundsen, and others who work tirelessly to evangelism API related web concepts, specifications, and standards. The importance of outreach and positive evangelism around standards reflect the reasons why I started API Evangelist--to make APIs more accessible to the masses.&nbsp;
This is why I have to get behind folks like Erik who step up to help evangelize standards. I do not have the dedication required to tune into the W3C, IANA, ISO, and other standards bodies, and super thankful for those who do. So if I can help any of you standard obsessed folks hone your approach to storytelling, and evangelism let me know. I'd love to see standards evangelism become commonplace--making them more friendly, accessible, and known across the tech sector.
[<a href="/2016/09/20/standards-evangelism/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-face-idk.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/20/i-am-feeling-the-same-about-yaml-as-i-did-with-json-a-decade-ago/">I Am Feeling The Same About YAML As I Did With JSON A Decade Ago</a></h3>
			<p><em>20 Sep 2016</em></p>
			<p>
I have been slowly evolving the data core of each of my research projects from JSON to YAML. I'm still providing JSON, and even XML, Atom, CSV, and other machine-readable representations as part of my research, but the core of each project, which lives in the Jekyll _data folder are all YAML moving forward.&nbsp;
When I first started using YAML I didn't much care for it. When the OpenAPI Specification introduced the YAML version, in addition to the JSON version, I wasn't all that impressed. It felt like the early days of JSON back in 2008 when I was making the switch from primarily XML to a more JSON-friendly&nbsp;environment. It took me a while to like JSON&nbsp;because I really liked my XML--now it is taking me a while to like YAML&nbsp;because I really like my JSON.
I do not anticipate that JSON will go the same way that XML did for me. I think it will remain a dominant machine-readable format in what I do, but YAML is proving to have more value as the core of my work--especially when it is managed with Jekyll and Github. I am enjoying having been in the industry long enough to see these cycles, and be in a position where I can hopefully think more thoughtfully&nbsp;about each one as it occurs.
[<a href="/2016/09/20/i-am-feeling-the-same-about-yaml-as-i-did-with-json-a-decade-ago/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://d3.js.yaml.jekyll.apievangelist.com/images/d3-js-bar-chart.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/20/d3-js-visualizations-using-yaml-and-jekyll/">D3.js Visualizations Using YAML and Jekyll</a></h3>
			<p><em>20 Sep 2016</em></p>
			<p>I am increasingly using D3.js as part of my storytelling process. Since all my websites run using Jekyll, and published entirely using Github repositories wich are shared as Github Page sites, it makes sense to standardize how I publish my visualizations. Jekyll provides a wealth of data management tools, including the ability to manage YAML dta stores in the _data folder. An approach I feel is not very well understand, and lacks real world examples regarding how to use when managing open data--I am looking to change that. I like my data visualizations beautiful, dynamic, with the data right behind--making D3.js the obvious choice. For this work, I took data intended for use as a bar and pie chart&nbsp;and published as YAML to this Github repositories _data folder. This approach to centrally storing machine-readable data, in the simple, more readable YAML format, makes the data behind visualizations much more accessible in my opinion. The problem with using D3.js visualization&nbsp;is that I need it in JSON format. Thankfully, using Jekyll and Liquid, I can easily establish dynamic versions of my data in JSON, XML, or any other format I need it in. I place these JSON pages in a separate folder I am just calling /data. Now I have the JSON I need to power my D3.js visualizations. To share the actual visualization, I created separate editions for my bar and pie charts, and have the HTML, CSS, and JavaScript for each chart, in its own file. There are two things being accomplished here. 1) I'm decoupling the data source in a way that makes it easier to swap in and out different D3.js visualizations, and 2) I'm centralizing the data management, making it easily managed by even a non-technical operator, who just needs to grasp how Jekyll and YAML works--which dramatically lowers the barriers to entry for managing the data needed for visualizations. There is definitely a learning curve involved. Jekyll, Github Pages, and YAML...[<a href="/2016/09/20/d3-js-visualizations-using-yaml-and-jekyll/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-github.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/20/a-trusted-github-authentication-layer-for-api-management/">A Trusted Github Authentication Layer For API Management</a></h3>
			<p><em>20 Sep 2016</em></p>
			<p>I am reworking the management layer for my APIs. For the last couple of years, I had aspirations of running my APIs with a retail layer generating revenue for API Evangelist--something which required a more comprehensive API management layer. In 2016, I'm not really interested in generating revenue from the APIs I operate, I'm just looking to put them to work in my own business, and if others want access I'm happy to open things up and broker some volume deals. To accomplish this I really do not need heavy security or service composition for my APIs, I'm just needed to limit who has access so they aren't just 100% public, and identify those who are using, and how much they are actually consuming. To facilitate this I am just going to use Github as a trusted layer for authentication. Using an OAuth proxy, I'll let my own applications authenticate using their respective Github user, and identify themselves using a Github OAuth token when making calls to each API.&nbsp; Each application I have operating on top of my APIs have its own Github account. Once they do the OAuth dance with my proxy, my system will then have a Github token identifying who they are. I won't need to validate the token is still good with each call, something I'll verify each hour or day, and cache locally to improve API performance. Anytime an unidentified token comes through, I'll just make a call to Github, and get the Github users associated, and check them against a trusted list of Github users who I have approved for accessing my APIs. I'm not really interested in securing access to all the content, data, and algorithms I'm exposing using APIs. I'm only looking to identify which applications are putting them to work&nbsp;and evaluate their amount of usage each day and month. This way I can monitor my own API consumption, while still opening things up to partners or...[<a href="/2016/09/20/a-trusted-github-authentication-layer-for-api-management/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-dominant-narrative.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/19/putting-the-concept-of-the-public-api-to-rest-as-a-dominant-narrative/">Putting The Concept Of The Public API To Rest As A Dominant Narrative</a></h3>
			<p><em>19 Sep 2016</em></p>
			<p>
APIs come in all different shapes and sizes. I focus on a specific type of APIs that leverage web technology for making data, content, and algorithms available over the Internet. While these APIs are available on the open Internet, who has the ability to discover, and put them to use will vary significantly. APIs have gained in popularity because of successful publicly available APIs like Twitter and Twilio, something that has contributed to these types of APIs being the dominant narrative of what APIs are.
A lack of awareness of what modern approaches to API management can do for securing web APIs as well as the dominance of this narrative that APIs need to be open like Twitter and Twilio tends to set the bar to unrealistic levels for API providers. Who has access to a web API is just one dimension of what APIs are, and sharing content, data, and algorithms securely via the web should be the focus. It's not whether&nbsp;or not we should do public or private APIs--it is about how you will be sharing your resources in a digital economy.
While I encourage ALL companies, institutions, and government agencies to be as transparent as they possibly can regarding the presence of their APIs, its documentation, and other resources--who actually&nbsp;can access them is entirely up to the discretion of each provider. You should treat ALL your APIs like they use public infrastructure (aka the web), secure them appropriately, and get to work making sure all your digital resources are accessible in this way, not being bogged down by useless legacy discussions.
Which is why I support putting the concept of the public API to rest as a dominant narrative around what is an API--you shouldn't hear me talking about public vs private anymore. If you do slap me.
[<a href="/2016/09/19/putting-the-concept-of-the-public-api-to-rest-as-a-dominant-narrative/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://gist.github.com/kinlane/325b2f3ca761b5deaf52666f78117b5d.js" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/19/providing-yaml-driven-xml-json-and-atom-using-jekyll-and-github/">Providing YAML driven XML, JSON, and Atom using Jekyll And Github</a></h3>
			<p><em>19 Sep 2016</em></p>
			<p>The power of Jekyll on Github Pages as a data management solutions is not a very widely held concept. I'm always amazed at how technologists and programmers don't understand Jekyll, let alone how it can be used as a data engine--maybe I can help a little by sharing my own usage. As I develop examples of this in action, I want to publish them as Github repositories that anyone can fork and reverse engineer to use in their own work. While it was not love at first sight for me, I'm increasingly becoming a fan of using YAML for storing and managing a significant portion of the data I use across my business. Part of the reasons I'm using YAML is its readability. The other reasons stem from the augmented benefits of using Jekyll and Github Pages to store and syndicate machine readable YAML for use across my storytelling--when you put YAML data into the _data folder for any Jekyll site, it opens up a new world of possibilities. Any YAML data I put into the _data folder immediately become objects I can work with across any HTML page within a Jekyll site, using Liquid. Where this really starts to impact my world is when I started dynamically generating other formats of data stored as YAML. First up is JSON. Here is the file I am using to generate a JSON representation of my central YAML file stored in _data folder. Which when I view in my browser, via the Jekyll driven, Github Pages published website&nbsp;I get a separate JSON representation: Next up is XML. Here is the file I am using to generate a XML representation of my central YAML file stored in _data folder. Which when I view in my browser, via the Jekyll driven, Github Pages published website&nbsp;I get a separate XML representation: Next up is Atom. Maybe I may want a feed of the latest products added to the catalog, so...[<a href="/2016/09/19/providing-yaml-driven-xml-json-and-atom-using-jekyll-and-github/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/minimum_api_portal.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/19/my-forkable-minimum-api-portal-definition/">My Forkable Minimum API Portal Definition</a></h3>
			<p><em>19 Sep 2016</em></p>
			<p>I am updating my minimum API portal definition&nbsp;so I can apply to my own API infrastructure, and since I operate 100% on Github using Github Page and Jekyll, I have made it a forkable API portal definition that anyone can put to work as their own API developer portal. This edition of my API portal definition uses Bootstrap for its UI, and Jekyll for the CMS, making it pretty extensible, and remixable once you fork it on Github. My goal was to make a simple, forkable API portal, that could act as a checklist for API providers looking to quickly set up a presence for their API. I know many companies, institutions, and government agencies do not have the resources&nbsp;to host one, let alone the time to pay attention to all the details--that is my job! To help API providers out, I have included what I feel is a&nbsp;complete API portal in the&nbsp;_config.yml for the Jekyll site. All you have to do is scroll down the API portal definition and comment out what you don't want, and fill in the areas you do, and the Jekyll site should do the rest. I've included the most common areas I like to see from all API providers in my definition. Portal Simple Description Getting Started Authentication Documentation Discovery Code Communication Plans Self-Service Support Direct Support Road Map Issues Change Log Legal This is just the first draft of my forkable API portal definition. I am going to apply to my Kin Lane, and API Evangelist API infrastructure, as well as a handful of independent APIs that I operate. Then I'm going to apply it to a couple of government APIs I want to simplify, like the USGS Water Services I am working on, to harden it a little bit. Sometimes all it takes is to better organize the information for an API, to help make it more accessible, and intuitive, reducing the friction when trying to get...[<a href="/2016/09/19/my-forkable-minimum-api-portal-definition/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-sketchbook.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/19/my-dream-api-sketchbook-and-portfolio/">My Dream API Sketchbook And Portfolio</a></h3>
			<p><em>19 Sep 2016</em></p>
			<p>I have a vision of an API notebook in my head I desperately want to get out. First of all, I want to come up with another name for it, which is a journey that always starts with playing around with synonyms. Direct synonyms of notebook include a diary, journal, log, workbook, pad, and binder-yes, all of that is relevant to what I would like to see. After that, a few other words resonated, including album, collection, portfolio, and registry. This isn't just a folder to put my API definitions. It will be the place where I go to find all the definitions of 3rd party API which Idepend on, as well as the APIs I'm designing, deploying, and operating as part of my own business. I want to be able to just record ideas, sketches, and thoughts I have as I'm thinking about APIs. I want to be able to annotate APIs that I find, and iterate, remix, and riff off of other API designers and architects work. Maybe an API sketchbook? An artistic design book is just the beginning. I'd also like it to be my professional sketchbook,where my business partners and customers can discover the API I depend on, and share with the world. I want it to be a directory of APIs that are relevant to my business. My sketchbook is where I'm creative, but it is also where I get business done. Maybe acting as an API portfolio? I want it to be fun, but also accommodate my professional existence as well. I just want a place where I can design and evolve the API definitions that impact my world. I want to be able to share them, as well as subscribe to other people's API designs. I want my designs versioned, and be able to play back their evolution, and see a timeline, and possibly the attributions of where I found my API inspiration, or leveraged an existing definition...[<a href="/2016/09/19/my-dream-api-sketchbook-and-portfolio/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-recycling.jpg" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/19/a-twilio-process-to-emulate-within-your-own-api-operations/">A Twilio Process To Emulate Within Your Own API Operations</a></h3>
			<p><em>19 Sep 2016</em></p>
			<p>Leading API providers do not always make me happy with they way they conduct themselves, but it always makes me smile that one of the top API providers consistently over the last five years, continues to do things right, and set a good example that I can write about. I am not delusional to think that everything is perfect behind the Twilio curtain, but a story&nbsp;from Gordon Wintrob (@gwintrob) about how Twilio's distributed team solves developer evangelism&nbsp;leaves me hopeful (once again) about the potential of APIs. There are several gems in this post, but one of them that stood out for me, and I think reflects the API potential which more companies should be emulating, is about how Twilio designs, develops&nbsp;and evolves new APIs. I think Gordon tells it the best:&nbsp; We also have a broader concept of our Developer Network, which handles a lot of the coding and writing for our public-facing documentation, blog posts, and our interactions with the community. Typically they&rsquo;ll give feedback on the budding ideas for the new API. This feedback comes long before it goes out to the first beta customers. The Developer Network brings a fresh set of eyes with less biased perspectives. They&rsquo;ll say things like, &ldquo;You know what? These parts of the API are awesome. This is what I would use it for.&rdquo; or &ldquo;These are the things that need work.&rdquo; That way we know how the API would work for a developer at a hackathon or trying to finish the story points in a sprint. How do we make it as easy as possible for them? Once the API or service comes together, we go to a closed beta process for a small group of customers. If we do a product announcement at all, then we&rsquo;ll have a &ldquo;request access&rdquo; button. We&rsquo;ll use that as a list of people that are really chomping at the bit to get coding. Then, after a period of time,...[<a href="/2016/09/19/a-twilio-process-to-emulate-within-your-own-api-operations/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/swagger_ui_petstore.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/16/standardizing-api-documentation-for-use-across-the-api-lifecycle/">Standardizing API Documentation For Use Across The API Lifecycle</a></h3>
			<p><em>16 Sep 2016</em></p>
			<p>I've been pushing for better API design tooling for some time now, something that significantly overlaps with movements I would also like to see around API documentation as well. In my opinion, we have made significant strides with the introduction of Swagger UI, as well as pushed the API documentation conversation to be part of the API design process by Apiary. This is something that was further re-enforced with the evolution of Swagger UI to be part of the Swagger Editor toolkit--where Swagger UI was used in real-time as part of the API design process, not just later down the road as API documentation for consumers. This demonstrates for me the importance of API documentation, not just in helping API consumers understand what an API does at integration time, but also for API providers at design, mock, test, and any other time during the API lifecycle. Functional, attractive looking, and often interactive documentation that describes what an API does is needed from design to deprecation, and is an area of the API world that I feel is pretty underserved at the moment. Swagger UI is a 100% client side JavaScript solution, which if you've ever cracked open the code, know that it cannot be used as a base template to move this&nbsp;conversation forward. The more attractive Slate solution is appealing right up until you realize it doesn't perpetuate the machine readable API core we will need to successfully integrate with every other stop along the API lifecycle. I haven't seen any movement out the efforts to make an OpenAPI Spec driven Slate, but I am pleased with the movements by Lucybot in their Lucybot Console. These are moving the needle a little bit&nbsp;but won't get us far down the road when it comes to API documentation that can be used across all stops along the lifecycle without friction. We will need API documentation that can be published in web, mobile, and &nbsp;desktop solutions that are...[<a href="/2016/09/16/standardizing-api-documentation-for-use-across-the-api-lifecycle/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-openapi-spec.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/16/learning-from-the-success-of-swagger-ui/">Learning From The Success Of Swagger UI</a></h3>
			<p><em>16 Sep 2016</em></p>
			<p>I feel like we haven't really sat down and studied the success of Swagger UI. I'm not talking about the OpenAPI Spec (fka Swagger Spec), I am only talking about the interactive API documentation that you can find on Github. Aside from the shitshow that was the movement of Swagger to be OpenSpec API, I'm thinking there are some lessons available around just the interactive API documentation itself. First, we have to acknowledge that many people think Swagger is the documentation, and do not understand the separate nature of the specification, and the UI layer that is meant to be API documentation, and used within API tooling like the Swagger Editor. While there are numerous benefits realized from the concept of the OpenAPI Spec, the number one reason people implement is to deploy the interactive API documentation. People need documentation for their API to communicate what it does with consumers, as well as other key stakeholders throughout the API life cycle. From what I can tell, this need is not being addressed in any new and innovative ways since the launch of Swagger UI, aside from the small group of API documentation service provider that have emerged. I can get behind paying for additional API lifecycle solutions as part of a service, but API document just seems like it should always be free by default--as well as evolved separately from the concerns of an aspiring startup. Let me try and sum up what made Swagger UI so good. It was forkable. You could fork the repo, make tweaks to the core Swagger JSON file, and if you had it published using Github Pages, you had documentation! I feel like this represents the minimum viable expenditures that API developers are willing to make when it comes to providing API documentation -- WSDL +1 (not very much). If you could understand how to tweak the JSON file, which many just see as a config file for Swagger...[<a href="/2016/09/16/learning-from-the-success-of-swagger-ui/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/swagger_ui_petstore.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/16/a-look-at-the-state-of-api-documentation-solutions/">A Look At The State Of API Documentation Solutions</a></h3>
			<p><em>16 Sep 2016</em></p>
			<p>A friend of mine was asking where he should get started with upgrading the documentation for an existing API, and was asking for assistance on what tools or services he should be considering. The state of things when it comes to API document shifts and changes, which is why I do my research the way I do, to ensure I have a single static location I can go find the latest for my research in API documentation, when I get questions like this. Discussing API documentation like many aspects of the API conversation can be difficult, due to a wide range of views on exactly what is API documentation. Some people consider the entire API portal, including the documentation, some include other code libraries, etc, but I define it as specific documentation describing the API surface area (auth &amp; request / response). As I begin my work, at first glance, the world of API documentation seems like it has evolved, but after a quick look through my research, I see it really hasn't evolved that much. Swagger UI Changing How We Document Our APIsIn 2011 how we documented our APIs changed significantly with the introduction of the Swagger specification and UI. The simple, visual, interactive documentation which was driven by the now OPenAPI Spec, shifted the conversation for how we provide documentation. All you had to do was generate the machine-readable specification, and the interactive documentation was automatically generated--solving one of the biggest obstacles for API providers. Apiary Providing Design, Mocking, Testing, and DocumentationBuilding on what Swagger UI provided, the service provider Apiary.io entered the game with their own version of the API documentation but shifted the conversation around documenting APIs earlier on in the life-cycle, during the design portion of the work. I consider Apiary to be first and foremost an API design service provider, but they are also a contender when it comes to API documentation because their solution spans multiple stops along...[<a href="/2016/09/16/a-look-at-the-state-of-api-documentation-solutions/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/bw_possibilities.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/15/the-next-api-lifecycle-opportunity-will-be-in-design-and-definitions/">The Next API Lifecycle Opportunity Will Be In Design And Definitions</a></h3>
			<p><em>15 Sep 2016</em></p>
			<p>Looking through the numbers for my API Evangelist research, and tallying up what I've learned along the way, I feel like the next opportunity out there will be about API design and definitions. The release of the API Stylebook, and Materia reflect this opportunity--serving the growing appetite for API design knowledge, and tooling being generated as businesses continue waking up to the need for APIs. The API management landscape has been well defined by pioneers like 3Scale and Apigee, and validated by their acquisitions by Red Hat and Google this summer. What is picking up momentum now, is the world of API design services, tooling, and specification set into motion by early pioneers of API design&nbsp;like Apiary.io. I am guessing it will play out over the next six years or so, taking longer than we expect, similar to the way API management has played out. This is when pioneers like Apiary will do well, but there will also be a steadily increasing need for new ways to help API providers craft their API designs, and leverage these definitions for use across almost every aspect of the API life cycle. Companies who have a good handle on their API surface area, and schema at play, will more successfully deploy, manage, test, scale, and orchestrate with this infrastructure. When you have all the moving parts well defined, the chances you will have a better handle on things only increases. Modern approaches to defining your APIs using API Blueprint and OpenAPI Spec are allowing us to better define the fast growing and evolving (almost too fast) layers of API we are developing and operating. API design services like Apiary and Restlet Studio are helping us better work with API definitions across the API life cycle. The API Stylebook is helping us share API design knowledge, and Materia&nbsp;is pushing the API design and deployment conversation forward. This is just the beginning of a wealth of new services, open source...[<a href="/2016/09/15/the-next-api-lifecycle-opportunity-will-be-in-design-and-definitions/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/api_lifecycle_66_tag_cloud.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/15/now-keeping-an-eye-on-66-areas-of-the-api-lifecycle/">Now Keeping An Eye On 66 Areas Of The API Lifecycle</a></h3>
			<p><em>15 Sep 2016</em></p>
			<p>As I was firing back up API Evangelist after a break this summer, I took the opportunity to add in a couple of new areas to my research, that I've had sitting on the backburner, bringing the number of areas of my API lifecycle research up to 66. Each area operates as an independent Github repository, possessing the JSON, YAML, and HTML for each area of my API industry monitoring, research, and storytelling. I approach my work this way&nbsp;because the individual repositories allow&nbsp;me to put my blinders on when it comes to each project, while also scaling my work to include as many possible areas that influence what we like to call the API economy. Not all areas are as mature as API management, which has been being defined for well over a decade, but each area plays its own significant role that in my mind makes it worthy enough to be its own separate project. You can always access the list of API lifecycle research projects from the home page of API Evangelist, and I try to provide a sort of pagination on the home page of each project, allowing for a leisurely&nbsp;stroll through all 66 areas of research. Each area will at least have some news I've curated, with some of the more mature areas also having companies, tools, and blog posts that I've written over the years. As I scroll down the list on the home page I can really see the API life cycle maturing and coming into focus. We still have decades of work ahead of us when it comes to properly designing, defining, and making our API discoverable, but I can see it all coming into focus. The problem is that I can also see the deficient areas like terms of service, privacy policies, patents, and copyright lacking in attention and discussion amidst each wave of growth.&nbsp; Which is why I list the areas of concern right alongside the...[<a href="/2016/09/15/now-keeping-an-eye-on-66-areas-of-the-api-lifecycle/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/datasheets_electronic_parts_components_search__octopart.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/15/if-you-have-an-online-product-catalog-you-should-have-an-api/">If You Have An Online Product Catalog You Should Have An API</a></h3>
			<p><em>15 Sep 2016</em></p>
			<p>Octopart is the products company that I regularly use as a reference for how product-focused companies should be doing APIs. Octopart's is an electronic parts company who have a physical, product centered company which is easy for people to understand, that also happens to do APIs well. Octopart doesn't do APIs just because they are cool, they do APIs because it enables the purchasing of their products in other systems. When you land on the Octopart website you are given a product catalog search, allowing humans to browse their virtual electronic parts warehouse, but as soon as you scroll below the fold you also see a link to their API. The Octopart API is nothing fancy, but it has all the essentials, including a simple REST API for accessing /brands, /categories, /parts, and /sellers in their&nbsp;virtual electronic parts warehouse.&nbsp; You can tell Octopart API integrations often center around the spreadsheets, an Octoparts showcases with their bill of material (BOM) solution, as well as open source Microsoft Excel and Google Spreadsheet add-ons, but when you browse their application gallery you&nbsp;can find other types of integrations with their catalog as well. Their approach to doing APIs seems like it should be a default for any company with a wealth of products to sell in an online environment. When browsing through the Octoparts API you don't feel like it is something that was launched as a momentary trendy idea, and it is something that gets just as much emphasis as the HTML edition of the product catalog. The API is keeping spreadsheet editions of the catalog in sync, as well as providing sync capabilities for their BOM-driven order workflow. In my opinion, it has all the characteristics a commerce API provider should possess in a modern digital business environment. While evangelizing APIs I often find I have to help unwind some of the miseducation companies have received about APIs from vendors trying to sell them external solutions. I...[<a href="/2016/09/15/if-you-have-an-online-product-catalog-you-should-have-an-api/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/screen_shot_2016_09_14_at_10.48.11_am.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/14/tracking-on-where-the-politics-of-apis-intersects-with-the-business-of-apis/">Tracking On Where The Politics Of APIs Intersects With The Business of APis</a></h3>
			<p><em>14 Sep 2016</em></p>
			<p>
While reviewing the details of Twilio's new enterprise plan, the one thing that stood out for me was the strong emphasis of the security and legal elements within this level of business integration. I've talked out security being included as part of plan details before, and is something I will keep talking about as leading providers continue to bring it front and center as part of API plans and operations.
Advanced security is the focal point for Twilio's new enterprise plan, providing audit events, and public key client validation, but shortly after showcasing these security features, the next section is all about providing finer grain access management including customizable role-based access control (RBAC), and Single Sing-On (SSO). Demonstrating how important security is overall to the average enterprise customer.
From what I'm seeing across API providers, security is definitely the number one concern, and something that I'm guessing will keep showing up as part of API pricing and pricing tiers. Every time I come across this practice in the wild I will record which elements are being included, and try to do a write up showcasing and sharing the approach. Good security practices and features won't be cheap, and as more companies tightene&nbsp;their belts, I am guessing it will keep bubbling up as paid features in the upper tiers of API access.&nbsp;
On the same note, the second area of growth I'm seeing, which is also present in TWilio's approach to their enterprise plan, is greater control over accounts, billing, and systems usage--something that includes API access, allowing API consumers to automate. Security is top of mind, but efficiently&nbsp;managing usage&nbsp;and dialing in revenue seems to be just as important to businesses looking looking to optimize their integrations with 3rd party API resources.
[<a href="/2016/09/14/tracking-on-where-the-politics-of-apis-intersects-with-the-business-of-apis/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="http://kinlane-productions2.s3.amazonaws.com/api_evangelist_site/blog/api_evangelist_views.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/14/sharing-some-basic-numbers-for-api-evangelist/">Sharing Some Basic Numbers For API Evangelist</a></h3>
			<p><em>14 Sep 2016</em></p>
			<p>I don't spend a lot of time worrying about the website traffic numbers for API Evangelist. Once a week I'll take a look at my Google Analytics or CloudFlare dashboards. I don't write for page views, but I do like to know which of my areas of research is of interest to the public, and generally what people are clicking on. To help me visit my numbers each week, I started publishing them to the API Evangelist repo, and sharing via a new numbers section on the site.&nbsp; I wanted to start with seeing the page views across all of my API research projects, giving me an idea of the interest levels in each area: Next, I wanted to see the page views that each guide or white paper was getting in the top left corner of my site: After the views, I wanted to get a better understanding of what people were clicking on when it came to&nbsp;these guides &amp; white papers: I also wanted to see which of the sponsor logos I include on the left-hand navigation were being clicked on by my readers: Numbers do not drive what I do. I research and write about what grabs my attention, not based upon what drives traffic. However, I do like sharing these basic numbers with my partners, and readers, so that they can get a feel for my reach, and what areas of my research are of interest to the public. The numbers reports are simple static views of activity across my sites. I will publish the JSON that drives the reports each week, along with the rest of my industry monitoring each week. Once I get enough data published to do some quarterly reports I will work to slice and dice, and see what I can come up with when it comes to visualizations. I'm enjoying playing with D3.js as a visual layer to the JSON data that I'm publishing to Github,...[<a href="/2016/09/14/sharing-some-basic-numbers-for-api-evangelist/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-press-apis.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/14/harvesting-companies-who-are-doing-apis-from-press-releases/">Harvesting Companies Who Are Doing APIs From Press Releases</a></h3>
			<p><em>14 Sep 2016</em></p>
			<p>Press releases continue to be one of the best ways for me to discover companies who have embarked on their API journey. From what I can tell, even with the shrinkage around funding for startups, the number companies offering up APIs is increasing. The big difference in the current wave of companies doing APIs. is these are just regular businesses, using APIs to provide access to resources in web, mobile, device, and partner integrations--a more business usage of APIs, as opposed to startups where the product is an API. This class of SMB, and SME API provider tend not to be so good at sharing the documentation, code, and other resources for their API operations via their sites, but do see the value in pointing it out when issuing a press release. The reasons vary for why they showcase their APIs, often times it is just in the little "about" paragraph, but sometimes it is the center&nbsp;of a partnership, an acquisition was made possible, or the focus of a new product or service. I wish companies were more into being public about it, but I'll take what I can get--I am guessing many just haven't thought about it, or don't have the resources to do it right. I'd say press releases are the number one way I discover companies who are doing APIs, with patent filings as a close second or third. After I publish eachpress release to API.Report, I have a CRON job running that extracts any URLs referenced in the page, spiders them looking for blog feeds, and Twitter or Github accounts, and makes sure they are entered into my monitoring system. If you are an API service provider looking for interesting leads of companies who are doing APIs let me know--I'm looking to formalize the list of companies I extract from press releases each week. I'm thinking about providing this list as a new API Evangelist service, and subsidize my research with...[<a href="/2016/09/14/harvesting-companies-who-are-doing-apis-from-press-releases/">Read More</a>]</p>
			<p><hr /></p>
	  
		  <div align="left" style="width:325px; height:250px; overflow:hidden; float: left; padding: 20px;"><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-mortgage-api.png" alt="API Evangelist" width="250" align="left" /></div>
			<h3><a href="/2016/09/14/building-the-case-for-redefining-the-mortgage-industry-using-apis/">Building The Case For Redefining The Mortgage Industry Using APIs</a></h3>
			<p><em>14 Sep 2016</em></p>
			<p>I am trying to get better at showcasing the early stories I find about APIs making their moves into new industries. It helps to have a post to reference when I add an area as an official research project, and begin tracking on companies more closely in the industry. I'm sure I've seen mortgage-related&nbsp;API rhetoric before, but a post in the National Mortgage News, provides a pretty clear mark in the sand&nbsp;to begin building the case for redefining the mortgage industry using APIs. The description of the mortgage industry sounds like other industries where many disparate systems and players exist, with a minefield of technical, business, and politic considerations littering the landscape. One reason I cherry picked this article out of the hundreds of posts I tune into each week, is the focus on a new area like mortgages, but also because it focuses on the reality on the ground, and acknowledges there is no single end-to-end solution that will fix what is going--despite many vendor claims. "No one actually has an end-to-end solution. And with anyone who claims to be an end-to-end solution, their customers are using other services to round out their capabilities." People will always seek out solutions to their integration issues, this is what shadow IT has shown us in recent history. It is clear the mortgage industry is facing what many other industries are facing, and that they are using many disparate systems, many operating in the cloud, and the need move things around more efficiently is becoming critical. There is no silver bullet in this environment, and APIs are increasingly providing the connectivity that is needed to 3rd party systems, as well as opening up and providing access to internal system resources. This post provides me with an early line in the sand, showing where I started tuning into the mortgage industry closer. I've added a couple of new keyword search phrases to my API monitoring, and if I...[<a href="/2016/09/14/building-the-case-for-redefining-the-mortgage-industry-using-apis/">Read More</a>]</p>
			<p><hr /></p>
	  

		<!-- Pagination links -->
		<table width="100%">
			<tr>
				<td align="left" width="33%">
				  
				    <a href="/blog/page14" class="previous">
				      &#8592; Previous
				    </a>
				  
			</td>
			<td align="center" width="33%">
			  <span class="page_number ">
			    Page: 15 of 37
			  </span>
			</td>
			<td align="right" width="33%">
		  
		    <a href="/blog/page16" class="next">Next &#8594;</a>
		  
			</tr>
			</tr>
		</table>

  </div>
</section>

              <footer>
	<hr>
	<div class="features">
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
	<hr>
	<p align="center">
		relevant work:
		<a href="http://apievangelist.com">apievangelist.com</a> |
		<a href="http://adopta.agency">adopta.agency</a>
	</p>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Home</a></li>
    <li><a href="/blog/">Blog</a></li>
  </ul>
</nav>

              <section>
	<div class="mini-posts">
		<header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/tyk/tyk-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.openapis.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/openapi.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.asyncapi.com/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/asyncapi/asyncapi-horiozontal.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://json-schema.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/json-schema.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
