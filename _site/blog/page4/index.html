<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
	<a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions2.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
	<ul class="icons">
		<li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
		<li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
		<li><a href="https://www.linkedin.com/company/api-evangelist/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
		<li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
	</ul>
</header>

    	        <section>
	<div class="content">

	<h3>The API Evangelist Blog</h3>
	<p>This blog is dedicated to understanding the world of APIs, exploring a wide range of topics from design to deprecation, and spanning the technology, business, and politics of APIs. <a href="https://github.com/kinlane/api-evangelist" target="_blank">All of this runs on Github, so if you see a mistake, you can either fix by submitting a pull request, or let us know by submitting a Github issue for the repository</a>.</p>
	<center><hr style="width: 75%;" /></center>
	
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/09/10/the-api-reality-in-our-heads-versus-the-reality-on-the-ground/">The API Reality In Our Heads Versus The Reality On The Ground</a></h3>
        <span class="post-date">10 Sep 2019</span>
        <p><img style="padding: 15px;" src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/udnie-IMG_7162.jpg" alt="" width="40%" align="right" /></p>
<p>I am spending some time grounding my views of the API landscape. Working my through all of my beliefs, and systematically blowing them to bits to see how they hold up against the stress of reality on the ground. This is something I&rsquo;ve become very good at when it comes to my personal beliefs in recent years, and something I&rsquo;ve been working to transfer to my professional world to help me keep a grip on what is going on. There are a number of reason why I fall prey to things that are not real in this game, and I&rsquo;m pretty aware of the shady things that occur in the business world, but when it comes to technology I find the stories it whispers in my ear prove to be particularly enchanting and seem to go from whisper to truth at a velocity I don&rsquo;t always understand.<br /><br />One of the things I need to develop a way of better evaluating in the moment is around the velocity at which things will happen. How fast adoption of APIs will occur within the mainstream. How quickly a company will adopt an API-first approach. And the time it will take a new tool to go from creation to adoption. Technology has this way of convincing me that everything is moving faster than ever before, and it is something that ends up as a residue on everything I touch, and is relative to how deeply I believe in this myth. As I approach a decade of doing this, I can say that API adoption and awareness has never played out in a timeline anywhere close to what I envisioned in the early days. At this point I&rsquo;d say that most things are at a 6X scale than I had imagined. Sure, there are exceptions, but when it comes to the normal pace of change, especially within the enterprise, it has taken about 6 times as long for things to take root.<br /><br />Beyond time, another area that I have to get better at accepting is when it comes to how technology convinces me to ignore the human side of things. It empowers me to efficiently overlook the human factors in all of this at a scale I&rsquo;ve never seen before. Technology, combined with my clueless white male privilege super powers makes for a pretty potent formula for missing the human factors of rolling out a new application within large organizations, and amongst the normals out in the real world. I am a master at working my way through all the technical and business details of an API rollout at an organization, pouring over the details making sure I haven&rsquo;t missed anything, and then I&rsquo;m genuinely surprised when 30 days in I&rsquo;m blindsided by some human being who is like&mdash;this shit is dumb. Then everyone else on their team goes yeah, this is dumb. Then I have a revolt on my hands. Why didn&rsquo;t I see this coming? Why hadn&rsquo;t I mapped out the teams better? What is it about technology and APIs that convinces me I have everything covered, when in reality I have such a massive blindspot for the most important detail&mdash;humans.<br /><br />Why can things make so much sense in my head, but then fall apart so horribly on the ground within the enterprise. Why don&rsquo;t I see all the negative ways in which my &ldquo;solution&rdquo; will be abused by the public? Why can&rsquo;t I see the negative consequences of my application when it gets released in the wild? These are the questions I&rsquo;m tattooing on my forearm so I make sure and consider them early on in the API ideation and development lifecycle. I don&rsquo;t fully understand why technological whispers in my ear (I only have one good one) are so powerful. I don&rsquo;t fully get why I am so attracted to tech in the first place, and still come back for more after so many misfires, and incomplete pictures painted. Sure, I&rsquo;ve also had a number of successes, but not near as many as I believed I would have. I don&rsquo;t think it is due to my abilities. I&rsquo;m pretty good at this shit. I think there is more to it. I think there is something inherent about technology that convinces us to overlook the essential human elements we will need for success. I don&rsquo;t think this is by accident. I think this is probably something that is baked into the DNA of technology. Regardless, it is something I&rsquo;m going to work to be more honest about, and understand as I continue to bang my head on this API thing.</p>
        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/09/10/the-api-reality-in-our-heads-versus-the-reality-on-the-ground/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/09/10/discovering-the-confluent-schema-registry/">Discovering The Confluent Schema Registry</a></h3>
        <span class="post-date">10 Sep 2019</span>
        <p><img style="padding: 15px;" src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/stories-new-80-140-800-500-0-max-0--5--5.jpg" alt="" width="40%" align="right" /></p>
<p>While spending time doing some research into schema management tooling I came across the <a href="https://www.confluent.io/confluent-schema-registry">Confluents Schema Registry</a>. The schema management solutions is one of the first formal tools I&rsquo;ve come across that is specifically designed for helping folks get their schema house in order when it comes to APIs. I&rsquo;m sure there are others out there, but this was the first solution I've documented that addresses this in an API context as well as having an API, providing some of the critical features needed to make sense of the crazy schema mess enterprise organizations find themselves in.<br /><br />Here is the language from the Confluent website describing what the registry is all about:</p>
<blockquote><em>Confluent Schema Registry provides a RESTful interface for developers to define standard schemas for their events, share them across the organization and safely evolve them in a way that is backward compatible and future proof.</em><br /></blockquote>
<p><a href="https://docs.confluent.io/current/schema-registry/schema_registry_tutorial.html#centralized-schema-management">The Confluence Schema Registry allows you to centralize your schema</a> and provides a REST API to integrate, save, and retrieve schemas, and delivers functionality for automatically converting JSON messages to make your data human friendly. Providing a pretty fundamental schema management solution that other API service providers should be thinking about. Clearly this one is for use with your Kafka infrastructure, but the model applies across any API you are deploying, whether it is HTTP, TCP, MQTT, or otherwise&mdash;Confluent just provides us with one compelling model to follow.<br /><br />Now that I have schema catalog added to my monitoring system vocabulary it will be notifying me of other news, blogs, and other signals when it comes to how API providers are managing their schema, as well as any other API service providers like Confluent who are investing in this area of the API lifecycle. It is an area that I&rsquo;ve been beating the drum about for a while now, and something I&rsquo;d like to see more investment in. If companies are not able to get their schema house in order, they aren&rsquo;t going to be very successful with their API efforts. The two are intertwined, and it doesn&rsquo;t matter how good your API design, deployment, management, testing, and documentation are, if you don&rsquo;t have a handle on the schema behind it all, you will continue to encounter friction.</p>
        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/09/10/discovering-the-confluent-schema-registry/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/09/09/continue-pushing-the-api-documentation-conversation-forward/">Continue Pushing The API Documentation Conversation Forward</a></h3>
        <span class="post-date">09 Sep 2019</span>
        <p><img style="padding: 15px;" src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/udnie-IMG_4564.jpg" alt="" width="40%" align="right" /></p>
<p>I am been finally seeing the investment across the API sector I wanted to see when it comes to API documentation. There are multiple API definition driven API documentation offerings available on the market now. Both open source and high quality commercial services. A couple years after Swagger UI made it&rsquo;s splash I began lobbing for more investment in open source API documentation tooling, and after four years I&rsquo;m starting to see it beginning to happen. However, let&rsquo;s not rest on laurels and make sure we keep investing and building on the momentum that we have established, and continue making API documentation more valuable to developers, but also to business users who are interested in putting API resources to work.<br /><br />One of the major improvements in API documentation that I would like to see in coming years centers around visualizations. I&rsquo;d like to see interactive documentation be augmented and extended using D3.js, and other visualization components, rendering API responses in a more visually pleasing way. Helping make API responses more meaningful to developers, and potentially to businesses users who are trying to understand the value an API delivers. Visualizations have the potential to make API documentation something that introduces and educates developers to how to integrate with an API, but also demonstrate and illustrate how the data, content, and other resources can be valuable. Bonus points for any tooling provider if the visual results are actually embeddable and shareable across websites and social media, allowing anyone to take the results of each API response and quickly make available to a developer or business users network.<br /><br />Beyond just visualizations, I&rsquo;d like to see more interactive API documentation to make results savable, exportable, and shareable. Allowing any developer or business user to easily make an API request, then export the results as a JSON, CSV, or possibly as a spreadsheet. Empowering API consumers to quickly use API documentation to understand what an API delivers, get at the valuable at the data, content, and other valuable resources, and take that value with them in a portable machine readable format. This type of functionality would move API documentation to be more executable like we see Postman doing with their API lifecycle tooling, documentation, and resulting Postman Collections. Ensuring API documentation is not just interactive, which is a default requirement these days, but it is executable functionality that can be shared and taken with you, allowing anyone to run the desired API request at any point in the future. <br /><br />These are just a handful of ways I&rsquo;d like to see the API documentation conversation continue to be moved forward. I have a lot of other ideas regarding how API documentation can be made more useful, and empowering for both developers and business users. Interactive API documentation like Swagger UI, Redoc, and others have gone a long way in helping make APIs more accessible and usable amongst developers. With another push, I think we can also make APIs more accessible and useful to business users, and dramatically increase the reach of APIs beyond just the tech community.&nbsp; Which will be necessary to realize the next wave of growth and adoption we are looking for when it comes to APIs, and ensuring they continue to be ubiquitous behind the desktop, web, mobile, device, and network based applications we depend on each day to get business done&mdash;ensuring APIs aren&rsquo;t just a behind the scenes developer tool, and can be used by virtually anyone.</p>
        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/09/09/continue-pushing-the-api-documentation-conversation-forward/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/09/09/bridging-grand-visions-of-an-api-lifecycle-with-people-on-the-ground-being-successful-in-their-work/">Bridging Grand Visions of an API Lifecycle With People on the Ground Being Successful In Their Work</a></h3>
        <span class="post-date">09 Sep 2019</span>
        <p><img style="padding: 15px;" src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/san-francisco-city-bridge-sf-city-bridge-copper-circuit.jpg" alt="" width="40%" align="right" /></p>
<p>While my work as the API Evangelist can burn me out some of the time, I generally find it intellectually challenging. The work takes me from industry to industry, country to country, and to the highest levels of technology, and down to the work that goes on across development teams within companies. APIs are everywhere. Really, they are. They are impacting each one of our personal and professional lives, and this is one of the things that keeps me coming back doing what I do. The diversity of API implementations, and levels at which I can engage with the industry keeps me interested, continuously learning and producing.<br /><br />I enjoy thinking about the API space from the 250K level. It is interesting to study what is, what has been, and where things might be going when it comes to APIs. I find it compelling to learn about how API providers, service providers, and investors see things, then reconcile that with what actually happens on the ground. Looking at API change across business sectors has lifted my view from 100K to 250K, allowing me to not just understand how the tech echo chamber sees APIs, but also how mainstream businesses and government agencies see APIs. Always pushing me in new directions, helping me see beyond any single silo I might get trapped in along the way.<br /><br />Watching the API lifecycle come Into focus over the last decade has been interesting. Watching API management shift how we generate revenue from our software, witnessed API design come to life, as well as mocking, testing, and other stops along the API lifecycle have all been educational experiences. I like to think about API-first, and how progressive developers are delivering high quality APIs from end to end. Studying, listening and writing about these concepts, then repeating, repeating, and repeating, until I push my own understanding to new heights. This is what API Evangelist has been for me. It has been something that has allowed me to constantly reinvent myself, and establish goals continuing to grow and understand the landscape, even after burning out a couple of time along thew ay.<br /><br />While I thoroughly enjoy thinking about the lofty API lifecycle stuff, I also thrive on understand what developers face on the ground. You&rsquo;ll always find me hacking away on a new API prototype, or playing with a new piece of infrastructure. And, you will always find me advocating for developers needs over those of business leaders&mdash;it is just how I am. I understand how essential it is for me to stay close to the action, while still also understanding how things work up at the top. I definitely can&rsquo;t ignore the business and political realities, but hearing what developers need to just be successful in their work matters a great deal. If it ain&rsquo;t making a developers life easier, it probably isn&rsquo;t worth keeping track of as part of my research&mdash;all of my lofty thoughts are rooted in this reality.<br /><br />In my experience most API providers and consumers don&rsquo;t care about APIs, services, tooling, and process as much as us analysts do. That is OK. They are more concerned with what they need to get their job done, and be successful at the company, organizations, institutions, and government agencies where they work. They tend to not care about startup exits, or the most relevant standard. They want friction reduced in their daily lives. They want to just get this project done, and done right, so they can move on to the next project. They may enjoy hearing.a good story from time to time about the best way to be doing APIs, but most of the time they just want it done for them. They just want the API designed, deployed, mocked, tested, managed, documented, and secured for them. They don&rsquo;t want to have to understand the nuance, pros and cons, and other challenges that come with moving throughout the API lifecycle.This is the honest reality of the API lifecycle.<br /><br />I feed off these realities, as well as the ups and downs of studying, thinking, and writing about APIs. I like thinking about the big picture, and learning about how individual or teams of developers are getting work done. I like knowing there is no one right way of doing APIs, and learning about all of the ways to deliver them throughout the API lifecycle. It is what keeps me coming back. I&rsquo;m also getting used to things not always going the way that I would like them to. I&rsquo;m getting used to the world not working as we think it will. I&rsquo;m fine with being wrong some of the time. It comes with the territory. It helps me not get to attached to any single vision of the API space, and leaves me more open to listening to what others are doing on the ground floor, and coming down from the clouds to see how the bits and bytes are moving around. Building bridges between what different folks are doing, and rooting my lofty visions of the API space in postitve developer and end-user outcomes is what keeps the API Evangelist train moving forward.</p>
        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/09/09/bridging-grand-visions-of-an-api-lifecycle-with-people-on-the-ground-being-successful-in-their-work/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/09/06/where-do-you-like-your-api-complexity/">Where Do You Like Your API Complexity?</a></h3>
        <span class="post-date">06 Sep 2019</span>
        <p><img style="padding: 15px;" src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/udnie-DSC_0109.jpg" alt="" width="40%" align="right" /></p>
<p>I prefer my API complexity at the path, query, then schema levels of my API design&mdash;specifically in that order. I don&rsquo;t mind a huge number of individual API calls to get the job done because I just script away this aspect of API complexity. However, I do fully understand that many folks prefer their complexity at the query and schema levels over having lots of individual paths. I find that developers love to rant about imperative API complexity, and in my experience the folks who don&rsquo;t like path level API complexity are also some of the most vocal types of folks who are very confident that their way is the right way, when in reality, there is no right way&mdash;just the way YOUR consumers will want or need to access the API resources you are serving up.<br /><br />In my experience, how someone learned about the web, and then APIs, dictate much of where they like their API complexity. If developers prefer their complexity at the query parameter layer their API doorway was the web. If developers prefer their complexity in the schema, their doorway was likely mobile. If you understand the role that paths can play in managing complexity, you&rsquo;ve probably have embraced the overall concept of web APIs. If you understand the opportunity and necessity of sensibly spreading complexity across the path, query, and schema layers of your API, you probably don&rsquo;t just have experience providing APIs, you probably have a lot of experience supporting many different types of API consumers&mdash;the key that unlocks the door to seeing the bigger picture of managing API complexity.<br /><br />Not all API consumers are created equal. They have different views of what an API is, and how you use one. If you&rsquo;ve supported a wide audience of API consumers,&nbsp; you realize there is no single silver bullet when it comes to where you offload your API complexity. You have to make tradeoffs at every turn when designing your API, and it&rsquo;s underlying schema. New users won&rsquo;t know your schema intimately, and respond well to simple, well defined API paths with the ability to dabble in complexity with a few query parameters. Your veteran users will know your schema and will see each API call made as cumbersome and demand a single robust way to get what they need. Not all API consumers will see API the same way you do, something that will exponentially increase depending on how public your APIs are.<br /><br />Personally I like to avoid complexity at all costs. However, I work hard to spread my API complexity equally across API paths, query parameters, as well as the schema. I&rsquo;d also add that I personally prefer leverage headers for much of the complexity we burden our query parameters and schema with. Things like authentication, pagination, sorting, filtering, etc. However, I also understand that many developers don&rsquo;t see headers, so I tread thoughtfully here, even though I personally prefer my complexity done using headers in some standardized way. API complexity is a topic I should visit more often here on the blog. I may even add it as a stop along the API lifecycle. Making it a conscious stop helps force me to pause and think about what I am doing, forcing me to think about complexity. Pushing me out of my API silo and this more about how others might view my API, moving beyond just my default API belief system.</p>
        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/09/06/where-do-you-like-your-api-complexity/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/09/06/api-management-should-not-just-limit-me-it-should-allow-me-to-scale/">API Management Should Not Just Limit Me, It Should Allow Me To Scale</a></h3>
        <span class="post-date">06 Sep 2019</span>
        <p><img style="padding: 15px;" src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/docks-docks-graham-sutherland.jpg" alt="" width="40%" align="right" /></p>
<p>I do a lot of thinking about API management. After almost a decade of contemplating how we manage our API infrastructure, I feel it is still the most important stop along the API lifecycle. I don&rsquo;t care how well designed, deployed, documented, and supported your APIs are, if you aren&rsquo;t tuned in using API management, you aren&rsquo;t going to be successful. API management provides you with the tools to you need to define and understand how your consumers will put your API resources to work. After almost 15 years of evolution, API management hasn&rsquo;t changed too much, but there is one core capability I&rsquo;d like to see evolve, expanding upon the often go to feature of API rate limiting.<br /><br />API rate limiting has been a staple of API management since the beginning, allowing you to limit how much fo any resource a group or individual consumer can get access to&mdash;limiting the rate at which they can make API calls. The reason for rate limiting will vary from provider to provider, but the most common reason is to conserve compute resources so that an API remains usable by all consumers. Next, I&rsquo;d say that pricing is the second most common reason for rate limiting, carving up API resources by access tier, and limiting the number of calls each API consumer can make per second, minute, day, or other time frame. While these concepts are still applicable to the business of APIs in 2019, I&rsquo;d like to see the concept evolve to keep up with how we deploy infrastructure in a containerized, Kubernetes, serverless cloudy landscape.<br /><br />Instead of capping what I can consume of your API, why not allow me to pay for more access, as well as more performance for a short period of time, or whatever duration I desire. You can still impose rate limits to measure everything I&rsquo;m consuming, but allow me to also give the OK and turn on the firehose. If I need to do some batch processing, get at a volume of data I need for some application, why get in the way? API management shouldn&rsquo;t just limit, it should empower me to scale and get what I need in any time period I need it. Let me pass in a header, or use an alternative DNS location to scale up my consumption of your API resources, leveraging API management to not just measure the amount of direct API resources I&rsquo;m consuming, but also the underlying API infrastructure resources I am putting to work. In today&rsquo;s cloud infrastructure landscape, it isn&rsquo;t be too difficult to more closely couple the API management and underlying database and compute layers, to automate the scaling of API resources.<br /><br />This concept wouldn&rsquo;t probably work for those API providers who use API rate limits to make digital resources seem more scarce, and leveraging API management to shift perceptions around pricing. However, the concept of using API rate limiting to manage the compute resources behind an API seems pretty outdated. We shouldn&rsquo;t be limiting access just because we can&rsquo;t afford more compute power behind. We should be offloading this to the consumers. If they want to scale the AWS Azure, and Google infrastructure behind our APIs, then let them. They just have to be responsible for the bill. It seems like modern API management infrastructure should help us broker this deal, empowering our consumers, while also allowing us to derive new sources of revenue from our existing API infrastructure. Which is one of the main purposes of employing API management in the first place, to help us define plans around how are API resources will be put to work in a way that fits with our wider API monetization strategy.</p>
        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/09/06/api-management-should-not-just-limit-me-it-should-allow-me-to-scale/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/09/06/api-evangelist-does-not-run-on-github-anymore/">API Evangelist Does Not Run On GitHub Anymore</a></h3>
        <span class="post-date">06 Sep 2019</span>
        <p><img style="padding: 15px;" src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/old-gas-pumps-oldgaspumps-dark-dali.jpg" alt="" width="40%" align="right" /></p>
<p>I migrated the main API Evangelist site off of GitHub the other day. The moved followed the migration of 100+ network sites of my API research a couple of weeks back. While I still have a handful of definitions and tooling published to GitHub, the migration of my main site signals a pretty huge shift in how I operate the site. I&rsquo;ve operated the site 100% on GitHub since 2014, using YAML as the backend data store, and Jekyll to publish the pages, blogs, and data-driven pages. I have always done this to keep the site as open and accessible as I possibly can, sharing all of the data behind what I was doing However, in 2019, due to increased GitHub API rate limits, Jekyll build timeout limits, and shifts in the purpose of API Evangelist, I don&rsquo;t see the value in me working to keep things open and available on GitHub anymore.<br /><br />To operate API Evangelist I am still going with a static approach, meaning all of the pages are published as static HTML, rather than making dynamic from a CMS or database--however, I won't be using Jekyll anymore. I will maintain all the content and data within my own home brew CMS and database, and I will publish things out on a schedule, and in response to specific events that occur. The move significantly reduces the complexity and workload on my part when it came to maintaining the many different repositories, schema, and increasinlgy complex publishing process. It is much easier to just publish HTML files to the file system of a Linux server than use Git and APIs to orchestrate changes across hundreds of repositories. It was something that was becoming untenable due to increased error rates with Jekyll builds when I committed a change, and impossible to do via the GitHub API with a shift in API rate limits recently.<br /><br />I&rsquo;m a little sad that it is all over. I enjoyed the performance of it all. I enjoyed the data backend being public, openly available, and even forkable. One thing that surprised me, is that even though my entire network of sites ran on GitHub, allowing anyone to submit a post, page, or other listing, very few ever did. I had a core of diehard editors who would correct my spelling and grammar (I love you all), but it never expanded beyond this group. With all the API providers and service providers out there, and nobody ever submitted a story for me to review and publish. Not sure if it is more of a statement on me, my approach, or the community. I just find it an interesting footnote in a long journey of exploration into the openness of content, data, and business model. Something that was an fun experiment, but now I&rsquo;ll go back to my roots, and depend on my own skills to manage and publish my research.<br /><br />Moving forward I will be operating API Evangelist in a more commercial manner. While I will still keep that independent lens, and be very opinionated in everything I do, I will be keeping much of the resulting research behind a pay wall. There just wasn&rsquo;t enough return on investment being so open, and it resulted in me burning out several times over the last nine years. While I enjoy operating on a variety of platforms, I also enjoy having full control over my digital presence, and operating my network of sites as a simple static network of sites on my own servers makes the most sense for where I am at with the evolution of API Evangelist. I&rsquo;ll remain as prolific as ever when it comes to publishing blog posts, and posting to social media, but most of the final research and analysis will end up being published as PDF guides, blueprints, opportunities, and landscape publications. Shifting what I do from being open on GitHub to being more closed on my own dedicated platform.</p>
        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/09/06/api-evangelist-does-not-run-on-github-anymore/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/09/05/the-different-ways-api-providers-use-the-openapi-servers-collection/">The Different Ways API Providers Use The OpenAPI Servers Collection</a></h3>
        <span class="post-date">05 Sep 2019</span>
        <p><img style="padding: 15px;" src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-stories-server-racks-clouds-copper-circuit.jpg" alt="" width="40%" align="right" /></p>
<p>I was looking through the OpenAPI definitions I have harvested via some automated scripts I have running, and I came across an API definition that had a variety of URLs available for their APIs, making this part of the definition something I want to study more, identifying the common patterns in use. I harvest a growing number of OpenAPI definitions and Postman Collections to help me stay in tune with who the interesting API providers are, and documenting what the common building blocks of APIs are, helping shine a light on the useful practices that exist across API providers within many different industries.</p>
<p>The OpenAPI server collection is beeing used to help automate switching between a variety of locations, and is most commonly used to differentiate between the different stages of an API server, as see I this example:</p>
<script src="https://gist.github.com/kinlane/e490429f2d619a9e9e1dd4b5970d0613.js"></script>
<p>This is just the most common usage of the OpenAPI server collection out there. I&rsquo;d say the second most common example is publishing multiple regions in which an API is available&mdash;leveraging DNS to to make an API more available, performant, and meeting local and regional regulations. After harvesting and processing a couple thousand OpenAPI 30 definitions following doing the same with slightly more Swagger 2.0 files the importance of moving from a single host in Swagger 2.0 to multiple potential servers in OpenAPI 3.0 revealed itself. Signaling that APIs aren&rsquo;t just being deployed and made available in a single location or way.</p>
<p>I will be regularly pulling the values for the server collection across all the OpenAPI definitions I index to develop a better understanding at how API providers are using it. It provides an interesting look at API providers roll out their infrastructure. I don&rsquo;t expect every API provider to be documenting their APIs this thoroughly, but since I&rsquo;m scanning GitHub for most of these API definitions, many of the API providers are publishing their OpenAPI definitions to GitHub because it is part of some CI/CD workflow, resulting in a more honest OpenAPI definition than what you might get with documentation. When I find anything interesting I will publish here as a story, slowly documenting the different ways in which API providers are making their APIs available, scaling them, and distributing them into different regions.</p>
        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/09/05/the-different-ways-api-providers-use-the-openapi-servers-collection/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/09/03/controlling-the-conversation-around-your-mobile-application-apis/">Controlling The Conversation Around Your Mobile Application APIs</a></h3>
        <span class="post-date">03 Sep 2019</span>
        <p><img style="padding: 15px;" src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/desert-dragon-light-dali.jpg" alt="" width="40%" align="right" /></p>
<p>I have seen it play out over and over since I began monitoring the API conversation. Companies who launch APIs to power a mobile application but refuse to, or are unaware of how they should be controlling the conversation around public API infrastructure. The most common reason for this is that companies do not view the APIs behind their mobile applications as public APIs, and that somehow because they are buried within their mobile application, that they are safe from hackers. Completely clueless of the fact that anyone can run any mobile application through a proxy and reverse engineer the API infrastructure behind any mobile application.<br /><br />Mobile application platforms that do not control the conversation around their public APIs are the ones who end up having security incidents down the road. This is due to the face that these providers end up having a pretty significant blind spot stemming from their lack of awareness and control of the conversation around their APIs, and someone ends up paying closer attention to their APIs and eventually someone finds a vulnerability to exploit. If you have a publicly available mobile application, then you have publicly available APIs, and you should be treating your APIs like they are public. I&rsquo;m not saying you should offer the public free and unfettered access to your APIs, I am simply saying that you should be operating a public API program around your APIs, controlling who has access, and shaping the message around what your APIs do, or don&rsquo;t do.<br /><br />To see examples of companies who do not have a handle on their API conversation, search for TikTok API, or for Tinder API, and you&rsquo;ll see that hackers own the conversation when it comes to the APIs for these platforms. When you aren&rsquo;t dealing with this side of your operations, rogue API operators step up and dominate the conversation on GitHub, Stack Exchange, NPM, and other places that coders hang out. We&rsquo;ve already seen Tinder have security and privacy issues, and I&rsquo;m betting that we&rsquo;ll see the same with TikTok, especially with their popularity making them such a vector for attack. Sensible security and privacy is a blind spot for platforms who see a public API presence as simply about offering free access to a public API.<br /><br />If you have a public mobile application and you search for your company name plus API and you do not own the top listings&mdash;it is a problem. It is a sign that you aren&rsquo;t thinking about the big picture when it comes to the resources you are making available via the web. This isn&rsquo;t about you becoming the next Twitter with a public API. This is about you looking at your API infrastructure from an external perspective, and controlling public opinion around your infrastructure. With this external perspective you will begin to look at API security, privacy, monitoring, performance, scalability, terms of service, ad other essential aspects of your infrastructure differently. If you are just operating and thinking your APIs are safe behind your mobile application, you will be getting p0wned every time someone publishes the blueprints to your Death Star, revealing your weaknesses, rather than just being public about it all in the first place.</p>
        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/09/03/controlling-the-conversation-around-your-mobile-application-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/08/26/benefits-of-treating-my-api-infrastructure-as-apifirst/">Benefits Of Treating My API Infrastructure As API-First</a></h3>
        <span class="post-date">26 Aug 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-square-23-160-800-500-0-max-0--5--1-square.jpg" width="45%" align="right" style="padding: 15px;" />
Most API providers I speak with see the value of consistently delivering API infrastructure to power desktop, web, mobile, device, and network applications. Less than 10% of these providers see the API infrastructure that powers their APIs, and ultimately their applications as APIs. Meaning, these providers do not view every stop along the API lifecycle as a set of APIs, ensuring that your API definitions, design, mocking, deployment, management, monitoring, testing, orchestration, security, and documentation all have APIs, and are able to be governed programmatically. Mostly it is because they are just getting started on their API journey and / or they just don’t have the bandwidth to be able to step back and look holistically at what they are trying to accomplish.

<p>In this truly API-first world, every architectural decision begins with a README in a Git repo, and the business, architectural, and development teams working together to hammer out a simple name, description, and list of capabilities for each individual architectural component. Once in agreement, then they can then get to work hammering out a contract for the schema, and the interfaces that are available for potential consumers. Once again, working between business, architecture, and development teams to come up with the simplest, most useful, and durable contract possible. One that will work for all stakeholders, and best represent the architectural capabilities of each component you will need to successfully operate each stop along your API life cycle.

<p>Once there is an overview and contract in place for each architectural component, you can begin mocking, testing, and documenting it for wider potential consumption by other stops along the API life cycle. With all stakeholders in agreement over the capabilities of each architecture component, and how it will work, you can begin wiring up specific back-end capabilities which might be as simple as data and content storage, or as complex as API management, monitoring, and testing. Leveraging 3rd party services, open source solutions, or custom artisan code to fully realize each architectural component you are defining, and now developing to deliver a specific aspect of how you do APIs. Making sure you treat your API infrastructure as an API, while also abstracting way specific solutions with your own layer that allows you swap out backend solutions as you need or desire.

<p>To help demonstrate what I am talking about, I am working on a series of stories that help me think about an API-first architectural approach to managing my API definitions. This is the most important stop along the API life cycle, and is something that touches every other stop, no matter where your journey might take you. I’ll be thinking through through how I create, store, manage, and evolve my API definitions using APIs. I have had an API for my APIs for some years now, but as I move from Swagger 2.0 to OpenAPI 3.0, and be more thoughtful about how I define event-driven APIs using AsyncAPI, as well as manage JSON Schema is use across these formats, my API API is overdue for a reworking. Additionally, I am augmenting the HTTP 1.1 layer of all of this with a Postman Collection and Environment management and automation layer, further pushing me to rethink how I’m doing things.

<p>Next steps are to create an OpenAPI for my new OpenAPI and JSON Schema defined API contracts. Once I have this contract, I’ll spend some time thinking about the automation and programmability of the API design stop along the API lifecycle–ensuring that my APIs follows a cadre of API Design patterns I have already established. After that I will move on to mocking, testing, documenting, and development deployment and management. The goal is to actually deliver my next generation API API, but also define the APIs that drive the underlying infrastructure I am using to deliver and operate my APIs. Luckily there is a lot of markdown and YAML work as part of all of this, and it is something that I can do within a README, and here on my blog over the course of a couple weeks. The process provides me with a pretty low cost way to plan out, then ultimately deliver and communicate with others around what this API actually does.

<p>Being able to work through every technical detail of my API infrastructure is one of the main benefits of treating my API infrastructure as an API. I can take the APIs of services and tooling I am using to deliver my API and use them as a seed for my own API abstraction layer. Taking GitHub, Postman, APIMATIC, ReDoc, Tyk, and other infrastructure providers I depend on and deliver my own API infrastructure, while also ensuring that I can orchestrate it all with my own API layer. Providing all the functionality I will need across the API life cycle as individual API capabilities that I can use to automate my API operations. Seeing my API infrastructure as just another set of API resources has changed how I view the landscape, and will has forever impacted how I interpret what an “application” is.  Once I introduced the recursive loop of APIs being APIs, then “application” doesn’t mean how I apply API resources on the desktop, web, mobile, device, and networks, it also speaks to how I apply APIs to APIs–leveling up with the benefits that APIs bring to the table.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/08/26/benefits-of-treating-my-api-infrastructure-as-apifirst/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/08/22/doing-a-diff-between-available-web-mobile-and-public-apis/">Doing A Diff Between Available Web, Mobile and Public APIs</a></h3>
        <span class="post-date">22 Aug 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/stories-copper-servers.jpg" width="45%" align="right" style="padding: 15px;" />
I spend a lot of time running web and mobile applications through a proxy to reverse engineer their APIs. I generally use Charles Proxy for routing my desktop, web, and mobile traffic through, which then automatically saves a JSON dump of sessions every five minutes, and syncs with Dropbox via my shared folders. From there I have a schedule service that will look in the shared Dropbox folder every hour, sift through the Charles Proxy JSON dump, and look for JSON, XML, CSV, and other common machine readable formats—which are then converted into OpenAPI definitions. Allowing me to reverse engineer desktop, web, and mobile applications as I use them, and map the API surface area for these targeted applications.

<p>Recently I started playing with doing the same thing as part of my use of Postman. You can use the <a href="https://learning.postman.com/docs/postman/sending_api_requests/capturing_http_requests/#using-the-postman-built-in-proxy">built-in proxy in the Postman native apps</a> or use the <a href="https://learning.postman.com/docs/postman/sending_api_requests/interceptor_extension">Interceptor extension for the Postman app</a>. Postman walks you through how to configure your laptop, mobile, and Postman application, and ultimately capture HTTP requests and save them to history or as a Postman Collection. Doing essentially the same thing I’m doing, but doing it with the Postman application, and leveraging collections instead of OpenAPI. I’d say there are pros and cons to both approaches, but Postman gives me the ability to manage environments, workspaces, and other essential concepts that would help take my API profiling work to the next level.

<p>One of the benefits of working with Postman collections is you get all the benefits of using the Postman app. Things like the built-in proxy for capturing traffic, but also the history, generate, fork, merge, and diff collections. My work profiling APIs is all about reverse engineering desktop, web, and mobile applications, as well as quickly translating API documentation in machine readable API definitions, like OpenAPI and Postman. When profiling an API, most of the time I have the API documentation open in one browser window, and my Postman application open—-operating them both in split screen. I’ll just take the paths and parameters from the documentation and enter them into my Postman application, and fidget with things until I make a successful API call. Then I repeat for every single path until I have a complete Postman Collection for a single APIs. Merging my more automated proxy profiling with my manual API documentation profiling makes a lot of sense—-then I can manage Postman Collections as part of Postman workspaces, complete with Postman environment definitions.

<p><a href="https://blog.postman.com/2019/01/16/forking-merging-a-conflict-resolution-solution/">Postman gives me forking, merging, diff, and conflict resolution for Postman Collections</a>. This is essential for mapping out the API landscape for a single API provider over time, but I find it is also useful for doing a diff between available web, mobile, and public APIs. Helping me paint a complete picture of a platform’s APIs by reverse engineering their APIs behind their web and mobile, then taking their own Postman Collection, OpenAPI definition, or manually create a Postman Collection from their API documentation, allowing me to create a diff between each of their channels. Painting a complete picture of a provider’s public API infrastructure, but also be showing the prioritization of resources when it comes to the different channels. I find this to be a pretty compelling way to paint a picture of not just tech companies, but almost any company in 2019 who has a web, mobile, and public API infrastructure. Wait, does having an API make you a tech company? IDK. We’ll answer that question another time.

<p>I enjoy learning new ways to accomplish existing tasks I’m working on daily. Sometimes it helps me to shift my view to a new service or tool so that I can gain a few new bells and whistles, but also the process forces me to look at things differently. This shift can push me into entirely new territory when it comes to understanding more about the technology, business, or politics of operating APIs. There are a lot of games played in the world of APIs, and even more being played in the shadows behind web and mobile applications. I need as much help as I can to help me shine a light into these dark corners of the web, which reside directly behind the desktop, web, browser, mobile, and device applications we are using in our personal and professional lives. Having a diff of the various APIs in use behind these channels goes a long way towards helping us understand the motivations of the companies behind these applications, pushing us be more aware of who the good or bad actors potentially are.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/08/22/doing-a-diff-between-available-web-mobile-and-public-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/08/22/an-api-policy-domain-specialist-at-twitter/">An API Policy Domain Specialist At Twitter</a></h3>
        <span class="post-date">22 Aug 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/white-house-lawn-white-house-window-propaganda-leaflets.jpg" width="45%" align="right" style="padding: 15px;" />
There are some jobs on the Internet I apply for no matter what my current situation is, and <a href="https://g.co/kgs/x1NtjW">an API policy domain specialist at Twitter</a> was one of them that popped up recently. I applied for the job within the first couple of hours after it came out, but haven’t heard from them. I can speculate on the reasons why, but I think a story about the job posting itself is actually more interesting, so I’ll focus there. It is the first time I’ve seen a job posting for this role, but I think it will eventually become a required role in the future for any company with a public API—-that is, if companies want avoid the trouble Twitter is going through right now, which again, is making Twitter the poster child for how to do APIs both right and wrong.

<p>To highlight what this role is all about, I think Twitter’s own posting sums it up well, so let’s start by just reviewing what you’ll be doing if you get this job at Twitter:


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/08/22/an-api-policy-domain-specialist-at-twitter/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/08/21/multiple-overlapping-api-life-cycles/">Multiple Overlapping API Life Cycle(s)</a></h3>
        <span class="post-date">21 Aug 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/stories-downtheline-dali-three.jpg" width="45%" align="right" style="padding: 15px;" />
One of the toughest parts about teaching people about APIs is that there are many different views of what the API life cycle can be depending on who you are, and what your intentions are. As an advocate or evangelist for a single API you are speaking externally to the API consumption life cycle, but internally you are focused on the API delivery life cycle. As an API Evangelist for many APIs, targeting providers, consumers, and anyone else who comes along, I find it a constant challenge to properly speak to my intended audience. One problematic part of my storytelling I regularly see emerge is that I speak of a single API life cycle, where in reality there are many overlapping life cycles. So, to help me think through all of this I wanted to explore what these overlapping tracks might be—coming up with four distinct iterations of overlapping API building blocks.

<h2 id="the-api-delivery-life-cycle">The API Delivery Life Cycle </h2>
<p>The most common way we refer to the API life cycle is from the perspective of the API provider, where it is all about delivering an API. Referencing the stops along the life cycle that are most relevant to someone who is  delivering a new API, or might be moving an API forward as part of the evolution of an existing resource. From my vantage point, I consider these to be the most common stops along the API delivery life cycle.

<ul>
  <li><strong>Definitions</strong> - Defining what an API does, crafting the JSON schema, OpenAPI, AsyncAPI, and other machine readable definitions of what is potentially being delivered, initializing the contract that will guide an API through the life cycle.</li>
  <li><strong>Design</strong> - Stepping back and considering the healthiest API Design practices to apply, working from a diverse API toolbox, and ensuring you have a good handle on who your audience is, and the design patterns they’ll respond to.</li>
  <li><strong>Mocking</strong> - Generating a mock instance of an API using the contract to test out design patterns, and begin socializing amongst technical and business stakeholders so that they can begin providing critical feedback on the design.</li>
  <li><strong>Documentation</strong> - Making sure there is auto-generated documentation available for all providers and consumers from the agreed upon API contract definition, ensuring there is machine and human readable documentation available at all times.</li>
  <li><strong>Testing</strong> - As the API contract stabilizes, tests can be generated to ensure that the contract is not veering of the intended course, providing guardrails that can be regularly rebuilt from the API contract, and ultimately used in production.</li>
  <li><strong>Deployment</strong> - Publishing of an API either using an open source framework, gateway, or through hand coding of the interfaces, providing a production ready instance of an individual API contract that was defined early on.</li>
  <li><strong>Management</strong> - Making sure an API requires proper authentication, has access levels and rate limits defined, as well as logging enabled, so that its access can be properly quantified, managed, and reported upon for provider and consumer.</li>
  <li><strong>Monitoring</strong> - Ensuring that each API is being monitored in real time from multiple regions, ensuring that an API is up and available, as well as passing all tests that have been defined as part of the development of each API contract.</li>
  <li><strong>Security</strong> - Knowing that requiring authentication will not be enough, and that all APIs are regularly scanned, probed, and pushed for any potential vulnerabilities, providing regular accounting of the security of al APIs in operation.</li>
</ul>

<p>I can quickly add on another 10-20 stops along the API delivery life cycle, but I want to highlight the core building blocks of delivering an API, not exhaustively document every possible building block. These are the primary steps any API provider will need to master before they can begin competently delivering API resources for consumption internally, or externally to partners and the general public. Providing a basic checklist that can be use to flesh out the details needed before an API can be called production-ready, then begin it’s life as the back-end for web, mobile, device, and network applications.

<h2 id="api-consumption-life-cycle">API Consumption Life Cycle</h2>
<p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/subway-subway-internet-numbers.jpg" width="45%" align="right" style="padding: 15px;" />
Next, I wanted to explore the flip side of the API life cycle, and the view from the position of the API consumer. This is the second most common way I talk about the API life cycle, but I find that I often get things mixed up between provider and consumer, and I know from experience that I do not always ensure that things are made clear. To help me put myself into the shoes of API consumers, I wanted to explore what some of the stops along the API life cycle from purely a consumption standpoint—here are a few that I’m thinking about today.

<ul>
  <li><strong>Discovery</strong> - Before you can consume an API you have to be able to find it, and be able to get what need to get up and running.</li>
  <li><strong>Documentation</strong> - You will have to be able to understand the details of what an API does, and ultimately what it delivers via documentation.</li>
  <li><strong>Plans</strong> - Once you understand what an API does, you will need to have a grasp on the tiers of access, and what you can afford to use.</li>
  <li><strong>Account</strong> - After you decide to use an API you will need to be able to signup and obtain an account and credentials that will allow for access.</li>
  <li><strong>Authentication</strong> - Now that you have keys, you will need to understand how you can use them as part of authentication for the APIs you need.</li>
  <li><strong>Clients</strong> - There should always be client tooling that helps consumers get up and running without writing code, with as little friction as possible.</li>
  <li><strong>Software Development Kits (SDK)</strong> - There should be SDKs available in a variety of programming languages allow consumers to seamlessly integrate.</li>
  <li><strong>Command Line Interface (CLI)</strong> - Some developers will prefer using a CLI to integrate and automate using an API as part of their systems.</li>
  <li><strong>Integration</strong> - Now we reach the point where we actually integrate with an API, something not all developers will successfully achieve as part of their own consumption life cycle.</li>
  <li><strong>Support</strong> - Now that a consumer has integrated they will need to know where they can receive support as part of their application, and ongoing use of API resources.</li>
  <li><strong>Engagement</strong> - There should be further opportunities for engagement beyond just the initial integration of an API, encouraging the deepening of the provider and consumer relationship.</li>
</ul>

<p>This provides me with a simple slice of the API consumption side of things to focus on in my storytelling. Having simple, organized groups of API building blocks like this, wrapped in a variety of contexts helps me be more consistent in my writing and speaking. Using consistent wording combined with repetition is important, and thoughtfully defining the API life cycle from the provider, and well as consumer side, in ways that everyone can easily understand is very important to me.

<h2 id="api-maintenance-life-cycle">API Maintenance Life Cycle</h2>
<p>We aren’t done with APIs once they are delivered and being consumed. There are regular maintenance tasks that should occur, and these are things that won’t necessarily be handed by the same team that delivers the APIs. Opening up a whole other dimension of the API life cycle that cannot be ignored, otherwise we run the risk of offering a less than quality experience for our consumers. Here are the maintenance portions of the API life cycle that I am considering, and documenting as part of this work.

<ul>
  <li><strong>Communication</strong> - An API is up and running and communication between provider and consumer is critical to ensure everyone is operating and doing what they need to be doing.</li>
  <li><strong>Support</strong> - Making sure all of the consumers are supported and getting the attention they need, providing the critical feedback loop between provider and consumer.</li>
  <li><strong>Issues</strong> - Publishing a real time accounting of open issues that exist to help API consumers understand the current status of the system, while also helping minimize support requests for known issues.</li>
  <li><strong>Monitoring</strong> - Understanding the overall health and status of all APIs, and making sure all monitors, tests, and the platform is meeting their SLA when it comes providing APIs.</li>
  <li><strong>Performance</strong> - Going beyond the monitoring and testing, and making sure a certain quality of service bar is met, and we have a complete grasp of how our customers experience API access.</li>
  <li><strong>Security</strong> - Reviewing security practices, and auditing authentication, usage, logging, scanning, and other dimensions of security across all APIs in operation.</li>
  <li><strong>Reporting</strong> - Staying in tune with API consumption, errors, and general access and usability across all API consumers, making sure an awareness is developed in how digital resources are used.</li>
  <li><strong>Road Map</strong> - Publishing an accounting of what the future will hold for API consumers, regularly thinking about what future plans, or lack of future plans will have on API consumers.</li>
  <li><strong>Evangelism</strong> - What is being doing to get the word out about the value APIs deliver, establishing a strategy for how you evangelize internally, with partners, and the public and help achieve platform goals.</li>
</ul>

<p>Abandoned APIs that do not have anyone in the drive seat are the most common type of API I come across. It is pretty easy to tell when an API provider has moved on, and just left the API up and running. In other cases, teams are just under resourced, and do not have the bandwidth available to invest in the API maintenance portion of the API life cycle. We all get excited about the design and delivery of a new API, but few of us like doing the mundane aspects of API management—this is why you should invest in a good API management solution, because nobody should be reinventing the wheel in this area.

<h2 id="api-governance-life-cycle">API Governance Life Cycle </h2>
<p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/stories-supreme-court-judgement.jpg" width="45%" align="right" style="padding: 15px;" />
Lastly, and one of the most under-developed portions of the API life cycle, is the governance and oversight of things. This is the portion of the API life cycle where you go beyond the tactical day to day stuff and work to establish a strategy for how you deliver, consume, and maintain APIs. Getting formal about how we do what we do, and make sure all stakeholders, especially consumers have a good grasp on API policy, and how things get done. Here are some of the building blocks I consider at the API governance stops along the API life cycle.

<ul>
  <li><strong>Definition</strong> - Which definitions are used? Where are the OpenAPI, schema, and other relevant patterns.</li>
  <li><strong>Design</strong> - What design patterns are in play across the API definitions, and what is the meaning behind the design of all APIs.</li>
  <li><strong>Deployment</strong> - What does deployment look like on-premise, in the cloud, and from region to region.</li>
  <li><strong>Management</strong> - Quantify the standard approaches to managing APIs from on-boarding to analysis and reporting.</li>
  <li><strong>Plans</strong> - How are access tiers and plans defined, providing 3rd party access to APIs, including that of aggregators and application developers.</li>
  <li><strong>Monitoring</strong> - What does monitoring of web APIs look like, and how is data aggregated and shared.</li>
  <li><strong>Testing</strong> - What does testing of web APIs look like, and how is data aggregated and shared.</li>
  <li><strong>Performance</strong> - What does performance evaluation of web APIs look like, and how is data aggregated and shared.</li>
  <li><strong>Security</strong> - What are the security practices in place for all APIs, and what is the current status of security.</li>
  <li><strong>Breaches</strong> - When there is a breach, what is the protocol, and practices surrounding what should happen–where is the historical data as well.</li>
  <li><strong>Terms of Service</strong> - What does terms of service across many APIs look like.</li>
  <li><strong>Privacy Policy</strong> - How is privacy protected across API operations, and how is this impacting operations.</li>
  <li><strong>Support</strong> - What are all the expected support channels, where are they located, and what the current metrics are.</li>
  <li><strong>Training</strong> - A detailed training walk-through for anyone looking to understand governance.</li>
  <li><strong>Certification</strong> - Providing certification for API publishers as well as API consumers when it comes to their platform participation.</li>
  <li><strong>Executive</strong> - A robust walk-through of the concepts at play for an executive from the 100K view of things.</li>
</ul>

<p>Your API delivery, consumption, and management house needs to be in order before you can begin realizing governance across everything. For many API providers it can be difficult to step back and look at things from the 100K level. However, increasingly it will become critical to make time to invest in API governance, otherwise you end up dealing with lower level fires that could have been addressed with a little more planning and coordination.

<h2 id="many-api-life-cycles">Many API Life Cycle(s)</h2>
<p>I’ve historically visualized the API lifecycle as a set of linear stops along a single life cycle, however over the years, as the world of APIs have come into better focus, I’m seeing the life cycle as a set of many different life cycles that work in concert for API provider as well as API consumers—cause, we all should really be both. I acknowledge that there are many different possible iterations upon the different types of API life cycles that will exist across companies, organizations, institutions, and government agencies. It will all depend on the expertise, services, tooling, and resources groups have when it comes to delivering, consuming, managing, and governing the APIs they depend on. However, I’m at least looking to define at least a handful of templates that can be used to guide API operators who are wanting to see the bigger picture.

<p>Next, I will work on a visual to go with these API lifecycle templates. I’m going to be sticking with my transit map approach to defining the API landscape, and plot these different API life cycles as different lines within the same overall landscape map. Working to show how they overlap and interact. My biggest challenge in all of this is to be able to articulate that this lifecycle is applied across many different APIs, and potentially many different teams publishing APIs, as well as many different types of consumers putting them to work. Historically I have tried to auto-generate these API transit maps, but for this work I think I will just hand-craft the maps to reflect the API life cycles I have defined. Providing a more universal way in which to help articulate, visualize, explore and learn more about how the API life cycle(s) work.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/08/21/multiple-overlapping-api-life-cycles/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/08/20/the-api-conferences-i-am-tracking-on-for-the-fall/">The API Conferences I Am Tracking On For The Fall</a></h3>
        <span class="post-date">20 Aug 2019</span>
        <p>As we approach the fall it is time to begin thinking about the conference season, and what the most relevant API conferences are. I haven’t been doing any events this year, but staying in tune with the conference circuit has always been important to my work. Who knows, maybe I will be spend some more time investing in API related events after taking a break for six month. When it comes to API events and conferences, here is what I am tracking on.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/08/20/the-api-conferences-i-am-tracking-on-for-the-fall/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/08/20/human-empathy-is-one-of-my-most-important-api-outreach-tools/">Human Empathy Is One Of My Most Important API Outreach Tools</a></h3>
        <span class="post-date">20 Aug 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/abe-lincoln-one-yellow-collage-file-00-00-00-00.jpg" width="45%" align="right" style="padding: 15px;" />
I am an empathic human being. It is one of my top strengths, as well as one of my top weaknesses. It is also one of the most important tools in my API toolbox. Being able to understand the API experience from the position of different people throughout the world of APIs is a cornerstone of the API Evangelist brand. Personally, I find APIs themselves to be empathy triggering, and something that has regularly forced me out of my silos, then allowing me t put myself in the shoes of my consumers. Something that when realized in a perpetual fashion can become a pretty powerful force for dialing in the services you offer, and establish, maintain, and strengthen connections with other people within the community.

<p>Being able to listen to people in the hallways of conferences, and within the meeting rooms across enterprise, institutions, and government agencies, then internalize, process, and position my writing from what I learn from people is how I have written on API Evangelist for the last nine years. I rarely am positioning my narrative my own vantage point, or that of a company. Most of the time I am channeling someone I’ve met along the way, speaking from their perspective, and analyzing the world of APIs as they would see it. While I wish that the world always resembled my view of the API landscape, from experience I know better, and that there are many diverse ways of seeing the value or damage APIs are responsible for.

<p>While API design, and the overall user experience around API service and tooling goes a long way to speak to end-users, I still think the human touch, and positioning our messaging from the vantage point of our consumers will have the greatest impact. Making a person connection will last much longer than any single blog post, advertisement, Tweet, image, video, or other common unit of engagement. Of course, what you gather from putting yourself in the shoes of your consumers should feed into all of these engagement areas, but ensuring they are rooted in the reality of consumers, possess the right amount of context, and speak in a personal tone will be critical to completing the empathic loop set into motion when talking with customers and conferences and within the companies, organizations, institutions, and government agencies where users work.

<p>The downside of relying on empathy is it can be exhausting, and it is only something you can properly accomplish if you care, making it very challenging to scale. I think many people can be taught empathy, but some will never get it, and even fewer will be really good at it. Those who are best at it will burnout much quicker, and will need more careful oversight, as well as opportunities to recharge. However, if you can work to ensure your in-person evangelism approach is empathy centered, and you work to weave what you learn into your overall messaging, outreach, and engagement practices, it can make a serious impact. This type of outreach can’t be faked. It has to come from an honest place. Which can be hard to find depending on where you work, the type of environment that exists, as well as the industries you target. I know that many folks who read this will dismiss this as too simplistic, and not easily measured, but I know from experience it is the most important thing I can bring to the table when reaching out to my audience.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/08/20/human-empathy-is-one-of-my-most-important-api-outreach-tools/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/08/19/postman-collection-as-a-single-quantifiable-shareable-executable-unit-of-representation-for-any-digital-capability/">Postman Collection As A Single Quantifiable, Shareable, Executable Unit Of Representation For Any Digital Capability</a></h3>
        <span class="post-date">19 Aug 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/stories-gears-numbers-blue.jpg" width="45%" align="right" style="padding: 15px;" />
In my world API definitions are more valuable than code. Code is regularly thrown away and rewritten. API definitions hold the persistent detail of what an API delivers, and contain all of the proprietary value when they are properly matured. OpenAPI has definitely risen to the top when it comes to which API definition formats you should be using, however, Postman Collections have one critical ingredient that makes them ultimately more usable, sharable, and meaningful to developers—-environmental context. This small but important difference is what makes Postman Collections so valuable as a single quantifiable, shareable, executable unit of representation for any digital capability.

<p>Like OpenAPI, Postman Collections describe the surface area of a web API, but they have that added layer to describe the environment you are running in, which makes it much more of a run-time and execute-time experience. This may seem like a minor detail, but developers who want instant gratification, a Postman Collection bundled with the Postman API lifecycle tooling, makes for a pretty powerful representation of a company’s, organization’s, institutions’s, or government agency’s digital capability. Allowing for API providers (or consumers) to describe what an API does in a machine readable format, bundle with it the environment context to actually execute the digital capability, and enable the unit of value to be realized within the Postman API development ecosystem.

<p>I can take any of my internal, or 3rd party public APIs I depend on, make successful calls to them, including authentication, tokens, and other environment variables, then export as a portable Postman Collection, and share with anyone I want using a simple URL, or by embedded the Run in Postman button within documentation, blog posts, and other resources. Then, any potential consumer can take that Postman Collection, load into their Postman client, and be able to realize the same digital capability I was using—-no documentation, on-boarding, or other friction required. You get instant gratification regarding putting the digital capability to work, exactly as I intended. This quantifiable, shareable, and executable nature of Postman Collections is what elevates them to a position held by no other API definition format out there.

<p>It is this dance between machine readable API definition, and API lifecycle tooling (client, documentation, testing, mocking, etc.), linked together with the environment context that continues to ensure Postman captures my attention. Ensuring the technical details of your API is captured in a machine readable format is something I don’t think all API providers fully comprehend. Enabling developers to put an API to work for them in a single click, instead of the usual on-boarding dance, digestion of documentation, selection of relevant programming language SDK, and other cognitive load associated with API integration—-is critical! APIs represent the digital capabilities of your company, organization, institution, or government agency—-Postman Collections are how you ensure your capabilities are quantified, shared, and executed by internal and 3rd party developers.

<p><i><strong>Disclosure:</strong> Postman is an API Evangelist sponsor.</i>


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/08/19/postman-collection-as-a-single-quantifiable-shareable-executable-unit-of-representation-for-any-digital-capability/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/08/19/a-second-wave-of-api-management-is-going-on/">A Second Wave of API Management is Going On</a></h3>
        <span class="post-date">19 Aug 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/stories-beach-rocks-currents-internet-numbers.jpg" width="45%" align="right" style="padding: 15px;" />
I fully surfed the first wave of API management. API Evangelist began by researching what Mashery, Apigee, and 3Scale had set into motion. API Evangelist continued to has exist through funding from 3Scale, Mulesoft, WSO2, and continues to exist because of the support of next generation providers like Tyk. I intimately understand what API management is, and why it is valuable to both API providers and consumers. API management is so relevant as infrastructure it is now baked into the AWS, Azure, and Google Clouds. However, if you listen to technological winds blowing out there, you will mostly hear that the age of API management is over with, but in reality it is just getting started. The folks telling these tales are purely seeing the landscape from an investment standpoint, and not from an actual boots on the ground within mainstream enterprise perspective—something that is going to burn them from an investment standpoint, because they are going to miss out on the second wave of API management that is going on.

<p>The basics of API haven’t changed from the first to the second wave, so let’s start with the fundamental building blocks of API management before I move into describing what the next wave will entail:

<ul>
  <li><strong>Portal</strong> - A single URL to find out everything about an API, and get up and running working the resources that are available.</li>
  <li><strong>On-Boarding</strong> - Think just about how you get a new developer to from landing on the home page of the portal to making their first API call, and then an application in production.</li>
  <li><strong>Accounts</strong> - Allowing API consumers to sign up for an account, either for individual, or business access to API resources.</li>
  <li><strong>Applications</strong> - Enable each account holder to register one or many applications which will be putting API resources to use.</li>
  <li><strong>Authentication</strong> - Providing one, or multiple ways for API consumers to authenticate and get access to API resources.</li>
  <li><strong>Services</strong> - Defining which services are available across one or many API paths providing HTTP access to a variety of business services.</li>
  <li><strong>Logging</strong> - Every call to the API is logged via the API management layer, as well as the DNS, web server, file system, and database levels.</li>
  <li><strong>Analysis</strong> - Understanding how APIs are being consumed, and how applications are putting API resources to use, identifying patterns across all API consumption.</li>
  <li><strong>Usage</strong> - Quantifying usage across all accounts, and their applications, then reporting, billing, and reconciling usage with all API consumers.</li>
  <li><strong>APIs</strong> - API access to accounts, authentication, services, logging, analysis, and usage of API resources.</li>
</ul>

<p>Many believe API management is primarily about securing APIs, with others seeing it purely as monetization, when in reality API management is about awareness. Establishing, maintaining, and evolving an awareness of the API-driven digital capabilities you possess, and how these capabilities are being applied on the desktop, within mobile phones, on Internet-connected devices, and at the network layer. Without this awareness you will not remain competitive in the online global economy. The first wave of API management was about selling these essential building blocks to the growing number of startups, and handful of progressive enterprise. The second wave of API management is about selling these building blocks to mainstream enterprises across staple industries like healthcare, banking, education, and beyond.

<p>API management remains the cornerstone of the API lifecycle, and while the first wave of API management providers will benefit from the second wave, it is the next generation of API management providers like Tyk and Kong who will truly reap the benefits. They are the ones who will be agile enough, aware enough, and innovative enough to meet the demands of mainstream enterprise companies, SMBs, and startups when it comes to delivering APIs at scale throughout their API journey. The core API management features will remain the essential building blocks that API management service providers will bring to the table, but there will be other areas in which rise to the occasion and serve.

<ul>
  <li><strong>Discovery</strong> - Helping enterprise make sense of the growing number of digital resources they possess.</li>
  <li><strong>Service Mesh</strong> - Establishing a fabric of services that are resilient, scalable, and meet consumer needs.</li>
  <li><strong>Micro</strong> - Possessing a light footprint so it can be deployed anywhere, by anyone looking to put to work.</li>
  <li><strong>Regional</strong> - the ability to rapidly deploy and scale to meet the specific needs of regional use cases.</li>
  <li><strong>Transformations</strong> - Leaning on the API management to evolve, transform, and move APIs forward.</li>
  <li><strong>Extensible</strong> - The ability to extend the capabilities within the API management layer for the long tail.</li>
</ul>

<p>These are just a handful of the API management features that next generation API service providers will need to possess. While the established API management providers will be able to deliver in some of these areas, they ultimately will not be able to move fast enough, and direct investment properly within these areas. I also worry that even some of the next generation solutions won’t be able to get the investment they need with the current perspective being that we exist in a post API management phase. Sadly, I think this is the reality of a world that is heavily influenced, driven, and captured by investment ideology. The result is that the solutions being delivered are out of touch with what enterprises actually need on the ground, and startups that chase the investment money are always on to the next big thing, leaving significant chunks of change on the table.

<p>For me, not much has changed since 2010 when it comes to APIs. I’d say the API lifecycle has expanded and come into focus a little bit more, but nothing revolutionary and only a handful of things that are evolutionary have actually emerged. The major shift in the landscape that has occurred is that in 2014 I was still talking to mostly startup, and in 2019, I’m mostly talking to enterprise. The mainstream enterprise has woken up to the potential of APIs, and they are needing the expertise, tooling, and services to get the job done. They need the awareness that API management brings, and to make sense of their digital capabilities. API management is a fundamental building block of the API economy, and just because the leading providers have been acquired and baked into the cloud doesn’t mean the opportunity is gone and over with. If you survived the SOA evolution, you know that there are plenty of building blocks being repurposed for a cloud, mobile, and device world—this is a world where API awareness is essential, and API management is how you achieve this level of awareness.

<p><i><strong>Disclosure:</strong> Tyk is an API Evangelist sponsor.</i>


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/08/19/a-second-wave-of-api-management-is-going-on/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/08/16/seeing-api-consumers-as-just-the-other-ones/">Seeing API Consumers As Just The Other Ones</a></h3>
        <span class="post-date">16 Aug 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/border-crossing-bordercrossing-dali-three.png" width="45%" align="right" style="padding: 15px;" />
As API providers, it can be easy to find ourselves in a very distant position from the consumers of our APIs. In recent weeks I have been studying the impacts of behavioral approaches to putting technology to work, something that has led me to the work of Max Meyer, and <a href="https://www.amazon.com/Psychology-Other-One-1921-Friedrich-Meyer/dp/1112557342?SubscriptionId=AKIAILSHYYTFIVPWUY6Q&amp;tag=duckduckgo-ffab-20&amp;linkCode=xm2&amp;camp=2025&amp;creative=165953&amp;creativeASIN=1112557342">his Psychology of the Other-One (1921)</a>. I haven’t read his book yet, but have finished other works citing his work on how to “properly” study how animals (including humans) behave. While the psychological impact of all of this interests me, I’m most interested in how this perspective has amplified and driven how we use technology, and specifically how APIs can be used to create or bridge the divide between us (API providers) and our (API consumers).

<p>While web and mobile technology is often portrayed as connecting and bringing people together, it also can be used to establish a separation between providers and consumers. We often get caught up in the scale and growth of delivering API infrastructure, and we forget that our API consumers are humans, and we can end up just seeing them as personas, humans, or just a demographic. Of course, as API providers, we can’t be expected to make a direct connection with every single consumer, but we also have to be wary of becoming so distant from their reality that we can’t make a connection with them at all. Leaving our products, services, and tooling something that doesn’t serve them in any way, and we fail to meet our own business objectives behind what we were building in the first place.

<p>There will aways be some distance between API provider and consumer. However, we have to regularly work to narrow this divide, otherwise negative forces can make their way in between us and consumers. If we simply see your API consumers and end-users as the “other ones”, it will make supporting, and investing in their success much more difficult. Trust with API consumers, and the end-users of the applications they develop is tough to achieve, and even harder to maintain-—something that is increasingly more difficult when you simply see them as the other ones, those over there, and just nameless faceless database entries. It is our job as API community managers, customer success engineers, evangelists, and marketers to ensure that this all to common divide doesn’t grow between us and our consumers.

<p>Technology has the potential to bring us together, and connect us in many new and interesting ways-—APIs are at the center of this technological evolution. However, without the proper care and attention, it also has the potential to push us further apart. Dehumanizing people along the way, and reducing them simply to database entries or just a series of transactions. As evangelists, we can’t let this happen. We have to work extra hard to get to know our consumers, and reach out to better understand who they are, what they need, and ensure we are in tune with their view of the API resources we are making available. This is something that applies to our intentional, unintentional, and malicious API consumers. The more visibility we have into who our API consumers are, and what they are up to, the more success we will have in achieving our objectives, and sensibly scaling our communities-—keeping the balance between us the API provider, and our consumers, and seeing them as more than just the “other ones”.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/08/16/seeing-api-consumers-as-just-the-other-ones/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/08/16/four-phases-of-internal-api-evangelism/">Four Phases Of Internal API Evangelism</a></h3>
        <span class="post-date">16 Aug 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-stories-new-van-gogh-starry-night-container-bridge-2.jpg" width="45%" align="right" style="padding: 15px;" />
General evangelism around what APIs are, as well as more precise advocacy around specific APIs or groups of API resources takes a lot of work, and repetition. Even as a seasoned API evangelist I can never assume my audience will receive and understand what it is that I am evangelizing, and I regularly find myself having to reassess the impact (or lack of) that I’m making, retool, refresh, and repeat my messaging to get the coverage and saturation I’m looking for. After a decade of doing this, I cannot tell which is more difficult, internal or external public evangelism, but I do find that after almost 10 years, I’m still learning from each audience I engage with—-proving to me that no single evangelism strategy will ever reliably work, and I need a robust, constantly evolving toolbox if I am going to be successful.

<p>I have many different tools in my internal evangelism toolbox, but I find that the most important aspect of what I do is repetition. I never assume that my audience understand what I’m saying after a single session, or simply by reading one wiki page, blog post, or white paper. Quality internal evangelism requires regular delivery and enforcement, and an acknowledgement that my first engagements with teams will not have the impact I desire. When it comes to internal evangelism, I tend to encounter the following phases when engaging with internal teams:

<ul>
  <li><strong>Silence</strong> - The first time I present material to internal teams I almost always encounter silence. Teams will often listen dutifully, but rarely will engage with me, ask questions, and want to get to the details of what is going on. I can rarely assess the state of things, and find that the silence stems from a range of emotions, ranging from not caring at all, to being very distrustful of what I am presenting. I can never assume that teams will care, trust me, and that silence is always a sign of the work that lies ahead of me, and I immediately get to work scheduling additional sessions with each team I’m trying to reach.</li>
  <li><strong>Challenges</strong> - Usually by our second or third encounter with internal teams I will begin to get a little more than just silence, however it almost always comes in some aggressive form. Developers love to challenge what you are proposing, tearing things down before they ever understand what is happening. It is actually part of the natural cycle for them. Technical folks are used to taking things apart, ripping them into pieces, to see what they are all about, and what they are made of. It is easy to take this type of response in a negative and personal way, but it is the way technical minded folks see the world, especially when faced with something they do not understand, and are uncomfortable with. To survive this phase you have to have a lot of self-confidence and know your stuff, otherwise you will be eaten alive.</li>
  <li><strong>Questions</strong> - After surviving the aggressive challenges about what APIs are all about and what they mean to a company, organization, institution, or government agency, I usually begin getting some more constructive questions. Moving beyond the desire to rip you to pieces, and actually start the process of actually understanding what is happening when it comes to providing and consuming of APIs. Again, you have to really know your stuff to be able to survive this line of questioning from often very smart, very technical, and inquisitive folks. However, if you come prepared, this is where you start seeing the ROI on your internal evangelism efforts.</li>
  <li><strong>Engagement</strong> - I usually do not see real engagement from teams for at least 1-6 sessions. Depending on the team, they’ll take more or less time to get through the painful aspects of understanding what is going on. Depending on the type of development team, what their experience levels are, and the environments they’ve been working in, they will respond very differently. You will have to be patient to be able to reach the phase where teams actually become engaged, are able to contribute to the conversation, and take what you’ve presented and apply it to their daily work. Moving beyond just evangelism, and actually beginning to see real business value from sharing of API knowledge.</li>
</ul>

<p>Depending on the organization culture, these four phases could take days, weeks, or months. Not all teams will be ready for what you are evangelizing. You cannot assume that teams understand what you are talking about, and that they see you as a messenger of a positive future. I’d say about 70% of the time I am seen as a hostile actor, coming in to disrupt, change ,and mess with people’s world. I don’t care how well honed my message, materials, and vision is, if I cannot connect with teams on a human and professional level–I will fail. I’ve been practicing my delivery of 101, intermediate, and advanced API material for a decade, and I still get eaten alive on a regular basis within startups, the enterprise, government agencies, and at conferences. There is no amount of preparation that will shield you from the intensity that internal development teams can bring to the table–this game isn’t for rookies.

<p>Internal API evangelism and advocacy within the enterprise is not something you can do from high up on the mountain. You have to be able to come down from your management, architect, and executive horse, and be able to see things from the view of those in the trenches trying to get work done on a daily basis. If you can’t be seen as someone looking to build bridges between management and development ranks, your information will never be received. No amount of evangelism will cross the Grand Canyon that exists between business and technical groups in some organizations, if you aren’t willing to build bridges. Something that will take some serious planning, repetition, and tactical agility when it comes to the delivery of relevant information that is tailored for your intended audience. Internal evangelism is hard work, and something that will take regular auditing and retooling before you will ever see the impact you desire.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/08/16/four-phases-of-internal-api-evangelism/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/08/15/an-observable-regulatory-provider-or-industry-api-management-gateway/">An Observable Regulatory Provider Or Industry API Management Gateway</a></h3>
        <span class="post-date">15 Aug 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/35201856153_61bc075e4b-nazi-invasion.jpg" width="45%" align="right" style="padding: 15px;" />
I wrote a separate piece on an API gateway specification standard recently, merging several areas of my research and riffing on a recent tweet from Sukanya Santhanam (@SukanyaSanthan1). I had all these building blocks laying around as part of my research on API gateways, but also from the other areas of the API lifecycle that I track on. Her tweet happened to coincide with other thoughts I had simmering, so I wanted to jump on the opportunity to publish some posts, and see if I could slow jam a conversation in this area. Now, after I defined what I’d consider to be a core API gateway set of building blocks, I wanted to take another crack at refining my vision for how we make it more observable and something that could be used as a tech sector regulatory framework.

<p>Copying and pasting from my API gateway core specification, here is what my v1 draft vision for an API gateway might be:

<ul>
  <li><strong>Paths</strong> - Allowing many different API paths to exist.</li>
  <li><strong>Schema</strong> - Allowing me to manage all of my schema.</li>
  <li><strong>Integrations</strong> - Providing backend lego architecture.
    <ul>
      <li><strong>Resource</strong> - Allow for integration with other APIs.</li>
      <li><strong>Database</strong> - Provide a stack of database integrations.</li>
      <li><strong>Other</strong> - Define whole buffet of integration definitions.</li>
    </ul>
  </li>
  <li><strong>Requests</strong> - Define all of my HTTP 1.1 requests
    <ul>
      <li><strong>Methods</strong> - Providing me with my HTTP verbs.</li>
      <li><strong>Path Parameters</strong> - Able to define path parameters.</li>
      <li><strong>Query Parameters</strong> - Able to define query parameters.</li>
      <li><strong>Bodies</strong> - Providing control over the request body.</li>
      <li><strong>Headers</strong> - Full management of HTTP request headers.</li>
      <li><strong>Encoding</strong> - Defining the media types in in use for requests.</li>
      <li><strong>Validate</strong> - Providing validation for all incoming requests.</li>
      <li><strong>Mappings</strong> - Allowing for mapping of requests to backend.</li>
      <li><strong>Transformations</strong> - Transformation before sending to backend..</li>
      <li><strong>Examples</strong> - Ensuring there are samples for each request.</li>
      <li><strong>Schema</strong> - Able to reference all schema used in requests.</li>
      <li><strong>Tags</strong> - Being able to organize API requests using tags.</li>
    </ul>
  </li>
  <li><strong>Responses</strong> - Define all of my HTTP 1.1 responses.
    <ul>
      <li><strong>Status Code</strong>s - Providing the ability to define HTTP status codes.</li>
      <li><strong>Headers</strong> - Full management of all HTTP response headers.</li>
      <li><strong>Encoding</strong> - Defining the media types in in use for responses.</li>
      <li><strong>Schema</strong> - Able to reference all schema used in responses.</li>
      <li><strong>Examples</strong> - Ensuring there are samples for each response.</li>
    </ul>
  </li>
  <li><strong>Stages</strong> - Able to stage APIs under any platform defined environment.</li>
  <li><strong>Publishing</strong> - Allowing for conscious publishing of APIs into production.</li>
  <li><strong>Versioning</strong> - Providing semantic versioning as header or in the path.</li>
  <li><strong>Policies</strong> - Defining policies for API, and schema access by consumers.</li>
  <li><strong>Licensing</strong> - Ensure that data and APIs are properly licensed for consumption.</li>
  <li><strong>Plans</strong> - Crafting a handful of standard access tiers for different consumers.</li>
  <li><strong>Rate Limiting</strong> - Define the rate limits for all APIs within each plan offered.</li>
  <li><strong>Domains</strong> - Allow for default and custom domains associated with APIs.</li>
  <li><strong>Certificates</strong> - Provide management and usage of certificates for encryption.</li>
  <li><strong>Tags</strong> - Allow APIs, as well as their individual paths, and requests to be tagged.</li>
  <li><strong>Dependencies</strong> - Inform on the dependencies between APIs, including 3rd party.</li>
  <li><strong>Regions</strong> - Allow for multi-region deployment of APIs, with full DNS support.</li>
  <li><strong>Contact</strong> - Ensure there is contact information for every API owner.</li>
  <li><strong>Logging</strong> - Standardize the logging for all API traffic to one or many locations.</li>
  <li><strong>Monitoring</strong> - Provide basic monitoring of all APIs from alternate locations.</li>
  <li><strong>Status</strong> - Offer a real time status dashboard and notification for all APIs.</li>
  <li><strong>Terms of Service</strong> - Allow for the publishing of one or many TOS applying to APIs.</li>
  <li><strong>Authentication</strong> - Provide a handful of standard authentication mechanisms.</li>
  <li><strong>Authorization</strong> - Enable fine grade authorization across APIs and schema.</li>
  <li><strong>Consumers</strong> - Allow for consumers to sign up and maintain access accounts.</li>
  <li><strong>Keys</strong> - Require consumers define their applications and use API keys with API calls.</li>
  <li><strong>Documentation</strong> - Automatically publish documentation for all APIs that are published.</li>
  <li><strong>Reporting</strong> - Provide reporting on all gateway activity across each API and the lifecycle.
    <ul>
      <li><strong>Platform</strong> - Deliver platform specific API consumption report.</li>
      <li><strong>Consumer</strong> - Provide consumer specific API consumption reports.</li>
    </ul>
  </li>
</ul>

<p>That is a pretty long list. I know there are other building blocks missing, but this represents my first pass through my API research. It reflects the building blocks I want available when I put an API gateway to work in any cloud, on-premise, or on-device use case. However, in this post I wanted to go beyond what I’d consider the core API gateway, and talk about how we make it more observable, and something that could be applied to regulate an industry. Not something that happens behind the scenes, but something that happens out in the open, bringing in some sunlight, and pulling back the curtain on the black boxes some companies enjoy operating. While this won’t solve all our problems, I think it will provide a pretty good first step towards bringing some much needed observability to the tech sector, using common solutions that already exist. Here are a handful of building blocks I think could contribute to this conversation.

<ul>
  <li><strong>Open Source</strong> - Ensure that the gateway is open source, and something that is developed out in the open, and can be forked an operated by anyone.</li>
  <li><strong>Organizational</strong> - Provide the suggested framework for the operating organization, and what staffing and other resources are required to operate an instance of this model, providing a (hopefully) neutral entity to bring to life.</li>
  <li><strong>Provider Directory</strong> - Require that all providers who have APIs published as part of the platform be profiled and available within a single directory, breaking down the resources they have published, as well as usage, monitoring, and other relevant information.</li>
  <li><strong>Consumer Directory</strong> - Require that all consumers who have access to the platform publish a profile, share how their are using APIs, and offer a summary of their authentication, authorization, usage levels, and data points.</li>
  <li><strong>Research Access</strong> - Provide access to researchers as part of the consumer management, but offering wider access to platform data and consumption based upon agreed upon plans.</li>
  <li><strong>Media Access</strong> - Allow for access my media organization who are looking to understand what is happening across a platform, and the impact on consumers via applications.</li>
  <li><strong>Industry Access</strong> - Provide access to industry analysts as part of the consumer management, but offering wider access to platform data and consumption based upon agreed upon plans.</li>
  <li><strong>Auditor Access</strong> - Provide complete auditor access to the entire platform, allowing certified auditors to come in and review resources, consumers, logs, and any other aspects of the gateway operations.</li>
  <li><strong>Schema Catalog</strong> - A complete catalog of all the types of data that is tracked and made available across all APIs, and used in desktop, web, mobile, and device applications.</li>
  <li><strong>Monetization</strong> - Allow for the monetization of provider participation, consumer, researcher, and industry access, allowing for well defined plans, rate limits, and observability into revenue that is generated.</li>
  <li><strong>Issues</strong> - Provide the ability for the public, consumers, researchers, and industry to submit issues in a safe, moderated, and accessible space, ensuring there is observability into all issues across the gateway.</li>
  <li><strong>Disputes</strong> -  Ensure there are trained professionals to help address disputes on the platform rising from issues reported by the public, consumers, researchers, and industry analysts using the gateway.</li>
  <li><strong>Reporting</strong> - Provide reporting to key stakeholders.
    <ul>
      <li><strong>Private</strong> - Provide comprehensive reporting accessible to trusted internal stakeholders.</li>
      <li><strong>Public</strong> - Publish a line of regular public reports on platform usage and consumption.</li>
      <li><strong>Researcher</strong> - Provide a set of reports just for researchers based upon plans and agreements.</li>
      <li><strong>Media</strong> - Provide a rich set of reports that help keep media understanding what is going on.</li>
      <li><strong>Auditors</strong> - Give auditors a full set of reports, including summary and detail access.</li>
    </ul>
  </li>
</ul>

<p>This model isn’t without precedent. Last year I spent a couple months studying the approach by UK regulators to bring more observability into the banking sector, and the formation of Open Banking organization (https://www.openbanking.org.uk/). Learning more about what open banking was in the UK (http://apievangelist.com/2018/02/21/what-is-open-banking-in-the-uk/),  how it provides a common API definition (http://apievangelist.com/2018/02/21/open-banking-in-the-uk-openapi-template/), who the common stakeholders were (http://apievangelist.com/2018/02/26/the-banking-api-actors-in-the-uk/), and exploring how I could emulate this approach as an open source API industry template (http://open.banking.blueprint.apievangelist.com/). In 2019, I want to go even further with this open source API Blueprint, understand how it can be used to define a common open source API gateway specification, while adding the necessary building blocks to ensure there is observability at the industry level.

<p>I am proposing that this model be defined, standardized, and applied at the single provider, or industry level. If required, an independent entity can be setup to operate the API gateway as an independent platform, funded by regulators, and the monetization layer that leverages a mix of providers, consumers, researchers, and industry analyst access to the platform. While there will be private layers of the platform, the goal is to provide as much as possible out in the open, operating as many public API platforms have been operating for the last 20 years. Ironically, some of the worst behaved technology platforms have operated using this model in the past, but sadly have been actively tightening down access levels. I’m proposing we take their formula, open source it, and operate it independently, out in the open, and make them pay for it.

<p>As I said before, this will not solve all problems. However it will define a layer that ALL desktop, web, mobile, and device applications can be required to go through. Requiring that provides only develop applications on top of APIs deployed within regulated API gateways. Requiring that all internal, partner, and 3rd party public consumers access API via the single, or regionally distributed gateways. If auditors ever find that an API provider is leverage APIs not listed in the API gateway directory, or their partners and the public have access to data that is not defined in schema within the API gateway—then there is a problem, and enforcement can follow. Of course, all of this isn’t as simple as proposed in this post, but it provides the scaffolding and blueprint for how it can be applied. There is no reason this can’t be applied to regulate technology companies, and applied to existing industries that are already regulated, helping bring more observability into already existing regulatory practices.

<p>Nothing I’m proposing here is rocket science, or theoretical. It is based upon proven practices of tech companies. All I’m saying now, is rather than just advising companies, organizations, institutions, and government follow these best practices, I’m saying that we should begin working to establish a standard, and craft policy that requires everyone to participate. I know many tech folks don’t like the idea of regulation, but for certain industries, and certain platforms, it might be a positive thing to inject some regulation into the equation, and begin doing things out in the open, rather than behind the curtain. If you have any questions or comments on this blueprint, feel free to email me at <a href="/cdn-cgi/l/email-protection" class="__cf_email__" data-cfemail="1970777f76597869707c6f78777e7c75706a6d377a7674">[email&#160;protected]</a>, or submit an issue on the GitHub repository for this blueprint.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/08/15/an-observable-regulatory-provider-or-industry-api-management-gateway/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/08/15/an-api-platform-reliability-engineer-at-stripe/">An API Platform Reliability Engineer At Stripe</a></h3>
        <span class="post-date">15 Aug 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-stories-algo-microservices.jpg" width="45%" align="right" style="padding: 15px;" />
I find that the most interesting and telling API building blocks come out of the companies who are furthest along in their API journey, and have made the conscious effort to heavily invest in their platforms. I try to regularly visit API platforms who are doing the most interesting work on a regular basis, because I am almost always able to find some gem of an approach that I can showcase here on the blog.

<p>This weeks gem is from <a href="https://g.co/kgs/wra6JH">API rockstar Stripe, and their posting for a reliability engineer for their API platform</a>. Here is a summary from their job posting:

<p><i>As a Reliability Engineer, you’ll help lead an active area of high impact by defining and building proactive ways to further hone the reliability of our API. You’ll collaborate with team members across Engineering, as well as with our business, sales and operations teams to determine areas of greatest leverage.</i>

<p><i>You Will:</i>

<ul>
  <li><i>Work with engineers across the company to identify key areas for reliability improvement.</i></li>
  <li><i>Gather requirements and make thoughtful tradeoffs to ensure we are focusing our efforts on the most impactful projects.</i></li>
  <li><i>Work on services and tools to proactively improve the quality and reliability of our production API.</i></li>
  <li><i>Debug production issues across services and multiple levels of the stack. Improve operational standards, tooling, and processes.</i></li>
</ul>

<p>I’ve studied thousands of APIs over almost a decade, and seeing a company invest this heavily in API reliability is a rare thing. For me, this demonstrates two things, that Stripe takes their API seriously, and that it takes a huge amount of investment and resources to do APIs right. Something I don’t think many API providers realize as they try to do APIs as a side project, and wonder why they aren’t seeing the results they want.

<p>I find that the job postings for API providers is one of the most telling signals I can harvest to understand how committed a company is to their APIs. Human Resources is one of the most areas to be investing in when it comes to your API operations, and the frequency and type of API job postings tells a lot about the API journey a company is on. Hiring API engineers is an important role to be hiring for, but it will also take hiring someone dedicated to reliability to make the impact with your platform that you desire.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/08/15/an-api-platform-reliability-engineer-at-stripe/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/08/05/some-of-the-api-gateway-building-blocks/">Some Of The API Gateway Building Blocks</a></h3>
        <span class="post-date">05 Aug 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/udnie-IMG_8312.jpg" width="45%" align="right" style="padding: 15px;" />
The inspiration for this post wasn’t fully mine, I’m borrowing and building upon what Sukanya Santhanam (@SukanyaSanthan1) <a href="https://twitter.com/SukanyaSanthan1/status/1151424256300859392">tweeted out the other day</a>. It is a good idea, and something that should be open sourced and moved forward. I’ve been studying with API management since 2010, and using gateways for over 15 years. I’ve watched gateways evolve, and partnered regularly with API management and gateway providers (Shout out to Tyk). Modern API gateways aren’t your grandfather’s SOA tooling, they’ve definitely gone through several iterations. While I still prefer hand rolling and forging my APIs out back in my woodshed on an anvil, I find myself working with a lot of different API gateways lately.

<p>I’ve kept feeling like I needed to map out the layers of what I’d consider to be a modern API gateway, and begin providing links to the most relevant API gateways out there, and the most common building blocks for an API gateway. Now that you can find API gateways baked into the fabric of the cloud, it is time that we work to standardize the definition of what they can deliver. I’m not looking to change what already is. Actually, I’m looking to just document and build on what already is. As with every other stop along the API lifecycle I’m looking to just map out the common building blocks, and establish a blueprint going forward the might influence existing API gateway providers, as well as any newcomers.

<p>After going through my <a href="http://gateway.apievangelist.com/">API gateway research</a> for a while, I quickly sketched out these common building blocks for helping deploy, manage, monitor, and secure your APIs:

<ul>
  <li><strong>Paths</strong> - Allowing many different API paths to exist.</li>
  <li><strong>Schema</strong> - Allowing me to manage all of my schema.</li>
  <li><strong>Integrations</strong> - Providing backend lego architecture.
    <ul>
      <li><strong>Resource</strong> - Allow for integration with other APIs.</li>
      <li><strong>Database</strong> - Provide a stack of database integrations.</li>
      <li><strong>Other</strong> - Define whole buffet of integration definitions.</li>
    </ul>
  </li>
  <li><strong>Requests</strong> - Define all of my HTTP 1.1 requests
    <ul>
      <li><strong>Methods</strong> - Providing me with my HTTP verbs.</li>
      <li><strong>Path Parameters</strong> - Able to define path parameters.</li>
      <li><strong>Query Parameters</strong> - Able to define query parameters.</li>
      <li><strong>Bodies</strong> - Providing control over the request body.</li>
      <li><strong>Headers</strong> - Full management of HTTP request headers.</li>
      <li><strong>Encoding</strong> - Defining the media types in in use for requests.</li>
      <li><strong>Validate</strong> - Providing validation for all incoming requests.</li>
      <li><strong>Mappings</strong> - Allowing for mapping of requests to backend.</li>
      <li><strong>Transformations</strong> - Transformation before sending to backend..</li>
      <li><strong>Examples</strong> - Ensuring there are samples for each request.</li>
      <li><strong>Schema</strong> - Able to reference all schema used in requests.</li>
      <li><strong>Tags</strong> - Being able to organize API requests using tags.</li>
    </ul>
  </li>
  <li><strong>Responses</strong> - Define all of my HTTP 1.1 responses.
    <ul>
      <li><strong>Status Code</strong>s - Providing the ability to define HTTP status codes.</li>
      <li><strong>Headers</strong> - Full management of all HTTP response headers.</li>
      <li><strong>Encoding</strong> - Defining the media types in in use for responses.</li>
      <li><strong>Schema</strong> - Able to reference all schema used in responses.</li>
      <li><strong>Examples</strong> - Ensuring there are samples for each response.</li>
    </ul>
  </li>
  <li><strong>Stages</strong> - Able to stage APIs under any platform defined environment.</li>
  <li><strong>Publishing</strong> - Allowing for conscious publishing of APIs into production.</li>
  <li><strong>Versioning</strong> - Providing semantic versioning as header or in the path.</li>
  <li><strong>Policies</strong> - Defining policies for API, and schema access by consumers.</li>
  <li><strong>Licensing</strong> - Ensure that data and APIs are properly licensed for consumption.</li>
  <li><strong>Plans</strong> - Crafting a handful of standard access tiers for different consumers.</li>
  <li><strong>Rate Limiting</strong> - Define the rate limits for all APIs within each plan offered.</li>
  <li><strong>Domains</strong> - Allow for default and custom domains associated with APIs.</li>
  <li><strong>Certificates</strong> - Provide management and usage of certificates for encryption.</li>
  <li><strong>Tags</strong> - Allow APIs, as well as their individual paths, and requests to be tagged.</li>
  <li><strong>Dependencies</strong> - Inform on the dependencies between APIs, including 3rd party.</li>
  <li><strong>Regions</strong> - Allow for multi-region deployment of APIs, with full DNS support.</li>
  <li><strong>Contact</strong> - Ensure there is contact information for every API owner.</li>
  <li><strong>Logging</strong> - Standardize the logging for all API traffic to one or many locations.</li>
  <li><strong>Monitoring</strong> - Provide basic monitoring of all APIs from alternate locations.</li>
  <li><strong>Status</strong> - Offer a real time status dashboard and notification for all APIs.</li>
  <li><strong>Terms of Service</strong> - Allow for the publishing of one or many TOS applying to APIs.</li>
  <li><strong>Authentication</strong> - Provide a handful of standard authentication mechanisms.</li>
  <li><strong>Authorization</strong> - Enable fine grade authorization across APIs and schema.</li>
  <li><strong>Consumers</strong> - Allow for consumers to sign up and maintain access accounts.</li>
  <li><strong>Keys</strong> - Require consumers define their applications and use API keys with API calls.</li>
  <li><strong>Documentation</strong> - Automatically publish documentation for all APIs that are published.</li>
  <li><strong>Reporting</strong> - Provide reporting on all gateway activity across each API and the lifecycle.
    <ul>
      <li><strong>Platform</strong> - Deliver platform specific API consumption report.</li>
      <li><strong>Consumer</strong> - Provide consumer specific API consumption reports.</li>
    </ul>
  </li>
</ul>

<p>I’m going to add these to my API gateway research. I’m sure there are other building blocks out there, but I think this is a good start. It reflects what I think makes API gateways different from API management. It has the design, deployment, and backend integration portion of the conversation, as well as the key API management features expected. I see API gateways as a Venn diagram of API lifecycle features. Providing a single blueprint, tooling, and appliance that will help you deliver, manage, distribute, and scale your API infrastructure.

<p><a href="https://github.com/api-evangelist/api-gateway-blueprint">You can find this outline published over at GitHub</a>. I will me managing it as a living document and opening up to feedback via GitHub issues. I’m going to evolve this as a core API gateway specification—eventually defining APIs and schema for each layer of the stack. This will take some time because I will have to profile several of the existing API gateway APIs and mine them for logical patterns. Borrowing from their API designs, and schema, layering them together to create a common set of building blocks. I’m going to also begin iterating upon what I’d call an observable regulatory set of API gateway building blocks to augment this existing list. Establishing my vision of not just an API gateway standard, which can be used internally, as well as openly as part of an industry-wide effort to deliver consistent APIs for a collective purpose.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/08/05/some-of-the-api-gateway-building-blocks/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/08/05/a-look-at-behavioral-api-patents/">A Look At Behavioral API Patents</a></h3>
        <span class="post-date">05 Aug 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/udnie-DSC_0035-2.jpg" width="45%" align="right" style="padding: 15px;" />
I have been studying uses of behavioral technology lately. Riffing off my partner in crimes work on the subject, but putting my own spin on it, and adding APIs (of course) into the mix. Applying on of my classic techniques, I figured I’d start with a patent search for “behavioral application programming interfaces”. I find patents to be a “wonderful” source of inspiration and understanding when it comes to what is going on with technology. Here are the top results for my patent search, with title, abstract, and a link to understand more about each patent.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=1&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">User-defined coverage of media-player devices on online social networks</a><br />
In one embodiment, a method includes detecting, by a media-player device including multiple antennas, a client system of a user is within a wireless communication range of the media-player device. In response to the detection, the media-player device broadcasts an authentication key for the user of the client system. The media-player device then registers the user to the media-player device based on the authentication key being verified by the client system. The media-player device further receives from the client system instructions to adjust a power level of each of the multiple antennas. The instructions are determined based on broadcast signals received at the client system and on a respective position of the client system associated with each received broadcast signal. The respective position of the client system is determined with respect to a position of the media-player device.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=2&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Controlling use of vehicular Wi-Fi hotspots by a handheld wireless device</a><br />
A system and method of controlling use of vehicular Wi-Fi hotspots by a handheld wireless device includes: detecting that the handheld wireless device is communicating via a Wi-Fi hotspot; determining at the handheld wireless device that that the Wi-Fi hotspot is provided by a vehicle; and enabling one or more default prohibitions against transmitting low-priority data from the handheld wireless device via a cellular wireless carrier system while the handheld wireless device communicates with the Wi-Fi hotspot provided by the vehicle.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=3&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">System and method for collecting data</a><br />
The passive data collection method is sometime more reliable because the direct query method might not be available or possible through a 3.sup.rd party channel application. Accordingly, an improved data collection method is provided. The method includes: running a channel application located on a first layer of an operating system of a user device; receiving an application interface (API) call, from the channel application, for a graphic rendering module located on a second layer of the operating system, wherein the graphic rendering module is a non-video playback module; intercepting metadata sent to the graphic rendering module; determining identifying information of a content based on the intercepted metadata; and storing the determined identifying information of the content.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=4&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">System and method for analytics with automated whisper mode</a><br />
A service session is facilitated via a packet switched network; in the service session, user equipment participates in an interactive communication exchange with an agent via a first interaction mode, and the interactive communication exchange is based on a user inquiry. The interactive communication exchange is monitored and a determination is made that a consultation service would facilitate resolution of the user inquiry. A service resource is associated with the service session responsive to determining that the consultation service would facilitate the resolution; the service resource provides consultation to the agent via a second interaction mode without exposing the consultation to the user equipment. The consultation elevates an experience level employed in the first service session towards resolution of the user inquiry.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=5&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Method, device, and system for displaying information associated with a web page</a><br />
Embodiments of the present application relate to a method, device, and system for displaying information. The method includes receiving a web page access request, in response to receiving the web page access request, displaying a first web page and obtaining designated information associated with the first web page, the first web page being associated with the web page access request and the designated information including content of the first web page, receiving an instruction to navigate to a second web page, in response to receiving the instruction to navigate to the second web page, communicating the designated information to a server associated with the second web page, and displaying the second web page, the second web page including information communicated by the server associated with the second web page.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=6&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Large-scale page recommendations on online social networks</a><br />
In one embodiment, a method includes accessing user-concept scores for a first set of users, wherein each user-concept score is associated with a user-concept pair; calculating recommended user-concept scores for a subset of user-concept pairs in a second set of users. The first set of users may be discrete from the second set of users. A recommendation-algorithm may compute the recommended user-concept scores for a user-concept pair by optimizing an objective function comprising a plurality of predicted rating functions. Each predicted rating function may be determined using a user score, a concept score, a user-bias value associated with the user, as well as a concept-bias value associated with the concept. Finally, the method may include sending recommendations for one or more concepts based on the recommended user-concept scores for the second set of users.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=7&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Technologies for secure personalization of a security monitoring virtual network function</a><br />
Technologies for secure personalization of a security monitoring virtual network function (VNF) in a network functions virtualization (NFV) architecture include various security monitoring components, including a NFV security services controller, a VNF manager, and a security monitoring VNF. The security monitoring VNF is configured to receive provisioning data from the NFV security services controller and perform a mutually authenticated key exchange procedure using at least a portion of the provisioning data to establish a secure communication path between the security monitoring VNF and a VNF manager. The security monitoring VNF is further configured to receive personalization data from the VNF manager via the secure communication path and perform a personalization operation to configure one or more functions of the security monitoring VNF based on the personalization data. Other embodiments are described and claimed.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=8&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Systems and methods for implementing intrusion prevention</a><br />
System and methods are provided for implementing an intrusion prevention system in which data collected at one or more remote computing assets is analyzed against a plurality of workflow templates. Each template corresponding to a different threat vector and comprises: (i) a trigger definition, (ii) an authorization token, and (iii) an enumerated countermeasure responsive to the corresponding threat vector. When a match between the data collected at the one or more remote computing assets and a trigger definition of a corresponding workflow template is identified, an active threat is deemed to be identified. When this occurs the authorization token of the corresponding workflow template is enacted by obtaining authorization from at least two authorization contacts across established trust channels for the at least two authorization contacts. Responsive to obtaining this authorization, the enumerated countermeasure of the corresponding workflow template is executed.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=9&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Identity and trustworthiness verification using online and offline components</a><br />
Methods and systems for verifying the identity and trustworthiness of a user of an online system are disclosed. In one embodiment, the method comprises receiving online and offline identity information for a user and comparing them to a user profile information provided by the user. Furthermore, the user's online activity in a third party online system and the user's offline activity are received. Based on the online activity and the offline activity a trustworthiness score may be calculated.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=10&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Protecting sensitive information from a secure data store</a><br />
In embodiments of the present invention improved capabilities are described for the steps of receiving an indication that a computer facility has access to a secure data store, causing a security parameter of a storage medium local to the computer facility to be assessed, determining if the security parameter is compliant with a security policy relating to computer access of the remote secure data store, and in response to an indication that the security parameter is non-compliant, cause the computer facility to implement an action to prevent further dissemination of information, to disable access to network communications, to implement an action to prevent further dissemination of information, and the like.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=11&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Local data aggregation repository</a><br />
Apparatuses, systems, methods, and computer program products are disclosed for a local repository of aggregated data. A hardware device comprises a local repository of data aggregated, for a user, from a plurality of third party service providers. A hardware device comprises a local authentication module configured to secure, on the hardware device, aggregated data and electronic credentials of a user for a plurality of third party service providers. A hardware device comprises an interface module configured to provide access controls to a user defining which of a plurality of other third party service providers the user authorizes to access aggregated data, and to provide the aggregated data to the authorized other third party service providers.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=12&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Mapping and display for evidence based performance assessment</a><br />
A computer-implemented method for providing a user with a performance indicator score includes receiving a first transaction message that includes historical clinical-trial performance data from one or more processors at a clinical research organization and receiving a second transaction message with health records data with parameters indicative of insurance claims data. The received historical clinical-trial performance data and the prescription data is translated into an updated database. Related records within the updated database are identified and one or more key performance indicators included in the data at the updated database for a first physician are identified. A score for each of the one or more key performance indicators are calculated and a performance indicator score record for the first physician is generated based on the calculated scores for each of the one or more key performance indicators. A multi-dimensional chart for organizing and evaluating investigators is generated.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=13&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Programmable write word line boost for low voltage memory operation</a><br />
A system and method for efficient power, performance and stability tradeoffs of memory accesses under a variety of conditions are described. A system management unit in a computing system interfaces with a memory and a processing unit, and uses boosting of word line voltage levels in the memory to assist write operations. The computing system supports selecting one of multiple word line boost values, each with an associated cross-over region. A cross-over region is a range of operating voltages for the memory used for determining whether to enable or disable boosting of word line voltage levels in the memory. The system management unit selects between enabling and disabling the boosting of word line voltage levels based on a target operational voltage for the memory and the cross-over region prior to updating the operating parameters of the memory to include the target operational voltage.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=14&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Ray compression for efficient processing of graphics data at computing devices</a><br />
A mechanism is described for facilitating ray compression for efficient graphics data processing at computing devices. A method of embodiments, as described herein, includes forwarding a set of rays to a ray compression unit hosted by a graphics processor at a computing device, and facilitating the ray compression unit to compress the set of rays, wherein the set of rays are compressed into a compressed representation.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=15&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Transactionally deterministic high speed financial exchange having improved, efficiency, communication, customization, performance, access,  trading opportunities, credit controls, and fault tolerance</a><br />
The disclosed embodiments relate to implementation of a trading system, which may also be referred to as a trading system architecture, having improved performance which further assures transactional determinism under increasing processing transaction loads while providing improved trading opportunities, fault tolerance, low latency processing, high volume capacity, risk mitigation and market protections with minimal impact, as well as improved and equitable access to information and opportunities.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=16&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Product notification and recommendation technology utilizing detected activity</a><br />
An exemplary system and method provides a product notification and recommendation technology for monitoring a computing device to detect particular use-cases of device activity and providing a notification through a user interface that indicates at least one product corresponding to the detected particular use-cases. In this way, the product notification and recommendation technology adds a new dimension of usage-based personalization to targeted advertising that results in timely product and service recommendations.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=17&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Presentation of content items in view of commerciality</a><br />
Methods, systems, and apparatus, including computer programs encoded on a computer storage medium, for enhancing selecting relevant and diverse advertisements. In one aspect, a method includes receiving an initial query, selecting one or more additional queries relating to the initial query, including selecting additional queries having a greatest commerciality, identifying one or more content items for each of the additional queries, the one or more content items forming a content block, and providing a content block and an associated additional query to a client device to be displayed along with search results associated with the initial query.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=18&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Protecting privacy of personally identifying information when delivering targeted assets</a><br />
Techniques are disclosed herein for protecting personally identifying information (PII) and behavioral data while delivering targeted assets. In one aspect, a profile is created based on a template and desired characteristics of users to receive one or more targeted assets. The template provides a framework for the user characteristics. One or more clients are provided the template. A manifest that identifies the targeted assets is encrypted based on the profile. The encrypted manifest is sent to the one or more clients. A user profile is generated at a client based on a template. The client attempts to decrypt the encrypted manifest based on the profile created at the client. The client sends a request for any targeted assets that were identified through the attempt to decrypt the encrypted manifest.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=19&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Vector-based characterizations of products and individuals with respect to customer service agent assistance</a><br />
Systems, apparatuses, and methods are provided herein for providing customer service agent assistance. A system for providing customer service agent assistance comprises a customer profile database storing customer partiality vectors for a plurality of customers, the customer partiality vectors comprise customer value vectors, a communication device, and a control circuit. The control circuit being configured to: provide a customer service agent user interface on a user device associated with a customer service agent, associate a particular customer with the customer service agent, retrieving at least one customer value vector for the particular customer from the customer profile database, and cause, via the communication device, the at least one customer value vector of the particular customer to be displayed on the customer service agent user interface of the user device.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=20&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Sparse neural control</a><br />
Aspects herein describe new methods of determining optimal actions to achieve high-level goals with minimum total future cost. At least one high-level goal is inputted into a user device along with various observational data about the world, and a computational unit determines, though a method comprising backward and forward sweeps, an optimal course of action as well as emotions. In one embodiment a user inputs a high-level goal into a cell phone which senses observational data. The cell phone communicates with a server that provides instructions. The server determines an optimal course of action via the method of backward and forward sweeps, and the cell phone then displays the instructions and emotions to the user.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=21&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Framework for classifying an object as malicious with machine learning for deploying updated predictive models</a><br />
According to one embodiment, an apparatus comprises a first analysis engine and a second analysis engine. The first analysis engine analyzes an object to determine if the object is malicious. The second analysis engine is configured to (i) receive results of the analysis of the object from the first analysis engine and (ii) analyze, based at least in part on the analysis by the first analysis engine, whether the object is malicious in accordance with a predictive model. Responsive to the first analysis engine and the second analysis engine differing in determinations as to whether the object is malicious, information associated with an analysis of the object by at least one of the first analysis engine and the second analysis engine is uploaded for determining whether an update of the predictive model is to occur. An update of the predictive model is subsequently received by the classification engine.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=22&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Continuous user authentication</a><br />
A method of enabling continuous user authentication, comprising: setting up an authentication server to provide authentication data to an enterprise server in parallel to a remote user session with the enterprise server, when the user is using a touch screen device; extracting samples from a user's behavior, to build a library of user specific parameters; and tracking user behavior to authenticate the user, the tracking comprises initial identification of a user of the touch screen device when starting a session with the enterprise server and continuous authentication of the user during the session with the enterprise server.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=23&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">System and method for decentralized autonomous healthcare economy platform</a><br />
A system and method for a decentralized autonomous healthcare economy platform are provided. The system and method aggregates all of the healthcare data into a global graph-theoretic topology and processes the data via a hybrid federated and peer to peer distributed processing architectures.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=24&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Safety features for high level design</a><br />
This disclosure relates generally to electronic design automation using high level synthesis techniques to generate circuit designs that include safety features. The algorithmic description representation can be specified in a first language and include at least one programming language construct associated with a first safety data type. Compiling the algorithmic description may involve identifying the at least one construct, accessing a first safety data type definition associated with the first safety data type, and generating a second representation of the circuit design based on the algorithmic description representation and the first safety data type definition. The second representation can be provided in a second language and include at least one safety feature for a portion of the circuit design associated with the at least one construct.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=25&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Configuring a programmable device using high-level language</a><br />
A method of preparing a programmable integrated circuit device for configuration using a high-level language includes compiling a plurality of virtual programmable devices from descriptions in said high-level language. That compiling includes compiling configurations of configurable routing resources from programmable resources of said programmable integrated circuit device, and compiling configurations of a plurality of complex function blocks from programmable resources of said programmable integrated circuit device. A machine-readable data storage medium may be encoded with a library of such compiled configurations. A virtual programmable device may include a stall signal network and routing switches of the virtual programmable device may include stall signal inputs and outputs.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=26&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">System and process for simulating the behavioral effects of timing violations between unrelated clocks</a><br />
According to one aspect, embodiments of the invention provide a CDC simulation system comprising a timing analysis module configured to receive a circuit design, analyze the circuit design to identify at least one CDC, and generate a report including information related to the at least one CDC, a CDC simulation module configured to communicate with the timing analysis module and to receive the report from the timing analysis module, and a test bench module configured to communicate with the CDC simulation module, to receive the circuit design, and to operate a test bench code to simulate the operation of the circuit design, wherein the CDC simulation module is further configured to edit a top level of the test bench code, based on the received report, such that the test bench module is configured to identify timing violations in the circuit design due to the at least one CDC.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=27&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Deep compositional frameworks for human-like language acquisition in virtual environments</a><br />
Described herein are systems and methods for human-like language acquisition in a compositional framework to implement object recognition or navigation tasks. Embodiments include a method for a model to learn the input language in a grounded and compositional manner, such that after training the model is able to correctly execute zero-shot commands, which have either combination of words in the command never appeared before, and/or new object concepts learned from another task but never learned from navigation settings. In embodiments, a framework is trained end-to-end to learn simultaneously the visual representations of the environment, the syntax and semantics of the language, and outputs actions via an action module. In embodiments, the zero-shot learning capability of a framework results from its compositionality and modularity with parameter tying.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=28&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Efficient word encoding for recurrent neural network language models</a><br />
Systems and processes for efficient word encoding are provided. In accordance with one example, a method includes, at an electronic device with one or more processors and memory, receiving a user input including a word sequence, and providing a representation of a current word of the word sequence. The representation of the current word may be indicative of a class of a plurality of classes and a word associated with the class. The method further includes determining a current word context based on the representation of the current word and a previous word context, and providing a representation of a next word of the word sequence. The representation of the next word of the word sequence may be based on the current word context. The method further includes displaying, proximate to the user input, the next word of the word sequence.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=29&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Displaying temporary profile content items on communication networks</a><br />
In one embodiment, a method includes accessing, from a data store of the communication network, user information associated with a first user of the communication network, identifying one or more entities associated with the communication network that are relevant to the first user based on the user information, and retrieving, for each identified entity, one or more content frames associated with the entity. The method includes ranking the one or more content frames based on the user information. The method also includes sending, to a client device of the first user, one or more of the content frames for display to the first user in ranked order, wherein each content frame is selectable by the first user to display the selected content frame in association with a particular content item for a specified period of time.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=30&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Method and system for mining frequent and in-frequent items from a large transaction database</a><br />
The technique relates to a system and method for mining frequent and in-frequent items from a large transaction database to provide the dynamic recommendation of items. The method involves determining user interest for an item by monitoring short item behavior of at least one user then selecting a local category, a neighborhood category and a disjoint category with respect to the item clicked by the at least one user based on long term preferences data of a plurality of users of the ecommerce environment thereafter selecting one or more frequent and infrequent items from each of the selected local, neighborhood and disjoint category items and finally generating one or more dynamic recommendations based on the one or more items selected from the local category, the neighborhood category and the disjoint category and the one or more selected frequent and infrequent items.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=31&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Clustering based process deviation detection</a><br />
Systems and methods for data analysis include correlating event data to provide process instances. The process instances are clustered, using a processor, by representing the process instances as strings and determining distances between strings to form a plurality of clusters. One or more metrics are computed on the plurality of clusters to monitor deviation of the event data.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=33&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Automated system and method to customize and install virtual machine configurations for hosting in a hosting environment</a><br />
Some embodiments provide a method for automated configuration of a set of resources for hosting a virtual machine at a particular node in a hosting system. The hosting system includes several nodes for hosting virtual machines. The method, at a first virtual machine operating using a first set of resources of the particular node, receives a user-specified virtual machine configuration for a second virtual machine to be hosted on a second set of resources of the particular node. The method retrieves, to the first virtual machine, a software image from a computer readable hardware medium storing several software images based on the user-specified virtual machine configuration. The method modifies the retrieved software image according to the user-specified virtual machine configuration. The method configures the second set resources using the modified software image.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=34&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Systems and methods of monitoring a network topology</a><br />
The technology disclosed relates to maintaining up to date software version data in a network. In particular, it relates to accessing a network topology that records node data and connection data including processes running on numerous hosts grouped into local services on the hosts, the local services running on multiple hosts grouped into service clusters and sub-clusters of service clusters, and network connections used by the service clusters to connect the hosts grouped into service connections. It further relates to collecting current software version information for the processes, updating the network topology with the current software version for particular process running on a particular host when it differs from a stored software version in the network topology, reassigning the particular host to a sub-cluster within the service cluster according to the current software version, and monitoring the updated sub-cluster within the service cluster.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=35&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Power management of memory chips based on working set size</a><br />
Briefly, in accordance with one or more embodiments, an apparatus comprises a memory comprising one or more physical memory chips, and a processor to implement a working set monitor to monitor a working set resident in the one or more physical memory chips. The working set monitor is to adjust a number of the physical memory chips that are powered on based on a size of the working set.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=36&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Mobile communication terminal providing adaptive sensitivity of a click event</a><br />
A mobile communication terminal has a controller and a touch display. The touch display is arranged to display at least a first graphical object and a second graphical object, receive a touch, and determine a touch position and a touch duration for the touch. The controller is configured to receive the touch position and the touch duration, determine a graphical object, among the first graphical object and the second graphical object, corresponding to the touch position, determine if the touch duration exceeds a reference time threshold, and if so, generate a click event for the corresponding graphical object. The first graphical object is associated with a first time threshold, and the second graphical object is associated with a second time threshold. The first time threshold is different from the second time threshold. The controller is further configured to retrieve the first time threshold if the corresponding graphical object is the first graphical object and use the first time threshold as the reference time threshold, and retrieve the second time threshold if the corresponding graphical object is the second graphical object and use the second time threshold as the reference time threshold. The first time threshold is either higher or lower than said second time threshold depending on one or more of the following: a size, shape or/and color of the corresponding graphical object, a distance from the corresponding graphical object to a neighboring graphical object, a relative location of the corresponding graphical object in a touch area of the touch display, and a level of the corresponding graphical object in a menu hierarchy.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=37&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Control system user interface</a><br />
Embodiments include systems and methods comprising a gateway located at a premise forming at least one network on the premise that includes a plurality of premise devices. A sensor user interface (SUI) is coupled to the gateway and presented to a user via a remote device. The SUI includes at least one display element. The at least one display element includes a floor plan display that represents at least one floor of the premise. The floor plan display visually and separately indicates a location and a current state of each premise device of the plurality of premise devices.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=38&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Optimizing transportation networks through dynamic user interfaces</a><br />
 The present disclosure relates to providing a dynamic graphical user interface for efficiently presenting users with relevant ride information throughout the fulfillment of a ride request. In some embodiments, the system detects a trigger event during a ride, and based on detecting the trigger event, the system expands or collapses an information portion within a graphical user interface. When in a collapsed state, for example, the information portion of the graphical user interfaces includes a first set of content. Upon detecting a trigger event, the system dynamically expands the information portion to provide a second set of content that includes information associated with the detected trigger.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=39&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Human-to-mobile interfaces</a><br />
A method of character recognition for a personal computing device comprising a user interface capable of receiving inputs that are to be recognized through data input means which are receptive to keyed, tapped or a stylus input, said device being adapted to facilitate a reduction in the number of physical keying actions, tapping actions or gestures required to create a data string to less than the number of characters within said data string: storing a set of data strings each with a priority indicator associated therewith, wherein the indicator is a measure of a plurality of derivatives associated with the data string; recognizing an event; looking up the most likely subsequent data string to follow the event from the set of data strings based on one or more of the plurality of derivatives; ordering the data strings for display based on the priority indicator of that data string.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=40&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Personalized autonomous vehicle ride characteristics</a><br />
Systems and method are provided for controlling a vehicle. In one embodiment, a method includes: obtaining ride preference information associated with a user, identifying a current vehicle pose, determining a motion plan for the vehicle along a route based at least in part on the ride preference information, the current vehicle pose, and vehicle kinematic and dynamic constraints associated with the route, and operating one or more actuators onboard the vehicle in accordance with the motion plan. The user-specific ride preference information influences a rate of vehicle movement resulting from the motion plan.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=41&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Feasible lane routing</a><br />
 Systems and method are provided for controlling a vehicle. The systems and methods calculate lane plan data including a set of lane plans defining a route from a start location to a destination location, solve a motion planning algorithm to produce solved lane plan data defining a solved lane plan and a trajectory therefor, receive forthcoming distance data representing a forthcoming distance, determine a feasible lane based on the solved lane plan data within the forthcoming distance, remove a lane plan from the lane plan data to produce feasible lane plan data including a feasible lane plan defining a route from the start location to the destination location, and control motion of the vehicle based on the feasible lane plan data.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=42&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Methods and systems for diagnosing eyes using aberrometer</a><br />
Configurations are disclosed for a health system to be used in various healthcare applications, e.g., for patient diagnostics, monitoring, and/or therapy. The health system may comprise a light generation module to transmit light or an image to a user, one or more sensors to detect a physiological parameter of the user's body, including their eyes, and processing circuitry to analyze an input received in response to the presented images to determine one or more health conditions or defects.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=43&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Mobile localization using sparse time-of-flight ranges and dead reckoning</a><br />
Mobile localization of an object having an object positional frame of reference using sparse time-of-flight data and dead reckoning can be accomplished by creating a dead reckoning local frame of reference, including an estimation of object position with respect to known locations from one or more Ultra Wide Band transceivers. As the object moves along its path, a determination is made using the dead-reckoning local frame of reference. When the object is within a predetermine range of one or more of the Ultra Wide Band transceivers, a "conversation" is initiated, and range data between the object and the UWB transceiver(s) is collected. Using multiple conversations to establish accurate range and bearing information, the system updates the object's position based on the collected data.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=44&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Method and apparatus for identifying defects in a chemical sensor array</a><br />
An apparatus including an array of sensors including a plurality of chemical sensors and a plurality of reference sensors, each chemical sensor coupled to a corresponding reaction region for receiving at least one reactant, and each reference sensor comprising a field effect transistor having a gate coupled to a corresponding reference line and an access circuit for accessing the chemical sensors and the reference sensors and a controller to apply bias voltages to the reference lines to select corresponding reference sensors, acquire output signals from the selected reference sensors, and identify one or more defects in the access circuit based on differences between the acquired output signals and expected output signals.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=45&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Systems and methods for restoring cognitive function</a><br />
Systems and methods for restoring cognitive function are disclosed. In some implementations, a method includes, at a computing device, separately stimulating one or more of lateral and medial entorhinal afferents and other structures connecting to a hippocampus of an animal subject in accordance with a plurality of predefined stimulation patterns, thereby attempting to restore object-specific memories and location-specific memories; collecting a plurality of one or more of macro- and micro-recordings of the stimulation of hippocampalentorhinal cortical (HEC) system; and refining the computational model for restoring individual memories in accordance with a portion of the plurality of one or more of macro- and micro-recordings.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=46&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Personal emergency response (PER) system</a><br />
A system includes one or more sensors to detect activities of a mobile object; and a processor coupled to the sensor and the wireless transceiver to classify sequences of motions into groups of similar postures each represented by a model and to apply the models to identify an activity of the object.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=47&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Providing visualization data to a co-located plurality of mobile devices</a><br />
A computer-implemented method according to one embodiment includes identifying a plurality of mobile devices, determining a relative location of each of the plurality of mobile devices, and assigning visualization data to each of the plurality of mobile devices, based on the relative location of each of the plurality of mobile devices.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=48&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Systems and methods for automatically detecting users within detection regions of media devices</a><br />
 Systems and methods are presented for detecting users within a range of a media device. A detection region may be defined that is within the range of the media device and smaller than the range. The detection region may be stored. It may be determined whether a user is within the detection region. The media device may be activated and settings associated with the user may be applied when a user is within the detection region. In some embodiments, settings associated with a user may be compared to provided media content when the user is within the detection region. The content may change when the settings conflict with the media content. Reminders may be provided to or directed to a plurality of users within the range of the media device.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=49&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Fractional-readout oversampled image sensor</a><br />
 Signals representative of total photocharge integrated within respective image-sensor pixels are read out of the pixels after a first exposure interval that constitutes a first fraction of a frame interval. Signals in excess of a threshold level are read out of the pixels after an ensuing second exposure interval that constitutes a second fraction of the frame interval, leaving residual photocharge within the pixels. After a third exposure interval that constitutes a third fraction of the frame interval, signals representative of a combination of at least the residual photocharge and photocharge integrated within the pixels during the third exposure interval are read out of the pixels.

<p><a href="http://patft.uspto.gov/netacgi/nph-Parser?Sect1=PTO2&amp;Sect2=HITOFF&amp;u=%2Fnetahtml%2FPTO%2Fsearch-adv.htm&amp;r=50&amp;f=G&amp;l=50&amp;d=PTXT&amp;p=1&amp;S1=(((behavioral+AND+application)+AND+programming)+AND+interface)&amp;OS=behavioral+AND+application+AND+programming+AND+interface&amp;RS=(((behavioral+AND+application)+AND+programming)+AND+interface)">Fraud detection in interactive voice response systems</a><br />
Systems and methods for call detail record (CDR) analysis to determine a risk score for a call and identify fraudulent activity and for fraud detection in Interactive Voice Response (IVR) systems. An example method may store information extracted from received calls. Queries of the stored information may be performed to select data using keys, wherein each key relates to one of the received calls, and wherein the queries are parallelized. The selected data may be transformed into feature vectors, wherein each feature vector relates to one of the received calls and includes a velocity feature and at least one of a behavior feature or a reputation feature. A risk score for the call may be generated during the call based on the feature vectors.

<p>The why and how of behavioral for each of these technological approaches varies. However, it does provide a nice slice of the pie when it comes to the different angles of how behavioral approaches is being applied. I purposely left the company name off of each of these to make folks click in to see who is behind each one. I’ll spoil it a little, and say the usual suspects like Facebook are behind some of them, but others are behind these different ways in which technological is being used to understand and shift our behavior. Of course, for good. ;-)

<p>I’ll be doing more work to understand what is behind the intent of these patents. My concern around understanding more about how and what is considered behavioral in the API space is more about surveillance, than ever is about truly understanding what the good and bad of behavioral is. This is just the first of many patent searches that I will conduct. I’ll keep evolving my vocabulary for searching and discovering APIs in this area, evolving my results over time, and learning from what companies are up to when it comes to behavioral manipulation via APIs.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/08/05/a-look-at-behavioral-api-patents/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/08/02/reverse-engineering-mobile-apis-to-show-a-company-their-public-apis/">Reverse Engineering Mobile APIs To Show A Company Their Public APIs</a></h3>
        <span class="post-date">02 Aug 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-square-97-193-800-500-0-max-0-1--1-square.jpg" width="45%" align="right" style="padding: 15px;" />
One story I tell a lot when talking to folks about APIs, is how you can reverse engineer a mobile phone to map out the APIs being used. As the narrative goes, many companies that I talk with claim they do not have any public APIs when first engage with them. Then I ask them, “Do you have any mobile applications?”. To which the answer is almost always, “Yes!”.  Having anticipated this regular conversation, if I am looking to engage with a company in the area of API consulting, I will have made the time to reverse engineer their application to produce a set of OpenAPI definitions that I can then share with them, showing that they indeed have public APIs.

<p>The process isn’t difficult, and I’ve written about this several times. To reverse engineer a mobile application, all you have to do is download the targeted application to your phone, configure your phone to use your laptop as a proxy, and be running Charles Proxy on your laptop. I’m not going to share a walkthrough of this, it is easy enough to Google and find the technical details of doing it, I’m more looking to just educate the average business person that this is possible. Once you have your mobile phone proxied through Charles, it will capture every call made home to the mother ship, logging the request and response structure of all APIs being used by the mobile application-—which uses public DNS for routing, making it a public API.

<p>I have an API that I developed which I can upload the resulting Charles Proxy output file, and convert all the API calls into an OpenAPI. Making quick work of documenting the APIs behind a mobile application. Which, when you share with someone who is under the belief that their mobile APIs are private APIs, it can make quite a splash. Usually you get a response, like “well, we don’t allow access to the general public for our APIs”. To which I respond, any consumer of your application is a consumer of your APIs. If you use public DNS to handle the transport for your APIs-—they are public APIs. There is no secret force field created by mobile applications to keep your APIs secure or protected. Some applications have invested in SSL pinning to prevent proxying with tooling like Charles Proxy, but there is still ways around—-albeit it requires a significant more work.

<p>As I’m spending more time crafting API discovery tooling and agents, I’m revisiting my work to reverse engineer mobile applications and generate OpenAPI from proxy logs. I’d like to find a way to emulate mobile applications and script the exploration of them. I find having to click through every option and feature within an application pretty mind numbing, and I’d like to automate it a little more. I feel like mobile, browsers, and  internet of things API discovery will be one of the next frontiers when it comes to mapping out the API landscape. I’m guessing there will always be truly public APIs launching, but the majority of APIs will continue to operate in the shadows of our browsers, mobile phones, and other devices that are becoming ubiquitous in our lives.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/08/02/reverse-engineering-mobile-apis-to-show-a-company-their-public-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/08/02/didnt-we-already-do-that/">Didn’t We Already Do That?</a></h3>
        <span class="post-date">02 Aug 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/amusement-park-amusement-park-2-satan-red.jpg" width="45%" align="right" style="padding: 15px;" />
When you are in the API game you hear this phrase a lot, “didn’t we already do that?”. It is a common belief system that because something was already done, that it means it will not work ever again. When you are operate solely in a computational world, you tend to see things as 1s and 0s, and if something was tried and “didn’t work”, there is no resetting of that boolean switch for some reason. We excel at believing things are done, without ever unpacking why something failed, or how the context may have changed.

<p>One of the things I’m going to do over the next couple months is go through the entire SOA toolbox and take accounting of everything we threw away, and evaluate what the possible reasoning were behind it—-good and bad. I strongly believe that many folks who abandoned SOA, willingly or unwillingly, and especially the people who enjoy saying, “didn’t we already do that”, haven never spent time unpacking why we did, why it didn’t work, let alone whether or not it might work today. I think this paradigm reflects one of the fundamental illnesses we encounter in the tech sector-—we have a dysfunctional and distorted relationship with the past (this is by design).

<p>I’ve written about this before, but I’ll say it again. Can you imagine saying, “didn’t we already do that” about other non-technical things. Fashion. Art. Music. Stories. Law. Why do we say it in technology? When it comes to SOA, there are many reasons why it didn’t work overall, and most of the reasons were not technical. So why would we not want to re-evaluate some of the technologies and practices to see if there would be a new opportunity to apply an old pattern or approach? This question isn’t just something I’ve heard a handful of times. There have been literally a hundred plus blog posts on API Evangelist where people have commented this—-especially when it comes to JSON Schema and OpenAPI.

<p>Anyways. I’m going to revisit my SOA history. My therapist said enough time has past and I’ve healed properly, so I can begin digging around in this part of my past. I’m guessing there are a lot of practices, tooling, and patterns we can rethink in a JSON, YAML, containerized, CI/CD, cloudy world. Alongside this work I’m going to be assessing what the preferred open source tool are for the API lifecycle, making sure it represents my vision of a diverse API toolbox, tracking on tooling for JSON Schema, OpenAPI, and AsyncAPI that will help us service the entire API lifecycle for both internal and external API delivery. There is a lot of work that has been done in the past that we should be learning from, and I’m more than happy to dive in, do the research, and shine a light on what we’ve left behind.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/08/02/didnt-we-already-do-that/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/08/01/the-future-of-apis-will-be-in-the-browser/">The Future Of APIs Will Be In The Browser</a></h3>
        <span class="post-date">01 Aug 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/nazi-invasion-dark-hallway.jpg" width="45%" align="right" style="padding: 15px;" />
I have been playing with the new <a href="https://developer.mozilla.org/en-US/docs/Web/API/Reporting_API">browser reporting API</a> lately, and while it isn’t widely supported, it does work in Chrome, and soon Firefox. I won’t go into too much technical detail, but the API provides an interesting look at reporting on APIs usage in the browser. Offering a unique view into the shadows of what is happening behind the curtain in our browser when we are using common web applications each day. I have been proxying my web traffic for a long time to produce a snapshot at the domains who are operating beneath the covers, but it is interesting for browsers to begin baking in a look at the domains who are violating, generating errors, and other shenanigans.

<p>As I’m contemplating the API discovery universe I can’t help but think of the how “API innovation” is occurring within the browser.  When I say “API innovation”, I don’t mean the kind that got us all excited from 2005 through 2010, or the golden days from 2010 through 2015-—I am talking the exploitative kind. Serving advertisers, trackers, and other exploitative practices. Most people would scoff at me calling these things APIs, but they are using the web to deliver machine readable information, so they are APIs. I’ve been tracking on the APIs I use behind the scene in my browser using Charles Proxy for a while now, but I’m feeling I should formalize my analysis.

<p>I’m thinking I’ll take a sampling of domain, maybe 250+, and automate the browsing of each page, while also running through Charles Proxy. Then aggregate all of the domains that are loaded, and categorize them by media type–just to give me a sampling of the APIs in operation behind the scenes of some common sites. I’m sure most are advertising or social related, but I’m guessing there are a lot of other surprises in there. While some of the APIs will be publicly showcased in some way, there are no doubt a number of APIs being used that do not have a public presence, documentation, or other visible element. While I am interested in learning how the public APIs I track on are used, I’m more interested in painting a picture of the shadow APIs that are running behind the JavaScript libraries, and other embeddable in use across the web.

<p>Developer portals and API documentation are not the only way to find APIs. Reverse engineering mobile and web applications will continue to be a significant player when it comes to understanding the next generation of APIs. When I look across the web, all I see are APIs. I know I’m biased, but I think there is something to this. I don’t think all companies are interested in doing APIs the same way many of us API evangelist, pundits, analysts and believers are. They like APIs, but aren’t that interested in showcasing their practices, and sharing patterns with the rest of the community. I’m guessing they are more interested in penetrating our worlds via our browsers, and capturing some of the valuable behavioral exhaust we all produce on a daily basis.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/08/01/the-future-of-apis-will-be-in-the-browser/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/08/01/about-giving-away-api-knowledge-for-free/">About Giving Away API Knowledge For Free</a></h3>
        <span class="post-date">01 Aug 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/gears-4882162452-fa3126b38d-b-wols.jpg" width="45%" align="right" style="padding: 15px;" />
I’m in the business of providing access to the API knowledge accumulated over the last decade. Despite what many people claim, I do not know everything about APIs, but after a decade I have picked up a few things along the way. Historically, I have really enjoyed sharing my knowledge with people, but I’m increasingly becoming weary of sharing to openly because of the nature of how business gets conducted online. Specifically, that there are endless waves of folks who want to tap what I know without giving anything in return, who work at companies who enjoy a lot of resources. I know people have read the startup handbook, which tells them to reach out to people who know and get their view, but when everyone does this, and doesn’t give anything in return, it is, well…I guess business as usual? Right? ;-(

<p>Taking a sampling from the usual week in my inbox, I’ll get a handful of requests reflecting these personas:

<ul>
  <li><strong>Analysts / Consultants</strong> - Other analysts reaching out to share information, and get my take on what I’m seeing. There is usually reciprocity here, so I’m usually happy to engage, especially if I know them personally, and have worked with them before.</li>
  <li><strong>Startup Founders</strong> - I get a wide range of startup founders reaching out, many of which I do not know, wanting to get validation of their idea, and understand the marketplace they are targeting—usually if I know them, or they come with a reference I’ll engage.</li>
  <li><strong>Venture Capitalists</strong> - There is a regular stream of VCs wanting to know what is happening, get my take on things, but they usually are just interested  listin validating what they already know, and get introduced to some new concepts.</li>
  <li><strong>Students</strong> - There is a growing number of students reaching out, and increasingly PHD students who are working on something API related as part of their studies.</li>
</ul>

<p>This represents the usual suspects. There are plenty of other outliers, but this represent the regular drumbeat of people making their way into my inbox. Depending on the day, my mood, and the way in which they reach out, I’ll decide to engage or not engage. However, as things are getting much tighter, especially as my time is at a premium each week, and my patience for the API sector decreases, I’m beginning to push back more. One of my biggest pet peeves is when people who have funding, or venture capitalists want to tap my knowledge without compensation. I’m guessing the privilege level with these folks is pretty high, and they are just used to engaging with other people of means—-completely oblivious to the fact that some of us don’t come from wealthy families, and are just making things work on our own.

<p>In coming months I’ll be publishing a range of guides, white papers, and blueprints for people to purchase. Also, as usual I am open to paid consulting time. Beyond that, you’ll have to gather what you can from my short form blog posts, and the research I openly publish across my network of sites. Don’t expect much from me if you cold email me-—however, feel free to do so. The more creative the outreach, and value demonstrated by your pitch might just influence me. However, if you are just cold emailing or calling me, without understanding that I am trying to piece together a living from my work, your outreach will probably not get the response you are looking for. I’m sorry to be so cold about this, but we can’t all just be giving away our knowledge for free, and a little support of my work goes a long way–it shows that you understand and respect how much time I’ve invested in what I know about the space.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/08/01/about-giving-away-api-knowledge-for-free/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/31/the-challenges-of-api-discovery-conversations-being-purely-technical/">The Challenges Of API Discovery Conversations Being Purely Technical</a></h3>
        <span class="post-date">31 Jul 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/containership-containership-blue-circuit-5.jpg" width="45%" align="right" style="padding: 15px;" />
Ironically one of the biggest challenges facing API discovery on the web, as well as within the enterprise, is that most conversations focus purely on the technical, rather than the human and often business implications of finding and putting APIs to work. The biggest movement in the realm of API discovery in the last couple years has been part of the service mesh evolution of API infrastructure, and how your gateways “discover” and understand the health of APIs or microservices that provide vital services to applications and other systems. Don’t get me wrong, this is super critical, but it is more about satisfying a technical need, which is also being fueled by an investment wave-—it won’t contribute to much to the overall API discovery and search conversation because of it’s limited view of the landscape.

<p>Runtime API discovery is critical, but there are so many other times we need API discovery to effectively operate the enterprise. Striving for technical precision at runtime is a great goal, but enabling all your groups, both technical and business to effectively find, understand, engage, and evolve with existing APIs should also be a priority. It can be exciting to focus on the latest technological trends, but doing the mundane profiling, documentation, and indexing of existing API infrastructure can have a much larger business impact. Defining the technical details of your API Infrastructure using OpenAPI, Postman, and other machine readable formats is just the beginning, ideally you are also working define the business side of things along the way.

<p>I find that defining APIs using OpenAPI and JSON Schema to be grueling work. However, I find documenting the teams and owners behind APIs, the licensing, dependencies (both technical and business), pricing, and other business aspects of an API to be even more difficult. Over the last decade we’ve gotten to work standardizing how define the technical surface area of our APIs, but we’ve done very little work to standardize how we license, price, own, collaborate, and track on the other business implications of delivering APIs. This is one reason Steve Willmott and I created <a href="http://apisjson.org/">the APIs.json format</a>, to help drive this discussion. Providing a machine readable API format to transcend the technical details of APIs, and allow us to better define the operational side of making sure APIs are discoverable.

<p>APIs.json is about defining everything about your APIs that JSON Schema, OpenAPI, and AsyncAPI will not. Where your documentation is, how to find SDKs, what the terms and conditions are, or maybe the licensing behind your API. We designed the API specification to be flexible, and something that can be extended. There are a handful of default property types you can use when applying the format, but ultimately it is about pushing you to define your own using x- extensions. Helping API providers think through what the common building blocks of their API operations are, and provide them with a simple JSON or YAML format for indexing all of these elements for use in your API catalog, or publishing to the root of your developer portal. Helping augment what OpenAPI, JSON Schema, and AsyncAPI have done, but providing a single place for you to hang all of your API artifacts.

<p>I’m working hard to continue refining my catalog of 3K+ APIs.json files. I’m working on better ways to validate or invalidate what I have indexed, and provide a single search interface for them. Once I’ve refreshed the catalog, and synced them with the evolution of the available APIs over at <a href="http://apis.io">APIs.io</a>, I will publish a fresh list of the companies I’m tracking on. I feel like one of the most critical business aspects of API discovery we consistently overlook, ignore, or are in denial of, is whether an API is still active, and anyone is home. This is a rampant illness in the cataloging of public APIs, but also something that you can find all over the enterprise. We need to do a better job of understand where are APIs are, but also be more honest about which APIs are used, do not have an owner, or are straight 404’ing and shouldn’t be listed in any active API catalog.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/31/the-challenges-of-api-discovery-conversations-being-purely-technical/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/31/differences-between-api-observability-over-monitoring-testing-reliability-and-performance/">Differences Between API Observability Over Monitoring, Testing, Reliability, and Performance</a></h3>
        <span class="post-date">31 Jul 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/35201856153_61bc075e4b-udnie.jpg" width="45%" align="right" style="padding: 15px;" />
I’ve been watching the API observability coming out of Stripe, as well as Honeycomb for a couple years now. Then observability of systems is not a new concept, but it is one that progressive API providers have embraced to help articulate the overall health and reliability of their systems. In control theory, observability is a measure of how well internal states of a system can be inferred from knowledge of its external outputs. Everyone (except me) focuses only on the modern approaches for monitoring, testing, performance, status, and other common API infrastructure building blocks to define observability. I insist on adding the layers of transparency and communication, which I feel are the critical aspects of observability—-I mean if you aren’t transparent and communicative about your monitoring, testing, performance, and status, does it really matter?

<p>I work to define observability as a handful of key API building blocks that every API provider should be investing in:

<ul>
  <li><strong>Monitoring</strong> - Actively monitoring ALL of your APIs to ensure they are up and running.</li>
  <li><strong>Testing</strong> - Performing tests to ensure APIs aren’t just up but also doing what they are intended to.</li>
  <li><strong>Performance</strong> - Adding an understanding of how well your APIs are delivering to ensure they perform as expected.</li>
  <li><strong>Security</strong> - Actively locking down, scanning, and ensuring all your API infrastructure is secure.</li>
</ul>

<p>Many folks rely on the outputs from these areas to define observability, but there are a couple more ingredients needed to make it observable:

<ul>
  <li><strong>Transparency</strong> - Sharing the practices and results from each of these areas is critical.</li>
  <li><strong>Communication</strong> - If you aren’t talking about these things regularly they do not exist.</li>
  <li><strong>Status</strong> - Providing real time status updates for al these areas is essential.</li>
</ul>

<p>You can be actively observing the outputs from monitoring, testing, performance, and security operations, but if this data isn’t accessible to other people on your team, within or company, partners, and for the public as required, then things aren’t observable. Of course, I’m not talking about making ALL API activity public, but I’m saying, if you are a public API, and you aren’t providing transparency, communication, and status of your monitoring, testing, performance, and security—-then you aren’t observable.

<p>I know many folks will disagree with me on this part, but that is ok. I am used to it. So far, I haven’t seen much embrace of the observability concept, with many providers either not understanding it, or not grasping the meaningful impact it will have on their operations. So I’m not holding my breath that folks will buy into my portion of it. However it is my self appointed role to make sure the bar is high, even if nobody adopts the same set of rules. In the end, API observability isn’t some new trendy buzzword, it is one of a handful of meaningful constructs that exist to help make us all better, but most likely will get lost in the shuffle of doing APIs each day. :-(


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/31/differences-between-api-observability-over-monitoring-testing-reliability-and-performance/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/30/peer-api-design-review-sessions/">Peer API Design Review Sessions</a></h3>
        <span class="post-date">30 Jul 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/abe-lincoln-one-smooth-ride-file-00-00-07-91.jpg" width="45%" align="right" style="padding: 15px;" />
Public APIs have always benefitted from something that internal APIs do not always received—-feedback from other people. While the whole public API thing didn’t play out as I originally imagined, there is still a lot of benefit in letting other see, and provide open feedback on your API. It is painful for many developers to receive feedback on their API, and it is something that can be even more painful when it is done publicly. This is why so many of us shy away from establishing feedback loops around our APIs. It is hard work to properly design, develop, and then articulate our API vision to an external audience. It is something I understand well, but still suffer from when it comes to properly accessioning peer review and feedback on my API designs.

<p>I prefer opening up to peer reviews of my API designs while they are still just mocks. I’m less invested in them at this point, and it is easier to receive feedback on them. It is way less painful to engage in an ongoing discussion fo what an API should (and shouldn’t) do early on, then it is to define the vision, deliver an API as code or within a gateway, and then have people comment on your baby that you have given birth to. It hurts to have people question your vision, and what you’ve put forth. Especially for us fragile white men who who aren’t often very good at accepting critical feedback, and want to just be left to our own devices. I’d much prefer just being a coder, but around 2008 through 2010 I saw the benefits to my own personal development when I opened up my work to my peers and let a little sunlight in. I am a better developer because of it.

<p>One tool in my API toolbox that is growing in importance is the peer, and open API design review sessions. Taking an OpenAPI draft, loading it into Swagger Editor, firing up a Zoom or Google Hangout, and inviting others to openly share in the design of an API. I find it isn’t something everyone is equipped to do, but many are open to learning, or at least curious about how it works. Curious is good. It is a start. I think many folks aren’t fluent in the API design process, and are often afraid to appear like they don’t know what they are doing, and having an open discussion throughout the API design process helps them learn out in the open. Using a process that helps everyone involved learn together, and lower their guard a little bit when it comes to new ideas, new ways of doing things, and discussing the overall developer experience (DX) of delivering a quality API.

<p>Peer API Design reviews is something I’d love to see more API design tooling support. If nothing else, more people just doing it with existing tools. You may not have fully embraced a complete API design first approach within your enterprise group, but openly discussing API design patterns is important. It is critical for any API developer to receive feedback on their design from other stakeholders, and other API development peers. It is important that we allow ourselves to open up to this feedback and sometimes criticism of our designs, based upon what others know, sharing potential views on how an API can reduce friction for consumers. Ideally, this process is also made accessible to non-developer stakeholders, and even business owners, but I’m thinking this is another post all by itself—-for right now, I just want to advocate for more peer API design review sessions.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/30/peer-api-design-review-sessions/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/30/api-for-processing-common-logging-formats-and-generating-openapi-definitions/">API For Processing Common Logging Formats And Generating OpenAPI Definitions</a></h3>
        <span class="post-date">30 Jul 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/abandonedbuildings_blue_circuit.jpg" width="45%" align="right" style="padding: 15px;" />
I’ve invested a lot of time in the last six months into various research, scripts, and tooling to help me with finding APIs within the enterprise. This work is not part my current role, but as a side project to help me get into the mindset of how to help the enterprise understand where their APIs are, and what APIs they are using. Almost every enterprise group I have consulted for has trouble keeping tabs on what APIs are being consumed across the enterprise, and I’m keen on helping understand what the best ways are to help them get their API houses in order.

<p>While there are many ways to trace out how APIs are being consumed across the enterprise, I want to start with some of the basics, or the low hanging when it came to API logging within the enterprise. I’m sure there are a lot of common logging locations to tackle, but my list began with some of the common cloud platforms in use for logging of operations to begin my work—focusing on the following three cloud logging solutions:

<ul>
  <li><strong>Amazon CloudFront</strong> - Beginning with the cloud leader, and looking at how the enterprise is centralizing their logs with CloudFront.</li>
  <li><strong>Google StackDriver</strong> - Next, I found Google’s multi-platform approach interesting and worth evaluating as part of this work.</li>
  <li><strong>Azure Logging</strong> - Of course, I have to include Azure in all of this as they are a fast growing competitor to Amazon in this space.</li>
</ul>

<p>After establishing a short list of cloud platforms logging solutions, I began looking at which of the common web server formats I should be looking for within these aggregate logging locations, trying to map out how the enterprise is logging web traffic. Providing me with a short list of the three most common web server formats I should be looking at when it comes to mapping the enterprise API landscape—-providing artifacts of the APIs that enterprise groups are operating.

<ul>
  <li><strong>Apache Log File</strong> - The most ubiquitous open source web server out there is the default for many API providers.</li>
  <li><strong>NGINX Log File</strong> - The next most ubiquitous open source web server is definitely something I should be looking for.</li>
  <li><strong>IIS Log File</strong> - Then of course, many Microsoft web server folks are still using IIS to serve up their API infrastructure.</li>
</ul>

<p>These three web server logging formats represent a significant slice of the API logging pie. If I can identify these logging formats across common cloud logging locations, I feel that I can provide a pretty significant solution for finding the APIs that are in use across the enterprise. However, I didn’t just want to be looking a the web server logging for understanding what APIs are being served up, I also wanted to look at the exhaust from how APIs are being consume by looking at these two web browser and proxy traffic formats:

<ul>
  <li><strong>HAR File</strong> - Allowing for the discovery of APIs that are used in web and browser applications across common use cases.</li>
  <li><strong>Charles Proxy JSON Session</strong> - Using a common proxy application to reverse engineer web and mobile application API calls.</li>
</ul>

<p>These cloud logging solutions, web server formats, as well as browser and proxy solutions give me a pretty interesting look at the API discovery pie. I have scripts to help identify these common formats, and then automatically produce OpenAPI definitions from them. It is pretty easy to run these scripts in a variety of ways to help automatically produce a catalog of OpenAPI definitions from them, automating the mapping of the API landscape within he enterprise. I have all of these scripts working for me in a variety of capacities, the next step is to further automate them, organize them into more of a usable suite of API tooling, then unleash them on a larger set of enterprise logs.

<p>All of my scripts currently run as APIs, as I’m API-first, but I’m currently exploring ways in which I can better execute them at the command line, and as autonomous solutions that can be installed within the enterprise, without any external connections or dependencies. I have a list of ways in which I want to add more value on top of these API discovery solutions, allowing me to generate revenue from them. However right now, I am more interested in ensuring they help automate the API landscape across the majority of enterprise logging solutions. Once I dial this in, I will be looking for more ways to implement the existing functionality, as well as evolve to cover other platforms and formats. I’m just looking to deliver a basic solution for understanding where the hell all the APIs are in the enterprise, before I look to bake in more advanced features.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/30/api-for-processing-common-logging-formats-and-generating-openapi-definitions/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/28/api-storytelling-within-the-enterprise/">API Storytelling Within The Enterprise</a></h3>
        <span class="post-date">28 Jul 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/long-factory-uncle-sam.jpg" width="45%" align="right" style="padding: 15px;" />
Storytelling is important. Storytelling within the enterprise is hard. Reaching folks on the open web is hard work to, but there is usually an audience that will eventually tune in, and over time you can develop and cultivate that audience. The tools you have at your disposal within the enterprise are much more prescribed and often times dictated–even controlled. I also find that they aren’t always as effective as they are made out to be, with the perception being one thing, and the reach, engagement, and noise being then much harder realities you face when trying to get a message out.

<p>Email might seem like a good idea, and is definitely a critical tool when reaching specific individuals or groups, but as a general company wide thing, it quickly becomes exponentially ineffective with each person you add on as CC. I’d say that you are better off creating a daily or weekly email newsletter if you are going to be sending across large groups of the enterprise rather than participating in the constant email barrage that occurs on a daily basis. Email is an effective tool when used properly, but I’d say I haven’t perfected the art of using email to reach my intended audience within the enterprise.

<p>My preferred storytelling format is relatively muted within the enterprise — people rarely read blogs in this world. Blog reading is something you do out on the web apparently. This means I have to get pretty creative when it comes to getting your stories out. It doesn’t mean you shouldn’t be using this format of storytelling, but you just can’t count on folks to regularly consume a blog, or subscribe to an RSS feed. You can still have a blog, but you have to find other ways of slipping the links into existing conversations, documentation, and other avenues in which people consume information within the enterprise.

<p>I would say this reality of reading within the enterprise is why I try to write more white papers and guides. I know that many folks across the enterprise prefer to consume their reading materials as a PDF on their laptop, desktop, or tablet. While this is definitely not my preferred way of consuming information, I have to remember that it is the primary way in which enterprise folks can cut through the noise, and find some quiet time to digest 6-8 pages of API blah blah blah during their busy day. While I will keep pumping out short form content on the blog, I will also be investing much more into creating longer form white papers and guides that have a greater opportunity of penetrating the enterprise.

<p>I know that enterprise folks are caught up in the daily shit-storm and can’t always get to my blog, or spend too much time on Twitter. Making content more portable, and something they can email around, download and potentially consume later is important. As I work within the enterprise more I am realizing how critical this is for folks, including myself. I found myself firing back up my Pocket app on my iPad, so that I can queue things up for later. Reminding how difficult it is to tell stories within the enterprise and that I cannot discount tools like the PDF when it comes to reaching my intended audience. You really have to understand your audience, and work to meet them at their level, regardless of the tools you use to get information and be influenced by the deluge of storytelling we are inundated with on a daily basis.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/28/api-storytelling-within-the-enterprise/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/24/apis-and-browser-bookmarklets/">APIs and Browser Bookmarklets</a></h3>
        <span class="post-date">24 Jul 2019</span>
        <p>I have quite a few API driven bookmarklets I use to profile APIs. I  recently quit using Google Chrome, so I needed to migrate all of them to  Firefox. I saw this work as an opportunity to better define and  organize them, as they had accumulated over the years without any sort  of strategy. When I need some new functionality in my browser I would  create a new API, and craft a bookmarklet that would accomplish whatever  I needed. I wish I had more browser add-on development skills,  something I regular try to invest in, but I find that bookmarklets are  the next best thing when it comes to browser and API interactions.</p>
<p>There are a number of tasks I am looking to accomplish when I&rsquo;m  browsing the web pages of an API provider. The first thing I want to do  is record their domain, then retrieve as much intelligence about the  company behind the domain in a single click of the bookmarklet. This was  the first bookmarklet and API I developed. Since then, I&rsquo;ve made  numerous others to record the pricing page, parse the terms of service,  OpenAPI, and other valuable API artifacts from across the landscape.  Bookmarklets are a great way to provide just a little more context  combined with a URL pointer, for harvesting, processing, and possibly  some human review. Allowing me to augment, enrich, and automate how I  consume information as I&rsquo;m roaming around the web, researching specific  topics, and do what I do.</p>
<p>At this point I am actually glad I didn&rsquo;t invest a lot of energy into  developing Chrome browser extension, because it wouldn&rsquo;t have easily  translated to a Firefox world. Since I have been investing in APIs plus  bookmarklets, I can easily import, or copy and paste my bookmarklets  over. I&rdquo;m spending the time to go through them, inventory them, and  better organize them for optimal usage, so the migration is a little  more work than just import and export. Another aspect of this work that I  am thankful for is that I abstracted away is the usage of other 3rd  party APIs. My very first bookmarklet which profiles the domain of the  website I&rsquo;m looking at has used several different business intelligence  solutions, all of which I have been priced out of using, so I&rsquo;ve  resorted to other ways to obtain the profile information I need&ndash;the API  continues to work despite the APIs I use under the hood.</p>
<p>Browsers are an area of my API research that is significantly  deficient. I am working to invest a little more time here, focusing on  the migration and evolution of my API driven bookmarklets, but also  playing around with the <a href="https://developer.mozilla.org/en-US/docs/Web/API/Reporting_API">Browser Reporting API</a>,  which is some pretty interesting HEADER voodoo. I can&rsquo;t help but feel  like the browsers will continue to play an increasingly important role  when it comes to APIs. Not just because of browser APIs like the  Reporting API, but also because of the hidden APIs web and mobile  applications use, as well as the above the tables APIs we leverage  within the browser&mdash;-like my bookmarklets. I find the browser a more  interesting place to study how APIs are being put to work than with  startups these days. I feel like it is where the &ldquo;innovation&rdquo; is  occurring these days, and sadly, it isn&rsquo;t the good kind of &ldquo;innovation&rdquo;  everyone so passionately believes in&mdash;-it is the more exploitative  ad-driven &ldquo;innovation&rdquo; that is pretty invasive in our lives.</p>
        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/24/apis-and-browser-bookmarklets/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/24/absolutism-around-api-tools-increases-friction-and-failure/">Absolutism Around API Tools Increases Friction And Failure</a></h3>
        <span class="post-date">24 Jul 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/legal-statue-legalstatue-smoking-cigarette.jpg" width="45%" align="right" style="padding: 15px;" />
I know you believe your tools are the best. I mean, from your vantage point, they are. I also know that when you are building a new API tool, your investors want you to position your tooling as the best. The one solution to rule them all. They want you to work hard to make your tools the best, but also make sure and demonize other tooling as being inferior, out of date, and something the dinosaurs use. I know this absolute belief in your tooling feels good and right, but you are actually creating friction for your users, and potentially failure or at least conflict within their teams. Absolutism, along with divide and conquer strategies for evangelizing API tooling works for great short term financial strategies, but doesn’t do much to help us on the ground actually developing, managing, and sustaining APIs.

<p>Ironically, there are many divers factors that contribute to why API tooling providers and their followers resort to absolutism when it comes to marketing and evangelizing their tools. Much of which has very little to do with the merits of the tools being discussed, and everything about those who are making the tools. I wanted to explore a few of them so they are available on the tip of my tongue while working within the enterprise.

<ul>
  <li><strong>No Due Diligence On What Is Out There</strong> - Most startups who are developing API tooling do not spend the time understanding what already exists across the landscape, and get outside of the echo chamber to learn what real world companies are using to get the job done each day.</li>
  <li><strong>No Learning Around Using Existing Tools</strong> - Even if startups are aware of existing tools, patterns, and processes, they rarely invest the time to actually understand what existing tools deliver—spending time to deeply understand how existing tools are being put to use by their would-be customers.</li>
  <li><strong>Lack Of Awareness Around The Problem</strong> - There is a reason investors prefer young engineers when it comes to developing the next set of disruptive tooling, because they rarely understand the scope of problems being solved, and provide great fuel for short to mid-term growth strategies.</li>
  <li><strong>Aggressive Male Dominated Environment</strong> - Young white men are perfect for this approach to delivering tooling that isn’t about the tool, but about a larger economic strategy, putting us passionate, privileged souls at the helm, and push them to do the disruptive bidding with very little awareness of the big picture.</li>
  <li><strong>No Empathy For Others You Encounter</strong> - API tooling that takes an absolutist approach is rarely about empowering others, or understanding and providing solutions to their problems—lacking in empathy for other tooling providers, tooling consumers, or the companies left with each round of tech debt.</li>
  <li><strong>Lack Of Diverse Experience In Industry</strong> - Entrepreneurs who ride each wave of API tooling absolutism and state their API tool is the one solution often lack experience in a variety of industries, and rarely have diverse experience outside of the - Silicon Valley echo chamber, and across multiple industries or geographic regions.</li>
  <li><strong>VC Backed With Aggressive Growth</strong> - The aggressive absolutist approach of each wave of API tooling is almost always fueled by aggressive funding cycles, and have very little to do with the actual application of API tooling—operating the puppet strings which most API tooling providers and consumers on the front line do not see.</li>
</ul>

<p>If you are in the business of tearing down someone else to deliver your tool, your tool will die by the same approach–someday. There is always a better funded, more aggressive solution to emerge on the market. Even if your tool has managed to achieve some level of market success, there will be a time when you let your guard down, and someone comes along to begin taking jabs at you. With each cycle of absolutism assault, the merits of the tooling mean very little. Perception always trumps reality, and there are always armies of developers waiting by in the wings to adopt what is new, and begin raising a pitchfork to attack what was. There is no allegiance and loyalty in this game.

<p>I know. I know. This is just business. I just don’t get the game. Smart people have to make money! Yes, there are also many of us who are responsible for keeping the lights on. That aren’t as disloyal as you are, willing to jump from job to job, startup to startup. There are many of us who have been doing this a lot longer than you, and are willing to be responsible for the tech debt we incur along the way, and we do not mind doing the hard work to clean up your messes. I know that API tool absolutism makes you feel knowledgable and in control now, but just wait until you’ve ridden a few waves, and you’ve had many of your valuable tooling taken away from you because of this game. Then you will begin to see the other side of this, and better understand the toll of this business approach. Eventually you will grow weary of it, but fortunate for you, there will always be a fresh crop of recruits to wage this battle, and there is no rest for the wicked. #liveByDisruption #DieByDisruption


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/24/absolutism-around-api-tools-increases-friction-and-failure/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/23/the-higher-level-business-politics-that-i-am-not-good-at-seeing-in-the-api-space/">The Higher Level Business Politics That I Am Not Good At Seeing In The API Space</a></h3>
        <span class="post-date">23 Jul 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/los-angeles-from-observatory-losangeles-from-observatory-purp-paper.jpg" width="45%" align="right" style="padding: 15px;" />
I have built successful startups. I’m good at the technology of delivering new solutions. I am decent at understanding and delivering much of business side of bringing new technological solutions to market. What I’m not good at is the higher level business politics that occur. These are the invisible forces behind businesses that I’m good at seeing, playing in, and almost always catch me off guard, too late, or just simply piss me off that they are going on behind the scenes. Unfortunately it is in this realm where most of the money is to be made doing APIs, resulting in me being written out of numerous successful API efforts, because I’m not up to speed on what is going on.

<p>Startups are great vehicles for higher level economic strategies. They are the playground of people with access to resources, and have economic visions that rarely jive with what is happening on the ground floors. Startup strategies count on a handful at the top understanding the vision, with most at the bottom levels not being able to see the vision, and small group of disposable stooges in the middle, ensuring that the top level vision is realized—at all costs. You can work full time at a startup, and even enjoy a higher level position, and still never see the political goings on that are actually motivating the investment in your startup. This is by design. The whole process depends on the lower levels working themselves to the bone, working on, marketing, and selling one vision, while there are actually many other things going on above, usually with a whole other set of numbers.

<p>After 30 years of playing in this game I still stuck at seeing the higher level influences. I’ve seen shiny API tooling solution after shiny API tooling solution arrive on the market, and I still fall for the shine. Ooooohhh, look at that. It will solve X, or Y problem. I really like the vision of those team members. Their timing is perfect. They seem to have the right funding, and mindshare of developers. Then I begin to see some of the usual tell-tale signs of direction coming from up above. It will be subtle signals, like the change in pricing tiers, a quickness to support a standard on import, but very slow to support export. A shift in the marketing strategy. A public “pivot”. There are a diverse of signals you can tune into that will help predict where an API startup is headed, often times away from the original tooling vision, and the needs of the end-users.

<p>With so much experience, you’d think I’d be better at this. I’m not good at it, because I hate playing these games. I like making money, but not in the way that follow the usual VC fueled playbook. To make money at scale you have to be willing to play by multiple playbooks, keeping one or more of them secret from your teams and end-users. This just isn’t me. I prefer being more transparent. I like building real businesses. I like developing real tools. This is what I’m good at. I’m not good at the higher level games required to build wealth for myself or others. It is this reality that leaves me so reluctant to share my knowledge with VCs, talk to and support new startups, and leaves me so cranky on a regular basis when I tell stories in the space. I know y’all think this is business as usual, and are looking to get your piece of the pie, but I operate at a different level, and refuse to go there.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/23/the-higher-level-business-politics-that-i-am-not-good-at-seeing-in-the-api-space/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/23/api-provider-and-consumer-developer-portals/">API Provider And Consumer Developer Portals</a></h3>
        <span class="post-date">23 Jul 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/server-cloud-server-racks-clouds-copper-circuit.jpg" width="45%" align="right" style="padding: 15px;" />
I’ve been studying API developer portals for almost a decade. I’ve visited the landing pages, portals, websites, and other incarnations from thousands of API providers. I have an intimate understanding of what is needed for API providers to attract, support, and empower API consumers. One area I’m deficient in, and I also think it reflects a wider deficiency in the API space, is regarding how to you make an API portal service both API providers and API consumers. Providing a single portal within the enterprise where everyone can come and understand how to deliver or consume an API.

<p>There are plenty of examples out there now when it comes to publishing an API portal for your consumers, but only a few that show you how to publish an API. I’d say the most common example are API marketplaces that allow both API consumers and providers to coexist, but this model isn’t exactly what you want within the enterprise. One thing the model lacks is the on-boarding of new developers when it comes to actually developing an API. Suffering from many of the same same symptoms API management service providers have historically suffered from—-not providing true assistance when it comes to delivering a quality API.

<p>When I envision an API portal that serves both providers and consumers, either publicly or privately, I envision just as much assistance when it comes to delivering a new API as we provide for new consumers of an API. Helping with API definition, design, deployment, management, testing, monitoring, documentation, and other critical stops along the API lifecycle. We need to see more examples of the split between API provider and consumers, equally helping both sides of the coin get up to speed, and be successful with what they are looking to achieve. I think we’ve spend almost 15 years investing in perfecting and monetizing the API portal with a focus not he consumer, and now we need to invest on helping make the portal easier for new API providers to step up and learn how to properly publish their API.

<p>The modern API management solution is still tailored for the mystical API provider who knows how do to everything, where most do not understand the full API lifecycle. It would be an opportunity for an API management provider to go beyond just one or a handful of stops along the API lifecycle, and properly invest in on boarding new APIs. I think one reason why all of this suffers is that venture capitalists have never prioritized education and training for both API providers or consumers—-directing API service providers to only lightly invest when it comes to these API educational resources. Now that APIs have gone mainstream, we are going to need an industrial grade enterprise solution for delivering API portals that help onboard both API providers and consumers, and provide them both with what they need to navigate the entire lifecycle of the API solutions they are providing and applying.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/23/api-provider-and-consumer-developer-portals/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/22/the-role-having-awareness-of-your-api-traffic-plays-in-api-security/">The Role Having Awareness Of Your API Traffic Plays In API Security</a></h3>
        <span class="post-date">22 Jul 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/IMG_4038_blue_circuit.jpg" width="45%" align="right" style="padding: 15px;" />
One of the biggest reasons we adopt new technology, and justify the development of new technology, is we do not want to do the heavy lifting when it comes to what we already have in place. A common illness when it comes to API security that I’ve been battling since 2015 is that you will have API security addressed once you adopted an API management solution. Your APIs require API keys, and thus they are secure. No further work necessary. The unwillingness or lack of knowledge regarding what is needed next, leaves a vacuum for new technology providers to come in and sell you the solution for what is next, when you should be doing more work to use the tools you already have.

<p>When it comes to API management, most vendors sold it as purely a security solution, and when companies implement it they become secure. Missing the entire point for why we do API management-—to develop an awareness of our API usage and consumption. Having keys for your APIs is not enough. You actually have to understand how those API consumers are putting API resources to work, otherwise your API security will always be deficient. Some of the fundamentals of API management you should be employing as part of your API security are:

<ul>
  <li><strong>Registration</strong> - Make all developers sign of for API usage, establishing the terms of use.</li>
  <li><strong>API Keys</strong> - Require all developers internal or external to use API keys for every application.</li>
  <li><strong>API Usage</strong> - Which APIs are being used by all API consumers putting them to use in applications.</li>
  <li><strong>API Errors</strong> - Understanding what the errors being generated are, and who is responsible for them.</li>
  <li><strong>Logging</strong> - The logging of all API traffic, reconciling against what you know as reported usage.</li>
  <li><strong>Invoicing</strong> - Invoicing of all consumers for their usage, even if they aren’t paying you money.</li>
  <li><strong>Reporting</strong> - Provide reports on API usage for all stakeholders, to regularly develop awareness.</li>
</ul>

<p>These are the fundamentals of API management, however API keys and tokens seem to be the part that people feel is API security. Where API security is really all about actually developing a real-time awareness of who is using your API resources. Leaving your finger on the pulse so that when anything changes, or error rates are elevated, you already have a base level of awareness and can easily respond by shutting off keys, or limiting overall access to resources by offending applications.

<p>There is much more that can be done in the name of API security. This is just a list of the elements of API management that contribute to API security, which are often neglected. Having API management does not equal API security. Properly applying API management contributes to API security, it is never API security by itself. If you aren’t doing API management properly, you are more likely to fall for the next generation of API security providers who are machine learning focused, promising to do the hard work of managing awareness for you, so you don’t have to. Your unwillingness to do the work in the first place, and properly understand the role that awareness of your traffic, makes you a ripe target for selling the next wave of API security solutionism. Good luck with that!


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/22/the-role-having-awareness-of-your-api-traffic-plays-in-api-security/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/22/happy-path-api-testing-bias/">Happy Path API Testing Bias</a></h3>
        <span class="post-date">22 Jul 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-stories-DSC-0084-dali-three.jpg" width="45%" align="right" style="padding: 15px;" />
I see a lot of happy path bias when it comes to the development of APIs, but specifically when it comes to crafting testing to ensure APIs are delivering as expected. Happy path is a term used in testing to describe the desired outputs a developer and product owner is looking for. Making the not so happy path being about testing for outcomes that a developer and product owner is not wanting to occur. When it comes to API development most developers and product owners are only interested in the happy path, and will almost always cut corners, minimize the investment in, or completely lack an imagination when it comes to less than happy path API testing.

<p>There are many reasons why someone will have a bias towards the happy path when developing an API. Every API provider is invested in achieving the happy path for delivering, providing, and consuming an API. This is what generates revenue. However, in this quest for revenue, we often become our own worst enemy. Shining a spotlight on the happy path, while being completely oblivious to what the not so happy paths will look like for end users. Why do we do this?

<ul>
  <li><strong>Greed</strong> - We are so interested in getting an API up and running, used in our applications, and generating behavioral surplus, we are more than willing to ignore all other possible scenarios if we can easily meet our revenue goals by ignoring the unhappy path and there are no consequences.</li>
  <li><strong>Tickets</strong> - Most development occurs using JIRA or other software development “tickets”, which tell developers what they are supposed to do to meet the requirements of their employment—tickets are written with the happy path in mind, and developers are rarely willing to do more.</li>
  <li><strong>Imagination</strong> - While many of us technologists think we are imaginative creatures, most of us are pretty stuck in a computational way of thinking, and elaborating, iterating, and exploring beyond the initial happy path design of our API just does not exist.</li>
  <li><strong>Use Software</strong> - Most of us developers do not actually use the platform we are developing, setting the stage for where we really don’t understand the problem being solve, further siloing us into seeing only the happy path that have been handed to us as part of initial product vision.</li>
  <li><strong>White Male</strong> - The majority of us API developers are white men, or developers who report to white men, leaving entire shadows regarding how our APIs will be used and abused—when you are privileged, the happy path is always easier to see and walk on.</li>
  <li><strong>Apathy</strong> - The majority of us are just doing our jobs, and we really do not have any excitement, passion, or interest in our jobs. We are just doing what we are told, and if our bosses do not specifically point out every single unhappy path, we don’t care.</li>
  <li><strong>Velocity</strong> - Things move fast at almost any company delivering APIs, and it is easy to not have time to be able to step back and sufficiently think about what the happy paths might be when we are delivering APIs that deliver some functionality amidst a fast pace environment.</li>
  <li><strong>Experience</strong> - Another reason for overlooking unhappy paths is we just do not have the experience to know about them. Startups and many technology focused companies like hiring young, low pay developers to get the job done, and they won’t always have the experience to see in the shadows.</li>
  <li><strong>By Design</strong> - The product owners do not want the less than happy or unhappy paths patched, as they are there by design, and support the overall business model, which is usually advertising. Encouraging abuse, and exploitation of APIs, or at least ensuring they are much lower priorities.</li>
</ul>

<p>There are few incentives to develop quality software these days. Revenue drives much of why we are delivering APIs, and incentivizing developers to think out of the box when it comes to API testing just doesn’t exist. Plus, it takes a lot of work to write first class tests alongside your code. Most developers are conditioned to see tests as secondary, and the thing you do only when you have the time. Making quality unhappy and less than happy path API testing always left on the cutting room floor, never making it into the final product.

<p>You can see this bias playing out in the APIs behind Facebook, Twitter, and other advertising driven platforms. The abuse of APIs are often overlooked if it generates clicks, traffic, and increases the eyeballs. Secondarily I’d say that the consequences for when unhappy paths are identified for APIs is almost non-existent. There is no accountability for poorly designed APIs, or APIs that allow for uses beyond their intended purpose. In this environment, most API providers will never prioritize API testing, and incentivize developers to properly explore how an API can be misused, abused, or just not deliver the functionality promised. Ensuring that much of API usage exists on the unhappy path by design.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/22/happy-path-api-testing-bias/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/18/what-makes-you-think-your-graphql-consumers-will-want-to-do-the-work/">What Makes You Think Your GraphQL Consumers Will Want To Do The Work</a></h3>
        <span class="post-date">18 Jul 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-stories-status-berlin-matrix.jpg" width="45%" align="right" style="padding: 15px;" />
Data work is grueling work. I’ve been working with databases since my first job developing student information databases in 1988 (don’t tell my wife). I’ve worked with Cobol, Foxpro, SQL Server, Filemaker, Access, MySQL, PostGres, and now Aurora databases over the last 30 years. I like data. I can even trick myself into doing massive data and schema refinement tasks on a regular basis. It is still grueling work that I rarely look forward to doing. Every company I’ve worked for has a big data problem. Data is not easy work, and the more data you accumulate, the more this work grows out of control. Getting teams of people to agree upon what needs to happen when it comes to schema and data storage, and actually execute upon the work in a timely, cost effective, and efficient way is almost always an impossible task. Which leaves me questioning (again), why GraphQL believers think they are going to be successfully in offshoring the cognitive and tangible work load to understand what data delivers, and then successfully apply it to a meaningful and monetizable business outcome.

<p>Don’t get me wrong. I get the use cases where GraphQL makes sense. Where you have an almost rabid base of known consumers, who have a grasp on the data in play, and possesses an awareness of the schema behind. I’m have made the case for GraphQL as a key architectural component within a platform before. The difference in my approach over the majority of GraphQL believers, is that I’m acknowledging there is a specific group of savvy folks who need access to data, and understand the schema. I’m also being honest about who ends up doing the heavy data lifting here—-making sure this group wants it. However, I have an entirely separate group of users (the majority) who do not understand the schema, and have zero interest in doing the hard work to understand the schema, navigate relationships, and develop queries—-they just want access. Now. They don’t want to have to think about the big picture, they want one single bit of data, or a series of bits.

<p>I’ve worked hard to engage in debate with GraphQL believers, and try to help provide them with advice on their approach. They aren’t interested. They operate within the echo chamber. They see a narrow slice of the API pie, and passionately believe their tool is the one solution. Like many API tooling peddlers who originate from within the echo chamber, they are hyper focused on the technology of managing our data. They think all data wranglers are like them. They think all data-driven companies are like theirs. They do not see the diverse types of API solutions that I see working across companies, institutions, organizations, and government agencies. They are not always aware of the business and political barriers that lie in between a belief in an API tool, and achieving a sustained implementation across the enterprise. They have that aggressive, tenacious startup way of penetrating operations, something that works well within the echo chamber, but it is an approach that will lose strength, and even begin to work against you outside the eco chamber within mainstream business operations.

<p>I definitely see some interesting and useful tooling coming out of the GraphQL community. GraphQL is a tool in my API toolbox right along with REST, Hypermedia, Siren, HAL, JSON API, Alps, JSON Schema, Schema.org, OpenAPI, Postman Collections, Async API, Webhooks, Kafka, NATS, NGINX, Docker, and others. It has a purpose. It isn’t the silver bullet for me getting a handle on large amounts of data. It is one thing I consider when I’m trying to make sense of large data sets, and depending on the platform, the resources I have on staff, and the consumers, or the industry I am operating in–I MAY apply GraphQL. However, the times I will be able to successfully get it in the door, past legal, approved by leadership, accepted by internal developers, and then accepted and applied successfully by external developers, will be much fewer because of the aggressive echo chamber, investor-driven approach, which does not help sell the tool to my enterprise customers who have been investing in evolving their schema, and developing a suite of internal and external APIs over the last 20+ years. You might help me sell to a Silicon Valley savvy company, but you aren’t helping me in the mainstream enterprise.

<p>This post will get the usual lineup of critical Tweets and comments. It’s fine. Once again the fact that I’m trying to help will be lost on believers. You will be more successful if you are honest about how the cognitive and  tangible workload is being shifted here. You are refusing to do the hard work to properly define and organize your schema, and provide meaningful imperative API capabilities that any developer can easily use, over providing a single declarative interface (and some tooling) to empower consumers to make sense of the schema, and access platform capabilities on their own. This works well with folks who are willing to take on the cognitive load of knowing the schema, knowing GraphQL, then are willing to accept the real work of crafting queries to get at what they desire. There are a whole lot of assumptions there that do not apply in all situations. I’d say about 12% of people I’ve worked with in my career would sign up for this. The rest of them, just want to get their task accomplished—get out of their way and give them an interface that is capable of accomplishing it for them.

<p>Nobody wants to do data shit work. If you do, get help. It is a job that is only growing because our ability to generate, harvest, and store data has grown. There is a belief that data provides answers, without being really honest about what it takes to actually clean, refine, and organize the data, let alone any truthfulness regarding whether or not the answers are ever even there after we do invest in doing the dirty data work. Everyone is drowning in data. Everyone is chasing more data. Few are managing it well. This type of environment makes new data tooling very sexy. However, sexy tools rarely change the actual behavior on the ground within the enterprise. People still aren’t going to change behaviors overnight. GraphQL tooling will not solve all of our data problems. Ideally, we would see more interoperability of tooling between GraphQL and other API design, deployment, management, testing, monitoring, mocking, and client tooling. Like we are seeing with IBM Loopback, Postman, and others. Ideally, we would see less rhetoric around GraphQL being the one solution to rule them all. Ideally, we’d see less investor-driven rhetoric, and more real world solutions for applying through the mainstream business world. But, I’m guessing I’ll see the same response I’ve been getting on these posts since 2016. ;-(


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/18/what-makes-you-think-your-graphql-consumers-will-want-to-do-the-work/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/18/what-is-an-application/">What Is An Application?</a></h3>
        <span class="post-date">18 Jul 2019</span>
        <p>I have struggled asking this question in many discussions I&rsquo;ve had  around the world, at technology conferences, on consulting projects, and  in the back rooms of dimly lit bars. What is an application? You get  ten different answers if you ask this question to ten different people.  I&rsquo;d say the most common response is to reference the applications on a  mobile device. These are the relevant. Most accessible. The most active  and visible form of application in our modern life. Older programmers  see them as desktop applications, where younger programmers see them as  web applications, with varying grades of server applications in between.  If you operate at the network layer, you&rsquo;ve undoubtedly bastardized the  term to mean several different things. Personally, I&rsquo;ve always tried to  avoid these obvious and tangible answers to this question, looking  beyond the technology.</p>
<p>My view of what an application is stems from a decade of studying the  least visible, and least tangible aspect of an application, its  programming interface. When talking to people about applications, the  first question I ask folks is usually, &ldquo;do you know what an API is&rdquo;? If  someone is API savvy I will move to asking, &ldquo;when it comes to  application programming interface (API), who or what is being  programmed? Is it the platform? The application? Or, is it the end-user  of the applications?&rdquo; I&rsquo;ve spent a decade thinking about this question,  playing it over and over in my end, never quite being satisfied with  what I find. Honestly, the more I scratch, the more concerned I get, and  the more I&rsquo;m unsure of exactly what an &ldquo;application&rdquo; is, and precisely  who are what is actually being programmed. Let&rsquo;s further this line of  thinking by looking at the definitions of &ldquo;application&rdquo;:</p>
<ul>
<li>noun - The act of applying.</li>
<li>noun - The act of putting something to a special use or purpose.</li>
<li>noun - A specific use to which something is put.</li>
<li>noun - The capacity of being usable; relevance.</li>
<li>noun - Close attention; diligence.</li>
<li>noun - A request, as for assistance, employment, or admission to a school.</li>
<li>noun - The form or document on which such a request is made.</li>
<li>noun - Computers A computer program designed for a specific task or use.</li>
<li>adjective - Of or being a computer program designed for a specific task or use.</li>
</ul>
<p>This list comes from my friends over at Wordnik  (https://www.wordnik.com/words/application), who I adore, have an API  (https://developer.wordnik.com/), and who have contributed to this  discussion by playing a leading role in introducing the API sector to  the OpenAPI (fka Swagger) specification to the API community. That is  whole other layer of significance when it comes to the semantics and  meaning behind the word application, which I will have to write more  about in a future post. In short, words matter. Really, words are  everything. The meaning behind them. The intent. The wider understanding  and belief. This is why APIs, and web applications like Wording are  important. Helping us make sense of the world around us. Anyways, back  to what is an application?</p>
<p>I like the first entry on the list &mdash; the act of applying. But, each  of these definitions resonate with my view of the landscape. Yet, I&rsquo;d  say that the most common answer to this question hover towards the  second half of this list, not the first half of it. Most application  developers would say they are programming the application&mdash;the interface  is for them. If you are an API developer you believe that are the one  programming the application, where higher up on the platform decision  making chain, they are the ones programming the application, and  applying their vision of the world. Something that isn&rsquo;t always visible  at the lower levels, by API developers delivering APIs, the application  developers consuming them, or the end-users of the tangible applications  being delivered. I see the end desktop, web, or mobile as an  application. I see the API as an application. I see the network  connecting the two as an application. I also see the wider ideology  being applied across all these layers, even when it is out of view to  the outer layers.</p>
<p>One of the biggest imbalances in my belief system around technology, a  result of be operating at the lower levels of business, institutional,  or government, is that I am the one &ldquo;applying&rdquo; and &ldquo;programming&rdquo;. I was  developing and delivering the application. These interfaces served me.  After studying the machine closer. Tracking on the cycles. Documenting  the results over the course of many cycles. I began to realize that  there is more to this &ldquo;application&rdquo; thing than what I&rdquo;m seeing. Maybe I  was too close to the gears and the noise of the machine to see the  bigger picture. I&rsquo;m in the role of application developer not because I&rsquo;m  good at what I do. I&rsquo;m there because I conveniently think I&rsquo;m in  control of this supply chain. As I worked my way up the supply chain,  and became an API developer I continue to believe that I was the one  &ldquo;applying&rdquo; and &ldquo;programming&rdquo;. However after over a decade of doing that,  I&rsquo;m realizing that I am not the one calling the shots. I&rsquo;m applying  something for someone else, and that applications were much more than  just an iPhone or Android application, or even a TCP, FTP, STMP, HTTP,  HTTP/2, or other protocol application. How you answer this questions  depends on where you operate within the machine.</p>
<p>After climbing my way up through the layers of the machine, and  finding my way to hatch at the top, finally getting some air in my  lungs&mdash;-I still do not find myself in a better position. With more  knowledge, just comes more concerns. Sometimes I wish I could climb back  down to the lower levels, and enjoy the warmth and comfort of the inner  workings of the machine. However, no I can&rsquo;t find comfort in the toil  that used to comfort me online late into the night. I can&rsquo;t ignore what  we are applying when we work in the service of the application. We are  doing the hard work to mine, develop, integrate, and extract value. We  are doing the dirty work of applying the vision of others. We are doing  the hard work of laying the digital railroad tracks for the Internet  tycoons. We are imposing their vision on the world. The special use or  purpose of APIs do not always serve us. The usability and relevance is  only minimally focused on us. The close attention and diligence is  centered on maximum extraction and value generation for the platform.  All wrapped in a computer program, designed for a specific task or use.  Obfuscating the true application, with digital eye candy that keeps us  always connected, and always open to something new being applied or  directed in our life.</p>
        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/18/what-is-an-application/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/17/the-many-ways-in-which-apis-are-taken-away/">The Many Ways In Which APIs Are Taken Away</a></h3>
        <span class="post-date">17 Jul 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/catacombs-catacombs-copper-circuit.jpg" width="45%" align="right" style="padding: 15px;" />
APIs are notorious for going away. There are so many APIs that disappear I really stopped tracking on it as a data point. I used keep track of APIs that were shuttered so that I could play a role in the waves of disgruntled pitchfork mobs rising up in their wake–it used to be a great way to build up your Hacker News points! But, after riding the wave a couple hundred waves of APIs shuttering, you begin to not really not give a shit anymore—-growing numb to it all. API deprecation grew so frequent, I wondered why anyone would make the claim that once you start an API you have to maintain it forever. Nope, you can shut down anytime. Clearly.

<p>In the real world, APIs going away is a fact of life, but rarely a boolean value, or black and white. There are many high profile API disappearances and uprising, but there are also numerous ways in which some API providers giveth, and then taketh away from API consumers.:

<ul>
  <li><strong>Deprecation</strong> - APIs disappear regularly both communicated, and not so communicated, leaving consumers scratching their heads.</li>
  <li><strong>Disappear</strong> - Companies regularly disappear specific API endpoints acting like they were never there in the first place.</li>
  <li><strong>Acquisition</strong> - This is probably one of the most common ways in which high profile, successful APIs disappear.</li>
  <li><strong>Rate Limits</strong> - You can always rate limit away users make APIs inaccessible, or barely usable for users, essentially making it go away.</li>
  <li><strong>Error Rates</strong> - Inject elevated error rates either intentionally or unintentionally can make an API unusable to everyone or select audience.</li>
  <li><strong>Pricing Tiers</strong> - You can easily be priced out of access to an API making it something that acts just like deprecating for a specific group.</li>
  <li><strong>Versions</strong> - With rapid versioning, usually comes rapid deprecation of earlier versions, moving faster than some consumers can handle.</li>
  <li><strong>Enterprise</strong> - APIs moving from free or paid tier, into the magical enterprise, “call us” tier is a common ways in which APIs go away.</li>
  <li><strong>Dumb</strong> - The API should not have existed in the first place and some people just take a while to realize it, and then shut down the API.</li>
</ul>

<p>I’d say Facebook, Twitter, and Google shutting down, or playing games in any of these areas have been some of the highest profile. <a href="http://apievangelist.com/2017/12/04/facebook-quietly-deprecates-the-audience-insight-api-used-to-automate-targeting-during-the-election/">The sneaky shuttering of the Facebook Audience Insight API was one example</a>, but didn’t get much attention. I’d say that <a href="http://apievangelist.com/2017/03/28/i-think-the-parse-twitter-page-sums-it-up-pretty-well/">Parse is one that Facebook</a> handled pretty well. Google did it with Google+ and Google Translate, but then brought back it with a paid tier. LinkedIn regularly locks down and disappears its APIs. Twitter has also received a lot of flack for limiting, restricting, and playing games with their APIs. In the end, many other APIs shutter after waves of acquisitions, leaving us with as my ex-wife says–“nothing nice”!

<p>Face.com, Netflix, 23andMe, Google Search, ESPN, and others have provided us with lots of good API deprecation stories, but in reality, most APIs go aware without much fanfare. You are more likely to get squeezed out by rate limits, errors, pricing, and other ways of consciously making an API unusable. If you really want to understand the scope of API deprecation visit the leading deprecated API directory ProgrammableWeb—-they have thousands of APIs listed that do not exist anymore. In the end it is very difficult to successfully operate an API, and most API providers really aren’t in it for the long haul. The reasons why APIs stay in existence are rarely a direct result of them being directly financially viable. Developers squawk pretty loudly when the free API they’ve been mooching off of disappears, but there are many, many, many other APIs that go away and nobody notices, or nobody talks about. In the world of APIs there are very few things you can count on being around for very long, and you should always have a plan B and C for every API you depend on.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/17/the-many-ways-in-which-apis-are-taken-away/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/17/paying-for-api-access/">Paying for API Access</a></h3>
        <span class="post-date">17 Jul 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-stories-old-door-lock-copper-circuit.jpg" width="45%" align="right" style="padding: 15px;" />
APIs that I can’t pay for more access grinds my gears. I am looking at you GitHub, Twitter, Facebook, and a few others. I spend $250.00 to $1500.00 a month on my Amazon bill, depending on what I’m looking to get done. I know I’m not the target audience for all of these platforms, but I’m guessing there is a lot more money on the table than is being acknowledged. I’m guessing that the reason companies don’t cater to this, is that there are larger buckets of money involved in what they are chasing behind the scenes. Regardless, there isn’t enough money coming my way to keep my mouth shut, so I will keep bitching about this one alongside the inaccessible pricing tiers startups like to employ as well. I’m going to keep kvetching about API pricing until we are all plugged into the matrix—-unless the right PayPal payment lands in my account, then I’ll shut up. ;-)

<p>I know. I know. I’m not playing in all your reindeer startup games, and I don’t understand the masterful business strategy you’ve all crafted to get rich. I’m just trying to do something simple like publish data to GitHub, or do some basic research on an industry using Twitter. I know there are plenty of bad actors out there who want also access to your data, but it is all something that could be remedied with a little pay as you go pricing, applied to some base unit of cost applied to your resources. If I could pay for more Twitter and GitHub requests without having to be in the enterprise club, I’d be very appreciative. I know that Twitter has begun expanding into this area, but it is something that is priced out of my reach, and not the simple pay as you go pricing I prefer with AWS, Bing, and other APIs I happily spend money on.

<p>If you can’t apply a unit of value to your API resources, and make them available to the masses in a straightforward way—-I immediately assume you are up to shady tings. Your business model is much more loftier than a mere mortal like me can grasp, let alone afford. I am just an insignificant raw material in your supply chain—-just be quiet! However, this isn’t rocket science. I can’t pay for Google Searches, but I can pay for Bing searches. I can’t pay for my GitHub API calls. I’m guessing at some point I’ll see Bing pricing go out of reach as Microsoft continues to realize the importance of investing at scale in the surveillance economy—-it is how you play in the big leagues. Or maybe they’ll be like Amazon, and realize they can still lead in the surveillance game while also selling things to the us lower level doozers who are actually building things. You can still mine data on what we are doing and establish your behavioral models for use in your road map, while still generating revenue by selling us lower level services.

<p>The problem here ultimately isn’t these platforms. It is me. Why the hell do I keep insisting on using these platforms. I can always extricate myself from them. I just have to do it. I’d much rather just pay for my API calls, and still give up my behavioral data, over straight extraction and not getting what I need to run my business each day. I feel like the free model, with no access to pay for more API calls is a sign of a rookie operation. If you really want to operate at scale, you should be obfuscating your true intentions with a real paid service. If you are still hiding behind the free model, you are just getting going. The grownups are already selling services for a fair price as a front, while still exploiting us at levels we can’t see, so ultimately they can compete with all of us down the road. Ok, in all seriousness, why can’t we get on a common model for defining API access, and setting pricing. I’m really tired of all the games. It would really simplify my life if I could pay for what I use of any resource. I’m happy to pay a premium for this model, just make sure it is within my reach. Thanks.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/17/paying-for-api-access/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/16/imperative-declarative-and-workflow-apis/">Imperative, Declarative, and Workflow APIs</a></h3>
        <span class="post-date">16 Jul 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/server-cloud-server-racks-clouds-smoking-cigarette.jpg" width="45%" align="right" style="padding: 15px;" />
At every turn in my API work I come across folks who claim that declarative APIs solutions are superior to imperative ones. They want comprehensive, single implementation, do it all their way approaches, over granular, multiple implementation API calls that are well defined by the platform. Declarative calls allow you to define a single JSON or YAML declaration that can then be consumed to accomplish many things, abstracting away the complexity of doing those many things, and just getting it done. Imperative API interfaces require many individual API calls to tweak each and every knob or dial on the system, but is something that is often viewed as more cumbersome from a seasoned integrator, but for newer, and lower level integrators a well designed imperative API can be an important lifeline.

<p>Declarative APIs are almost always positioned against imperative APIs. Savvier, more experienced developers almost always want declarations. Where newer developers and those without a full view of the landscape, often prefer well designed imperative APIs that do one thing well. From my experience, I always try to frame the debate as imperative and declarative where the most vocal developers on the subject prefer to frame it as declarative vs imperative. I regularly have seasoned API developers “declare” that I am crazy for defining every knob and dial of an API resource, without any regard for use cases beyond what they see. They know the landscape, don’t want to be burdened them with having to pull every knob and dial, just give them one interface to declare everything they desire. A single endpoint with massive JSON or YAML post or abstract it all away with an SDK, Ansible, GraphQL, or a Terraform solution. Demanding that a platform meet their needs, without ever considering how more advanced solutions are delivered and executed, or the lower level folks who are on boarding with a platform, and may not have the same view of what is required to operate at that scale.

<p>I am all for providing declarative API solutions for advanced users, however, not at the expense of the new developers, or the lower level ones who spend their day wiring together each individual knob or dial, so it can be used individually, or abstracted away as part of more declarative solution. I find myself regularly being the voice for these people, even though I myself, prefer a declarative solution. I see the need for both imperative and declarative, and understand the importance of good imperative API design to drive and orchestrate quality declarative API implementations, and even more flexible workflow solutions, which in my experience are often what processes, breaks down, and executes most declarations that are fed into a system. The challenge in these discussions are that the people in the know, who want the declarative solutions, are always the loudest and usually higher up on the food chain when it comes to getting things done. They can usually rock the boat, command the room, and dictate how things will get delivered, rarely ever taking into consideration the full scope of the landscape, especially what lower level people encounter in their daily work.

<p>For me, the quality of an API always starts with the imperative design, and how well you think through the developer experience around every dial and knob, which will ultimately work to serve (or not), a more declarative and workflow approach. They all work together. If you dismiss the imperative, and bury it within an SDK, Ansible, or Terraform solution, you will end up with an inferior end solution. They all have to work in concert, and at some point you will have to be down in the weeds turning knobs and dials, understanding what is possible to orchestrate the overall solution we want. It is all about ensuring there is observability and awareness in how our API solutions work, while providing very granular approaches to putting them to work, while also ensuring there are simple, useable comprehensive approaches to moving mountains with hundreds or thousands of APIs. To do this properly, you can’t be dismissing the importance of imperative API design, in the service of your declarative—-if you do this, you will cannibalize the developer experience at the lower levels, and eventually your declarations will become incomplete, clunky, and do not deliver the correct “big picture” vision you will need of the landscape.

<p>When talking API strategy with folks I can always tell how isolated someone is based upon whether they see it as declarative vs imperative, or declarative and imperative. If it is the former, they aren’t very concerned with with others needs. Something that will undoubtedly introduce friction as the API solutions being delivered, because they aren’t concerned with the finer details of API design, or the perspectives of the more junior level, or newer developers to the conversation. They see these workers in the same way they see imperative APIs, something that should be abstracted away, and is of no concern for them. Just make it work. Something that will get us to the same place our earlier command and control, waterfall, monolith software development practices have left us. With massive, immovable, monolith solutions that are comprehensive and known by a few, but ultimately cannot pivot, change, evolve, or be used in new ways because you have declared one or two ways in with the platform should be used. Rather than making things more modular, flexible, and orchestrate-able, where anyone can craft (and declare) a new way of stitching things together, painting an entirely new picture of the landscape with the same knobs and dials used before, but done so in a different way than was ever conceived by previous API architects.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/16/imperative-declarative-and-workflow-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/16/hoping-for-more-investment-in-api-design-tooling/">Hoping For More Investment In API Design Tooling</a></h3>
        <span class="post-date">16 Jul 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-stories-new-68-158-800-500-0-max-0-1--1.jpg" width="45%" align="right" style="padding: 15px;" />
I was conducting an accounting of my API design toolbox, and realized it hasn’t changed much lately. It is still a very manual suite of tooling, and sometimes services, that help me craft my APIs. There are some areas I am actively investing in when it comes to API design, but honestly there really isn’t that much new out there to use. To help me remember how we got here, I wanted to take a quick walk through the history of API design, and check in on what is currently available when it comes to investing in your API design process.

<p>API design has historically meant REST. Many folks still see it this way. While there has been plenty of books and articles on API design for over a decade, I attribute the birth of API design to Jakub and Z at Apiary (https://apiary.io). I believe they first cracked open the seed of API design, and the concept API design first. Which is one of the reasons I was so distraught when Oracle bought them. But we won’t go there. The scars run deep, and where has it got us? Hmm? Hmm?? Anyways, they set into motion an API design renaissance which has brought us a lot of interesting thinking on API design, a handful of tooling and services, some expansion on what API design means, but ultimately not a significant amount of movement overall.

<p>Take a look at what AP+I Design (https://www.apidesign.com/) and API Plus (https://apiplus.com/) have done to architecture, API has done for the oil and gas industry (https://www.api.org/), and API4Design has done for the packaging industry (http://api4design.com/)—I am kidding. Don’t get me wrong, good things have happened. I am just saying we can do more. The brightest spot that represents the future for me is over at:

<ul>
  <li><a href="http://stoplight.io"><strong>Stoplight.io</strong></a> - They are not just moving forward API design, they are also investing in the full API lifecycle, including governance. I rarely plug startups, unless they are doing meaningful things, and Stoplight.io is.</li>
</ul>

<p>After Stoplight.io, I’d say some of the open source tooling that still exists, and has been developed over the last five years, gives me the most hope that I will be able to efficiently design APIs at scale across teams:

<ul>
  <li><a href="https://editor.swagger.io/"><strong>Swagger Editor</strong></a> - I still find myself using this tool for most of my quick API design work.</li>
  <li><a href="http://apistylebook.com/"><strong>API Stylebook</strong></a> - What Arnaud has done reflects what should be standard across all industries.</li>
  <li><a href="https://mermade.github.io/openapi-gui/"><strong>OpenAPI GUI</strong></a> - A very useful and progressive OpenAPI GUI editor.</li>
  <li><a href="https://www.apicur.io/"><strong>Apicurio</strong></a> - Another useful GUI OpenAPI editor which shouldn’t be abandoned (cough cough).</li>
  <li><a href="http://webconcepts.info/"><strong>Web Concepts</strong></a> - The building blocks of every API design effort is located here.</li>
</ul>

<p>I use these tools in my daily work, and think they reflect what I like to see when it comes to API design tooling investment. I would be neglectful if I didn’t give a shout out to a handful of companies doing good work in this area:

<ul>
  <li><a href="https://www.postman.com/"><strong>Postman</strong></a> - While not coming from a pure API design vantage point, you can do some serious design work within Postman.</li>
  <li><a href="https://www.apimatic.io/"><strong>APIMATIC</strong></a> - It takes good API design to deliver usable SDKs, and APIMATIC provides a nice set of design tooling for their services.</li>
  <li><a href="https://www.reprezen.com/"><strong>Reprezen</strong></a> - They have invested heavily in their overall API design workflow and are a key player in the OpenAPI conversation.</li>
  <li><a href="https://restlet.com/"><strong>Restlet</strong></a> - My friends over at Restlet are still up to good things even thought they are part of Talend.</li>
</ul>

<p>While I am not quite ready to showcase and highlight these companies because they don’t always reflect the positive API community influence I’d like to see, I don’t want to leave them out for what they bring to the table:

<ul>
  <li><a href="https://app.swaggerhub.com/"><strong>SwaggerHub</strong></a> - They are doing interesting things, even if I’m still bummed over the whole Swagger -&gt; OpenAPI bullshit, which I will never forget!</li>
  <li><a href="https://www.mulesoft.com/platform/api/anypoint-designer"><strong>Mulesoft</strong></a> - Their AnyPoint Designer is worth noting in this discussion. I will leave it there.</li>
</ul>

<p>While writing this, and taking a fresh look at the search results for API design, editors, and tooling, and looking through my archives, I came across a couple players I have never seen before, either because I haven’t been tuned in, or because they are new:

<ul>
  <li><a href="https://openapi.design/#/about"><strong>OpenAPI Designer</strong></a> - An interesting new player to the OpenAPI editor game.</li>
  <li><a href="https://visual-paradigm.com/features/code-engineering-tools/#rest-api-generation"><strong>Visual Paradigm</strong></a> - Another interesting approach to delivering APIs – we may have to test drive.</li>
</ul>

<p>I do not want to stop there. Maybe there are other API design toolbox forces at play, influencing, shifting, or directing the API design conversation in other ways, using different protocols and approaches:

<ul>
  <li><a href="https://graphql.org/"><strong>GraphQL</strong></a> - Maybe the GraphQL believers are right? Maybe it is the true solution to designing our APIs? What if? OMG</li>
  <li><a href="https://kafka.apache.org/"><strong>Kafka</strong></a> - I think an event-driven approach has shifted the conversation for many, moving classic API design into the background.</li>
  <li><a href="https://grpc.io/">gRPC</a> - Maybe we are moving towards a more HTTP/2 RPC way of delivering APIs and API design is becoming irrelevant.</li>
</ul>

<p>It is possible that these solutions are siphoning off the conversation in new directions. Maybe the lack of investment is due to other influences that go well beyond the technology, or a specific approach to defining and designing the API problems. What might be some out of the box reasons the API design conversation hasn’t moved forward:

<ul>
  <li><strong>Investment</strong> -  Most of the movement I’ve seen has occurred as well as stagnated because of the direction of venture capital.</li>
  <li><strong>Hard</strong> - Maybe it is because it is hard, and nobody wants to do it, making it something that is difficult too monetize.</li>
  <li><strong>Expertise</strong> - We need more training and the development of API design expertise to help lead the way, and show us how it is done.</li>
  <li><strong>Bullshit</strong> - Maybe API design is bullshit, and we are delusional to think anything will ever become of API design in the first place.</li>
  <li><strong>My Vision</strong> - Maybe my vision of API design tooling and services is too high of a bar, or unrealistic in some way.</li>
</ul>

<p>Ultimately, I think API design is difficult. I think we need more investment in small open source API design tooling that do one thing well. I think other API paradigms will continue to distract us, but also potentially enrich us. I think my vision of API design is obtainable, but out of view of the current investment crowd. I think API design vision is either technical, or it is business. There is very little in between. This is why I highlight Stoplight.io. I feel they are critically thinking about not just API design, but also the rest of the lifecycle. I’d throw Postman into this mix, but they are more API lifecycle than pure design, but I do think they reflect more of the type of services and tooling I’d like to see.

<p>I do not think resource centered web API design is going anywhere. From what I”m seeing, it is going mainstream. It is simple. Low cost. It gets the job done with minimal investment. I think we should invest more into open source API design solutions. I think we need continued investment in API design services like Stoplight.io, Postman, Reprezen, Restlet, and others. I think we need to shift the conversation to also include GraphQL, Kafka, and gRPC. I think investors can do a better job, but I”m not going to hold my breathe there. In the end, I go back to my hand-crafted, artisanal, API design workbench out back where I have cobbled together a few open source tools, on top of a Git foundation. Honestly, it is all I can afford, but I’ll keep playing with other tools I have access to, to see if something will shift my approach.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/16/hoping-for-more-investment-in-api-design-tooling/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/15/what-is-an-api-contract/">What Is An API Contract?</a></h3>
        <span class="post-date">15 Jul 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/adam-smith-adam-smith-purp-paper.jpg" width="45%" align="right" style="padding: 15px;" />
I am big on regularly interrogating what I mean when I use certain phrases. I’ve caught myself repeating and reusing many hollow, empty, and meaningless phrases over my decade as the API Evangelist. One of these phrases is, “an API contract”. I use it a lot. I hear it a lot. What exactly do we mean by it? What is an API contract, and how is it different or similar to our beliefs and understanding around other types of contracts? Is it truth, or is just a way to convince people that what we are doing is just as legitimate as what came before? Maybe it is even more legitimate, like in a blockchain kind of way? It is an irreversible, unbreakable, digital contract think bro!

<p>If I was to break down what I mean when I say API contract, I’d start with being able to establish to a shared articulation of what an API does. We have an OpenAPI definition which describes the surface area of the request and response of each individual API method being offered. It is available in a machine and human readable format for both of us to agree upon. It is something that both API provider and API consumer can agree upon, and get to work developing and delivering, and then integrating and consuming. An API contract is a shared understanding of what the capabilities of a digital interface are, allowing for applications to be programmed on top of.

<p>After an API contract establishes a shared understanding, I’d say that an API contract helps mitigate change, or at leasts communicates it—-again, in a human and machine readable way. It is common practice to semantically version your API contracts, ensuring that you won’t remove or rename anything within a minor or patch release, committing to only changing things in a big way with each major release. Providing an OpenAPI of each version ahead of time, allowing consumers to review that new version of an API contract before they ever commit to integrating and moving to the next version. Helping reduce the amount of uncertainty that inevitably exists when an API changes, and consumers will have to respond with changes in their client API integrations.

<p>Then I’d say an API contract moves into service level agreement (SLA) territory, and helps provide some guarantees around API reliability and stability. Moving beyond any single API, and also speaking to wider operations. An API contract represents a commitment to offering a reliable and stable service that is secure, observability, and the provider has consumers best interest in mind. A contract should reflect a balance between the provider and the consumer interests, and provide a machine and human readable agreement that reflects the shared understanding of what an API delivers—for an agreed upon price. Any API contract reflects the technical and business details of us doing business in this digital world.

<p>Sadly, an API contract is often wielded in the name of all of these things, but there really is very little accountability or enforcement when it comes to API contracts. It is 100% up to the API provider to follow through and live up to the contract, with very little an API consumer can do if the contract isn’t met. Resulting in many badly behaved API providers, as well as monstrous API consumers. Right now, API contract is thrown around by executives, evangelists, analysts and pundits, more than they are ever actually used to govern what happens on the ground of API operations. Only time will tell if API contracts are just another buzzword that comes and goes, or if they become common place when it comes to doing business online in a digital world.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/15/what-is-an-api-contract/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/12/my-primary-api-search-engines/">My Primary API Search Engines</a></h3>
        <span class="post-date">12 Jul 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-stories-crypto-machine-bletchley-copper-circuit.png" width="45%" align="right" style="padding: 15px;" />
I am building out several prototypes for the moving parts of an API search engine I want to build, pushing my usage of APIs.json and OpenAPI, but also trying to improve how I define, store, index, and retrieve valuable data about thousands of APIs through a simple search interface. I’m breaking out the actual indexing and search into their own areas, with rating system being another separate dimension, but even before I get there I have to actually develop the primary engines for my search prototypes, feeding the indexes with fresh signals of where APIs exist across the online landscape. There isn’t an adequate search engine out there, so I’m determined to jumpstart the conversation with an API search engine of my own. Something that is different from what web search engines do, and tailored to the unique mess we’ve created within the API industry.

<p>My index of APIs.json and OpenAPI definitions, even with a slick search interface is just a catalog, directory, or static index of a small piece of the APIs that are out there. I see a true API search engine as three parts

<ol>
  <li>The Humans Searching for APIs - Providing humans with web application to search for new and familiar APIs.</li>
  <li>The Search Engine Searching For APIs - Ensuring that the search engine is regularly searching for new APIs.</li>
  <li>Other Systems Searching For APIs - Providing an API for other systems to search for new and familiar APIs.</li>
</ol>

<p>Without the ability for the search engine to actually seek out new APIs, it isn’t a search engine in my opinion—-it is a search application. Without an API for searching for APIs, in my opinion, it isn’t an API search engine. It takes all three of these areas to make an application a true API search engine, otherwise we just have another catalog, directory, marketplace, or whatever you want to call it.

<p>To help me put the engine into my API search engine, I’m starting with a handful of sources I’ve cultivated over the last five years studying the API industry. Providing me with some seriously rich sources of information when it comes to identifying new APIs:

<ul>
  <li><strong>GitHub Code Search API</strong> - I have a vocabulary I use for uncovering artifacts that provide clues to where APIs exist. I can also expand this search to topics, repos, and other dimensions of GitHub search, but I’m going to make sure I’m exhausting and optimizing core search for all I can before I move on.  GitHub provides me with a handful of nuggets when it comes to finding APIs:
    <ul>
      <li><strong>Swagger</strong> - Machine readable JSON and YAML API definitions.</li>
      <li><strong>OpenAPI</strong> - Machine readable JSON and YAML API definitions.</li>
      <li><strong>Postman</strong> - Machine readable JSON API definitions.</li>
      <li><strong>API Blueprint</strong> - Machine readable markdown definitions.</li>
      <li><strong>RAML</strong> - Machine readable YAML definitions.</li>
      <li><strong>HAR</strong> - Machine readable traffic snapshots.</li>
      <li><strong>Domains</strong> - Domains doing interesting things with APIs.</li>
      <li><strong>People</strong> - People doing interesting things with APIs.</li>
    </ul>
  </li>
  <li><strong>Bing Web Search API</strong> - I use my vocabulary to uncover domains and artifacts that provide clues to where APIs exist. Unlike GitHub, I have to pay for these API calls, so I’m being much more careful about how I spider, and coherently defining the vocabulary I use to uncover this landscape.  Bing provides me with a handful of nuggets when it comes to finding APIs:
    <ul>
      <li><strong>Domains</strong> - A look into many different domains who are talking APIs in specific verticals.</li>
      <li><strong>GitHub</strong> - I find that Bing has some interesting indexes of GitHub — wondering how this will evolve.</li>
    </ul>
  </li>
  <li><strong>Twitter</strong> - I use my vocabulary to identify new domains where people are talking about APIs, and additional signs of APIs.
    <ul>
      <li><strong>Domains</strong> - A look into many different domains who are talking APIs in specific verticals.</li>
      <li><strong>People</strong> - People doing interesting things with APIs.</li>
    </ul>
  </li>
  <li><strong>Domain</strong> - I have an exhaustive list of domains to spider for API artifacts, autogenerating OpenAPI index along the way.
    <ul>
      <li><strong>URLs</strong> - I look for a handful of valuable URLs for use as part of my index.
        <ul>
          <li><strong>Twitter</strong> - Their Twitter accounts.</li>
          <li><strong>GitHub</strong> - Their GitHub accounts.</li>
          <li><strong>LinkedIn</strong> - Their LinkedIn accounts.</li>
          <li><strong>Feeds</strong> - Their Atom and RSS feeds.</li>
          <li><strong>Definitions</strong> - Any API definitions I can find.</li>
          <li><strong>Documentation</strong> - Where their documentation is.</li>
          <li><strong>Other Links</strong> - I have a long list of other links I look for</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<p>These four areas represent the primary engines for my API search engine. I currently have these engines running on AWS, processing GitHub and Bing searches, and I’m currently refining my existing Twitter, and relevant domain harvesting engines. While Bing and GitHub are harvesting API signals, and indexing API artifacts like OpenAPI, Postman, and others, I’m going to overhaul my Twitter and domain approaches. Twitter has always been a treasure trove of API signals, but like on GitHub, it is getting harder to obtain these API signals at scale—-as their value increases, things are getting tighter with API access. Also, running a proper domain harvesting campaign across thousands of domains isn’t easy, and will require some refactoring to do at the scale I need for this type of effort.

<p>While I have two of these up and running, indexing new APIs, I still have a significant amount of work to invest in each engine. What I have now is purely of prototype. It will take several cycles until I get each engine performing as desired, and then I’m expecting ongoing tweaks, adjustments, and refinements to be made daily, weekly, and monthly to get the results I’m looking for. There are many areas of deficiency in the API sector that bother me, but not having a simple way to search for new and existing APIs is one are I cannot tolerate any longer. I am happy that ProgrammableWeb has been around all these years, but they haven’t moved the needle in the right way. I also get why people do API marketplaces, but I’m afraid they aren’t moving the needle in a positive direction either. I’d say that APIs.Guru (https://apis.guru/openapi-directory/) is the most progressive vision when it comes to API search in the last decade–with all the innovation supposedly going on, that is just sad. I am guessing that venture capital does not always equal meaningful things we need will get built.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/12/my-primary-api-search-engines/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/11/taking-a-fresh-look-at-the-nuance-of-api-search/">Taking A Fresh Look At The Nuance Of API Search</a></h3>
        <span class="post-date">11 Jul 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/stories-beach-rocks-currents-internet-numbers.jpg" width="45%" align="right" style="padding: 15px;" />
I have a mess of APIs.json and OpenAPI definitions I need to make sense of. Something that I could easily fire up an ElasticSearch instance, point at my API “data lake”, and begin defining facets and angles for making sense of what is in there. I’ve done this with other datasets, but I think this round I’m going to go a more manual route. Take my time to actually understand the nuance of API search over other types of search, take a fresh look at how I define and store API definitions, but also how I search across a large volume of data to find exactly the API I am looking for. I may end up going back to a more industrial grade solution in the end, but I am guessing I will at least learn a lot along the way.

<p>I am using API standards as the core of my API index—APIs.json and OpenAPI. I’m importing other formats like API Blueprint, Postman, RAML, HAR, and others, but the core of my index will be APIs.json and OpenAPI. This is where I feel solutions like ElasticSearch might overlook some of the semantics of each standard, and I may not immediately be able to dial-in on the preciseness of the APIs.json schema when it comes to searching API operations, and OpenAPI schema when it comes to searching the granular details of what each individual API method delivers. While this process may not get me to my end solution, I feel like it will allow me to more intimately understand each data point within my API index in a way that helps me dial-in exactly the type of search I envision.

<p>The first dimensions are of my API search index are derived from APIs.json schema properties I use to define every entity within my API search index:

<ul>
  <li><strong>Name</strong> - The name of a company, organization, institution, or government agency.</li>
  <li><strong>Description</strong> - The details of what a particular entity brings to the table.</li>
  <li><strong>Tags</strong> - Specific tags applied to an entity, or even a collection of entities.</li>
  <li><strong>Kin Rank</strong> - What Kin thinks of the entity being indexed with APIs.json.</li>
  <li><strong>Alexa Rank</strong> - What Alex thinks of the entity being indexed with APIs.json.</li>
  <li><strong>Common Properties</strong> - Using common properties like blog, Twitter, and GitHub.</li>
  <li><strong>Included</strong> - Other related APIs that are included within the index.</li>
  <li><strong>Maintainers</strong> - Details about who is the maintainer of the API definition.</li>
  <li><strong>API Name</strong> - The name of specific API program or project that an entity possesses.</li>
  <li><strong>API Description</strong> - The details of a specific API program or project that an entity possesses.</li>
  <li><strong>API Tags</strong> - How the individual API program or project is tagged for organization.</li>
  <li><strong>API Properties</strong> - The details of specific properties of an API like documentation, pricing, etc.</li>
</ul>

<p>After indexing the 100K view with APIs.json, providing references to the different layers of API operations, I’m indexing the following OpenAPI schema properties:

<ul>
  <li><strong>Title</strong> - The title of an individual API program or project that an entity possesses.</li>
  <li><strong>Description</strong> - The description of an individual API program or project that an entity possesses.</li>
  <li><strong>Domain</strong> - The subdomain, or top level domain that an API operates within.</li>
  <li><strong>Version</strong> - The version of each individual API.</li>
  <li><strong>Tags</strong> - The tags that are applied to a specific API program or project that an entity possesses.</li>
  <li><strong>API Path</strong> - The actual path of each API.</li>
  <li><strong>API Method Summary</strong> - The summary for an individual API method.</li>
  <li><strong>API Method Description</strong> - The description for an individual API method.</li>
  <li><strong>API Method Operation ID</strong> - The operation id for an individual API method.</li>
  <li><strong>API Method Query Parameters</strong> - The query parameters for an individual API method.</li>
  <li><strong>API Method Headers</strong>  - The headers for an individual API method.</li>
  <li><strong>API Method Body</strong> - The body of an individual API method.</li>
  <li><strong>API Method Tags</strong> - The tags applied to each individual API methods.</li>
  <li><strong>Schema Object Name</strong> - The name of each of the schema objects.</li>
  <li><strong>Schema Object Description</strong> - The description of each of the schema objects.</li>
  <li><strong>Schema Properties</strong> - The properties of each of the schema objects.</li>
  <li><strong>Schema Tags</strong> - The tags of each of the schema objects.</li>
</ul>

<p>These details provide to be by the OpenAPI definition for each API provides me with the long tail of my search, going beyond just the names and description of each API, allowing me to turn on or turn off different facets of the OpenAPI specification when indexing, and delivering search results.  My biggest challenges in building this index center around:

<ul>
  <li><strong>Completeness</strong> - I struggle with being able to invest the resources to properly complete the profile for each API.</li>
  <li><strong>Inconsistency</strong> - Navigating the inconsistency of APIs, trying to nail down a single definition across thousands of the is hard.</li>
  <li><strong>Performance</strong> - The performance of basic JavaScript search against such a large set of YAML / JSON documents isn’t optimal.</li>
  <li><strong>Accuracy</strong> - The accuracy of API methods is difficult to ascertain without actually getting a key and firing up Postman, or other script.</li>
  <li><strong>Up to Date</strong> - Understanding when information has become out of date, obsolete, or deprecated is a huge challenge with search.</li>
</ul>

<p>Right now I have about 2K APIs defined with APIs.json, with a variety of OpenAPI artifacts to support. With more coming in each day through my search engine spiders, trolling GitHub and the open web for signs of API life. I’m working to refine my current index of APIs, making sure they are complete-enough for making available publicly. Then I want to be able to provide a basic keyword search tool, then slowly add each of these individual data points to some sort of advanced filter setting for this search tool. I’m not convinced I’ll end up with a usable solution in the end, but I convinced that I will flesh out more of the valuable data points that exist within an APIs.json and OpenAPI index.

<p>This prototype will at least give me something to play with when it comes to crafting a JavaScript interface for the YAML API index I am publishing to GitHub. I feel like these API search knobs will help me better define my search index, and craft cleaner OpenAPI definitions for use in this API search index. As the index grows I can dial in the search filters, and look for the truly interesting patterns that exist across the API landscape. Then I’m hoping to add an API ratings layer to further help me cut through the noise, and identify the truly interesting APIs amidst the chaos and trash. Not all APIs are created equal and I will need a way to better index, rank, and then ultimately search for the APIs I need. While also helping me more easily discover entirely new types of APIs that I may not notice in my insanely busy world.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/11/taking-a-fresh-look-at-the-nuance-of-api-search/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/10/the-json-schema-tooling-in-my-life/">The JSON Schema Tooling In My Life</a></h3>
        <span class="post-date">10 Jul 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/udnie-DSC_0033.jpg" width="45%" align="right" style="padding: 15px;" />
I am always pushing for more schema order in my life. I spend way too much time talking about APIs, when a significant portion of the API foundation is schema. I don’t have as many tools to help me make sense of my schema, and to improve them as definitions of meaningful objects. I don’t have the ability to properly manage and contain the growing number of schema objects that pop up in my world on a daily basis, and this is a problem. There is no reason I should be making schema objects available to other consumers if I do not have a full handle on what schema objects exist, let alone a full awareness of everything that has been defined when it comes to the role that each schema object plays in my operations.

<p>To help me better understand the landscape when it comes to JSON Schema tooling, I wanted to take a moment and inventory the tools I have bookmarked and regularly use as part of my daily work with JSON Schema:

<ul>
  <li><strong>JSON Schema Editor</strong> - https://json-schema-editor.tangramjs.com/ - An editor for JSON Schema.</li>
  <li><strong>JSON Schema Generator</strong> - https://github.com/jackwootton/json-schema - Generates JSON Schema from JSON</li>
  <li><strong>JSON Editor</strong> - https://json-editor.github.io/json-editor/ - Generates form and JSON from JSON Schema.</li>
  <li><strong>JSON Editor Online</strong> -https://github.com/josdejong/jsoneditor/ - Allows me to work with JSON in a web interface.</li>
  <li><strong>Another JSON Schema Validator (AJV)</strong> - https://github.com/epoberezkin/ajv - Validates my JSON using JSON Schema.</li>
</ul>

<p>I am going to spend some time consolidating these tools into a single interface. They are all open source, and there is no reason I shouldn’t be localizing their operation, and maybe even evolving and contributing back. This helps me understand some of my existing needs and behavior when it comes to working with JSON Schema, which I’m going to use to seed a list of my JSON Schema needs, as drive a road map for things I’d like to see developed. Getting a little more structure regarding how I work with JSON Schema.

<ul>
  <li><strong>Visual Editor</strong> - Being able to visual render and edit JSON Schema in browser.</li>
  <li><strong>YAML / JSON Editor</strong> - Being able to edit JSON Schema in YAML or JSON.</li>
  <li><strong>YAML to JSON Converter</strong> - Converting my YAML JSON Schema into JSON.</li>
  <li><strong>JSON to YAML Converter</strong> - Converting my JSON JSON Schema into YAML.</li>
  <li><strong>JSON to JSON Schema Generator</strong> - Generate JSON Schema from JSON object.</li>
  <li><strong>JSON Schema to JSON Generator</strong> - Generate a JSON object from JSON Schema.</li>
  <li><strong>JSON Validation Using JSON Schema</strong> - Validate my JSON using JSON Schema.</li>
  <li><strong>Enumerators</strong> - Help me manage enumerators used across many objects.</li>
  <li><strong>Search</strong> - Help me search across my JSON Schema objects, wherever they are.</li>
  <li><strong>Guidance</strong> - Help me create better JSON Schema objects with standard guidelines.</li>
</ul>

<p>This is a good start. If I can bring some clarity and coherence to these areas, I’m going to be able to step up my API design and development game. If I can’t, I’m afraid I’m going to be laying a poor foundation for any API I’m designing in this environment. I mean, how can I consciously provide access to any schema object that I don’t have properly defined, indexed, versioned, and managed? If I don’t fully grasp my schema objects, my API design is going to be off kilter, and most likely be causing friction with my consumers. Granted, I could be offloading the responsibility for making sense of my schema to my consumers using a GraphQL solution, but I’m more in the business of doing the heavy lifting in this area, as it pertains to my business—-I’m the one who should know what is going on with each and every object that passes through my business servers.

<p>I wish there was a schema tool out there to help me do everything that I need. Unfortunately I haven’t seen it. The tooling that has rose up around the OpenAPI specification helps us better invest in schema objects when they are in the service of our API contracts, but nothing just for the sake of schema management. I will keep taking inventory of what tooling is available, as well as what I am needing when it comes to JSON Schema management. Who knows, something might pop up out there on the landscape. Or, more realistically I’m hoping little individual open source solutions keep popping up, allowing me to stitch them together and create the experience I’m looking for. I’m a big fan of this approach, rather than one service provider swooping in and providing the one tool to rule them all, only to get acquired and then be shut down–breaking my heart all over again.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/10/the-json-schema-tooling-in-my-life/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/10/navigating-api-rate-limit-differences-between-platforms/">Navigating API Rate Limit Differences Between Platforms</a></h3>
        <span class="post-date">10 Jul 2019</span>
        <p>I always find an API providers business model to be very telling  about the company&rsquo;s overall strategy when it comes to APIs. I&rsquo;m  currently navigating the difference between two big API providers,  trying to balance my needs spread across very different approaches to  offering up API resources. I&rsquo;m working to evolve and refine my API  search algorithms and I find myself having to do a significant amount of  work due to the differences between GitHub and Microsoft Search.  Ironically, they are both owned by the same company, but we all know  their business models are seeking alignment as we speak, and I suspect  my challenges with GitHub API is probably a result of this alignment.</p>
<p>The challenges with integrating with GitHub and Microsoft APIs are  pretty straightforward, and something I find myself battling regularly  when integrating with many different APIs. My use of each platform is  pretty simple. I am looking for APIs. The solutions are pretty simple,  and robust. I can search for code using the GitHub Search API, and I can  search for websites using the Bing Search API. Both produce different  types of results, but what both produce is of value to me. The challenge  comes in when I can pay for each API call with Bing, and I do not have  that same option with GitHub. I am also noticing much tighter  restriction on how many calls I can make to the GitHub APIs. With Bing I  can burst, depending on how much money I want to spend, but with GitHub  I have no relief value&mdash;I can only make X calls a minute, per IP, per  user.</p>
<p>This is a common disconnect in the world of APIs, and something I&rsquo;ve  written a lot about. GitHub (Microsoft) has a more &ldquo;elevated&rdquo; business  model, with the APIs being just an enabler of that business model. Where  Bing (Microsoft) is going with a much more straightforward API  monetization strategy&mdash;pay for what you use. In this comparison my needs  are pretty straightforward&mdash;-both providers have data I want, and I&rsquo;m  willing to pay for it. However, there is an additional challenge. I&rsquo;m  also using GitHub to manage the underlying application for my project.  Meaning after I pull search results from GitHub and Bing, and run them  through my super top secret, magical, and proprietary refinement  algorithm, I publish the refined results to a GitHub repository, and  manage the application in real time using Git, and GitHub APIs&mdash;which  counts against my API usage.</p>
<p>I used to manage all my static sites and applications 100% on GitHub,  using the APIs to orchestrate the data behind each Jekyll-driven site.  For the last five years I&rsquo;ve run API Evangelist, and waves of simple  data-driven static applications on GitHub like this. It has been a good  ride. A free ride. One I fear is coming to a close. I can no longer  deploy static data-driven Jekyll apps on the platform, and confidently  manage using the GitHub API anymore. It is something I do not expect to  continue getting for free. I&rsquo;d be happy to pay for my account on a per  organization, per repo, and per API call basis. In the end, I&rsquo;ll  probably begin just relying on Git for bulk builds of each application I  run on GitHub, and eventually begin migrating them to my own servers,  running Jekyll on my own, and custom developing an API for managing the  more granular changes across hundreds of micro applications that run on  Jekyll using YAML data. It would be nice for GitHub to notice this type  of application development as part of their business model, but I&rsquo;m  guessing it isn&rsquo;t mainstream enough for folks to adopt, and GitHub to  cater to.</p>
<p>Getting back to the search portion of this post. I am finding myself  writing a scheduling algorithm that spread out my API calls across a 24  hour period. I guess I can also leverage the different GitHub accounts I  have access to and maybe spread the harvesting across a couple EC2  instance, but I&rsquo;d rather just do what Bing offers me, and put in my  credit card. I am sure there are other ways I can find to circumvent the  GitHub API rate limits, but why? I would rather just be above board and  put in my credit card to be able to scale how I&rsquo;m using the platform.  One of the biggest challenges to API integration at scale in the future  will be API providers who do not offer relief valves for their  consumers. Significantly increasing the investment required to integrate  with an API in a meaningful way, making it much more difficult to  seamless use just a handful of APIs, let alone hundreds or thousands of  them. This challenge is nothing new, and just one example of how the  business of APIs can get in the way of the technology of APIs&mdash;-slowing  things down along the way.</p>
        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/10/navigating-api-rate-limit-differences-between-platforms/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/09/the-details-of-my-api-rating-formula/">The Details Of My API Rating Formula</a></h3>
        <span class="post-date">09 Jul 2019</span>
        <p>Last week I put some thoughts down about the basics of my API rating  system. This week I want to go through each of those basics, and try to  flesh out the details of how I would gather the actual data needed to  rank API providers. This is a task I&rsquo;ve been through with several  different companies, only to be abandoned, and then operated on my own  for about three years, only to abandon once I ran low on resources. I&rsquo;m  working to invest more cycles into actually defining my API rating in a  transparent and organic way, then applying it in a way that allows me to  continue evolving, while also using to make sense of the APIs I am  rapidly indexing.</p>
<p>First, I want to look at the API-centric elements I will be  considering when looking at a company, organization, institution,  government agency, or other entity, and trying to establish some sort of  simple rating for how well they are doing APIs. I&rsquo;ll be the first to  admit that ratings systems are kind of bullshit, and are definitely  biased and hold all kinds of opportunity for going, but I need  something. I need a way to articulate in real time how good of an API  citizen an API provider is. I need a way to rank the searches for the  growing number of APIs in my API search index. I need a list of  questions I an ask about an API in both a manual, or hopefully automated  way:</p>
<ul>
<li><strong>Active / Inactive</strong> - APIs that have no sign of life need a lower rating.      
<ul>
<li><strong>HTTP Status Code</strong> - Do I get a positive HTTP status code back when I ping their URL(s)?</li>
<li><strong>Active Blog</strong> - Does their blog have regular activity on it, with relevant and engaging content?</li>
<li><strong>Active Twitter</strong> - Is there a GitHub account designated for the API, and is it playing an active role in its operations?</li>
<li><strong>Active GitHub</strong> - Is there a GitHub account designated for the API, and is it playing an active role in its operations?</li>
<li><strong>Manual Visit</strong> - There will always be a need for a regular visit to an API to make sure someone is still home.</li>
</ul>
</li>
<li><strong>Free / Paid</strong> - What something costs impacts our decision to use or not.      
<ul>
<li><strong>Manual Visit</strong> - There is no automated way to understand API pricing.</li>
</ul>
</li>
<li><strong>Openness</strong> - Is an API available to everyone, or is a private thing.      
<ul>
<li><strong>Manual Review</strong> - This will always be somewhat derived from a manual visit by an analyst to the API.</li>
<li><strong>Sentiment Analysis</strong> - Some sentiment about the openness could be established from analyzing Twitter, Blogs, and Stack Exchange.</li>
</ul>
</li>
<li><strong>Reliability</strong> - Can you depend on the API being up and available.      
<ul>
<li><strong>Manual Review</strong> - Regularly check in on an API to see what the state of things are.</li>
<li><strong>Sentiment Analysis</strong> - Some sentiment about the reliability of an API could be established from analyzing Twitter, Blogs, and Stack Exchange.</li>
<li><strong>Monitoring Feed / Data</strong> - For some APIs, monitoring could be setup, but will cost resources to be able to do accurately.</li>
</ul>
</li>
<li><strong>Fast Changing</strong> - Does the API change a lot, or remain relatively stable.      
<ul>
<li><strong>Manual Review</strong> - Regularly check in on an API to see how often things have changed.</li>
<li><strong>Change Log Feed</strong> - Tune into a change log feed to see how often changes are Ade.</li>
<li><strong>Sentiment Analysis</strong> - Some sentiment about the changes to an API could be established from analyzing Twitter, Blogs, and Stack Exchange.</li>
</ul>
</li>
<li><strong>Social Good</strong> - Does the API benefit a local, regional, or wider community.      
<ul>
<li><strong>Manual Review</strong> - It will take the eye of an analyst to truly understand the social impact of an API.</li>
</ul>
</li>
<li><strong>**Exploitative</strong> - Does the API exploit its users data, or allow others to do so.      
<ul>
<li><strong>Manual Review</strong> - It will take a regular analyst review to understand whether an API has become exploitative.</li>
<li><strong>Sentiment Analysis</strong> - Some sentiment about the  exploitative nature of an API could be established from analyzing  Twitter, Blogs, and Stack Exchange.</li>
</ul>
</li>
<li><strong>Secure</strong> - Does an API adequately secure its resources and those who use it.      
<ul>
<li><strong>Manual Review</strong> - Regularly check in on an API to see how secure things are.</li>
<li><strong>Sentiment Analysis</strong> - Some sentiment about the security of an API could be established from analyzing Twitter, Blogs, and Stack Exchange.</li>
<li><strong>Monitoring Feed / Data</strong> - For some APIs, monitoring could be setup, but will cost resources to be able to do accurately, unless provided by provider.</li>
</ul>
</li>
<li><strong>Privacy</strong> - Does an API respect privacy, and have a strategy for platform privacy.      
<ul>
<li><strong>Manual Review</strong> - Regularly check in on an API to see how privacy is addressed, and what steps the platform has been taking to address.</li>
<li><strong>Sentiment Analysis</strong> - Some sentiment about the privacy of an API could be established from analyzing Twitter, Blogs, and Stack Exchange.</li>
</ul>
</li>
<li><strong>Monitoring</strong> - Does a platform actively monitor its platform and allow others as well.      
<ul>
<li>**Manual Review **- Regularly check in on an API to see how secure things are.</li>
<li><strong>Sentiment Analysis</strong> - Some sentiment about the security of an API could be established from analyzing Twitter, Blogs, and Stack Exchange.</li>
<li><strong>Monitoring Feed / Data</strong> - For some APIs, monitoring could be setup, but will cost resources to be able to do accurately, unless provided by provider.</li>
</ul>
</li>
<li><strong>Observability</strong> - Is there visibility into API platform operations, and its processes.      
<ul>
<li><strong>Manual Review</strong> - It will always take an analyst to understand observability until there are feeds regarding every aspect of operations.</li>
</ul>
</li>
<li><strong>Environment</strong> - What is the environment footprint or impact of API operations.      
<ul>
<li><strong>Manual Review</strong> - This would take a significant  amount of research into where APIs are hosted, and disclosure regarding  the environment impact of data centers, and the regions they operate  in.</li>
</ul>
</li>
<li><strong>Popular</strong> - Is an API popular, and something that gets a large amount of attention.      
<ul>
<li><strong>Manual Review</strong> - Analysts can easily provide a review of an API to better understand an APIs popularity.</li>
<li><strong>Sentiment Analysis</strong> - Some sentiment about the presence of an API could be established from analyzing Twitter, Blogs, and Stack Exchange.</li>
<li><strong>Twitter Followers</strong>** - The number of Twitter followers for an account dedicated to an API provides some data.</li>
<li><strong>Twitter Mentions</strong> - Similarly the number of mentions of an API providers Twitter account provides additional data.</li>
<li><strong>GitHub Followers</strong> - The number of GitHub followers provides another dimension regarding how popular an API is.</li>
<li><strong>Stack Exchange Mentions</strong> - The question and answer site always provides some interesting insight into which APIs are being used.</li>
<li><strong>Blog Mentions</strong> - The number of blog posts on top tech blogs, as well as independent blogs provide some insight into popularity.</li>
</ul>
</li>
<li><strong>Value</strong> - What value does an API bring to the table in generalized terms.      
<ul>
<li><strong>Manual Review **</strong>- The only way to understand  the value an API brings to the table is for an analyst to evaluate the  resources made available. Maybe some day we&rsquo;ll be able to do this with  more precision, but currently we do not have the vocabulary for  describing.</li>
</ul>
</li>
</ul>
<p>I am developing a manual questionnaire I can execute against while  profiling every API. I have already done this for many APIs, but I&rsquo;m  looking to refine for 2019. I will also be automating wherever I can,  leverage other APIs, feeds, and some machine learning to help me augment  my heuristic analyst rank with some data driven elements. Some of these  will only change when I, or hopefully another analyst reviews them, but  some of this will be more dependent on data gathered each month. It  will take some time for a ranking system based upon these elements to  come into focus, but I&rsquo;m guessing along the way I&rdquo;m going to learn a  lot, and this list will look very different in twelve months.</p>
<p>Next, I wanted to look at the elements of the rating system itself  which I think are essential to the success of an API ranking system  based upon the elements above. I&rsquo;ve seen a number of efforts fail when  it comes to indexing and ranking APIs. It is not easy. It is a whole lot  of work, without an easy path to monetization like Google established  with advertising. Many folks have tried and failed, and I feel like some  of these elements will help keep things grounded, and provide more  opportunity for success, if not at least sustainability.</p>
<ul>
<li><strong>YAML Core</strong> - I would define the rating system in YAML.      
<ul>
<li><strong>Rating Formula</strong> - The rating formula is  machine readable and available as YAML, taking everything listed above  and automating the application of it across APIs using a standard YAML  definition.</li>
<li><strong>Rating Results</strong> - Publishing a YAML dump of  the results of rating for each API provider, also providing a machine  readable template for understanding how each API provider is being  ranked.</li>
</ul>
</li>
<li><strong>GitHub Base</strong> - Everything would be in a series of repositories.      
<ul>
<li><strong>GitHub Repo</strong> - A GitHub repository is the unit of compute and storage for the rating.</li>
<li><strong>Git Management</strong> - I am using GitHub to apply the rating system across all APIs in my search index.</li>
<li><strong>GitHub API Management</strong> - I am automating the granular editing of the YAML core using the GitHub API.</li>
</ul>
</li>
<li><strong>Observable</strong> - The entire algorithm, process, and results are open.      
<ul>
<li><strong>Search Transparency</strong> - I will be tracking keyword searches, minus IP and user agent, then publishing the results to GitHub as YAML.</li>
<li><strong>Minimal Tracking</strong> - There will be minimal tracking of end-users searching and applying the ranking, with tracking being provider focused.</li>
</ul>
</li>
<li><strong>Evolvable</strong> - It would be essential to evolve and adapt over time.      
<ul>
<li><strong>Semantic Versioned</strong> - The search engine will be semantically versioned, providing a way of understanding it as it evolves.</li>
<li><strong>YAML</strong> - Everything is defined as YAML which is semantically versioned, so nothing is removed or changed until major releases.</li>
</ul>
</li>
<li><strong>Weighted</strong> - Anyone can weight the questions that matters to them.      
<ul>
<li><strong>Data Points</strong> - All data points will have a  weight applied as a default, but ultimately will allow end-users to  define the weights they desire.</li>
<li><strong>Slider Interface</strong> - Providing end-users with a sliding interface for defining the importance of each data point to them, and apply to the search.</li>
</ul>
</li>
<li><strong>Completeness</strong> - Not all the profiles of APIs will be as complete as others.      
<ul>
<li><strong>Data Points</strong> - The continual addition and  evolution of data points, until we find optimal levels of ranking across  industries, for sustained periods of time.</li>
</ul>
</li>
<li><strong>Ephemeral</strong> - Understanding that nothing lasts forever in this world.      
<ul>
<li><strong>Inactive</strong> - Making sure things that are inactive reflect this state.</li>
<li><strong>Deprecation</strong> - Always flag something as deprecated, reducing in rank.</li>
<li><strong>Archiving</strong> - Archive everything that has gone away, keeping indexes pure.</li>
</ul>
</li>
<li><strong>Community</strong> - It should be a collaboration between key entities and individuals.      
<ul>
<li><strong>GitHub</strong> - Operate the rating system out in the open on GitHub, leveraging the community for evolving.</li>
<li><strong>Merge Request</strong> - Allow for merge requests on the search index, as well as the ratings being applied.</li>
<li><strong>Forks</strong> - Allow for the workability of the API search, leveraging ranking as a key dimensions for how things can be forked.</li>
<li><strong>Contribution</strong> - Allow for community contribution to the index, and the ranking system, establishing partnerships along the way.</li>
</ul>
</li>
<li><strong>Machine Readable</strong> - Able for machines to engage with seamlessly.      
<ul>
<li><strong>YAML</strong> - Everything is published as YAML to keep things simple and machine readable.</li>
<li><strong>APIs.json</strong> - Follow a standard for indexing API operations and making them available.</li>
<li><strong>OpenAPI</strong> - Follow a standard for indexes the APIs, and making them available.</li>
</ul>
</li>
<li><strong>Human Readable</strong> - Kept accessible to anyone wanting to understand.      
<ul>
<li><strong>HTML</strong> - Provide a simple HTML application for end-users.</li>
<li><strong>CSS</strong> - Apply a minimalist approach to using CSS.</li>
<li><strong>JavaScript</strong> - Drive the search and engagement with client-side JavaScript, powered by APIs.</li>
</ul>
</li>
</ul>
<p>This provides me with my starter list of elements I think will set  the tone for how this API search engine will perform. Ultimately there  will be a commercial layer to how the API search and ranking works, but  the goal is to be as transparent, observable, and collaborative around  how it all works. A kind of observability that does not exist in web  search, and definitely doesn&rsquo;t in anything API search related. I&rsquo;ll give  it to DuckDuckGo, for being the good guys of web search, which I think  provides an ethical model to follow, but I want to also be open with the  rating system behind, to avoid some of the illness that commonly exists  within rating agencies of any kind.</p>
<p>Next stop, will be about turning the rating elements into a YAML  questionnaire that I can begin systematically applying to the almost  2,000 APIs I have in my index. With most of it being a manual process, I  need to get the base rating details in place, begin asking them, and  then version the questionnaire schema as I work my way through all of  the APIs. I have enough experience with profiling APIs to know that what  questions I ask, how I ask them, and what data I can gather about API  will rapidly evolve once I begin trying to satisfy questions again real  world APIs. How fast I can apply my API rating system to the APIs I have  indexed, as well as quickly turn around and refresh over time will  depend on how much time and resources I am able to manifest for this  project. Something that will come and go, as this is just a side project  for me, to keep me producing fresh content and awareness of the API  space.</p>
        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/09/the-details-of-my-api-rating-formula/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/08/thinking-differently-when-approaching-openapi-diffs-and-considering-how-to-layer-each-potential-change/">Thinking Differently When Approaching OpenAPI Diffs And Considering How To Layer Each Potential Change</a></h3>
        <span class="post-date">08 Jul 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-stories-server-cloud1-feed-people.jpg" width="45%" align="right" style="padding: 15px;" />
I have a lot of OpenAPI definitions, covering about 2,000 separate entities. For each entity, I often have multiple OpenAPIs, and I am finding more all the time. One significant challenge I have in all of this centers around establishing a master “truth” OpenAPI, or series of definitive OpenAPIs for each entity. I can never be sure that I have a complete definition of any given API, so I want to keep vacuuming up any OpenAPI, Swagger, Postman, or other artifact I can, and compare it with the “truth” copy” I have on indexed. Perpetually layering the additions and changes I come across while scouring the Internet for signs of API life. This perpetual update of API definitions in my index isn’t easy, and any tool that I develop to assist me will be in need constant refinement and evolution to be able to make sense of the API fragments I’m finding across the web.

<p>There are many nuances of API design, as well as the nuances of how the OpenAPI specification is applied when quantifying the design of an API, making the process of doing a “diff” between two OpenAPI definitions very challenging. Rendering common “diff” tools baked into GitHub, and other solutions ineffective when it comes to understanding the differences between two API definitions that may represent a single API. These are some of the things I’m considering as I’m crafting my own OpenAPI “diff” tooling:

<ul>
  <li><strong>Host</strong> - How the host is stored, defined, and applied across sandbox, production, and other implementations injects challenges.</li>
  <li><strong>Base URL</strong> - How OpenAPI define their base url versus their host will immediately cause problems in how diffs are established.</li>
  <li><strong>Path</strong> - Adding even more instability, many paths will often conflict with host and base URL, providing different fragments that show as differences.</li>
  <li><strong>Verbs</strong> - Next I take account of the verbs available for any path, understanding what the differences are in methods applied.</li>
  <li><strong>Summary</strong> - Summaries are difficult to diff, and almost always have to be evaluated and weighted by a human being.</li>
  <li><strong>Description</strong> -  Descriptions are difficult to diff, and almost always have to be evaluated and weighted by a human being.</li>
  <li><strong>Operation ID</strong> - These are usually autogenerated by tooling, and rarely reflect a provider defined standard, making them worthless in “diff”.</li>
  <li><strong>Query Properties</strong> - Evaluating query parameters individually is essential to a granular level diff between OpenAPI definitions.</li>
  <li><strong>Path Properties</strong> - Evaluating path parameters individually is essential to a granular level diff between OpenAPI definitions.</li>
  <li><strong>Headers</strong> - Evaluating headers individually is essential to a granular level diff between OpenAPI definitions.</li>
  <li><strong>Tags</strong> - Most providers do not tag their APIs, and they are often not included, and rarely provide much value when applying a “diff”.</li>
  <li><strong>**Request Bodies</strong> - Request bodies provide a significant amount of friction for diffs depending on the complexity and design of an API.</li>
  <li><strong>Responses</strong> - Responses often provide an incomplete view of an API, and rarely are robust enough to impact the “diff” view.</li>
  <li><strong>Status Codes</strong> - Status codes should be evaluated on an individual basis, providing a variety of ways to articulate these statuses.</li>
  <li><strong>Content Types</strong> - Content types these days are often application/json, but do provide some opportunities to define unique characteristics.</li>
  <li><strong>Schema Objects</strong> - Schema is often not defined, and rarely used as part of a diff unless OpenAPIs are generated from log, HAR, and other files.</li>
  <li><strong>Schema Properties</strong> - Schema properties are rarely present in OpenAPIs, making them not something that comes  up on the “diff” radar.</li>
  <li><strong>Security Definitions</strong> - Security definitions are the holy grail of automating API indexing, but are rarely present in OpenAPI, and only in Postman Collections.</li>
  <li><strong>References</strong> - The use of $ref, or absence of $ref and doing everything inline poses massive challenges to coherently considering “diff” results.</li>
  <li><strong>Scope</strong> - The size of the OpenAPI snippet being applied as part of a “diff” helps narrow what needs to be considered by a human or machine.</li>
</ul>

<p>This reflects the immediate concerns I have approaching the development of a custom “diff” tool for OpenAPI. First I am just trying to establish a strategy for stripping back the layers of OpenAPI definitions, and established a sort of layered user interface for me to manually accept or reject changes to an OpenAPI. An interface that will also allow me to define a sort of rules vocabulary for increasingly automating the decision making process. I’d love it if eventually the diff tool would show me just a single diff, present me with the change it thinks I should make, and allow me to just agree and move to the next “diff”. I have a lot of work to get things to this point.

<p>Like API search, I feel like API diff is something I have to reduce to its basics, and then fumble my way towards finding an acceptable solution. I don’t feel there is a single “diff” tool for JSON or YAML that will have the eye that I demand for analyzing, presenting, and either manually or automatically merging a diff. Like the other layers of my API search engine, diff is something I need to think through, iterate upon, and repeat until I come up with something that helps me merge “diffs” efficiently across thousands of APIs, and hopefully eventually automates and abstract away the most common differences between the APIs that I am spidering and indexing. Like every other area it is something I’m only working on when I have time, but something I will eventually come out the other end with a usable OpenAPI diff tool, that can help me make sense of all the API definitions I’m bombarded with on a daily basis.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/08/thinking-differently-when-approaching-openapi-diffs-and-considering-how-to-layer-each-potential-change/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/05/why-the-open-data-movement-has-not-delivered-as-expected/">Why The Open Data Movement Has Not Delivered As Expected</a></h3>
        <span class="post-date">05 Jul 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/fast-lights-freeway-redes-fast-flux-623x425-internet-numbers.jpg" width="45%" align="right" style="padding: 15px;" />
I was having a discussion with my friends working on API policy in Europe about API discovery, and the topic of failed open data portals came up. Something that is a regular recurring undercurrent I have to navigate in the world of APIs. Open data is a subset of the API movement, and something I have first-hand experience in, building many open data portals, contributing to city, county, state, and federal open data efforts, and most notably riding the open data wave into the White House and working on open data efforts for the Obama administration.

<p>Today, there are plenty of open data portals. The growth in the number of portals hasn’t decreased, but I’d say the popularity, utility, and publicity around open data efforts has not lived up to the hype. Why is this? I think there are many dimensions to this discussion, and few clear answers when it comes to peeling back the layers of this onion, something that always makes me tear up.

<ul>
  <li><strong>Nothing There To Begin With</strong> - Open data was never a thing, and never will be a thing. It was fabricated as part of an early wave of the web, and really never got traction because most people do not care about data, let alone it being open and freely available.</li>
  <li><strong>It Was Just Meant To Be A Land Grab</strong> - The whole open data thing wasn’t about open data for all, it was meant to be open for business for a few, and they have managed to extract the value they needed, enrich their own datasets, and have moved on to greener pastures (AI / ML).</li>
  <li><strong>No Investment In Data Providers</strong> - One f the inherent flaws of the libertarian led vision of web technology is that government is bad, so don’t support them with taxes. Of course, when they open up data sets that is goo for us, but supporting them in covering compute, storage, bandwidth, and data refinement or gathering is bad, resulting in many going away or stagnating.</li>
  <li><strong>It Was All Just Hype From Tech Sector</strong> - The hype about open data outweighs the benefits and realities on the ground, and ultimately hurt the movement with unrealistic expectations, setting efforts back many years, and are now only beginning to recover now that the vulture capitalists are on to other things.</li>
  <li><strong>Open Data Is Not Sexy</strong> - Open data is not easy to discover, define, refine, manage, and maintain as something valuable. Most government, institutions, and other organizations do have the resources to do properly, and only the most attractive of uses have the resources to pay people to do the work properly, incentivizing commercial offerings over the open, and underfunded offerings.</li>
  <li><strong>Open Data Is Alive and Well</strong> - Open data is doing just fine, and is actually doing better, now that the spotlight is off of them. There will be many  efforts that go unnoticed, unfunded, and fall into disrepair, but there will also be many fruitful open data offerings out there that will benefit communities, and the public at large, along with many commercial offerings.</li>
  <li><strong>Open Data Will Never Be VC Big</strong> - Maybe open data share the spotlight because it just doesn’t have the VC level revenue that investors and entrepreneurs are looking for. If it enriches their core data sets, and can be used to trying their machine learning models, it has value as a raw material, but as something worth shining a light on, open data just doesn’t rise to the scope needed to be a “product” all by itself.</li>
</ul>

<p>My prognosis on why open data never has quite “made it”, is probably a combination of all of these things. There is a lot of value present in open data as a raw material, but a fundamental aspect of why data is “open”, is so that entrepreneurs can acquire it for free. They aren’t interested in supporting city, county, state, and federal data stewards, and helping them be successful. They just want it mandated that it is publicly available for harvesting as a raw material, for use in the technology supply chain. Open data primarily was about getting waves of open data enthusiasts to do the heavy lifting when it came to identifying where the most value raw data sources exist.

<p>I feel pretty strong that we were all used to initiate a movement where government and institutions opened up their digital resources, right as this latest wave of information economy was peaking. Triggering institutions, organizations, and government agencies to bare fruit, that could be picked by technology companies, and used to enrich their proprietary datasets, and machine learning models. Open doesn’t mean democracy, it mostly means for business. This is the genius of the Internet evolution, is that it gets us all working in the service of opening things up for the “community”. Democratizing everything. Then once everything is on the table, companies grab what they want, and show very little interest in giving anything back to the movement. I know I have fallen for several waves of this ver the last decade.

<p>I think open data has value. I think community-driven, standardized sets of data should continue to be invested in. I think we should get better at discovery mechanisms involving how we find data, and how we enable our data to be found. However, I think we should also recognize that there are plenty of capitalists who will see what we produce as a valuable raw resource, and something they want to get their hands on. Also, more importantly, that these capitalists are not in the businesses of ensuring this supply of raw resource continues to exist in the future. Like we’ve seen with the environment, these companies do not care about the impact their data mining has on the organizations, institutions, government agencies, and communities that produced them, or will be impacted when efforts go unfunded, and unsupported. Protecting our valuable community resources from these realities will not be easy as the endless march of technology continues.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/05/why-the-open-data-movement-has-not-delivered-as-expected/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/03/api-interoperability-is-a-myth/">API Interoperability is a Myth</a></h3>
        <span class="post-date">03 Jul 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/san-francisco-city-bridge-sf-city-bridge-copper-circuit.jpg" width="45%" align="right" style="padding: 15px;" />
There are a number of concepts we cling to in the world of APIs. I’ve been guilting of inventing, popularizing, and spreading many myths in my almost decade as the API Evangelist. One of them that I’d like to debunk and be more realistic about is when it comes to API interoperability. When you are focused on just the technology of APIs, as well as maybe the low-level business of APIs, you are an API interoperability believer. Of course everyone wants API interoperability, and that all APIs should work seamlessly together. However, if you at all begin operating at the higher levels of the business of APIs, and spend any amount of time studying the politics of why and how we do APIs at scale, you will understand that API interoperability is a myth.

<p>This reality is one of the reasons us technologists who possess just a low-level understanding of how the business behind our tech operation, are such perfect tools for the higher level business thinkers, and people who successfully operate and manipulate at the higher levels of industries, or even at the policy level. We are willing to believe in API interoperability, and work to convince our peers that it is a thing, and we all work to expose, and open up the spigots across our companies, organizations, institutions, and government agencies. Standardized APIs and schema that play nicely with each other are valuable, but only within certain phases of a companies growth, or as part of a myth-information campaign to convince the markets that a company is a good corporate citizen. However, once a company achieves dominance, or the winds change around particular industry trends, most companies just want to ensure that all roads lead towards their profitability.

<p>Believing in API interoperability without a long term strategy means you are opening up your company to value extraction by your competitors. I don’t care how good your API management is, if your API makes it frictionless to integrate with because you use a standard format, it just means that the automated value harvesters of companies will find it frictionless to get at what you are serving, and more easily weaponize and monetize your digital resources. It pains me to say this, but it is the reality. If you are in the business of making your API easier to connect with, you are in the business of making it easier for your competitors to extract value from you. Does this mean we shouldn’t do APIs, and make them interoperable? No, but it does mean that we shouldn’t be ignorant of the cutthroat, exploitative, and aggressive nature of businesses that operate within our industries. Does it mean we shouldn’t invest in standards? No, but we should be aware that not every company sitting at the table shares the same interests as us, and could be playing a longer game that involves lock-in, proprietary nuances, or even slowing the standards movement in their favor.

<p>I think that storage APIs are a great example of this. In the early days of cloud storage APIs, I remember everyone saying they were AWS S3 compatible—even Google and Microsoft highlighted this. However, as things have progressed, everyone adds their own tweaks, changes, and nuances that make it much harder to get your terabytes of data off their platform. It was easy to get it in, and keep it synced across your providers, but eventually the polarities change, and all roads lead to lock-in, and are not in the service of interoperability. This is just businesses. I’m not condoning it, I am only repeating what my entrepreneurial friends tell me. If you make it easy for your customers to use other services, you are eroding their loyalty to your brand, and eventually they will leave. So you have to make it harder for them over time. Just incrementally. Forget to grease the door hinges. Change the way the doorknob turns. Make the door narrower. Stop following the international or local standards for how you design a door, call it innovation, and reduce the ways in which your customers can easily get out the door.

<p>I call this the Hotel California business model. You can check-in, but you can never leave. Wrap it all in a catchy tune, even call yourself a hotel, but in reality you’ve gotten hooked a technological myth, and you will never actually be able to ever find the door. Anyways, c’mon, I fucking hate the Eagles, don’t we just have some Credence we could play? Anyways, I got off track. Nobody, but us low-level delusional developers believe in API interoperability. The executives don’t give a shit about it. Unless it supports the latest myth-information campaign. In the long run, nobody wants their APIs to work together, we all just want EVERYONE to use OUR APIs! Sure, we also want to be seen as working together on standards groups, and that our APIs are the the standard EVERYONE should follow, ensuring interoperability with us at the center. But, nobody truly believes in API interoperability. If you do, I recommend you do some soul searching regarding where you exist in the food chain. I’m guessing you are a lower level pawn, doing the work of the puppet master in your industry. This is why you won’t find me on many standards bodies, or me blindly pushing interoperability at scale. It doesn’t exist. It isn’t real. Let’s get to work on more meaningful policy level things that will help shape the industry.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/03/api-interoperability-is-a-myth/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/02/your-api-and-schema-is-that-complex-because-you-have-not-done-the-hard-work-to-simplify/">Your API and Schema Is That Complex Because You Have Not Done The Hard Work To Simplify</a></h3>
        <span class="post-date">02 Jul 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/machine-road-machine-road-blue-circuit-3.jpg" width="45%" align="right" style="padding: 15px;" />
I find myself looking at a number of my more complex API designs, and saying to myself, “this isn’t complicated because it is hard, it is complicated because I did not spend the time required to simplify it appropriately”. There are many factors contributing to this reality, but I find that more often than not it is because I’m over-engineering something, and I am caught up in the moment focusing on a purely computation approach, and not considering the wider human, business, and other less binary aspects of delivering APIs.

<p>While I am definitely my own worst enemy in many API deliver scenarios, I’d say there are a wide range of factors that are influencing how well, or poorly that I design my API resources, with just a handful of them being:

<ul>
  <li><strong>Domain</strong> - I just do not have the domain knowledge required to get the job done properly.</li>
  <li><strong>Consumer</strong> - I just do not have the knowledge I need of my end consumers to do things right.</li>
  <li><strong>Bandwidth</strong> - I just do not have the breathing room to properly sit down and make it happen.</li>
  <li><strong>Narcissism</strong> - I am the best at this, I know what is needed, and I deem this complexity necessary.</li>
  <li><strong>Lazy</strong> - I am just too damn lazy to actually dig in and get this done properly in the first place.</li>
  <li><strong>Caring</strong> - I just do not give a shit enough to actually go the extra distance with API design.</li>
  <li><strong>Dumb</strong> - This API is dumb, and I really should not be developing it in the first place.</li>
</ul>

<p>These are just a few of the reasons why I settle for complexity over simplicity in my API designs. It isn’t right. However, it seems to be a repeating pattern in some of my work. It is something that I should be exploring more. For me to understand why my work isn’t always of highest quality possible I need to explore each of these areas and understand where I can make improvements, and which areas I cannot. Of course I want to improve in my work, and reach new heights with my career, but I can’t help be dogged by imperfections that seem out of my control…or are they?

<p>I have witnessed API simplicity. APIs that do powerful and seemingly complicated things, but in an easy and distilled manner. I know that it is possible to do, but I can’t help but feel that 90% of my API designs fall short of this reality. Some get very close, while others look like amateur hour. One thing is clear. If I’m going to deliver high quality simple and intuitive APIs, I’m going to have to work very hard at it. No matter how much experience I have, I can only improve the process so much, and there will always be a significant amount of investment required to take things to the next level.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/02/your-api-and-schema-is-that-complex-because-you-have-not-done-the-hard-work-to-simplify/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/01/the-complexity-of-api-discovery/">The Complexity of API Discovery</a></h3>
        <span class="post-date">01 Jul 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-stories-containership-copper-circuit.jpg" width="45%" align="right" style="padding: 15px;" />
I can’t get API discovery out of my mind. Partly because I am investing significant cycles in this area at work, but it is also something have been thinking about for so long, that it is difficult to move on. It remains one of the most complex, challenging, and un-addressed aspects of the way the web is working (or not working) online today. I feel pretty strongly that there hasn’t been investment in the area of API discovery because most technology companies providing and consuming APIs prefer things be un-discoverable, for a variety of conscious and un-conscious reasons behind these belief systems. 

<h3 id="what-api-discovery-means-depends-on-who-you-are">What API Discovery Means? Depends On Who You Are…</h3>
<p>One of the reasons that API discovery does not evolve in any significant ways is because there is not any real clarity on what API discovery is. Depending on who you are, and what your role in the technology sector is, you’ll define API discovery in a variety of ways. There are a handful of key actors that contribute to the complexity of defining, and optimizing in the area of API discovery.
 
<ul>
  <li><strong>Provider</strong> - As an API provider, being able to discover your APIs, informs your operations regarding what your capabilities are, building a wider awareness regarding what a company, organization, institution, or government agency does, helping eliminate inefficiencies, and allows for more meaning decisions to be made at run-time across operations.</li>
  <li><strong>Consumer</strong> - What you need as an internal consumer of APIs, or maybe a partner, or 3rd party developer will significantly shift the conversation around what API discovery means, and how it needs to be “solved”. There is another significant dimension to this discussion, separating human from other system consumption, further splintering the discussion around what API discovery is when you are a consumer of APIs.</li>
  <li><strong>Analyst</strong> - As an analyst for specific industries, or technology in general, need to understand the industries they are watching, and how API tooling is being applied, helping them develop an awareness of what is happening, and understand what the future might hold.</li>
  <li><strong>Investor</strong> - A small number of investors are in tune with APIs, and even grasp what API discovery means to their portfolio, and the industries they are investing in, generally being unaware of how API discovery will set the tone for how markets behave–providing the nutrients (or lack of) markets need to understand what is happening across industries.</li>
  <li><strong>Journalist</strong> - Most journalists grasp the importance of Facebook and Twitter, but only a small percentage understand what APIs are, and how ubiquitous they are, and the benefits they bring to the table when it comes to helping them in their investigations, and research, let alone how it can benefit them in their work when it comes to syndication and exposure for their work–making API discovery pretty critical to what they do.</li>
  <li><strong>University</strong> - I am seeing more universities depending on accessible APIs when it comes to research, and an increase in the development of API related curriculum–just as important, I am also seeing the increased development of open source API tooling out of university environments.</li>
  <li><strong>Government</strong> - APIs will play an increasing role in regulation, taxation, and government funded / implemented research, making API discovery key to finding the data they need, and being able to have their finger on the pulse of what citizens and businesses are up to on any given week.</li>
  <li><strong>End-User</strong> - Last, but definitely not least, the end-user should b e concerned with API discovery, and using it as a low water mark for where they should be doing business, and requiring that the platforms they use have APIs, and have their best interests in mind when allowing for 3rd party access to their data.</li>
</ul>

<h3 id="how-apis-are-discovered">How APIs Are Discovered?</h3>
<p>Across these different views of the API discovery landscape, there are a variety of ways in which APIs are discovered, and made discoverable. Providing some formal, and some not so formal ways to define the the API landscape, and develop an awareness of the API ecosystems that have risen up within different industries, within institutions and government agencies. These are the ways I am focused on finding APIs, and making APIs findable, inside and outside the firewall.

<p>Our motivations for finding APIs inside or outside the firewall are not always in sync with our motivations for having our APIs be found. This affects our view of the landscape when it comes to how hard API discovery is, and what the possible solutions for it will be. I find people’s view on API discovery to be very relative to their view of the landscape, and very few people in the API sector travel widely enough, exchange ideas externally enough, and lift themselves up high enough to be able to understand API discovery well enough to provide the right tools and services to move the conversation forward.

<h3 id="within-the-firewall">Within the Firewall</h3>
<p>API discovery with the firewall is a real struggle. Most companies I know do not know where all of their APIs are. There is no single up to date truth of where each API is, what it does, let alone the machine readable details of what it delivers. These are a few of the ways in which I have seen groups tackle API discovery within the firewall: 

<ul>
  <li><strong>Directories</strong> - Many employ a catalog, directory, or database of APIs, micro services, and other relevant solutions.</li>
  <li><strong>Git Repositories</strong> - Git within the enterprise is a very viable way to manage a large volume of artifacts you can use for discovery.</li>
  <li><strong>Word of Mouth</strong> - Talking to people is always a great way to understand what APIs exist across the enterprise landscape.</li>
  <li><strong>Documentation</strong> - The documentation for APIs often become the focal point of API discovery because it can be searched.</li>
  <li><strong>Log Files</strong> - Harvest what is actually coming across the wire, parse APIs that are in use, and documenting them in some way.</li>
</ul>

<p>The biggest challenge with API discovery within the firewall is having dedicated resources to keep up with the discovery, documenting, while also keeping catalogs, repositories, documentation, and other key sources of API discovery information up to date. In my experience, rarely do teams ever get the budget to properly invest in API discovery, with things often done as part of skunk works, or in every day operations—as teams can–which is never enough.

<h3 id="outside-the-firewall">Outside the Firewall</h3>
<p>This is the aspect of API discovery that gets the most attention when it comes to API discovery. These are the API rock stars, directors, marketplaces, and API showcasing that occurs across tech blogs. API discovery outside the firewall has access to far more tools and services to leverage when getting the word out, or helping you find what you are looking for. The challenge becomes, like most other things on the open web, how do you cut through the noise, and get your APIs found, or find the APIs you are looking for. 

<ul>
  <li><strong>Directories</strong> - You use some of the existing API directories out there. Providing you access to a small subsection of the APIs that these operators can find, and manually publish to their API catalogs. There really isn’t a single source of truth when it comes to API directories, but there are some that have been around longer than others.</li>
  <li><strong>Marketplaces</strong> - There have been numerous waves of investment into API marketplaces, trying to centralize the discovery and integration with APIs, hoping to simplify APIs enough that consumers use you as their doorway to the API world—giving the API marketplace provider a unique look at how APIs are consumed.</li>
  <li><strong>Documentation</strong> - Public API documentation is how your APIs will be found using Google, which is the number one way people are going to find your API—using a Google search. I’m surprised that Google hasn’t tackled the issue of API discovery already, but I’m guessing they haven’t deemed it valuable enough to tackle, and will most likely swoop in and take dominate once some smaller providers plant the seeds.</li>
  <li><strong>Definitions</strong> - API discovery has gotten easier, and more robust with the introduction of API definitions like Swagger, OpenAPI, API Blueprint, RAML, Postman collections and other machine readable formats. Going beyond just static or even dynamic API documentation, and providing a machine readable artifact that can be indexed and used to drive API search.</li>
  <li><strong>Domains</strong> - You can drive a lot of interesting API discovery using domains, and building indexes of different types of domains, then conduct several types of searches to see if they have any APIs. Using search engine APIs, Twitter, Github, and other social media APIs to find APIs across the landscape—using domains as the anchor for refining and making API discovery queries more precise.</li>
  <li><strong>Scraping</strong> - If you have the URL for an companies, organization, institution, or government agencies API documentation page you can also scrape that page, or pages for more information about how any API operations, and what it does—adding to the index of APIs to be search against.</li>
  <li><strong>Search Engines</strong> - While Google doesn’t have a good API anymore, Bing and DuckDuckGo do.It is easy to build up an index of queries to search Bing to uncover potential domains, documentation, definitions, and other artifacts to enrich an API discovery index. With the right key phrase glossary, you can quickly automate a pretty comprehensive search for new APIs.</li>
  <li><strong>GitHub</strong> - The social coding platform is a rich one of API related data. Similar to search engines you can easy build a search query vocabulary to uncover a number of domains, documentation, definitions, and other artifacts to enrich an API discovery index.</li>
  <li><strong>Integrated Development Environment (IDE)</strong> - The IDE is an untapped market when it comes to helping developers find APIs, and to help API providers reach developers. Microsoft has been increasing API discovery features, while also being extended with plugins by the community. There is no universal API search engine baked into the top IDEs, a definite missed opportunity.</li>
  <li><strong>News</strong> - It is pretty easy to harvest press releases, blog posts, and other news sources and use them as rich sources of information for new APIs. Helping seed a list of potential domains to look for documentation and other API artifacts. Publishing a press release, or posting to their blog is the most common way that API providers get the word out, unfortunately it tends to be the only thing they do.</li>
  <li><strong>Tweets</strong> - Twitter is also a rich source of information about APIs, with a variety of accounts, hashtags, and other ways to make queries for new APIs more precise. Using the social media platform as.a way of tuning into potential new APIs, as well as the activity around existing APIs in the index.</li>
  <li><strong>LinkedIn</strong> - More business, institutions, and government entities are talking about their APIs on LinkedIn, publishing posts, job listings, and other API related goings on. Making it a pretty rich way to find new APIs, and companies who are embarking, or making their way along their API journey.</li>
  <li><strong>Security Alerts</strong> - Sadly, security alerts is one way I learn about companies and their APIs—when they become un-secure. Providing information about API providers who operate in the shadows, as well as more information to index when actually rating APIs in the index, but that is another story.</li>
</ul>

<p>Even once you discover the APIs you are looking for, often times more context is required, and you may or may not have the time or expertise to assess what an API delivers, and what it will take to get up and running with an API.

<ul>
  <li><strong>Documentation</strong> - Where the documentation resides for the API.</li>
  <li><strong>Signup</strong> - Where a user can signup to use an API.</li>
  <li><strong>Pricing</strong> - What is the pricing for using an API.</li>
  <li><strong>Support</strong> - Where do you get support if you need help.</li>
  <li><strong>Terms of Service</strong> - Where do I find the legalize behind AP operations.</li>
</ul>

<p>These are just five of the most common questions APIs consumers are going to ask when they are looking at an API, and would benefit from direct links to these essential building blocks as part of any API search results.  I have over a hundred questions that I like to ask of an API as I’m reviewing, which all reflect what a potential API consumer will be asking when they come across an API out in the wild.

<h3 id="four-primary-dimensions-of-complexity">Four Primary Dimensions Of Complexity</h3>
<p>I’d say that these are the four main dimensions of complexity I see out there when it comes to API discovery, which makes it really hard to provide a single API discovery solution, and why there hasn’t been more investment in this area. It is difficult to make sense of APIs, and what people are looking for. It is hard to get all API providers on the same page when it comes to investing in API discovery as part of their regular operations. API discovery is something I’ll keep investing in, but it is something that will need wider investment from the community, as well as some bigger players to step up and help move the conversation forward.

<p>Other than API marketplaces, and the proliferation of API definitions, I haven’t seen any big movements in the API discovery conversation. We still have ProgrammableWeb. We still have Google. Not much more. With the number of APIs growing, this is only going to become more of a pain point. I’m interested in investing in API discovery not because I want to help everyone find APIs, or have their APIs found. Im more interested in shining a light on what is going on. I am looking to understand the spread of APIs across the digital landscape, and better see where they are pushing into our physical worlds. My primary objective is not to make sure all APIs are found so they can be used. My primary objects is to make sure all APIs are found so we have some observability into how the machine works.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/01/the-complexity-of-api-discovery/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/07/01/the-basics-of-my-api-rating-formula/">The Basics of My API Rating Formula</a></h3>
        <span class="post-date">01 Jul 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/aws-s3-square-supreme-court-judgement.jpg" width="45%" align="right" style="padding: 15px;" />
I have been working on various approaches to rating APIs since about 2012. I have different types of algorithms, even having invested in operating one from about 2013 through 2016, which I used to rank my daily intake of API news. Helping me define what the cream on top of each industry being impacted by APIs, while also not losing site of interesting newcomers to the space. I have also had numerous companies and VCs approach me about establishing a formal API rating system—many of whom concluded they could do fairly easily and went off to try, then failed, and gave up. Rating the quality of APIs is subjective and very hard.

<p>When it comes to rating APIs I have a number of algorithms to help me, but I wanted to step back and think of it from a more simpler human vantage point, and after establishing a new overall relationship with the API industry. What elements do I think should exist within a rating system for APIs:

<ul>
  <li><strong>Active / Inactive</strong> - APIs that have no sign of life need a lower rating.</li>
  <li><strong>Free / Paid</strong> - What something costs impacts our decision to use or not.</li>
  <li><strong>Openness</strong> - Is an API available to everyone, or is a private thing.</li>
  <li><strong>Reliability</strong> - Can you depend on the API being up and available.</li>
  <li><strong>Fast Changing</strong> - Does the API change a lot, or remain relatively stable.</li>
  <li><strong>Social Good</strong> - Does the API benefit a local, regional, or wider community.</li>
  <li><strong>Exploitative</strong> - Does the API exploit its users data, or allow others to do so.</li>
  <li><strong>Secure</strong> - Does an API adequately secure its resources and those who use it.</li>
  <li><strong>Privacy</strong> - Does an API respect privacy, and have a strategy for platform privacy.</li>
  <li><strong>Monitoring</strong> - Does a platform actively monitor its platform and allow others as well.</li>
  <li><strong>Observability</strong> - Is there visibility into API platform operations, and its processes.</li>
  <li><strong>Environment</strong> - What is the environment footprint or impact of API operations.</li>
  <li><strong>Popular</strong> - Is an API popular, and something that gets a large amount of attention.</li>
  <li><strong>Value</strong> - What value does an API bring to the table in generalized terms.</li>
</ul>

<p>These are just a handful of the most relevant data points I’d rank APIs on. Using terms that almost anyone can understand. You might not fully understand the technical details of what an API delivers, but you should be able to walk through these overall concepts that will impact your company, organization, institution, or government agency when putting an API to work. Now the more difficult questions around this API rating system, is how do you make it happen. Gathering data to satisfy all of these areas is easier said than done.

<p>Getting the answer to some of these question will be fairly easy, and we can come up with some low tech ways to handle. However, some of them are near impossible to satisfy, let alone do it continually over time. It isn’t easy to gather the data needed to answer these questions. In my opinion no single entity can deliver what is needed, and it will ultimately need to be a community thing if we are going to provide any satisfactory answer to these API rating questions, regularly satisfy them on a regular schedule, and then honestly acknowledge when an API goes dormant, or away altogether. To properly do this, it would have to be done out in the open, and a few things I’d consider introducing would be:

<ul>
  <li><strong>YAML Core</strong> - I would define the rating system in YAML.</li>
  <li><strong>YAML Store</strong> - I would store all data gathered as YAML.</li>
  <li><strong>GitHub Base</strong> - Everything would be in a series of repositories.</li>
  <li><strong>Observable</strong> - The entire algorithm, process, and results are open.</li>
  <li><strong>Evolvable</strong> - It would be essential to evolve and adapt over time.</li>
  <li><strong>Weighted</strong> - Anyone can weight the questions that matters to them.</li>
  <li><strong>Completeness</strong> - Not all the profiles of APIs will be as complete as others.</li>
  <li><strong>Ephemeral</strong> - Understanding that nothing lasts forever in this world.</li>
  <li><strong>Collaborative</strong> - It should be a collaboration between key entities and individuals.</li>
  <li><strong>Machine Readable</strong> - Able for machines to engage with seamlessly.</li>
  <li><strong>Human Readable</strong> - Kept accessible to anyone wanting to understand.</li>
</ul>

<p>I will stop there. I have other criteria I’d like to consider when defining a ranking system. Specifically, a public ranking system, for public APIs. Actually, I’d consider a company, organizations, institution, and government agency ranking system with an emphasis on APIs. Do they do APIs or not? If they do, how many of the questions can we answer pretty easily, with as few resources as possible, while being able to reliably measure using these data points on into the future. It is something we need. It is something that will be very difficult to setup, taking a significant amount of investment over time. It will also rely on the contribute of other entities and individuals. It is something that won’t be easy to make happen. That shouldn’t stop us from doing it.

<p>That is the basics of my API rating formula. Version 2019. This is NOT an idea for a startup. There is revenue to be generated here, but not if approached through the entrepreneurial playbook. It will fail. I’ve seen it happen over and over. This is a request for the right entities and individuals to come together and make it happen. It is dumb that there is no way of understanding which APIs are good or bad. It is also dumb that there is no healthy and active API search engine after APIs having gone so mainstream—another sign of the ineffectiveness of doing not just API rating, but API discovery as a venture backed startup. Sorry, there are just some infrastructural things we’ll all need to invest in together to make this all work at scale. Otherwise we are going to just end up with a chaotic, unreliable network of API-driven services behind our applications. If you’d like to talk API ratings with me, drop me an email at <a href="/cdn-cgi/l/email-protection" class="__cf_email__" data-cfemail="91f8fff7fed1f0e1f8f4e7f0fff6f4fdf8e2e5bff2fefc">[email&#160;protected]</a>, or tweet at @apievangelist.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/07/01/the-basics-of-my-api-rating-formula/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/06/25/why-schema-org-does-not-see-more-adoption-across-the-api-landscape/">Why Schema.org Does Not See More Adoption Across The API Landscape</a></h3>
        <span class="post-date">25 Jun 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/machine-road-machine-road-blue-circuit-3.jpg" width="45%" align="right" style="padding: 15px;" />
I’m a big fan of Schema.org. A while back I generated an OpenAPI 2.0 (fka Swagger) definition for each one and published to GitHub. I’m currently cleaning up the project, publishing them as OpenAPI 3.0 files, and relaunching the site around it. As I was doing this work, I found myself thinking more about why Schema.org isn’t the goto schema solution for all API providers. It is a challenge that is multi-layered like an onion, and probably just as stinky, and will no doubt leave you crying.

<p>First, I think tooling makes a big difference when it comes to why API providers haven’t adopted Schema.org by default across their APIs. If more API design and development tooling would allow for the creation of new APIs using Schema.org defined schema, I think you’d see a serious uptick in the standardization of APIs that use Schema.org. In my experience, I have found that people are mostly lazy, and aren’t overly concerned with doing APIs right, they are just looking to get them delivered to meet specifications. If we provide them with tooling that gets the API delivered to specifications, but also in a standardized, they are doing to do it.

<p>Second, I think most API providers don’t have the time and bandwidth to think of the big picture like using standardized schema for all of their resources. Most people are under pressure to more with less, and standards is something that can be easily lost in the shuffle when you are just looking to satisfy the man. It takes extra effort and space to realize common standards as part of your overall API design.  This is a luxury most companies, organizations, and government agencies can not afford, resulting in many ad hoc APIs defined in isolation.

<p>Third, I think some companies just do not care about interoperability. Resulting in them being lazy, or not making it a priority as stated in the first and second points. Most API providers are just concerned with you using their API, or checking the box that they have an API. They do not connect the dots between standardization and it being easier for consumers to put their resources to work. Many platforms who are providing APIs are more interested in lock-in, providing proprietary SDKs and tooling on top of their API. Selling them on the benefits of interoperable open source SDKs and tooling just falls on deaf ears—leaving most API providers to perpetually reinvent the wheel when there is an existing well defined one within reach.

<p>I wish ore API folks cared about Schema.org. I wish I had the luxury of using in more of my own work. If it is up to me, I will always adopt Schema.org for my core API designs, but unfortunately I’m not always the one in charge of what gets decided. I will continue to invest in OpenAPI definitions for all of the Schema.org defined schema. This allows me to have within reach when I’m getting ready to define a new API. If Schema.org ready API definitions are in a neat stack on my desk, the likelihood that I’m going to put to work in the API tooling I’m developing, and actually as the base for an API I’m delivering, increases significantly. Helping me standardize my API vocabulary to something that reaches beyond the tractor beam of daily API bubble.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/06/25/why-schema-org-does-not-see-more-adoption-across-the-api-landscape/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/06/24/avoiding-complexity-and-just-deploying-yaml-json-and-csv-apis-using-github-or-gitlab/">Avoiding Complexity and Just Deploying YAML, JSON, and CSV APIs Using GitHub or GitLab</a></h3>
        <span class="post-date">24 Jun 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/meadowbutterfly-meadow-butterfly-internet-numbers.jpg" width="45%" align="right" style="padding: 15px;" />
I find that a significant portion of I should be doing when defining, designing, developing, and delivering an API is all of avoiding complexity. Every step away along the API journey I am faced with opportunities to introduce complexity, forcing me to constantly question and say no to architectural design decisions. Even after crafting some pretty crafty APIs in my day, I keep coming back to JSON or YAML within Git, as the most sensible API architectural decision I can make.

<p>Git, with JSON and YAML stored within a repository, fronted with a Jekyll front-end does much of what I need. The challenge with selling this concept to others is that it is a static publish approach to APIs, instead of a dynamic pull of relevant data. This approach isn’t for every API solution, I’m not in the business selling one API solution to solve all of our problems. However, for many of the API uses I’m building for, a static Git-driven approach to publishing machine readable JSON or YAML data is a perfect low cost, low tech solution to delivering APIs.

<p>A Git repository hosted on GitHub or GitLab will store a significant amount of JSON, YAML, or CSV data. Something you can easily shard across multiple repositories within an organization / group, as well as across many repositories within many organization / groups. Both GitHub and GitLab offer free solutions, essentially letting you publish as many repositories as you want. As I said earlier, this is not a solution to all API needs, but when I’m looking to introduce some constraints to keep things low cost, simple, and easy to use and manage—a Git-driven API is definitely worth considering. However, going static for your API will force you to think about how you automate the lifecycle of your data, content, and other resources.

<p>The easiest way to manage JSON, CSV, or YAML data you have on GitHub or GitLab is to use the GitHub or GitLab API, allowing you to update individual JSON, CSV, or YAML files in real-time. If you want to do it in batch process by checking out the repository, making all the changes you want and committing back as a single Git commit using the command line—I’ve automated a number of these to reduce the number of API calls I’m making. If you want to put in an editorial layer you can require submission via pull / merge request, requiring there be multiple eyes on each update to the data behind a static API. It is an imperative, and a declarative API, with an open source approval workflow by default—all for free.

<p>Once you have your data in the Git repository you can make it available using the RAW links provided by GitHub or GitLab. However, I prefer to publish a Jekyll front-end, which can act as the portal landing page for the site, but then you can also manually or dynamically create neat paths that route users to your data using sensible URLs—the best part is you can add a cname and publish your own domain. Making your API accessible to humans, while also providing intuitive, easy to follow URLs to the static data that has been published using the GitHub or GitLab API, or the underlying Git infrastructure to do in bulk.

<p>This is the cheapest, most productive way to deliver simple data and content APIs. The biggest challenges are that you have to begin thinking a little differently about how you manage your data. You have to move from a pull to a push way of delivering data, and embrace the existing CI/CD way of doing things embraced by both GitHub and GitLab. For me, using Git to deliver APIs provides a poor mans way to not just deliver an API, but also ensure it is secure, performant, and something I can automate the management of using existing tools developers are depending on. Over the last couple of years I’ve pushed the limits of this approach by publishing thousands of OpenAPI-driven API discovery portals, and it is something I’m going to be refining and using as the default layer for delivering simple APIs that allow me to avoid unnecessary complexity.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/06/24/avoiding-complexity-and-just-deploying-yaml-json-and-csv-apis-using-github-or-gitlab/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/06/19/organizing-my-apis-using-openapi-tags/">Organizing My APIs Using OpenAPI Tags</a></h3>
        <span class="post-date">19 Jun 2019</span>
        <p>I like my OpenAPI tags. Honestly, I like tags in general. Almost  every API resource I design ends up having some sort of tagging layer.  Too help me organize my world, I have a centralized tagging vocabulary  that I use across my JSON Schema, OpenAPI, and AsyncAPI, to help me  group, organize, filter, publish, and maintain my catalog of API and  schema resources.</p>
<p>The <a href="https://github.com/OAI/OpenAPI-Specification/blob/master/versions/3.0.2.md#tagObject">tag object for the OpenAPI specification</a> is pretty basic, allowing you to add tags for an entire API contract,  as well as apply them to each individual API method. Tooling, such as  API documentation uses these tags to group your API resources, allowing  you to break down your resources into logical bounded contexts. It is a  pretty basic way of defining tags, that can go a long ways depending on  how creative you want to get. I am extending tags with an OpenAPI vendor  extension, but I also see that there is <a href="https://github.com/OAI/OpenAPI-Specification/issues/1367">a issue submitted suggesting they move the specification forward by allowing for the nesting of tags</a>&ndash;potentially taking OpenAPI tagging to the next level.</p>
<p>I&rsquo;m allowing for a handful of extensions to the OpenAPI specification to accomplish the following:</p>
<ul>
<li><strong>Tag Grouping</strong> - Help me nest, and build multiple tiers of tags for organization APIs.</li>
<li><strong>Tag Sorting</strong> - Allowing me to define a sort order that goes beyond an alphabetical list.</li>
</ul>
<p>I am building listing, reporting, and other management tools based up OpenAPI tagging to help me in the following areas:</p>
<ul>
<li><strong>Tag Usage</strong> - Reporting how many resources are available for each tag, and tag group.</li>
<li><strong>Tag Cleanup</strong> - Helping me de-dupe, rename, deal with plural challenges, etc.</li>
<li><strong>Tag Translations</strong> - Translating old tags into new tags, helping keep things meaningful.</li>
<li><strong>Tag Clouds</strong> - Generating D3.js tag clouds from the tags applied to API resources.</li>
<li><strong>Packages</strong> - Deployment of NPM packages based upon different bounded contexts defined by tags.</li>
</ul>
<p>I am applying tags to the following specifications, stretching my  OpenAPI tagging approach to be more about a universal way to organize  all my resources:</p>
<ul>
<li><strong>JSON Schema</strong> - All schema objects have tags to keep organized.</li>
<li><strong>OpenAPI</strong> - Each API method have tags, for easy grouping.</li>
<li><strong>AsyncAPI</strong> - Each pub/sub, event, and message API have APIs.</li>
<li><strong>APIs.json</strong> - Collections of APIs have tags for discoverability.</li>
</ul>
<p>Tags are an important dimension of API discoverability when it comes  to my API definitions. They provide rich metadata that I can use to make  sense of my API infrastructure. Without them, the quality of my API  definitions tend to trend lower. By evolving the tagging schema, and  investing in tooling to help me make sense across the API definitions  I&rsquo;m depending on, I can push the boundaries of how I tag, and evolve it  to be the core of how I manage my API definitions.</p>
<p>I&rsquo;ll keep watching how others are tagging their APIs, although I  don&rsquo;t see too much innovation, and low levels of usage by other API  providers as part of their API definitions. I&rsquo;ll also keep an eye out  for other ways in which tag schema are being extended, helping  potentially define the future of how the leading API specifications  enable tagging. I have a short list of tooling I am developing to help  make my life easier, but I&rsquo;m working hard to just make sure I&rsquo;m applying  tags across my API resources in a consistent way. I find this is the  most valuable aspect of API tagging, but eventually I&rsquo;m guessing that  the tooling will make the real difference when it comes to slicing and  dicing, and making sense of my API infrastructure at scale.</p>
        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/06/19/organizing-my-apis-using-openapi-tags/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/06/17/doing-the-hard-work-to-define-apis/">Doing The Hard Work To Define APIs</a></h3>
        <span class="post-date">17 Jun 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/gears-4882162452-fa3126b38d-b-umberto-bocc.jpg" width="45%" align="right" style="padding: 15px;" />
Two years later, <a href="https://apievangelist.com/2017/01/09/the-api-driven-marketplace-that-is-my-digital-self/">I am still working to define the API driven marketplace that is my digital self</a>. Understanding how I generate revenue from my brand (vomits in mouth a little bit), but also fight off the surveillance capitalists from mining and extracting value from my digital existence. It takes a lot of hard work to define the APIs you depend on to do business, and quantify the digital bits that you are transacting on the open web, amongst partners, and unknowingly with 3rd parties. As an individual, I find this to be a full time job, and within the enterprise, it is something that everyone will have to own a piece of, which in reality, is something that is easier said than done.

<p>Convincing enterprise leadership of the need to be aware of every enterprise capability being defined at the network, system, or application level is a challenge, but doable. Getting consensus on how to do this at scale, across the enterprise will be easier said than done. Identifying how the network, system, and applications across a large organization are being accessed, what schema, messages, and other properties are being exchanged is not a trivial task. It is a massive undertaking to reverse engineer operations, and identify the core capabilities being delivered, then define and document in a coherent way that can be shared with others, and included as part of an organization messaging campaign.

<p>Many will see work to define all enterprise API capabilities as a futile task–something that is impossible to deliver. Many others will not see the value of doing it in the first place, and unable to realize the big picture, they will see defining of APIs and underlying schema as meaningless busy work. Even when you do get folks on-board with the important, having the discipline to see the job through becomes a significant challenge. If moral is low within any organization group, and team members do not have visibility into the overall strategy, the process of defining gears that make the enterprise move forward will be seen as mind numbing accounting work–not something most teams will respond positively about working on.

<p>Once you do define your APIs, you then have to begin investing in defining what the future will hold when it comes to unwinding, evolving, maturing the API infrastructure you are delivering and depending on. This is something that can’t fully move forward until a full or partial accounting of enterprise API capabilities has occurred. If you do not know what is, you will always have trouble defining or controlling what will be. One of the reasons we have so much technical debt is we prefer to focus on what comes next rather than attending to the maintenance required to keep everything clean ,coherent, organized, and well defined. It is always easier to focus on the future, than it is to reconcile with mess we’ve created in the past. This is why it is so easy to sell each wave of enterprise technology solutions, promising to do this work for you–when in reality, most times, you are just laying down the next layer of debt.

<p>Whether it is our personal lives, or our professional worlds, defining the APIs we depend on, as well as the APIs that aren’t useful and become parasitic, as well as the schema objects they produce and exchange is a lot of hard work. Hard work most of us will neglect and outsource for convenience. Making the work become even harder down the road–nobody will care about doing this as we do. The really fascinating part of all of this for me, is that with each cycle of technology that passes through, we keep doubling down on technology being the solution to yesterday’s problem, even though it is the core of yesterday’s problem. In my experience, once you really begin investing in the hard work to define the APIs you depend on, you begin to realize that you don’t need so many of them. That the number of API connections you depend on can actually begin to hurt you, which is something that can wildly grow if you aren’t in tune with what your API landscape consists of in your personal and professional worlds.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/06/17/doing-the-hard-work-to-define-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/06/16/there-is-no-single-right-way-to-do-apis/">There Is No Single Right Way To Do APIs</a></h3>
        <span class="post-date">16 Jun 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/rockingchair-face-2-atari-asteroids.jpg" width="45%" align="right" style="padding: 15px;" />
My time working in the API sector has been filled with a lot of lessons. I researched hard, paid attention, and found a number of surprising realities emerge across the API landscape. The majority of surprises have been in the shadows caused by my computational belief scaffolding I’ve been erecting since the early 1980s. A world where there has to be absolutes, known knowns, things are black and white, or more appropriately 1 or 0. If I studied all APIs, I’d find some sort of formula for doing APIs that is superior to everyone else’s approach to doing APIs. I was the API Evangelist man–I could nail down the one right way to do APIs. (Did I mention that I’m a white male autodidact?)

<p>I was wrong. There is no one right way to do APIs. Sure, there are many different processes, practices, and tools that can help you optimize your API operations, but despite popular belief, there is no single “right way” to do define, design, provide, or consume APIs. REST is not the one solution. Hypermedia is not the one solution. GraphQL is not the one solution. Publish / Subscribe is not the one solution. Event-driven is not the one solution. APIs in general are the the one solution. Anyone telling you there is one solution to doing APIs for all situations is selling you something. Understanding your needs, and what the pros and cons of different approaches are, is the only thing that will help you.

<p>If you are hyper focused on the technology, it is easy to believe in a single right way of doing things. Once you start having to deliver APIs in a variety of business sectors and domains, you will quickly begin to see your belief system in a single right way of doing things crumble. Different types of data require different types of approaches to API enablement. Different industries are knowledgable in different ways of API enablement, with some more mature than others. Each industry will present its own challenges to API delivery and consumption, that will require a different toolbox, and mixed set of skills required to be successful. Your social network API strategy will not easily translate to the healthcare industry, or other domain.

<p>With a focus on the technology and business of APIs, you can still find yourself dogmatic around a single right way of doing things. Then, if you find yourself doing APIs in a variety of organizations, across a variety of industries, you quickly realize how diverse your API toolbox and approach will need to be. Organizations come with all kinds of legacy technical debt, requiring a myriad of approaches to ensuring APIs properly evolve across the API lifecycle. Once a technological approach to delivering software gets baked into operations, it becomes very difficult to unwind, and change behavior at scale across a large organization–there is no single right way to do APIs within a large enterprise organization. If someone is telling you there is, they are trying to sell you the next generation of technology to bake into your enterprise operations, which will have to be unwound at some undisclosed date in the future–if ever.

<p>I’ve always considered my API research and guidance to be a sort of buffet–allowing my readers choose the mix that works for them. However, I still found myself providing industry guides, comprehensive checklists, and other declarative API narratives that still nod towards there being a single, or at least a handful of right ways of doing APIs. I think that APIs will ultimately be like cancer, something we never quite solve, but there will be huge amounts of money spent trying to deliver the one cure. I’ll end with the comparison there, because I don’t want to get me on a rant regarding the many ways in which APIs and cancer will impact the lives of everyday people. In the end, I will still keep studying and understanding different approaches to doing APIs (both good or bad), but you’ll find my narrative to be less prescriptive when it comes to any single way of doing APIs, as well as suggesting that doing APIs in the first place is the right answer to any real world problem.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/06/16/there-is-no-single-right-way-to-do-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/06/12/api-definitions-are-important/">API Definitions Are Important</a></h3>
        <span class="post-date">12 Jun 2019</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algorotoscope-master/christianity-christianity-under-construction-copper-circuit.jpg" width="35%" align="right" style="padding: 15px;" />
I found myself scrolling down the home page of API Evangelist and thinking about what topic(s) I thought were still the most relevant in my mind after not writing about APIs for the last six months. Hands down it is API definitions. These machine and human readable artifacts are the single most important thing for me when it comes to APIs I’m building, and putting to work for me.

<p>Having mature, machine readable API definitions for all API that you depend on, is essential. It also takes a lot of hard work to make happen. It is why I went API define first a long time ago, defining my APIs before I ever get to work designing, mocking, developing, and deploying my APIs. Right now, I’m heavily dependent on my:

<ul>
  <li><strong>JSON Schema</strong> - Essential for defining all objects being used across API contracts.</li>
  <li><strong>OpenAPI</strong> - Having OpenAPI contracts for al my web APIs is the default–they drive everything.</li>
  <li><strong>AsyncAPI</strong> - Critical for defining all of my non HTTP 1.1 API contracts being provided or consumed.</li>
  <li><strong>Postman Collections</strong> - Providing me with the essential API + environment definitions for run-time use.</li>
  <li><strong>APIs.json</strong> - Helping me define all the other moving parts of API operations, indexing all my definitions.</li>
</ul>

<p>While there is plenty of other stops along the API lifecycle that are still relevant to me, my API definitions are hands down the most valuable intellectual property I possess. These four API specifications are essential to making my world work, but there are other more formalized specifications I’d love to be able to put to work:

<ul>
  <li><strong>Orchestrations</strong> - I’d liked to see a more standardized, machine readable way for working with many API calls in a meaningful way. I know you can do this with Postman, and I’ve done with OpenAPI, and like Stoplight.io’s approach, but I want an open source solution.</li>
  <li><strong>Licensing</strong> - I am not still actively using API Commons, but I’d like to invest in a 2.0 version of the API licensing specification, moving it beyond just the API licensing, and consider SDK, and other layers.</li>
  <li><strong>Governance</strong> - I’d like to see a formal way of expressing API governance guidance that can be viewed by a human, or executed as part of the pipeline, ensuring that all API contracts conform to a set of standards.</li>
</ul>

<p>These area hit on the main areas that concern for me when it comes to defining the contracts I need to further automate the integration and deployment of API resources in my life. While there are definitely other stops along the API lifecycle on my mind, I spend the majority of my time creating, refining, communicating, and moving forward API definitions that define and drive every other stop along the API lifecycle.

<p>API definitions represent API sanity for me. If they aren’t in order, there is disorder. An immature API definition requires investment, socialization amongst stakeholders, and iterating upon before it will ever be considered for publishing. I’ll be exploring the other things that matter for me along the API lifecycle, and then I’m guessing that the rest of this stuff I’ve been researching over the last eight years will either disappear, or just be demoted on the site. We’ll see how this refresh rolls out, but I’m guestimating about 25% of my research will continue to move forward after this reboot.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/06/12/api-definitions-are-important/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2019/06/10/api-evangelist-is-open-for-business/">API Evangelist Is Open For Business</a></h3>
        <span class="post-date">10 Jun 2019</span>
        <p><img src="https://kinlane-productions2.s3.amazonaws.com/api-evangelist-site/open-nen.jpg" width="25%" align="right" style="padding: 15px;" />
<p>After six months of silence I've decided to fire API Evangelist back up again. I finally reached a place where I feel like I can separate out the things that caused me to step back in the first place. Mostly, I have a paycheck now, some health insurance, and I don't have to pretend I give a shit about APIs, startups, venture capital, and the enterprise. I'm being paid well to do an API job. I can pay my rent. I can go to the doctor when my health takes a hit. My basic needs are met.

<p>Beyond that, I'm really over having to care about building an API community, making change with APIs, and counteracting all of the negative effects of APIs in the wild. I can focus on exactly what interests me about technology, and contribute to the 3% of the API universe that isn't doing yawnworthy, half-assed, or straight up shady shit. I don't have to respond to all the emails in my inbox just because I need to eat, and have a roof over my head. I don't have to jump, just because you think your API thing is the next API thing. I can just do me, which really is the founding principle of API Evangelist.

<p>Third, I got a kid to put through college, and I'm going to make y'all pay for it. So, API Evangelist is open for business. I won't be producing the number of stories I used to. I'll only be writing about things I actually find interesting, and will explore other models for generating content, traffic, and revenue. So reach out, and pitch me. I'm looking for sponsors, and open to almost anything. Don't worry, I'll be my usual honest self and tell you whether I'm interested or not, and have strong opinions on what should be said, but pitch me. I'm open for business, I'll entertain any business offer keep API Evangelist in forward motion, and generating revenue for me.

<p>If you are interested in sponsoring API Evangelist, it is averaging 2K page views a day, but normally averages 5K a day when it is in full active mode. The Twitter account has 10K followers, and the audience is a pretty damn good representation of the API pie if I don't say so myself. It is a damn shame to squander what I've built over the last nine years just because I like to ride on a sparkly high horse. If I've learned anything during my time as the API Evangelist, it is that revenue drives ALL decisions. So get in on the action. Let me know what you are thinking, and I'll get to work adding your logo to the site, and turning on the other sponsorship opportunities. Ping me at <a href="/cdn-cgi/l/email-protection#0861666e67486978616d7e69666f6d64617b7c266b6765"><span class="__cf_email__" data-cfemail="adc4c3cbc2edccddc4c8dbccc3cac8c1c4ded983cec2c0">[email&#160;protected]</span></a> to get the ball rolling.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2019/06/10/api-evangelist-is-open-for-business/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/11/27/asking-the-honest-questions-when-it-comes-to-your-api-journey/">Asking The Honest Questions When It Comes To Your API Journey</a></h3>
        <span class="post-date">27 Nov 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/kin-mountain_feed_people.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I engage with a lot of enterprise organizations in a variety of capacities. Some are more formal consulting and workshop engagements. While others are just emails, direct messages, and random conversation in the hallways and bars around API industry events. Many conversations are free flowing, and they trust me to share my thoughts about the API space, and provide honest answers to their questions regarding their API journey. Where others express apprehension, concern, and have trouble engaging with me because they are worried about what I might say about their approach to doing APIs within their enterprise organizations. Some have even told me that they’d like to formally bring me in for discussions, but they can’t get me pass legal or their bosses–stating I have a reputation for being outspoken.

<p>While in Vegas today, I had breakfast with Paolo Malinverno, analyst from Gartner, he mentioned the Oscar Wilde quote, “Whenever people agree with me I always feel I must be wrong.” Referring to the “yes” culture than can manifest itself around Gartner, but also within industries and the enterprise regarding what you should be investing in as a company. That people get caught up in  up in culture, politics, and trends, and don’t always want to be asking, or be asked the hard questions. Which is the opposite of what any good API strategist, leader, and architect should be doing. You should be equipped and ready to be asked hard questions, and be searching out the hard questions. This stance is fundamental to API success, and you will never find what you are seeking when it comes to your API journey if you do not accept that many questions will be difficult.

<p>The reality that not all API service providers truly want to help enterprise organizations genuinely solve the business challenges they face, and that many enterprise technology leaders aren’t actually concerned with truly solving real world problems, has been one of the toughest pills for me to swallow as the API Evangelist over the last eight years. Realizing that there is often more money to be made in not solving problems, not properly securing systems, or systems being performant, efficient, and working as expected. While I think many folks are too far down in the weeds of operations and company culture to fully make the right decision, I also think there are many people who make the wrong technological decision because it is the right business decision in their view. They do it to please share holders, investors, their boss, or just going with the flow when it comes to the business culture within their enterprise, and the industry that they operate in.

<p>Honestly, there isn’t a lot of money to be made asking the hard questions, and addressing the realities of getting business done using APIs within the enterprise. Not all companies are willing to pay you to come in and speak truth to what is going on. Pointing out the illnesses that exist within the enterprise, and potentially provide solutions to what is happening. People are afraid what you are going to ask. People don’t want to pay someone to rock the boat. I find it to be a rare occurrence to find large enterprise organizations who are willing to look in the mirror and be held accountable for their legacy technical debt, and be forced to make the right decisions when it comes to moving forward with the next generation of investment. Which is why most organizations will stumble repeatedly in their API journeys, be more susceptible to the winds of technological trends and investment cycles, all because they aren’t willing to surround themselves with the right people who are willing to speak truth.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/11/27/asking-the-honest-questions-when-it-comes-to-your-api-journey/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/11/25/a-diverse-api-toolbox-driving-hybrid-integrations-across-an-eventdriven-landscape/">A Diverse API Toolbox Driving Hybrid Integrations Across An Event-Driven Landscape</a></h3>
        <span class="post-date">25 Nov 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/machine-road_atari_missle.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I’m heading to Vegas in the morning to spend two days in conversations with folks about APIs. I am not there for <a href="https://reinvent.awsevents.com/">AWS re:Invent</a>, or <a href="https://www.gartner.com/en/conferences/na/applications-us">the Gartner thingy</a>, but I guess in a way I am, because there are people there for those events, who want to talk to me about the API landscape. Folks looking to swap stories about enterprise API investment in possessing a diverse API toolbox for driving hybrid integrations in an event-driven landscape. I’m not giving any formal talks, but as with any engagement, I’m brushing up on the words I use to describe what I’m seeing across the space when it comes to the enterprise API lifecycle.

<p><strong>The Core Is All About Doing Resource Based, Request And Response APIs Well</strong><br />
I’m definitely biased, but I do not subscribe to popular notions that at some point REST, RESTful, web, and HTTP APIs will have to go away. We will be using web technology to provide simple, precise, useful access to data, content, and algorithms for some time to come, despite the API sectors continued evolution, and investment trends coming and going. Sorry, it is simple, low-cost, and something a wide audience gets from both a provider and consumer perspective. It gets the job done. Sure, there are many, many areas where web APIs fall short, but that won’t stop success continuing to be defined by enterprise organizations who can do microservices well at scale. Despite relentless assaults by each wave of entrepreneurs, simple HTTP APIs driving microservices will stick around for some time to come.

<p><strong>API Discovery And Knowing Where All Of Your APIs Resources Actually Are</strong><br />
API discovery means a lot of things to a lot of people, but when it comes to delivering APIs well at scale in a multi-cloud, event-driven world, I’m simply talking about knowing where all of your API resources are. Meaning, if I walked into your company tomorrow, could you should me a complete list of every API or web service in use, and what enterprise capabilities they enable? If you can’t, then I’m guessing you aren’t going to be all that agile, efficient, and ultimately effective with doing your APIs at scale, and be able to orchestrate much, and identify what the most meaningful events are that occur across the enterprise landscape. I’m not even getting to the point of service mesh, and other API discovery wet dreams, I’m simply talking about being able to coherently articulate what your enterprise digital capabilities are.

<p><strong>Always Be Leveraging The Web As Part Of Your Diverse API Toolbox</strong><br />
Technologists, especially venture fueled technologists love to take the web for granted. Nothing does web scale better than, the web. Understand the objectives behind your APIs, and consider how you are leveraging the web, negotiate, cache, and build on the strengths of the web. Use the right media type for the job, and understand the tradeoffs of HTML, CSV, XML, JSON, YAML, and other media types. Understand when hypermedia media types might be more suitable for document, media, and other content focused API resources. Simple web APIs make a huge difference when they further speak to their intended audience and allow them to easily translate an API call into a workable spreadsheet, or navigate to the previous or next episode, installment, or other logical grouping with sensible hypermedia controls. Good API design is more about having a robust and diverse API design toolbox to choose from, than it is ever about the dogma that exists around any specific approach, philosophy, protocol, or venture capital fueled trend.

<p><strong>Have A Clear Vision For Who Will Be Using Your APIs</strong><br />
One significant mistake that API designers, developers, and architects make over and over again, is not having a clear vision of who will be using the APIs they are building. Defining, designing, delivering, and operating an API that is based upon what the provider wants over what the consumers will need. Using protocols, ignoring existing patterns, and adopting the latest trend that have nothing to do with what API consumers will be needing or capable of putting to work. Make sure you know your consumers, and consider giving them more control with query languages like GraphQL and Falcor, allowing them to define the type of experience they want. Work to have a clear vision of who will be consuming an API, even if you don’t know who they are. Starting simple with basic web APIs that help easily on-board new users who are unfamiliar with the domain and schema, while also allowing for the evolution give power-users who are in the know, more access, more control, and a stronger voice in the vision of what your APIs deliver or do not.

<p><strong>Responding In Real Time, Not Just Upon Request</strong><br />
A well oiled request and response API infrastructure is a critical base for any enterprise organization, however, a sign of a more mature, scalable API operations is always the presence of event-driven infrastructure including webhooks, streaming solutions, and multi-protocol, multi-message approaches to moving data and content around, and responding algorithmically based upon real time events occurring across the domains. Investing in event-driven infrastructure is not simply about using Kafka, it is about having a well-defined, well-honed web API base, with a suite of event-driven approaches in ones toolbox for also providing access to internal, partner, and last mile public and 3rd party resources using an appropriate set of protocols, and message formats. Something that might be as simple as a webhook subscription to changes, getting a simple HTTP push when something changes, to maintaining persistent HTTP connections to get an HTTP push when something changes, all the way to high volume HTTP and TCP connections to a variety of topical channels using Kafka, or other industrial grade API-driven solutions like gRPC, and beyond.

<p><strong>Have A Reason For When You Switch Protocols</strong><br />
There are a number of reasons why we switch protocols, moving off HTTP towards a TCP way of getting things done, with most reasoning being more emotional than they are ever technical. When I ask people why they went from HTTP APIs to Kafka, or Websockets, there is rarely a protocol based response. They did it because they needed things done in real time, through the existence of specific channels, or just simple because Kafka is how you do big data, or Websockets is how you do real time data. There wasn’t much scrutiny of who the consumers are, what was gained by moving to TCP, and what was lost by moving off HTTP. There is little awareness of the work Google has done around gRPC and HTTP/2, or what has happened recently around HTTP/3, formerly known as Quick UDP Internet Connections (QUIC). I’m no protocol expert, but I do grasp the role that these protocols play, and understand that the fundamental foundation of APIs is the web, and the importance of having a well thought out strategy when it comes to using the Internet for delivering on the API vision across the enterprise.
<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/containership_copper_circuit.jpg" width="45%" align="right" style="padding: 15px;" />
<p><strong>Ensuring All Your API Infrastructure Is Reliable</strong><br />
It doesn’t matter what your API design processes are, and what tools you are using if you cannot do it reliably. If you aren’t monitoring, testing, securing, and understanding performance, consumption, and limitations across ALL of your API infrastructure, then there will never be the right API solution. Web APIs, hypermedia, GraphQ, Webooks, Server-Sent Events, Websockets, Kafka, gRPC, and any other approach will always be inadequate if you cannot reliably operate them. Every tool within your API design toolbox should be able to be effectively deployed, thoughtfully managed, and coherently monitored, tested, secured, and delivered as a reliable service. If you don’t understand what is happening under the hood with any of your API infrastructure, out of your league technically, or kept in the dark through vendor magic, it should NOT be a tool in your toolbox, and be something that left in the R&amp;D lab until you can prove that you can reliably deliver, support, scale, and evolve something that is in alignment with, and has purpose augmenting and working with your existing API infrastructure.

<p><strong>Be Able To Deliver, Operate, And Scale Your APIs Anywhere They Are Needed</strong><br />
One increasingly critical aspect of any tool in our API design is whether or not we can deploy and operate it within multiple environments, or find that we are limited to just a single on-premise or cloud location. Can your request and response web API infrastructure operate within the AWS, Google, or Azure clouds? Does it operate on-premise within your datacenter, locally for development, and within sandbox environments for partners and 3rd party developers? Where your APIs are deployed will have have just as big of an impact on reliability and performance as your approach to design and the protocol you re using. Regulatory and other regional level concerns may have a bigger impact on your API infrastructure, than using REST, GraphQL, Webhooks, Server-Sent Events, or Kafka. Being able to ensure you can deliver, operate, and scale APIs anywhere they are needed is fast becoming a defining characteristic of the tools that we possess in our API toolboxes.

<p><strong>Making Sure All Your Enterprise Capabilities Are Well Defined</strong><br />
The final, and most critical element of any enterprise API toolbox, is ensuring that all of your enterprise capabilities are defined as machine readable API contracts, using OpenAPI, AsyncAPI, JSON Schema, and other formats. API definitions should provide human and machine readable contracts for all enterprise capabilities that are in play. These contracts contribute to every stop along the API lifecycle, and help both API providers and consumers realize everything I have discussed in this post. OpenAPI provides what we need to define our request and response capabilities using HTTP, and Async provides what we need to define our event-driven capabilities, providing the basis for understanding what we are capable of delivering using our API toolboxes, and responding to via the hybrid integration solutions we’ve engineered, and automated using our event-driven solutions. Defining the surface area of our API infrastructure, but also the API operations that surround the enterprise capabilities we are enabling internally, with partners, and publicly via our enterprise API efforts.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/11/25/a-diverse-api-toolbox-driving-hybrid-integrations-across-an-eventdriven-landscape/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/11/23/yaml-api-management-artifacts-from-aws-api-gateway/">YAML API Management Artifacts From AWS API Gateway</a></h3>
        <span class="post-date">23 Nov 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/old-door-lock-2_marcel_duchamp.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I’ve always been a big supporter of creating machine readable artifacts that help define the API lifecycle. While individual artifacts can originate and govern specific stops along the API lifecycle, they can also bring value when applied across other stops along the API lifecycle, and most importantly when it comes time to govern everything. The definition and evolution of individual API lifecycle artifacts is the central premise of <a href="http://apisjson.org">my API discovery format APIs.json</a>–which depends on there being machine readable elements within the index of each collection of APIs being documented, helping us map out the entire API lifecycle.

<p><a href="https://www.openapis.org/">OpenAPI</a> provides us with machine readable details about the surface area of our API which can be used throughout the API lifecycle, but it lacks other details about the surface area of our API operations. So when I do come across interesting approaches to extending the OpenAPI specification which are also injecting a machine readable artifact into the OpenAPI that support other stops along the API lifecycle, I like to showcase what they are doing. I’ve become very fond of one within the OpenAPI export of any AWS API Gateway deployed API I’m working with, which provides some valuable details that can be used as part of both the deployment and management stops along the API lifecycle:
<pre><code>
x-amazon-apigateway-integration:
	uri: "http://example.com/path/to/the/code/behind/"
	responses:
		default:
			statusCode: "200"
	requestParameters:
		integration.request.querystring.id: "method.request.path.id"
	passthroughBehavior: "when_no_match"
	httpMethod: "GET"
	type: "http"
</code></pre>
<p>This artifact is associated with each individual operation within my OpenAPI. It tells the AWS gateway how to deploy and manage my API. When I first import this OpenAPI into the gateway, it will deploy each individual path and operation, then it helps me manage it using the rest of the available gateway features. From this OpenAPI definition I can design, then autogenerate and deploy the code behind each individual operation, then deploy each individual path and operation to the AWS API Gateway and map them to the code behind. I can do this for custom APIs I’ve deployed, as well as Lambda brokered APIs–I prefer the direct way, because it is still easier, stabler, more flexible and cost effective for me to write the code behind each of my API operations, than to go full serverless.

<p>However, this artifact demonstrates for me the importance of artifacts associated with each stop along the API lifecycle. This little bit of OpenAPI extended YAML gives me a significant amount of control when it comes to the automation of deploying and managing my APIs. There are even more properties available for other layers of the AWS Gateway not included in this example, but is something that I will keep mapping out. Having these types of machine readable artifacts present within our OpenAPI specifications for describing the surface area of our APIs, as well as present within our APIs.json indexes for describing the surface area of our API operations will be critical to further automating, scaling, and defining the API lifecycle as it exists across the enterprise.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/11/23/yaml-api-management-artifacts-from-aws-api-gateway/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/11/23/the-api-journey/">The API Journey</a></h3>
        <span class="post-date">23 Nov 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/journey/journey-bridge.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I’ve been researching the API space full time for the last eight years, and over that time I have developed a pretty robust view of what the API landscape looks like. <a href="http://apievangelist.com/#api-lifecycle">You can find almost 100 stops along what I consider to be the API lifecycle on the home page of API Evangelist</a>. While not every organization has the capacity to consider all 100 of these stops, they do provide us with a wealth of knowledge generated throughout my own journey. Where I’ve been documenting what the API pioneers have been doing with their API operations, how startups leverage simple web API infrastructure, as well as how the enterprise has been waking up to the API potential in the last couple of years.

<p>Over the years I’ve tapped this research for my storytelling on the blog, and for the white papers and guides I’ve produced. I use this research to drive my talks at conferences, meetups, and the workshops I do within the enterprise. I’ve long had a schema for managing my research, tracking on the APIs, companies, people, tools, repos, news, and other building blocks I track across the API universe. Now, after a year of working with them on the ground at enterprise organizations, I’m partnering with <a href="http://streamdata.io">Streamdata.io (SDIO)</a> to continue productizing my approach to the API lifecycle, which we are calling Journey, or specifically SDIO Journey.

<p>Our workshops are broken into four distinct areas of the lifecycle:

<ul>
  <li><strong>Discovery</strong> (Goals, Definition, Data Sources, Discovery Sources, Discovery Formats, Dependencies, Catalog, Communication, Support, Evangelism) - Defining your digital resources are and what your enterprise capabilities are.</li>
  <li><strong>Design</strong> (Definitions, Design, Versioning, Webhooks, Event-Driven, Protocols, Virtualization, Testing, Landing Page, Documentation, Support, Communication, Road Map, Discovery) - Going API first, as well as API design first when it comes to the delivery of all of your API resources.</li>
  <li><strong>Development</strong> (Definitions, Discovery, Virtualization, Database, Storage, DNS, Deployment, Orchestration, Dependencies, Testing, Performance, Security, Communication, Support) - Considering what is needed to properly develop API resources at scale, and move from design to production.</li>
  <li><strong>Production</strong> (Definitions, Discovery, Virtualization, Authentication, Management, Logging, Plans, Portal / Landing Page, Getting Started, Documentation, Code, Embeddables, Licensing, Support, FAQs, Communication, Road Map, Issues, Change Log, Legal, Monitoring, Testing, Performance, Tracing, Security, Analysis, Maintenance) - Thinking about the production needs of an API operation, extracting the building blocks from successful APIs available across the web.</li>
  <li><strong>Outreach</strong> (Purpose, Scope, Defining Success, Sustaining Adoption, Communication, Support, Virtualization, Measurement, Structure) - Getting more structured around how you handle outreach around your APIs, whether they are internal, partner, or public API resources.</li>
  <li><strong>Governance</strong> (Design, Testing, Monitoring, Performance, Security, Observability, Discovery, Analysis, Incentivization, Competition) - Looking at how you can begin defining, measuring, analyzing, and providing guidance across API operations at the highest levels.</li>
</ul>

<p>We are currently working with several API service providers to deliver SDIO Journey workshops within their enteprise organizations, helping bring more API awareness to their pre-sales, sales, business, and executive groups. While also working to deliver independent Journey workshops for their customers, helping them see the bigger picture when it comes to the API lifecycle, but also begin establishing their own formal strategy for how they can execute on their own personal vision and version of it. Helping enterprise organization learn from the research I’ve gathered over the last eight years, and begin thinking more constructively, and being more thoughtful and organized about how they approach the delivery, iteration, and sustainment of APIs across the enterprise.

<p>I have turned SDIO Journey into a set of basic APIs that allow me to build, replicate, and deliver our Journey workshops. I’m preparing for a handful of workshops before the end of the year with <a href="http://axway.com">Axway</a>, and for API Days in Paris, but then in 2019, continue productizing and delivering these API workshops, helping encourage other enterprise organizations to invest more in their own API Journey, get more structured in how they think about the delivering of microservices across the enterprise. Helping them realize that the transformation they are going through right now isn’t going to stop. It is something that will be ongoing, and require their organization to learn to accept perpetual change and evolution in how they deliver the data, content, and algorithmic resources they’ll need to do business across the enterprise. While also evolving their understanding that all of this is more about people, business, and politics more than it will ever be about technology all by itself.

<p>If you have any questions about the SDIO Journey workshops we are doing, feel free to reach out, and I’ll get you more details about how to get involved.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/11/23/the-api-journey/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/11/21/what-does-the-next-chapter-of-storytelling-look-like-for-api-evangelist/">What Does The Next Chapter Of Storytelling Look Like For API Evangelist?</a></h3>
        <span class="post-date">21 Nov 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-butterfly-vertical.png" width="45%" align="right" style="padding: 15px;" />
<p>I find myself refactoring API Evangelist again this holiday season. Over the last eight years of doing API Evangelist I’ve had to regularly adjust what I do to keep it alive and moving forward. As I  close up 2018, I’m finding the landscape shifting underneath me once again, pushing me to begin considering what the next chapter of API Evangelist will look like. Pushing me to adjust my presence to better reflect my own vision of the world, but hopefully also find balance with where things are headed out there in the real world.

<p>I started API Evangelist in July of 2010 to study the business of APIs. As I was researching things in 2010 and 2011 I first developed what I consider to be the voice of the API Evangelist, which continues to be the voice I use in my storytelling here in 2018. Of course, it is something that has evolved and matured over the years, but I feel I have managed to remain fairly consistent in how I speak about APIs throughout the journey. It is a voice I find very natural to speak, and is something that just flows on some days whether I want it to or not, but then also something I can’t seem to find at all on other days. Maintaining my voice over the last eight years has required me to constantly adjust and fine tune, perpetually finding the frequency required to keep things moving forward.

<p>First and foremost, API Evangelist is about my research. It is about me learning. It is about me crafting stories that help me distill down what I’m learning, in an attempt to articulate to some imaginary audience, which has become a real audience over the years. I don’t research stuff because I’m being paid (not always true), and I don’t tell stories about things I don’t actually find interesting (mostly true). API Evangelist is always about me pushing my skills forward as a web architect, secondarily about me making a living, and third about sharing my work publicly and building an audience–in short, I do API Evangelist to 1) learn and grow, 2) pay the bills, and 3) cultivate an audience to make connections.

<p>As we approach 2019, I would say my motivations remain the same, but there is a lot that has changed in the API space, making it more challenging for me to maintain the same course while satisfying all these areas in a meaningful way. Of course, I want to keep learning and growing, but I’d say a shift in the API landscape toward the enterprise is making it more challenging to make a living. There just aren’t enough API startups out there to help me pay the bills anymore,  and I’m having to speak and sell to the enterprise more. To do this effectively, a different type of storytelling strategy is required to keep the paychecks coming in. Something I don’t think is unique to my situation, and is something that all API focused efforts are facing right now, as the web matures, and the wild west days of the API come to a close. It was fun while it lasted–yee haw!!

<p>In 2019, the API pioneers like SalesForce, Twitter, Facebook, Instagram, Twilio, SendGrid, Slack, and others are still relevant, but it feels like API storytelling is continuing it’s migration towards the enterprise. Stories of building an agile, scrappy startup using APIs isn’t as compelling as they used to be. They are being replaced by stories of existng enterprise groups become more innovative, agile, and competitive in a fast changing digital business landscape. The technology of APIs, the business of APIs, and the stories that matter around APIs have all been caught up in the tractor beam of the enterprise. In 2010, you did APIs if you were on the edge doing a startup, but by 2013 the enterprise began tuning into what is going on, by 2016 the enterprise responded with acquisitions, and by 2018 we are all selling and talking to the enterprise about APIs.

<p>Despite what many people might believe, I’m not anti-enterprise. I’m also not pro-startup. I’m for the use of web infrastructure to deliver on ethical and sensible private sector business objectives, strengthen expectations of what is possible in the public sector, while holding both sectors accountable to each other. I understand the enterprise, and have worked there before. I also understand how it is evolving over the last eight years through API discussions I have been having had with enterprise folks, workshops I’ve conducted within various public and private sector groups, and studying this latest shift in technology adoption across large organizations. Ultimately, I am very skeptical that large business enterprises can adapt, decouple, evolve, and embrace API and microservice principles in a way that will mean success, but I’m interested in helping educate enterprise teams, and assist them in crafting their enterprise-wide API strategy, and contribute what I can to incentivize change within these large organizations.
<p><img src="https://s3.amazonaws.com/kinlane-productions2/kin-lane/141-Post+Con+2018-Speakers.jpg" width="45%" align="right" style="padding: 15px;" />
<p>A significant portion of my audience over the last eight years is from the enterprise. However, I feel like these are the people within the enterprise who have picked up their heads, and consciously looked for new ways of doing things. My audience has always been fringe enterprise folks operating at all levels, but API Evangelist does not enjoy mainstream enterprise adoption and awareness. A significant portion of my storytelling speaks to the enterprise, but I recognize there is a portion of it that scares them off, and doesn’t speak to them at all. One of the questions I am faced with is around what type of tone do I strike as the API Evangelist in this next chapter? Will it be a heavy emphasis on the politics of APIs, or will it be more about the technology and business of APIs? To continue learning and growing in regards to what is happening on the ground with APIs, I’m going to need enterprise access. To continue making a living doing APIs, I’m going to need more enterprise access. The question for me is always around how far I put my left foot in the enterprise or government door, and how far I keep my right found outside in the real world–where there is no perfect answer, and is something that requires constant adjustment.

<p>Another major consideration for me is always around authenticity. An area I posses a natural barometer in, and while I have a pretty high tolerance for API blah blah blah, and writing API industry white papers, when I start getting into areas of technology, business, or politics where I feel like I’m not being authentic, I automatically begin shutting down. I’ve developed a bulshit-o-meter over the years that helps me walk this line successfully. I’m confident I can maintain and not sell out here. My challenge is more about continuing to do something that matters to someone who will continue investing in my work, and having relevance to the audience I want to reach, and less about keeping things in areas that I’m interested in. I will gladly decline conversations, relationships, and engagements in unethical areas, shady government or business practices, avoid classified projects, and pay for play concepts along the way. Perpetually pushing me to always strike a balance between something that interests me, that pushes my skills, bring value to the table, has a meaningful impact, enjoys a wide reach, while also paying the bills. Which reflects what I’m thinking through as I write this blog post, demonstrating how I approach my own professional development.

<p>So, what does the next chapter of storytelling look like for API Evangelist? I do not know. I know it will have more of a shift towards the enterprise. Which means a heavy emphasis on the technology and business of APIs. However, I’m also thinking deeply about how I present the political side of the API equation, and how I voice my opinions and concerns when it comes to privacy, security, transparency, observability, regulation, surveillance, and ethics that swirls around APIs. I’m guessing they can still live side by side in some way, I just need to be smarter about the politics of it, and less rantier and emotional. Maybe separate things into a new testament for the enterprise that is softer, wile also maintaining a separate old testament for the more hellfire and brimstone. IDK. It is something I’ll continue mulling over, and make decisions around as I continue to shift things up here at API Evangelist. As you can tell my storytelling levels are lower than normal, but my traffic is still constant, reflecting other shifts in my storytelling that have occurred in the past. I’ll be refactoring and retooling over the holidays, and no doubt have more posts about the changes. If you have any opinions on what value you get from API Evangelist, and what you’d like to see present in the next chapter, I’d love to hear from you in the comments below, on Twitter, or personally via email.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/11/21/what-does-the-next-chapter-of-storytelling-look-like-for-api-evangelist/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/11/16/the-ability-to-link-to-api-service-provider-features-in-my-workshops-and/">The Ability To Link To API Service Provider Features In My Workshops And</a></h3>
        <span class="post-date">16 Nov 2018</span>
        <p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/109_201_800_500_0_max_0_-5_-5.jpg" width="45%" align="right" style="padding: 15px;" />
<p>All of my API workshops are machine readable, driven from a central YAML file that provides all the content and relevant links I need to deliver what I need during a single, or multi-day API strategy workshop. One of the common elements of my workshops are links out to relevant resource, providing access to services, tools, and other insight that supports whatever I’m covering in my workshop. There are two parts to this equation, 1) me knowing to link to something, and 2) being able to link to something that exists.

<p>A number of API services and tooling I use don’t follow web practices and do not provide any easy way to link to a feature, or other way of demonstrating the functionality that exists. The web is built on this concept, but along the way within web and mobile applications, we’ve have seemed to lose our understanding for this fundamental concept. There are endless situations where I’m using a service or tool, and think that I should reference in one of my workshops, but I can’t actually find any way to reference as a simple URL. Value buried within a JavaScript nest, operating on the web, but not really behaving like you depend on the web.

<p>Sometimes I will take screenshots to illustrate the features of a tool or service I am using, but I’d rather have a clean URL and bookmark to a specific feature on a services page. I’d rather give my readers, and workshop attendees the ability to do what I’m talking about, not just hear me talk about it. In a perfect world, every feature of a web application would have a single URL to locate said feature. Allowing me to more easily incorporate features into my storytelling and workshops, but alas many UI / UX folks are purely thinking about usability and rarely thinking about instruct-ability, and being able to cite and reference a feature externally, using the fundamental building blocks of the web.

<p>I understand that it isn’t easy for all application developers to think externally like this, but this is why I tell stories like this. To help folks think about the externalities of the value they are delivering. It is one of the fundamental features of doing business on the web–you can link to everything. However, I think we often forgot what makes the web so great, as we think about how to lock things down, erect walled gardens around our work, something that can quickly begin to work against us. This is why doing APIs is so important as it can helps us think outside of the walls of the gardens we are building, and consider someone else’s view of the world. Something that can give us the edge when it comes to reaching a wider audience with whatever we are creating.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/11/16/the-ability-to-link-to-api-service-provider-features-in-my-workshops-and/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/11/06/flickr-and-reconciling-my-history-of-apis-storytelling/">Flickr And Reconciling My History Of APIs Storytelling</a></h3>
        <span class="post-date">06 Nov 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/flickr/361347580_2d9d02b83d_z.jpg" width="45%" align="right" />
<p>Flickr was <a href="http://apievangelist.com/2010/10/06/flickr-api-review/">one of the first APIs that I profiled back in 2010</a> when I started API Evangelist. Using their API as a cornerstone of my research, <a href="http://apievangelist.com/2011/02/09/history-of-apis-flickr-api/">resulting in their API making it into my history of APIs storytelling</a>, continuing to be a story I’ve retold hundreds of times in the conversations I’ve had over the eight years of being the API Evangelist. Now, after the second (more because of Yahoo?) acquisition, <a href="https://www.businesswire.com/news/home/20181101005328/en/Flickr-Announces-New-Photographer-Centric-Improvements-Flickr-Pro">Flickr users are facing significant changes regarding the number of images we can store on the platform, and what we will be charged for using the platform</a>–forcing me to step back, and take another look at the platform that I feel has helped significantly shape the API space as we know it.

<p>When I step back and think about Flickr, it’s most important contribution to the world of APIs was all about the resources it made available. Flickr was the original image sharing API, powering the growing blogosphere at the beginning of this century. Flickr gave us a simple interface for humans in 2004, and an API for other applications just six months later, that provided us all with a place to upload the images we would be using across our storytelling on our blogs. Providing the API resources that we would be needed to power the next decade of storytelling via our blogs, but also set into the motion the social evolution of the web, demonstrating that images were an essential building block of doing business on the web, and in just a couple of years, on the new mobile devices that would become ubiquitous in our lives.

<p>Flickr was an important API resource, because it provided access to an important resource–our images. The API allowed you to share these meaningful resources on your blog, via Facebook and Twitter, and anywhere else you wanted. In 2005, this was huge. At the time, I was working to make a shift from being an developer lead, to playing around with side businesses built using the different resources that were becoming available online via simple web APIs. Flickr quickly became a central actor in my digital resource toolbox, and I was using it regularly in my work. As an essential application, Flickr quickly got out of my way by offering an API. I would still use the Flickr interface, but increasingly I was just publishing images to Flickr via the API, and embedding them in blogs, and other marketing, becoming what we began to call social media marketing, and eventually was something that I would rebrand as API Evangelist while making it more about the tooling I was using, than the task I was accomplishing.

<p>After thinking about Flickr as a core API resource, next I always think about the stories I’ve told about Flickr’s Caterina Fake who coined the phase, “business development 2.0”. As I tell it, back in the early days of Flickr, the team was getting a lot of interest in the product, and unable to respond to all emails and phone calls. They simply told people to build on their API, and if they were doing something interesting, they would know, because they had the API usage data. Flickr was going beyond the tech and using an API to help raise the bar for business development partnerships, putting the burden on the integrator to do the heavy lifting, write the code, and even build the user base, before you’d get the attention of the platform. If you were building something interesting, and getting the attention of users, the Flickr team would be aware of it because of their API management tooling, and they would reach out to you to arrange some sort of partner relationship.

<p><img src="https://kinlane-productions2.s3.amazonaws.com/flickr/flickr-beta.png" align="right" width="40%" style="padding: 15px;" />
<p>It makes for a good story. It resonates with business people. It speaks to the power of doing APIs. It is also enjoys a position which omits so many other negative aspects of doing startups, which as a technologist becomes too easy to look the other way when you are just focused on the tech, and as a business leader after the venture capital money begins flowing. Business development 2.0 has a wonderful libertarian, pull yourself up by your bootstrap ring to it. You make valuable resources available, and smart developers will come along and innovate! Do amazing things you never thought of! If you build it, they will come. Which all feeds right into the sharecropping, and exploitation that occurs within ecosystems, leading to less than ethical API providers poaching ideas, and thinking that it is ok to push public developers to work for free on their farm. Resulting in many startups seeing APIs as simply a free labor pool, and source of free road map ideas, manifesting concepts like the “<a href="https://apievangelist.com/2018/07/09/operating-your-api-in-the-cloud-kill-zone/">cloud kill zone</a>”. Business development 2.0 baby!!

<p>Another dimension of this illness we like to omit is around the lack of a business model. I mean, the shit is free! Why would we complain about free storage for all our images, with a free API? It is easier for us to overlook the anti-competitive approaches to pricing, and complain down the road when each acquisition of the real product (Flickr) occurs, than it is to resist companies who lack a consumer level business model, simply because we are all the product. Flickr, Twitter, Facebook, Gmail, and other tools we depend on are all free for a reason. Because they are market creating services, and revenue is being generated at other levels out of our view as consumers, or API developers. We are just working on Maggie’s Farm, and her pa is reaping all the benefit. When it come’s to Flickr, Maggie and her {a cashed out a long time ago, and the farm keeps getting sold and resold, all while we still keep working away in the soil, giving them our digital bits that we’ve cultivate there, until conditions finally become unacceptable enough to run us off.

<p>I’ve begun moving off of Flickr a couple years ago. I stopped using them for blog photo hosting in 2010. I stopped uploading photos there regularly over the last couple years. The latest crackdown doesn’t mean much to me. It will impact my storytelling to potentially lose such an amazing resource of openly licensed photos. However, I’ve saved each photo I use, and it’s attribution locally–hopefully my attribution link doesn’t begin to 404 at some point. Hopefully other openly licensed photo collections emerge on the horizon, and ideally SmugMug doesn’t do away with openly licensed treasure trove they are stewards of now. The latest acquisition and business model shift occurring across the Flickr platform doesn’t hit me too hard, but the situation does give me an opportunity to step back and reassess my API storytelling, and the role that Flickr plays in my API Evangelist narrative. Giving me another opportunity to eliminate bullshit and harmful myths from my storytelling and myth making–which I feel like is getting pretty close to leaving me with nothing left to tell when it comes to APIs.

<p>In the end, if I just focus purely on the tech, and ignore the business and politics of APIs, I can keep telling these bullshit. This is the real Flickr lesson for me. I’d say there is two reasons we perpetuate stories like this. One, “because we just didn’t know any better”. Which is pretty weak. Two, it is how capitalism works. It is why us dudes, especially us white dudes thrive so well in a Silicon Valley tech libertarian world, because this type of myth making benefits us, even when it repeatedly sets us up for failure. This is one of the things that makes me throw up a little (a lot) in my mouth when I think about the API Evangelist persona I’ve created. This entire reality makes it difficult for me to keep doing this API Evangelist theater each day. APIs are cool and all, but when they are wielded as part of this larger money driven stream of consciousness, we (individuals) are always going to lose. In the end, why the fuck do I want to be a mouthpiece for this kind of exploitation. I don’t.

<p><em><strong>Photo Credit:</strong> <a href="https://www.flickr.com/photos/kinlane/361347580/in/dateposted-public/">Kin Lane (The First Photo I Uploaded to Flickr)</a></em>


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/11/06/flickr-and-reconciling-my-history-of-apis-storytelling/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/11/01/the-impact-of-travel-on-being-the-api-evangelist/">The Impact Of Travel On Being The API Evangelist</a></h3>
        <span class="post-date">01 Nov 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/IMG_7598.jpg" width="45%" align="right" style="padding: 15px;" />
<p>Travel is an important part of what I do. It is essential to striking up new relationships, and reenforcing old ones. It is important for me to get out of my bubble, expose myself to different perspectives, and see the world in different ways. I am extremely grateful for the ability to travel around the US, and the world the way that I do. I am also extremely aware of the impact that travel has on me being the API Evangelist–the positive, the negative, and the general shift in my tone in storytelling after roaming the world.

<p>One of the most negative impact that traveling has on my world is on my ability to complete blog posts. If you follow my work, when I’m in the right frame of mind, I can produce 5-10 blog posts across the domains I write for, on a daily basis. The words just do not flow in the same way when I am on the road. I’m not in a storyteller frame of mind. At least in the written form. When I travel, I am existing in a more physical and verbal sense as the API Evangelist, something that doesn’t always get translated into words on my blog(s). This is something that is ok for short periods of time, but after extended periods of time on the road, it is something that will begin to take a toll on my overall digital presence.

<p>After the storytelling impact, the next area to suffer when I am on the road, is my actual project work. I find it very difficult to write code, or think at architectural levels while on the road. I can flesh out and move forward smaller aspects of the projects I’m working on, but because of poor Internet, packed schedules, and the logistics of being on the road, my technical mind always suffers. This is something that is also related to the impact on my overall storytelling. Most of the stories I publish on a daily basis evolve out of me moving forward actual projects as part of my API Evangelist work. If I am not actually developing a strategy, designing a specific API, or working on API definitions, discovery, governance, or one of the loftier aspects of my work, the chances I’m telling interesting stories will significantly be diminished.

<p>Once I land back home, one of the first orders of business is to unclog the pipes with a “travel is hard” story. ;-) Pushing my fingers to work again. Testing out the connections between my brain and my fingers. While I also open up my IDE, command line, API universe dashboard, and begin refining my paper notes about what the fuck I was actually doing before I got on that airplane. Make it all work again is tough. Even the simplest of tasks seem difficult, and many of the projects I’m working on just seem too big to even know where to even begin. However, with a little effort, focus, and lack of a plane, train, or meeting to be present for, I’ll find my way forward again, slowly picking back up the momentum I enjoy as the API Evangelist. Researching, coding, telling stories, and pushing forward my projects so that they can have an impact on the space, and continue paying the bills to keep this vessel moving forward in the direction that I want.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/11/01/the-impact-of-travel-on-being-the-api-evangelist/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/10/22/what-are-your-enterprise-api-capabilities/">What Are Your Enterprise API Capabilities?</a></h3>
        <span class="post-date">22 Oct 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/machine-road_copper_circuit.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I spend a lot of time helping enterprise organizations discover their APIs. All of the organizations I talk to have trouble knowing where all of their APIs are–even the most organized of them. Development and IT groups have just been moving too fast over the last decade to know where all of their web services, and APIs are. Resulting in large organizations not fully understanding what all of their capabilities are, even if it is something they actively operate, and may drive existing web or mobile applications.

<p>Each individual API within the enterprise represents a single capability. The ability to accomplish a specific enterprise tasks that is valuable to the business. While each individual engineer might be aware of the capabilities present on their team, without group wide, and comprehensive API discovery across an organization, the extent of the enterprise capabilities is rarely known. If architects, business leadership, and any other stakeholder can’t browse, list, search, and quickly get access to all of the APIs that exist, the knowledge of the enterprise capabilities will not be able to be quantified or articulated as part of regular business operations.

<p>In 2018, the capabilities of any individual API is articulated by it’s machine readable definition. Most likely OpenAPI, but could also be something like API Blueprint, RAML, or other specification. For these definitions to speak to not just the technical capabilities of each individual API, but also the business capabilities, they will have to be complete. Utilizing a higher level strategic set of tags that help label and organize each API into a meaningful set of business capabilities that best describes what each API delivers. Providing a sort of business capabilities taxonomy that can be applied to each API’s definition and used across the rest of the API lifecycle, but most importantly as part of API discovery, and the enterprise digital product catalog.

<p>One of the first things I ask any enterprise organization I’m working with upon arriving, is “do you know where all of your APIs are?” The answer is always no. Many will have a web services or API catalog, but it almost always is out of date, and not used religiously across all groups. Even when there are OpenAPI definitions present in a catalog, they rarely contain the meta data needed to truly understand the capabilities of each API. Leaving developer and IT operations existing as black holes when it comes to enterprise capabilities, sucking up resources, but letting very little light out when it comes to what is happening on the inside. Making it very difficult for developers, architects, and business users to articulate what their enterprise capabilities are, and often times reinventing the wheel when it comes to what the enterprise delivers on the ground each day.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/10/22/what-are-your-enterprise-api-capabilities/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/10/22/join-me-for-a-fireside-chat-at-the-paris-api-meetup-this-wednesday/">Join Me For A Fireside Chat At The Paris API Meetup This Wednesday</a></h3>
        <span class="post-date">22 Oct 2018</span>
        <p><a href="https://www.meetup.com/ParisAPI/events/255614957/"><img src="https://s3.amazonaws.com/kinlane-productions2/events/paris-api-meetup/DqJd3bkJ.jpeg" width="45%" align="right" style="padding: 15px;" /></a>
<p>I am in Europe for most of October, and while I am in Paris we thought it would be a good idea to pull together a last minute API Meetup. Romain Simiand (<a href="https://twitter.com/RomainSimiand">@RomainSimiand</a>), the API Evangelist at <a href="https://www.people-doc.com/">PeopleDoc</a> was gracious enough to help pull things together, and the <a href="http://streamdata.io">Streamdata.io</a> team is stepping up to help with food and drink. Pulling together a last minute gathering at PeopleDoc in Paris, and bringing me on stage to talk about the technology, business, and politics of APIs, well as about some of my recent work on API discovery, and event-driven architecture.

<p><a href="https://www.meetup.com/ParisAPI/events/255614957/">You can find more details on the Paris API Meetup site</a>, with directions on how to find PeopleDoc. Make sure you RSVP so that we know you are coming, and of course, please help spread the word. We are over 30 people attending so far, but I think we can do better. I’m happy to get on stage and help drive the API discussion, but I’d prefer to have a healthy representation of the Paris API community asking questions, helping me understand what is happening across the area when it comes to APIs. I always have plenty of knowledge to share, but it becomes exponentially more valuable when people on the ground within communities are asking questions, and making it relevant to what is happening within the day to day operations of companies in the local area.

<p>While I enjoy doing conference keynotes and panels, my favorite format of event is the Meetup. Bringing together less than 100 people have a discussion about APIs. I always find that I learn the most in this environment, and able to actually engage with developers and business folks about what really matters when it comes to APIs. The larger the audience the more it is just about me broadcasting my message, and when it is a smaller and more intimate venue, I feel like I can better connect with people. In my opinion, this is how all API events should be–small, intimate, and a real world conversation about APIs. Not just an API pundit pushing their thoughts out, ensuring that all participants feel like they are actually part of the conversation.

<p>If you are in the Paris region, or can make the time to hope on a plane or train and make it to Paris this Wednesday, I love to hang out. If you can’t make it, I’ll be back for API Days Paris in December, but it will be a bigger event, and it might be more difficult to carve out the time to hang. So, bring your API questions, and come over to the PeopleDoc office this Wednesday, and we’ll have a proper discussion about the technology, business, or politics of APIs. Helping drive the API discussion going on in France, continuing to push it forward. Making France a leader when it comes to doing business in the growing API economy. I look forward to seeing you all in Paris this week!


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/10/22/join-me-for-a-fireside-chat-at-the-paris-api-meetup-this-wednesday/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/10/22/i-participated-in-an-api-workshop-with-the-european-commission-last-week/">I Participated In An API Workshop With The European Commission Last Week</a></h3>
        <span class="post-date">22 Oct 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/events/apis4dgov/DpyR9qrXoAAYo4r.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I was in Ispra, Italy last week for a two day workshop on APIs with the European Commission. <a href="https://ec.europa.eu/digital-single-market/en/news/new-study-digital-government-apis-apis4dgov-project">The European Commission’s DG CONNECT together with the Joint Research Centre (JRC) launched a study</a> with the purpose to gain further understanding of the current use of APIs in digital government and their added value for public services, and they invited me to participate. I was joined by Mehdi Medjaoui (<a href="https://twitter.com/medjawii">@medjawii</a>), David Berlind (<a href="https://twitter.com/dberlind">@dberlind</a>), and Mark Boyd (<a href="https://twitter.com/mgboydcom">@mgboydcom</a>), along with EU member states, and European cities, to help provide feedback and strategies for consideration by the commission.

<p>This European Commission study is looking at <em>“innovative ways to improve interconnectivity of public services and reusability of public sector data, including dynamic data in real-time, safeguarding the data protection and privacy legislation in place.”</em> Looking to:

<ul>
  <li>assess digital government APIs landscape and opportunities to support the digital</li>
  <li>transformation of public sector</li>
  <li>identify the added value for society and public administrations of digital government APIs (key enablers, drivers, barriers, potential risks and mitigates)</li>
  <li>define a basic Digital Government API EU framework and the next steps</li>
</ul>

<p>David Berlind from ProgrammableWeb gave a couple talks, with myself, Mehdi, and Mark following up. The rest of the time spent was hearing presentations from EU member states, and other municipal efforts–learning more about the successes and the challeges they face. What I heard reflected what I’ve experienced in federal government, as well as city, county, and state level API efforts I’ve participated in across the United States. &lt;p&gt;<img src="https://s3.amazonaws.com/kinlane-productions2/events/apis4dgov/IMG_7464.jpg" width="45%" align="right" style="padding: 15px;" />&lt;/p&gt;All groups were struggling to win over leaders and the public, modernize legacy system, build on top of open data efforts, and push forward the conversation using a modern approach to delivering web APIs.

<p>I am eager to see what comes out of the European Commission API project. While there are still interesting things happening in the United States, I feel like there is an opportunity for the EU to leap frog us when it comes to meaningful API adoption within government. While many cities, counties, and states are still investing in open data and APIs, the investment at the federal level has stagnated with the current administration. There are still plenty of agencies moving forward the API conversation, but the leadership is coming from the GSA, and from within individual agencies, not from the executive branch. What is happening at the European Commission has the potential to be adopted by all the countries in the European Union, and making a pretty significant impact in how government works using APIs.

<p>I’ll be staying in touch with the group leading the effort, and making myself available for future gatherings. There was talk of holding another gathering at API Days in Paris, and I am sure there will be further workshops as the project evolves. Clearly the European Commission has a huge amount of work ahead of them, but the fact that they are coming together like this, and highlighting, as well as learning from the existing work going on across the member states, shows significant promise. I made it clear as we were wrapping up regarding the importance of continued storytelling between the member states, as well as out of the European Commission. Emphasizing it will take a regular drumbeat of activity, and sharing of the work in real-time, for all of this to evolve as they desire. However, with the right cadence, the API effort out of Europe could make a pretty significant impact across the EU, and beyond.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/10/22/i-participated-in-an-api-workshop-with-the-european-commission-last-week/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/10/11/api-evangelist-api-lifecycle-workshop-on-api-discovery/">API Evangelist API Lifecycle Workshop on API Discovery</a></h3>
        <span class="post-date">11 Oct 2018</span>
        <p><a href="http://locations.api.lifecycle.workshop.apievangelist.com/"><img src="https://s3.amazonaws.com/kinlane-productions2/workshops/kin-lane-api-days-spain.jpg" width="45%" align="right" style="padding: 15px;" /></a>
<p>I’ve been doing more <a href="http://locations.api.lifecycle.workshop.apievangelist.com/">workshops on the API lifecycle within enterprise groups lately</a>. Allowing me to refine my materials on the ground within enterprise groups, further flesh out the building blocks I recommend to API groups to help them craft their own API strategy. One of the first discussions I start with large enterprise groups is always in the area of API discovery, or commonly asked as, “do you know where all your APIs are?”

<p>EVERY group I’m working with these days is having challenges when it comes to easy discovery across all the digital resources they possess, and put to use on a daily basis. I’m working with a variety of companies, organizations, institutions, and government agencies when it comes to the API discovery of their digital self:

<ul>
  <li><strong>Low Hanging Fruit</strong> (<a href="http://locations.api.lifecycle.workshop.apievangelist.com/outline-items/#Low%20Hanging%20Fruit">outline</a>) - Understanding what resources are already on the public websites, and applications, by spidering existing domains looking for data assets that should be delivered as API resources.</li>
  <li><strong>Discovery</strong> (<a href="http://locations.api.lifecycle.workshop.apievangelist.com/outline-items/#Discovery">outline</a>) - Actively looking for web services and APIs that exist across an organization, industry, or any other defined landscape. Documenting, aggregating, and evolving what is available about each API, while also publishing back out and making available relevant teams.</li>
  <li><strong>Communication</strong> (<a href="http://locations.api.lifecycle.workshop.apievangelist.com/outline-items/#Communication">outline</a>) - Having a strategy for reaching out to teams and engaging with them around API discovery, helping the remember to register and define their APIs as part of wider strategy.</li>
  <li><strong>Definitions</strong> (<a href="http://locations.api.lifecycle.workshop.apievangelist.com/outline-items/#Definition">outline</a>) - Work to make ensure that all definitions are being aggregated as part of the process so that they can be evolved and moved forward into design, development and production–investing in all of the artifacts that will be needed down the road.</li>
  <li><strong>Dependencies</strong> (<a href="http://locations.api.lifecycle.workshop.apievangelist.com/outline-items/#Dependencies">outline</a>) - Defining any dependencies that are in play, and will play a role in operations. Auditing the stack behind any service as it is being discovered and documented as part of the overall effort.</li>
  <li><strong>Support</strong> (<a href="http://locations.api.lifecycle.workshop.apievangelist.com/outline-items/#Support">outline</a>) - Ensure that all teams have support when it comes to questions about web service and API discovery, helping them initially, as well as along the way, making sure all their APIs are accounted for, and indexed as part of discovery efforts.</li>
</ul>

<p>API discovery will positively or negatively impact the rest of the API lifecycle at any organization. Not knowing where all of your resources are, and not having them properly defined for discovery at critical design, development, production, and integration movements, is an illness all companies are suffering from in 2018. We’ve deployed layers of services to deliver on enterprise growth, and put down a layer of web APIs to service the web, mobile, and increasingly device-based applications we’ve been delivering. Resulting in a tangled web of services, we need to tame before we can move forward properly.

<p>Let me know if you need help with API discovery where you work. It is the fastest growing aspect of my API workshop portfolio. Aside from security, I feel like API discovery is the biggest challenge facing large enterprise groups learning to be more agile, flexible, and pushing forward with a microservices, and event-driven way of doing business. I definitely don’t have all the solutions when it comes to API discovery, but I knew have a lot of experience to share around how we are defining our enterprise capabilities and resources, and making them more discoverable across our entire API catalog.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/10/11/api-evangelist-api-lifecycle-workshop-on-api-discovery/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/10/11/api-evangelist-api-lifecycle-workshop-on-api-design/">API Evangelist API Lifecycle Workshop on API Design</a></h3>
        <span class="post-date">11 Oct 2018</span>
        <p><a href="http://locations.api.lifecycle.workshop.apievangelist.com/"><img src="https://s3.amazonaws.com/kinlane-productions2/workshops/43043431_10156747264069813_2487933138479611904_n.jpg" width="45%" align="right" style="padding: 15px;" /></a>
<p>I’ve been doing more <a href="http://locations.api.lifecycle.workshop.apievangelist.com/">workshops on the API lifecycle within enterprise groups lately</a>. Allowing me to refine my materials on the ground within enterprise groups, further flesh out the building blocks I recommend to API groups to help them craft their own API strategy. One area of the API lifecycle I find more groups working on these days, centers around a design-first approach to the API lifecycle.

<p>While not many groups I work with achieved a design-first approach doing APIs, almost all of them I talk to express interest in making this a reality at least within some groups, or projects. The appeal of being able to define, design, mock, and iterate upon an API contract before code gets written is very appealing to enterprise API groups, and I’m looking to help them think through this part of their API lifecycle, and work towards making API design first a reality at their organization.

<ul>
  <li><strong>Definition</strong> (<a href="http://locations.api.lifecycle.workshop.apievangelist.com/outline-items/#Definition" target="_blank">outline</a>) - Using definitions as the center of the API design process, developing an OpenAPI contract for moving things through the design phase, iterating, evolving, and making sure the definitions drive the business goals behind each service.</li>
  <li><strong>Design</strong> (<a href="http://locations.api.lifecycle.workshop.apievangelist.com/outline-items/#Design" target="_blank">outline</a>) - Considering the overall approach to design for all APIs, executing upon design patterns that are in use to consistently deliver services across teams. Leveraging a common set of patterns that can be used across services, beginning with REST, but also evetually allowing the leveraging of hypermedia, GraphQL, and other patterns when it comes to the deliver of services.</li>
  <li><strong>Versioning</strong> (<a href="http://locations.api.lifecycle.workshop.apievangelist.com/outline-items/#Versioning" target="_blank">outline</a>) - Managing the definition of each API contract being defined as part of the API design stop for this area of the lifecycle, and having a coherent approach to laying out next steps.</li>
  <li><strong>Virtualization</strong> (<a href="http://locations.api.lifecycle.workshop.apievangelist.com/outline-items/#Virtualization" target="_blank">outline</a>) - Providing mocked, sandbox, and virtualized instances of APIs and other data for understanding what an API does, helping provide an instance of an API that reflects exactly how it should behave in a production environment.</li>
  <li><strong>Testing</strong> (<a href="http://locations.api.lifecycle.workshop.apievangelist.com/outline-items/#Testing" target="_blank">outline</a>) - Going beyond just testing, and making sure that a service is being tested at a granular level, using schema for validation, and making sure each service is doing exactly what it should, and nothing more.</li>
  <li><strong>Landing Page</strong> (<a href="http://locations.api.lifecycle.workshop.apievangelist.com/outline-items/#Landing%20Page" target="_blank">outline</a>) - Making sure that each individual service being designed has a landing page for acccessing it’s documentation, and other elements during the design phase.</li>
  <li><strong>Documentation</strong> (<a href="http://locations.api.lifecycle.workshop.apievangelist.com/outline-items/#Documentation" target="_blank">outline</a>) - Ensuring that there is always comprehensive, up to date, and if possible interactive API documentation available for all APIs being designed, allowing all stakeholders to easily understand what an API is going to accomplish.</li>
  <li><strong>Support</strong> (<a href="http://locations.api.lifecycle.workshop.apievangelist.com/outline-items/#Support" target="_blank">outline</a>) - Ensuring there is support channels available for an API, and stakeholders know who to contact when providing feedback and answering questions in real, or near real time, pushing forward the design process.</li>
  <li><strong>Communication</strong> (<a href="http://locations.api.lifecycle.workshop.apievangelist.com/outline-items/#Communication" target="_blank">outline</a>) - Making sure there is a communication strategy for moving an API through the design phase, and making sure stakeholders are engaged as part of the process, with regular updates about what is happening.</li>
  <li><strong>Road Map</strong> (<a href="http://locations.api.lifecycle.workshop.apievangelist.com/outline-items/#Road%20Map" target="_blank">outline</a>) - Providing a list of what is being worked on with each service being designed, and pushed forward, providing a common list for everyone involved to work from.</li>
  <li><strong>Discovery</strong> (<a href="http://locations.api.lifecycle.workshop.apievangelist.com/outline-items/#Discovery" target="_blank">outline</a>) - Make sure all APIs are discoverable after they go through the design phase, ensuring each type of API definition is up to date, and catalogs are updated as part of the process.</li>
</ul>

<p>I currently move my own APIs forward in this way using a variety of open source tooling, and GitHub. I’m working with some groups to do this in Stoplight.io, as well as Postman. I don’t think there is any “right way” to go API define and design first. I’m here to just educate teams about what is going on out there. What some of the services and tools that help enable an API design first reality, and talk through the technological, business, and political challenges are preventing a team, or entire enterprise group from becoming API design first.

<p>Let me know if you need help thinking through the API design strategy where you work. I’ve been studying this area since it emerged as a discipline in 2012, led by API service providers like Apiary, but continue with other next generation platforms like Stoplight.io, APIMATIC, and others. For me, API design is less about REST vs Hypermedia vs GraphQL, and more about the lifecycle, services, tooling, and API definitions you use. I’m happy to share my view of the API design landscape with your group, just let me know how I can help.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/10/11/api-evangelist-api-lifecycle-workshop-on-api-design/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/10/09/the-layers-of-completeness-for-an-openapi-definition/">The Layers Of Completeness For An OpenAPI Definition</a></h3>
        <span class="post-date">09 Oct 2018</span>
        <p>Everyone wants their OpenAPIs to be complete, but what that really means will depend on who you are, what your knowledge of OpenAPI is, as well as being driven by your motivation for having an OpenAPI in the first place. I wanted to take a crack at articulating a complete(enough) definition for OpenAPIs I create, based upon what I’m needing them to do.

<p><strong>Info &amp; Base</strong> - Give the basic information I need to understand who is behind, and where I can access the API.
<script src="https://gist.github.com/kinlane/5e52d6063a0744d711795beb6e60365f.js"></script>

<p><strong>Paths</strong> - Provide an entry for every path that is available for an API, and should be included in this definition.
<script src="https://gist.github.com/kinlane/1aa1a3f492da5f18bc7947b62589c8f8.js"></script>

<p><strong>Parameters</strong> - Provide a complete list of all path, query, and header parameters that can be used as part of an API.
<script src="https://gist.github.com/kinlane/29d0247d6ff4aaa39db4dc793df4a2f9.js"></script>

<p><strong>Descriptions</strong> - Flesh out descriptions for all the path and parameter descriptions, helping describe an API does.
<script src="https://gist.github.com/kinlane/160ae6aafdf5a5fb7114b5dd2ac37981.js"></script>

<p><strong>Enums</strong> - Publish a list of all the enumerated values that are possible for each parameter used as part of an API.
<script src="https://gist.github.com/kinlane/444731f0214cab5efcc3ae77011823ba.js"></script>

<p><strong>Definitions</strong> - Document the underlying schema being returned by creating a JSON schema definition for the API.
<script src="https://gist.github.com/kinlane/e833af3e1df40c716289c6cb81a64b88.js"></script>

<p><strong>Responses</strong> - Associate the definition for the API with the path using a response reference, connecting the dots regarding what will be returned.
<script src="https://gist.github.com/kinlane/01b5805b9d2cf60e163f708b9a8e5916.js"></script>

<p><strong>Tags</strong> - Tag each path with a meaningful set of tags, describing what resources are available in short, concise terms and phrases.
<script src="https://gist.github.com/kinlane/13331609e54f0dc88383144f08b01f50.js"></script>

<p><strong>Contacts</strong> - Provide contact information for whoever can answer questions about an API, and provide a URL to any support resources.
<script src="https://gist.github.com/kinlane/0009c35551f94d9dead677b3555ee7ed.js"></script>

<p><strong>Create Security Definitions</strong> - Define the security for accessing the API, providing details on how each API request will be authenticated.
<script src="https://gist.github.com/kinlane/aa56655a10bda8ee26ced5f98434d4fd.js"></script>

<p><strong>Apply Security Definitions</strong> - Apply the security definition to each individual path, associating common security definitions across all paths.
<script src="https://gist.github.com/kinlane/2c99e9b3f8382d2ea60a8db838dfdad2.js"></script>

<p><strong>Complete(enough)</strong> - That should give us a complete (enough) API description.
<script src="https://gist.github.com/kinlane/061f8c83226027c98079aa5fe3857ff2.js"></script>

<p>Obviously there is more we can do to make an OpenAPI even more complete and precise as a business contract, hopefully speaking to both developers and business people. Having OpenAPI definitions are important, and having them be up to date, complete (enough), and useful is even more important. OpenAPIs provide much more than documentation for an API. They provide all the technical details an API consumer will need to successfully work with an API.

<p>While there are obvious payoffs for having an OpenAPI, like being able to publish documentation, and generate code libraries. There are many other uses for an OpenAPI like loading into <a href="https://www.postman.com/">Postman</a>, <a href="https://stoplight.io/">Stoplight</a>, and many other API services and tooling that helps developers understand what an API does, and reduce friction when they integrate, and have to maintain their applications. Having an OpenAPI available is becoming a default mode of operation, and something every API provider should have.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/10/09/the-layers-of-completeness-for-an-openapi-definition/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/10/09/a-quick-manual-way-to-create-an-openapi-from-a-get-api-request/">A Quick Manual Way To Create An OpenAPI From A GET API Request</a></h3>
        <span class="post-date">09 Oct 2018</span>
        <p>I have numerous tools that help me create OpenAPIs from the APIs I stumble across each day. Ideally I’m crawling, scraping, harvesting, and auto-generating OpenAPIs, but inevitably the process gets a little manual. To help reduce friction in these manual processes, I try to have a variety of services, tools, and scripts I can use to make my life easier, when it comes to create a machine readable definition of an API I am using–in this scenario it is <a href="https://www.xignite.com/product/market-data-alerts#/DeveloperResources/request">the xignite CloudAlerts API</a>.

<p align="center"><a href="https://www.xignite.com/product/market-data-alerts#/DeveloperResources/request"><img src="https://s3.amazonaws.com/kinlane-productions2/api-definition-stories/xignite-api-url.png" width="75%" align="center" /></a>

<p>One way I’ll create an OpenAPI from a simple GET API request, providing me with a machine readable definition of the surface area of that API, is using <a href="https://www.postman.com/">Postman</a>. When you have the URL copied onto your clipboard, open up your Postman, and paste the URL with all the query parameters present.

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions2/api-definition-stories/postman-paste-url-in-1.png" width="75%" align="center" />

<p>You’ll have to save your API request, and add it to a collection, but then you can choose to share the collection, and retrieve the URL to this specific requests Postman Collection.

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions2/api-definition-stories/postman-collection-share-2.png" width="75%" align="center" />

<p>This gives you a machine readable definition of the surface area of this particular API, defining the host, baseURL, path, and parameters, but it doesn’t give me more detail about the underlying schema being returned. To begin crafting the schema for the underlying definition of the API, and connect it to the response for my API definition, I’ll need an OpenAPI–which I can create from my Postman Collection using <a href="https://apimatic.io/transformer">API Transformer from APIMATIC</a>.

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions2/api-definition-stories/apimatic-api-transformer.png" width="75%" align="center" />

<p>After pasting the URL for the Postman Collection into the API transformer form, you can generate an OpenAPI from the definition. Now you have an OpenAPI, except it is missing the underlying schema, which I will just grab the response from my last request, and convert it into JSON schema using <a href="https://www.jsonschema.net/">JSONSchema.net</a>. I’ll just grab the properties section of these, as the bottom definitions portion of the OpenAPI specification is just JSON Schema.

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions2/api-definition-stories/jsonchema-net.png" width="75%" align="center" />

<p>I can merge my JSON schema with my OpenAPI, adding it to the definition collection at the bottom. With a little more love, adding a more coherent title, description, and fluffing up some of the summaries, descriptions, tags, etc., I now have a fairly robust profile of this particular API. Ideally, this is something the API provider would do, but in the absence of an OpenAPI or Postman Collection, this is a pretty quick and dirty way to produce an OpenAPI and Postman Collection from a simple GET API, but the formula works for other types of API requests as well–leaving me with a machine readable definition for an API I will be integrating with.

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions2/api-definition-stories/xignite-cloud-alerts.png" width="75%" align="center" />

<p>There are definitely other ways of scraping API documentation, processing .HAR files generated from a proxy, but I think this is a way that anyone, even a non-developer can accomplish. I did my version in JSON, but the same process will work for YAML, making the resulting definition a little more human readable, while still maintaining it’s machine readability. I like documenting these little processes so that my readers can put to use, but it also provides a nice definition that I can use to remember how I get things done–my memory isn’t what it used to be.

<p>The resulting API definitions from this process are:

<ul>
  <li><a href="https://gist.githubusercontent.com/kinlane/394bb4cc4a42f44911e229217370f903/raw/4815f58a219bd629977da07b2494c9b5655c4aa7/xignite-cloud-alerts.json">OpenAPI</a></li>
  <li>]Postman Collection](https://www.postman.com/collections/02a6658c25631d716407)</li>
</ul>


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/10/09/a-quick-manual-way-to-create-an-openapi-from-a-get-api-request/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/10/08/api-evangelist-and-streamdata-io-api-lifecycle-workshops/">API Evangelist And Streamdata.io API Lifecycle Workshops</a></h3>
        <span class="post-date">08 Oct 2018</span>
        <p><a href="http://locations.api.lifecycle.workshop.apievangelist.com/"><img src="https://s3.amazonaws.com/kinlane-productions2/workshops/kin-lane-api-days-spain.jpg" width="45%" align="right" style="padding: 15px;" /></a>
<p>I have been partnering with <a href="http://streamdata.io">Streamdata.io</a> to evolve how I work with enterprise groups on their API lifecycle strategy. After working closely with the Streadata.io sales team, it became clear that many enterprise organizations weren’t quite ready for the event-driven infrastructure Streamdata.io provides. Most groups were in desperate need of stepping back and developing their own formal strategy for delivering APIs across the enterprise, before they could every scale their operations and take advantage of things being more event-driven and real time.

<p>In response, I set out to evolve <a href="http://apievangelist.com/">my own API lifecycle research</a>, gathered over the last eight years of studying the API space, and make it more accessible to the enterprise, as self-service short form and long form content, in-person workshops, and forkable blueprints that any enterprise can set in motion on their own. The results is a series of evolvable API projects, that we are using to drive our ongoing workshop engagements with enterprise API groups, focusing in on six critical areas of the API lifecycle:

<ul>
  <li><strong>Discovery</strong> (<a href="http://locations.api.lifecycle.workshop.apievangelist.com/outline-items/#Discovery">demo</a>) - Knowing where all of your APIs and services are across groups.</li>
  <li><strong>Design</strong> (<a href="http://locations.api.lifecycle.workshop.apievangelist.com/outline-items/#Design">demo</a>) - Focus in on an a design and virtualized API lifecycle before deployment.</li>
  <li><strong>Development</strong> (<a href="http://locations.api.lifecycle.workshop.apievangelist.com/outline-items/#Development">demo</a>) - Understanding the many ways in which APIs can be developed &amp; deployed.</li>
  <li><strong>Production</strong> (<a href="http://locations.api.lifecycle.workshop.apievangelist.com/outline-items/#Production">demo</a>) - Thinking critically about properly operating API infrastructure.</li>
  <li><strong>Governance</strong> (<a href="http://locations.api.lifecycle.workshop.apievangelist.com/outline-items/#Governance">demo</a>) - Understanding how to measure, report, and evolve API operations.</li>
</ul>

<p>Not all of our workshops will cover all of these areas. Depending on the time we have available, the scope of the team participating in a workshop(s), and how far along teams are in their overall API journey, the workshops might head in different directions, and expand or contract the depth in which we dive into each of these area (ie. not everyone is ready for governance). After several workshops this year, we have found these areas of the API lifecycle to be the most meaningful ways to organize a workshop, and help enterprise group think more critically about their API strategy.

<p><strong>Craft An API Strategy For Your Enterprise</strong><br />
The purpose of our API lifecycle workshops is to help enterprise organizations develop a strategy. Bring in outside API knowledge, learn more about where an enterprise API group is in their overall API journey, and leave them with a structured artifact that helps them step back and look at the entire lifecycle of their APIs. Moving the API conversation across the enterprise forward in a meaningful way with three distinct actions:

<ul>
  <li><strong>Starter API Lifecycle Strategy</strong> - Provide a template API lifecycle strategy in GitHub or GitLab as README, and YAML file. Providing a framework to consider as you craft your own API strategy, providing a starting point for your journey. Generated from eight years of research on the API space, providing a living document that can be used to execute and evolve the overall API lifecycle strategy for an enterprise organization.</li>
  <li><strong>API Lifecycle Workshop</strong> - Schedule and conduct a single, or multi-day API lifecycle workshop on-site, with as many enterprise and / or partner stakeholders as possible. We will come on site, and walk teams through each stop along a modern API lifecycle, helping customize, personalize, and make the API strategy better fit the enterprises strategy.</li>
  <li><strong>Evolve API Lifecycle Strategy</strong> - Coming out of the workshop, you will be given an updated API lifecycle YAML document. Providing a human and machine readable framework that represents your API lifecycle strategy, helping provide a scaffolding for future discussions. Producing a usable artifact out of the gathering, encapsulating the research and experience we bring to the table, adding what we learned during the workshop, and hopefully continually being used to drive the API strategy road map.</li>
  <li><strong>Provide Execution &amp; Support</strong> - After the workshop is done, and we have provided an updated API lifecycle, we are still here to support. We can answer questions via the repository we leave an API strategy artifact, as well as email. We are happy to conduct virtual meetings to help check in on where you are at, and of course we are happy to always come back and conduct future workshops as you need.</li>
</ul>
<p><a href="http://locations.api.lifecycle.workshop.apievangelist.com/"><img src="https://s3.amazonaws.com/kinlane-productions2/workshops/43043431_10156747264069813_2487933138479611904_n.jpg" width="45%" align="right" style="padding: 15px;" /></a>
<p>We are happy to continue the conversation around the API lifecycle artifact we will leave with you. We don’t expect you to use everything we propose. We are more interested in teaching you about what is possible, and continuing to work with you to refine, evolve, and make the API lifecycle your own. We’ve just worked hard to identified many different ways to operating API infrastructure at scale, and continue to help standardize and make it more accessible by large enterprise organizations.

<p><strong>Helping You On Your Journey</strong><br />
The resulting API lifecycle strategy we leave behind after the workshop is done will possess all the knowledge we’ve aggregated across API research, gathered across leading API providers, and polished by conducting API workshops within the enterprise. Embedded within this API lifecycle artifact we’ll leave you with some added elements that will help you in your journey, going beyond just advice on process, and helping the rubber meet the road.

<ul>
  <li><strong>Links</strong> - Provide a wealth of references to external resources, attached to each stop along the API lifecycle, bring our API research into your organization, allowing you to put to use inline as you are building your API strategy.</li>
  <li><strong>Services</strong> - Embedding links to API services that you are already using, and introducing you to other useful API services along the way. Making sure specific services are associated with each stop along the API lifecycle, across the different areas, and even sub-linking to specific features that can help accomplish a specific aspect of API operations.</li>
  <li><strong>Tooling</strong> - Embedding links to open source API tools, specifications, and other solutions that can be used to accomplish a specific aspect of operating an API platform. Brining in open source solutions that can be considered as you are crafting the API strategy for your enterprise organization.</li>
</ul>

<p>While not all organizations will be ready to use a YAML API lifecycle artifact as part of their API orchestration, it helps to have the API lifecycle well defined, even if many steps are still manual. It helps teams think more critically about how they approach the deliver of APIs, while also being something that can be downloaded, forked, and reused by different groups across the enterprise. Eventually it is something that can be further automated, measured, and used to help quantify the maturity level of each APIs, as well as API across distributed teams.

<p><strong>Next Steps For Developing A Strategy</strong><br />
If you are interested in what we are offering with our API lifecycle workshops, there are few things you can do to get things started, to help us bring an API lifecycle workshop your way:

<ul>
  <li><a href="/cdn-cgi/l/email-protection#741d1a121b3415041d1102151a1311181d07005a171b19"><strong>Email Me</strong></a> - Happy to answer any questions, and get you the information you need to sell the workshop to your team, and get you in the calendar.</li>
  <li><a href="https://docs.google.com/forms/d/e/1FAIpQLSfhb9ToanbWHESr4e2_lP3TTvUIifL3YYP6mNy7yUGqS-3kpA/viewform?usp=sf_link">Take Survey</a> - Take a quick survey about your operations, helping us tailor a workshop for your needs.</li>
  <li><a href="http://locations.api.lifecycle.workshop.apievangelist.com/">Demo Workshop</a> - Take a stroll around one of our demos that we use in our workshop, introducing you to what we’ll be discussing.</li>
</ul>

<p>While I work from a common set of workshop material when designing these workshops, I work to tailor each engagement for the company, organization, institution, or government agency I’m working with. All of my API lifecycle workshop blueprints are meant to be forkable, customizable, and something anyone can turn into their own working API lifecycle strategy.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/10/08/api-evangelist-and-streamdata-io-api-lifecycle-workshops/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/09/28/api-developer-outreach-research-for-the-department-of-veterans-affairs/">API Developer Outreach Research For The Department of Veterans Affairs</a></h3>
        <span class="post-date">28 Sep 2018</span>
        <p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/109_214_800_500_0_max_0_1_-1.jpg" width="45%" align="right" style="padding: 15px;" />
<p><i>This is a write-up for research I conducted with my partner Skylight Digital. The team conducted a series of interviews with leading public and private sector API platforms regarding how they approached developer outreach, and then I wrote it up as a formal report, which the Skylight Digital team then edited and polished. We are looking to provide as much information as possible regarding how the VA, and other federal agencies should consider crafting their API outreach efforts.</i>

<p>This is <a href="https://skylight.digital/">Skylight’s</a> report for the U.S. Department of Veterans Affairs (VA) microconsulting project, “<a href="https://github.com/department-of-veterans-affairs/VA-Micropurchase-Repo/issues/26">Developer Outreach</a>.” The VA undertook this project in order to better understand how private- and public-sector organizations approach Application Programming Interface (API) developer outreach.

<p>In preparing this report, we drew on nearly a decade’s worth of our own API developer outreach expertise, as well as information obtained through interviews with seven different organizations. For each interview, we followed an interview script (see Appendix A) and took notes. The Centers for Medicare &amp; Medicaid Services (CMS) Blue Button API program (see Appendix B), the Census Bureau (see Appendix C), the OpenFEC program (see Appendix D), and Salesforce (see Appendix E) all agreed to releasing our notes publicly. The other three organizations (a large social networking site, a government digital services delivery group, and a cloud communications platform) preferred to keep them private.

<p>We structured this report to largely reflect the interview conversations that we held with people who are involved in developer outreach programs and activities. These conversations focused around the following questions:

<ol>
  <li>
    <p>What is the purpose and scope of your API developer outreach program?
  </li>
  <li>
    <p>What does success look like for you?
  </li>
  <li>
    <p>What are the key elements or practices (e.g, documentation, demos, webinars, conferences, blog posts) that you are using to drive and sustain effective adoption of your API(s)?
  </li>
  <li>
    <p>Do you make use of an API developer sandbox to drive and sustain adoption? If so, please describe how you’ve designed that environment to be useful and usable to developers.
  </li>
  <li>
    <p>What types of metrics do you use to measure adoption effectiveness and to inform future decisions about how you evolve your program?
  </li>
  <li>
    <p>How is your outreach program structured and staffed? How did it start out and evolve over time?
  </li>
  <li>
    <p>Imagine that you are charged with ensuring the complete failure of an API developer outreach program. What things would you do to ensure that failure?
  </li>
  <li>
    <p>What big ideas do you have for evolving your outreach program to make it even more effective?
  </li>
</ol>

<p>if we were forced to distill all of the interviews and all of our existing information down to a single essential piece of advice, it would be this: involve the programmers who are going to be using your APIs. By “involve,” we mean:

<ol>
  <li>
    <p>Engage them early.
  </li>
  <li>
    <p>Support the users with documentation, hackathons, forums, and other types of engagement activities.
  </li>
  <li>
    <p>Measure the happiness of the programmers who are using your APIs, as well as the number of times that they use them and how they use them.
  </li>
  <li>
    <p>Prioritize your APIs so as to maximize the utility to would-be programmers.
  </li>
</ol>

<p>In the pages below, you will find a large number of specific suggestions culled from extensive interviews and our collective personal experience. All of these specific techniques are in service to the idea of designing the API program with the programmers who will use the API in mind at all times.

<h2 id="purpose-and-scope-of-developer-outreach-programs">Purpose and scope of developer outreach programs</h2>

<h3 id="what-we-learned">What we learned</h3>

<p>Different API programs have different purposes, and these purposes are fulfilled by varying levels of API outreach resources. Just as organizations have invested differing amounts of resources into their web presence over the last 25 years, the API providers we talked to are each at different stages in their API journey. This variety presented us with a range of informative stories concerning outreach programs.

<h4 id="the-purpose-behind-apis">The purpose behind APIs</h4>

<p>Our interviews revealed a number of potential purposes for API development, a number of which are presented below:

<ul>
  <li>
    <p><strong>Build it and they will come:</strong> It is common for companies, organizations, institutions, and government agencies to launch an API effort with no specifically-stated purpose. They make resources available, build awareness amongst developers, and encourage innovation. While such organizations may not know what the future will hold, they believe that their investment in their API platform in a sensible and pragmatic way will attract developers and the inevitably innovative applications they will bring.
  </li>
  <li>
    <p><strong>A focus on APIs as a product:</strong> Some interviewees we spoke to are fully invested in their API efforts, treating their APIs as a product in and of themselves. They develop formal programs around their API operations, treat API consumers as end customers, and consistently ensure their programs have the resources they need. In short, they treat API operations like a business and run them as efficiently as possible, even if commercial sales is not the end goal.
  </li>
  <li>
    <p><strong>A focus on APIs as an enabler:</strong> Other interviewees we consulted focus on ensuring the availability of APIs as a means to support an existing web or mobile application; in a sense, the API is relegated to a secondary role relative to the primary application’s existence. APIs for these kinds of organizations serve to drive traffic, adoption, and integration when it comes to a larger vision, but the APIs themselves are simply enablers for the overall platform.
  </li>
  <li>
    <p><strong>A focus on the developer:</strong> Beyond the API as a product — and the web and mobile apps they power — API efforts tend to focus on the developer. Development-focused APIs emphasize what developers will bring to the table when they have the resources and support they need, and are thus central to the investment and engagement necessary for successful outreach/development.
  </li>
  <li>
    <p><strong>Attract unique entities:</strong> API platforms are often aimed at attracting the attention of interesting/progressive companies, universities, institutions, and other entities. These external entities can often put API resources to use in new and innovative ways, and in doing so, they can bring attention and potential partnerships to the platform.
  </li>
  <li>
    <p><strong>Leverage the network effect:</strong> Some API providers we interviewed expressed an interest in joining a wider network of collaborative open data and API efforts. They felt that working in concert with others, by bringing together many different federal, state, or municipal agencies, would benefit the platforms. Further, they felt this would allow the API initiatives to be led by either policy or private interests.
  </li>
  <li>
    <p><strong>Save developers time:</strong> Overall, the most positive motivation for API development expressed by existing providers was to streamline API onboarding and integration. Further development would help internal, partner, and 3rd party public developers be more successful in putting API resources to work in their web, mobile, and device applications.
  </li>
</ul>

<h4 id="the-scope-of-api-investment">The scope of API investment</h4>

<p>The programs we spoke to each had differing levels of investment, access to resources, and primary purposes. Since the scope of an API investment is naturally a function of these factors, this meant that the organizations we interviewed had different intended scopes for their API projects. We have collated a selection of these scopes below.

<ul>
  <li>
    <p><strong>Starting small:</strong> A common theme across API operations we spoke with was the importance of starting small and building a stable foundation before attempting larger infrastructure development. Beginning with a basic, yet valuable set of API resources, fleshing out the entire lifecycle, and processing around what is necessary to be successful before scaling and increasing the scope of API operations was routinely described as a critical part of any API scope.
  </li>
  <li>
    <p><strong>Invest as it makes sense:</strong> A lack of resources is a common element of slow growth and unrealized API operations. Bringing significant levels of investment to projects while simultaneously applying appropriate human and technological capital to API operations were universally mentioned by our interviewees as critical to seeing desired results.
  </li>
  <li>
    <p><strong>Working with what you have:</strong> Almost all the API programs we spoke to worked in resource-starved environments. They wished to do more and to be more, but lacked the time, human investment, and technical resources to make it happen. This forced the organizations to work with what they had, making the most of their limited opportunities while hoping for eventual greater contributions.
  </li>
  <li>
    <p><strong>A focus on improvement:</strong> All the API programs we interviewed expressed that they wanted to be doing more with their programs. They want to formalize their approach and increase their resource and labor investment in the areas in which they have seen success. Ultimately, their target scope was to focus not on just meeting expectations, but on moving towards excellence and mastery of what it takes to scale their API efforts.
  </li>
</ul>

<p>Every API effort clearly has its own personality, and while there are common patterns, the environment, team, and amount of resources available appears to dictate much of the scope of developer outreach programs. However, the scope of any effort will always start small and move forward incrementally as confidence grows.

<h3 id="what-our-thoughts-are">What our thoughts are</h3>

<p>We learned a lot talking to API providers about the purpose and scope of their API operations. Our interviews also reinforced much of what we know about the API operations of leading providers like Amazon and Google. When it comes to the motivations for developing APIs, ensuring an appropriate level of investment and planning for future scalable growth are necessary steps in giving an API the best chance to succeed.

<p>Augmenting what we learned from providers, we recommend focusing on the following areas in order to achieve API purposes, grow and scale API operations, and integrate API technology into the fabric of an organization’s overall operations.

<ul>
  <li>
    <p><strong>APIs are a journey:</strong> Always view API operations as a journey and not a destination, setting expectations appropriately from the beginning. Do not raise the bar too high when it comes to achieving goals, reaching metrics, and getting the job done. API operations will always be an ongoing effort, with both wins and losses along the way.
  </li>
  <li>
    <p><strong>Always start small:</strong> Reiterating what we researched in the API sector as well as what we confirmed in our interviews, it is important to build a small, stable base before moving forward. This does not mean you cannot develop rapidly, but before you do, make sure you’ve researched the API and planned for its success, understanding what will be needed throughout the lifecycle of all APIs you intend to deliver.
  </li>
  <li>
    <p><strong>Center on the end user:</strong> It is always important that every goal and purpose you set for your API platform center on its end-users. While the platform and developer ecosystem is always a consideration, the end-user is the central focus of the “why” and the “how” for API technologies, especially consider the scope of operating an API in both the private and the public sector.
  </li>
  <li>
    <p><strong>A dedicated team:</strong> Even if it is a small group, always work to dedicate a team to API operations. While APIs will ultimately be an organization-wide effort, it will take a dedicated team to truly lead the API to success. Without centralized, dedicated leadership, APIs will never attain true relevance within an organization, leaving to be a side project that rarely delivers as expected.
  </li>
  <li>
    <p><strong>Everyone pitches in:</strong> While a dedicated team is a requirement, an API’s development should always invite individuals from across an organization to join in the process. Ideally, API operations are a group effort led by a central team. It is important to encourage individual API teams to feel a sense of ownership over their API, just as it is important to encourage business users to participate in development conversations.
  </li>
  <li>
    <p><strong>Striving for excellence:</strong> API operations will always be forced to deal with a lack of resources. That is simply a fact of dealing in APIs. However, each API program should be seeking excellence, working to understand what is needed to move APIs forward in a healthy way. Improving upon what has already been done, refining processes that are in motion, and making sure all APIs are iteratively improved continually benefits a platform’s end users.
  </li>
  <li>
    <p><strong>Continued investment:</strong> Always be regularly investing in the API platform. Continued input of both human resources and financial resources helps to ensure that a platform is not starved along the way. Otherwise the API-in-question will constantly fall short and threaten the viability of the platform in the long term.
  </li>
</ul>

<p>While the purpose of an API may depend on the mission of its developing organization, in the end, APIs always exist to serve end-users while protecting the interest of a platform. The scope of any such API will depend on the commitment of an organization, and as such, there is no “right answer” to the question of determining the purpose of any single API platform. The only true constraint is the assurance that the API remains in alignment with an organization’s mission as it grows and scales.

<h2 id="how-success-is-defined">How success is defined</h2>

<p>With a variety of possible purposes, scopes, and approaches, an API’s success can be defined in a myriad of ways. Depending on the motivations behind the API, and the investment made, the measure of success will depend on what matters to the organization. However, there are some common patterns to defining success, which we extracted from both the interviews we conducted and the research we performed as part of this outreach study.

<h3 id="what-we-learned-1">What we learned</h3>

<p>Along with what we learned about the purpose and scope of API platforms, we discovered more about the different ways in which API providers are defining success. We have collected the highlights among these metrics below.

<ul>
  <li>
    <p><strong>Building awareness:</strong> API success revolves around building awareness that an API exists, as well as awareness of the value of the API resources that are being made available. Awareness is not simply a consumer side consideration, though; providers, too, must possess an awareness of the value of their resources in relation to both other API developers and consumers alike.
  </li>
  <li>
    <p><strong>Attracting new users:</strong> Bringing attention to an API and attracting new users is one of the most common ways of measuring the success of API operations. While new users won’t always immediately become active users, their interest and involvement will bring attention to and awareness of what the API platform can deliver. Attracting new users is one of the easiest and most accessible ways to measure the success of any API, according to our interviews, but importantly, none of the platform providers we spoke to recommended that an organization should stop there.
  </li>
  <li>
    <p><strong>Incentivizing active users:</strong> While attracting new users is easy, producing active users is much harder. The easier it is to onboard with an API and make the first series of API calls, the greater the likelihood that API consumers will integrate the platform into their own resources and work, which is a critical metric for any API provider.
  </li>
  <li>
    <p><strong>Applications:</strong> Applications and development are the cornerstones that incentivize API providers to invest in APIs, and across the board, our interviewees cited application integration and involvement as a prime candidate for determining an API’s success. This could be quantified both in terms of new applications relying on the API platform, as well as active application processes that integrate with the API’s platform. In either case, measuring usage was considered an excellent means to justify the existence and growth of an API platform.
  </li>
  <li>
    <p><strong>Entities:</strong> Getting the attention of companies, organizations, institutions, and other government agencies is an important part of the the API journey. In particular, developing an awareness of and encouraging the usage and adoption of APIs among groups already leveraging the technology is an important metric by which success can be determined.
  </li>
  <li>
    <p><strong>End users:</strong> Of course, API providers articulated the importance of serving end-users. Besides serving an organization’s mission, the true purpose of an API is to satisfy an end-user, and so measuring success based upon how much value is created and delivered to these users and customers, and even the public at large, can directly verify that an API is living up to its billing.
  </li>
  <li>
    <p><strong>Stakeholders:</strong> Further discussions with API providers implied that success is also defined in terms of involvement with other stakeholders. Ensuring that the definition of success was crafted in an inclusive way allows everyone involved and impacted by the project to input their voice. This widens the target audience to make sure success is a large umbrella that covers as many individuals within an organization as possible.
  </li>
  <li>
    <p><strong>New resources:</strong> An additional area that was used to define success was the number of new API resources added to a platform. If an organization is currently in the development phase and deploying APIs into production, it is likely that a platform already has a handle on what it takes to successfully deliver APIs throughout their lifecycle. Making new APIs a great way to understand the velocity of any platform, and how well it is ultimately doing.
  </li>
</ul>

<p>Measuring the success of an API platform is a much more ambiguous goal than determining scope, purpose, and investment. Our interviews revealed that success often means different things to different providers. Moreover, an organization’s understanding of success is also something that will evolve over time. We learned a lot from API providers about pragmatic views on what API success looks like, and now we would like to translate that into some basic guidance on how to help ensure the success of providing APIs.

<h3 id="what-our-thoughts-are-1">What our thoughts are</h3>

<p>Defining, measuring, and quantifying the success of API operations is not easy. As discussed above, measuring success functionally amounts to hitting a moving target. It is important to start with the basics, be realistic, and define success in a meaningful way. Adding to what has been gathered from interviews with API providers, we recommend a consideration of the following factors when it comes to defining just exactly what success is.

<ul>
  <li>
    <p><strong>Know your resources:</strong> Understand the resources you are making available via an API. Ensure that they are well defined and made accessible in ways that consider security, privacy, and the interests of all stakeholders. Do not just open up APIs for the sake of delivering APIs — make sure the resources are well defined, designed, and serve a purpose.
  </li>
  <li>
    <p><strong>Manage your APIs:</strong> API management is essential to measuring success. It is extremely difficult to define success without requiring all developers to authenticate, log, quantify, and analyze their consumption. Measuring these kinds of consumption activities helps to quantify the value produced by the API and its related platform, and an understanding of this value serves as the foundation for any API’s future success.
  </li>
  <li>
    <p><strong>Have a plan:</strong> There is no success without a plan. A set of plans are required to apply at the management level in order to quantify the addition of new accounts, define whether they are active or not, and understand how applications are putting resources to work. Providing a plan for how resources are made available, and how they are consumed, generates a framework to think about and measure what API success means.
  </li>
  <li>
    <p><strong>Measure portal activity:</strong> Treat your API developer portals as you would any other web property and actively measure its traffic. Apply data-analytic solutions to track sessions, time, and visitors, and use this information to contribute to a sales and marketing funnel that can be used to understand how developers are using portal resources. Importantly, this kind of analysis can also discover points of friction that developers may be encountering when trying to use your API platform.
  </li>
  <li>
    <p><strong>Analyze and report:</strong> Produce weekly, monthly, quarterly, and annual reports from data gathered across the API stack, API portal, and from social media. Developing an understanding of what is happening based upon actual data, and consistently reporting upon findings with all stakeholders in API operations, ensures both transparency of API knowledge and information access to formulate plans for growth.
  </li>
  <li>
    <p><strong>Discuss and evangelize:</strong> Have a strategy for taking any analysis or reporting from API operations and disseminating it amongst stakeholders. With these resources distributed, consider conducting regular on- and off-line discussions around what they mean. Work with everyone involved to understand the activity across a platform, and use these discussions to transform the understanding of success as platform awareness grows.
  </li>
  <li>
    <p><strong>Make things observable:</strong> Make every building block of API operations observable. Ensure that everything has well defined inputs and outputs, and consider how these can be used to better understand whether the platform is working or not. Allowing every single aspect of the platform to be able to contribute to an overall definition of what success means by providing real-time and historic data around how resources are being used can signal important insights about an API and how it might be improved.
  </li>
</ul>

<p>The success of an API platform will mean different things to different groups and will evolve over time as awareness around an organization’s resources grows. Know your resources, properly manage your APIs, and have a plan, but make sure you are constantly reassessing exactly what success means while having ongoing conversations with stakeholders. With more experience, you will find that API platform success becomes much more nuanced, but importantly, it also becomes easier to define once you know what it is that you want.

<h2 id="key-practices-for-driving-and-sustaining-adoption-of-apis">Key practices for driving and sustaining adoption of APIs</h2>

<p>After a decade of leading tech companies operating API programs, and a little over five years of government agencies following their lead, a number of common practices emerged that helped drive the adoption of APIs and support relationships between provider and consumer. We spent some time talking to API providers about their approaches, while also bringing our existing research and experience to the table, and have collected our responses and analysis below.

<h3 id="what-we-learned-2">What we learned</h3>

<p>This is one area where we believe that our existing research outweighed what we learned in talking to API providers, but the conversations did reinforce what we know while also illuminating some new ways to look at operational components. Here are the key practices our interviewees provided for driving and sustaining the adoption of APIs.

<ul>
  <li>
    <p><strong>Documentation:</strong> Documentation is the single most important element that needs to accompany an API that is being made available. This transforms the process of learning about what an API can do from static to interactive (such as by using OpenAPI specifications) and renders the API a hands-on experience.
  </li>
  <li>
    <p><strong>Code:</strong> Providing samples, SDKs, start solutions, and other code elements is vital to making sure developers understand through demonstration how to integrate with APIs in a variety of programming languages.
  </li>
  <li>
    <p><strong>Content:</strong> Content is critical. Invest in blog posts, samples, tutorials, case studies, and anything else you think will assist your consumers in their journey. We heard over and over how important a regular stream of content is for attracting new developers, keeping active ones engaged, and putting API resources to work.
  </li>
  <li>
    <p><strong>Forums:</strong> Provide a forum where developers can find existing answers to their questions while also being able to ask new questions. Offering a safe, up to date, well moderated place to engage in asynchronous conversations around an API platform ensures that dialogue is always happening, which means that use and progress are in continued development.
  </li>
  <li>
    <p><strong>Conferences:</strong> Conducting workshops and attending relevant conferences where potential API consumers will be is an important practice in furthering the outreach of an API platform. Engage with your community — both consumers and developers — instead of just pushing content to them online.
  </li>
  <li>
    <p><strong>Proactive:</strong> Make sure you are proactive in operating your API platform by constantly marketing your work to developers (remember, continually attracting new attention is vital). At the same time, work to provide existing developers with what they will need based upon common practices across the API sector. Investing in developers by giving them resources they will need before they have to ask for it makes an API’s community feel alive and healthy.
  </li>
  <li>
    <p><strong>Reactive:</strong> While proactivity is important, an API team must also be able to react to any questions, feedback, and concerns submitted by API consumers and other stakeholders. Ensuring people do not have to wait very long for an answer to their question makes consumers, developers, and stakeholders alike feel like they are considered a relevant and important part of the API community.
  </li>
  <li>
    <p><strong>Feedback loops:</strong> Having feedback loops in place are essential to driving and sustaining the adoption of APIs. Without one or more channels for consumers to provide feedback, as well as responsive and attentive API providers who analyze how the feedback can fit into the overall API plan, API operations will never quite rise to the occasion.
  </li>
  <li>
    <p><strong>Management:</strong> Almost all API providers we talked to articulated that having a proper strategy for API management, as well as an investment in services and tooling, was essential to onboarding new consumers. Additionally, this kind of investment facilitates an understanding of how to engage with and incentivize the usage of API resources by existing users. Without the ability to authenticate, define access tiers, quantify application usage, log all activity, and report upon usage and activity across dimensions, it is extremely difficult to scale platform adoption.
  </li>
  <li>
    <p><strong>Webinars:</strong> For an enterprise audience, webinars were a viable way to educate new users about what an API platform offers, as well as helping to bring existing API consumers up to speed on new features. Not all communities are well-suited for webinar attendance, but for those that are, it is a valuable tool in the API toolbox.
  </li>
  <li>
    <p><strong>Tutorials:</strong> Providing detailed tutorials on how to use an API, understand business logic, and take better advantage of platform resources were all common elements of existing API provider options. Breaking down the features of the platform and providing simple walkthroughs that help consumers put those features to work can streamline the integration and onboarding process that users face when working with APIs.
  </li>
  <li>
    <p><strong>Domain:</strong> Our interviewees mentioned that having a dedicated domain or subdomain for an API developer portal significantly helped in attracting new users by providing a known location for existing developers to find the resources they are looking for.
  </li>
  <li>
    <p><strong>Explorer:</strong> In some cases, it is important to provide a more hands-on, visual way to explore resources available within an API rather than simply listing or describing such features in documentation. For new and particularly inexperienced users of API technologies, resources that can “connect the dots” between the API’s functional support and the actual implemented pathway of using a particular API tool can be immeasurably important in user retention.
  </li>
</ul>

<p>We learned that many API providers in the public sector are actively learning from API providers in the private sector. They employ many of the same elements used by leading API providers who have been doing it for a while now. However, we also found evidence of innovation by some of the public sector API providers we interviewed, especially in the realm of onboarding and retaining new users.

<h3 id="what-our-thoughts-are-2">What our thoughts are</h3>

<p>Below, we have constructed a list of common building blocks that should be considered when developing, operating, and evolving any API platform. These recommendations are the results of formalizing what we learned as part of the interview process, as well as leveraging eight years worth of research. Our objective is to give API providers the elements they need to attract and engage with new users, while also pushing existing users to be more active. We have broken down our recommendations into eleven separate areas, which are further discussed below.

<h4 id="portal">Portal</h4>

<p>It is important to provide a single known location where API providers and consumers can work together to integrate the offered resources into a variety of web, mobile, device, and network applications, as well as directly into other systems. Several components play into the successful adoption and consumption of APIs published to a single portal.

<ul>
  <li>
    <p><strong>Overview:</strong> A simple overview explaining what a platform does and clearly defining the value the API offers to consumers.
  </li>
  <li>
    <p><strong>Getting started:</strong> A simple series of steps that help onboard a new user so that they can begin putting API resources to work.
  </li>
  <li>
    <p><strong>Documentation:</strong> Interactive documentation for all APIs and schema (preferably created in OpenAPI or another interactive API specification format).
  </li>
  <li>
    <p><strong>Errors:</strong> A simple, clear list of all the possible errors an API consumer will encounter, starting with HTTP status codes and then proceeding to any specialized schema used to articulate when API errors occur.
  </li>
  <li>
    <p><strong>Explorer:</strong> A visual representation of the resources available within the API that allows consumers to search, browse, and explore all available resources without needing to know or write code. Note: it is always helpful to provide a direct link to replicate a search using the API.
  </li>
</ul>

<p>These elements set the foundation for any API operations, providing the basic elements that will be needed to onboard with an API. They establish an interface for the other features that will be needed to incentivize deep and sustaining integrations with any platform.

<h4 id="definitions">Definitions</h4>

<p>Besides the basic functionality described above, industries have turned toward a suite of machine-readable definitions to drive API integrations. Due to the ubiquity of a number of these definitions, we have collected a handful of specification formats that we recommend making a part of the base of any API operations.

<ul>
  <li>
    <p><strong>OpenAPI:</strong> An interactive documentation standard that describes the functionality of an API in a machine-readable way.
  </li>
  <li>
    <p><strong>Postman:</strong> A standard collection that provides a transactional, runtime-oriented definition of the feature interface of an API for use in client tooling.
  </li>
  <li>
    <p><strong>JSON Schema:</strong> A widely used specification that describes the objects, parameters, and other structural elements of the consumption of API resources.
  </li>
  <li>
    <p><strong>APIs.json:</strong> A discovery document that provides a machine-readable index of API operations with references to the portal, documentation, OpenAPI, Postman, and other building blocks of an API platform.
  </li>
</ul>

<h4 id="code">Code</h4>

<p>It is common practice for API providers to invest in a variety of code-focused resources to help jumpstart the onboarding process for API developers. This reduces the number of technical steps necessary for the technology to successfully integrate with other platforms. Here are the building blocks we recommend considering when crafting the code portion of any developer outreach strategy.

<ul>
  <li>
    <p><strong>Github/Gitlab:</strong> Use a social coding platform to manage many of the technical components used to support API developers.
  </li>
  <li>
    <p><strong>Samples:</strong> Publish simple examples of making individual API calls in a variety of programming languages.
  </li>
  <li>
    <p><strong>SDKs:</strong> Provide more comprehensive software development kits in a variety of programming languages for developers to use when integrating with API resources.
  </li>
  <li>
    <p><strong>PDKs:</strong> Provide platform development kits that help developers integrate with existing solutions they may already be using as part of their operations.
  </li>
  <li>
    <p><strong>MDKs:</strong> Provide mobile development kits that help jumpstart the development of mobile applications that take advantage of a platform APIs.
  </li>
  <li>
    <p><strong>Starters:</strong> Publish complete applications that provide starter kits that developers can use to jumpstart their API integrations.
  </li>
  <li>
    <p><strong>Embeddables:</strong> Provide buttons, badges, widgets, and bookmarklets for any API consumer to use when quickly integrating with API resources.
  </li>
  <li>
    <p><strong>Spreadsheets:</strong> Offer spreadsheet connectors that allow API consumers to use platform APIs within Microsoft Excel and Google Sheets.
  </li>
  <li>
    <p><strong>Integrations:</strong> Invest in a suite of existing integrations with other platforms that API consumers can take advantage of, providing low-code or no-code solutions for integrating with APIs.
  </li>
</ul>

<p>While we have presented a variety of code-related resources, we want to point out the caveat that these tools should only be employed if an organization possesses the resources to properly maintain and support them. These elements can provide some extremely valuable coding solutions for developers and consumers to put to work, but if not properly done, they can also quickly become a liability, driving developers away.

<h4 id="event-driven">Event-driven</h4>

<p>In addition to simpler request-and-response delivery and documentation methods, we also recommend thinking about the following event-driven possibilities, which can also be used to incentivize deeper engagement and workflow with an API.

<ul>
  <li>
    <p><strong>Webhooks:</strong> These can provide ping and data push opportunities, which allow API consumers to be notified when any event occurs across an API platform.
  </li>
  <li>
    <p><strong>Streams:</strong> Providing high-level or individual streams of data allow for long-running HTTP or TCP connections with API resources.
  </li>
  <li>
    <p><strong>Event types:</strong> In many cases, it is helpful to publish a list of the types of possible API events, as well as opportunities for subscribing to webhook or streaming channels.
  </li>
  <li>
    <p><strong>Topics:</strong> Similarly, developers and consumers alike may find a published list of platform-related topics useful, particularly one that allows API consumers to search, browse, and discovery exactly the topical channels they are interested in.
  </li>
</ul>

<p>These event-based tools help augment existing APIs and make them more usable by API consumers at scale. They facilitate a meaningful application experience for end-users, allowing them to stay tuned to specific topics. This in turn fine-tunes the experience for developers,which further drives adoption, ultimately establishing more loyal consumers at the API integration and application user levels.

<h4 id="management">Management</h4>

<p>One of the cornerstones for defining, quantifying, and delivering successful API onboarding and engagement is API management. The following list contains some core elements of API management that should be considered as any API provider is planning, executing, and evolving their operational strategy.

<ul>
  <li>
    <p><strong>Authentication:</strong> Providing clear options for onboarding using Basic Auth, API Keys, JWT, or OAuth keeps things standardized, well-explained, and frictionless to implement.
  </li>
  <li>
    <p><strong>Plans/tiers:</strong> Establishing well-defined tiers of API consumers in terms of how they access all available API resources can inform the provision of structured plans that define how an API’s resources are being utilized.
  </li>
  <li>
    <p><strong>Applications:</strong> Individual applications should be at the center of consumer API engagement. In particular, applications that help onboard new users so that they can begin consuming API resources are imperative.
  </li>
  <li>
    <p><strong>Usage reporting:</strong> Tools and metrics that provide real-time and historical data, as well as access to multi-dimensional reporting across all API consumers is useful in analyzing the API’s usage and performance. This information can be helpful to developers in defining the stage of their API journey, as well as any additional resources they might wish to consider.
  </li>
</ul>

<p>There are many other aspects of API management, but these building blocks reflect the consumer-facing elements that help onboard new users and drive increased engagement with existing consumers. API management is an area in which API providers should not be reinventing proven methods that already work: the best practices established over the last decade by leading API providers already account for strong engagement and retention levels for users and service providers.

<h4 id="communications">Communications</h4>

<p>Engagement is important for consumers not only with the tools of the API, but the communications and news surrounding the API. Streams of information across multiple channels can help unite a communications strategy for any API platform. We have collected the best examples of such information feeds below.

<ul>
  <li>
    <p><strong>Blog:</strong> An active blog with an Atom feed, one for each individual API and/or overall platform.
  </li>
  <li>
    <p><strong>Twitter:</strong> A dedicated Twitter account for the entire API platform, providing updates and support.
  </li>
  <li>
    <p><strong>GitHub:</strong> A GitHub organization dedicated to the platform, with accounts for each API team member. The organization leverages the platform for content as well as code management.
  </li>
  <li>
    <p><strong>Reddit:</strong> A helpful forum for answering questions, sharing content, and engaging with consumers on the social bookmarking platform.
  </li>
  <li>
    <p><strong>Hacker News:</strong> Another helpful discussion board for answering questions, sharing content, and engaging with consumers.
  </li>
  <li>
    <p><strong>LinkedIn:</strong> A business social network enterprise devoted to engaging with consumers. An established LinkedIn page for the platform can be useful for regularly publishing content, as well as engaging in conversations via the platform.
  </li>
  <li>
    <p><strong>Facebook:</strong> Similar to LinkedIn, a Facebook page for the platform is helpful in engaging with API consumers via their social media presence. It can be used to regularly publishing content and engage in network-broadcast conversations via social platforms.
  </li>
  <li>
    <p><strong>Press:</strong> A platform section detailing the latest releases, as well as a feed that users can subscribe to in order to receive a regular stream of news on the platform.
  </li>
</ul>

<p>A coherent communication, content, and social media strategy will be the number one driver of new users to the platform while also keeping existing developers engaged. These communication building blocks provide a regular stream of information and signals that API consumers can use to stay informed and engaged while putting API resources to use in their applications.

<h4 id="direct-support">Direct support</h4>

<p>Besides communication, direct support channels are essential to completing the feedback loop for the platform. There are a handful of common channels API providers use to provide direct support to API consumers, allowing them to receive support from platform operations. We recommend the following selections.

<ul>
  <li>
    <p><strong>Email:</strong> Establish a single, shared email address for the platform in which all platform support team can provide assistance.
  </li>
  <li>
    <p><strong>Twitter:</strong> Provide support via Twitter, pushing it beyond just a communication channel and making it so that API consumers can directly interact with the platform team.
  </li>
  <li>
    <p><strong>GitHub:</strong> Do not just use GitHub for code or content management: leverage individual team member accounts to actively support using GitHub issues, wikis, and other channels the social coding platform provides.
  </li>
  <li>
    <p><strong>Office hours:</strong> Provide regular office hours where API consumers can join a hangout or other virtual group chat application in order to have their questions answered.
  </li>
  <li>
    <p><strong>Webinars:</strong> Provide regular webinars around platform specific topics, allowing API consumers to engage with team members via a virtual platform that can be recorded and used for in-direct, more asynchronous support in addition to live feedback.
  </li>
  <li>
    <p><strong>Paid:</strong> Provide an avenue for API consumers to pay for premium support and receive prioritization when it comes to having their questions answered.
  </li>
</ul>

<p>With a small team, it can be difficult to scale direct support channels properly. It makes sense to activate and support only the channels you know you can handle until there are more resources available to expand to new areas. Ensuring that all direct support channels are reactive in terms of communication will help deliver value by bringing the feedback loop back full circle.

<h4 id="indirect-support">Indirect support</h4>

<p>After direct support options, there should always be indirect/self-support options available to help answer API consumers’ questions. Such options allow users to get support on their own while still leveraging the community effect that exists on a platform. There are a handful of proven indirect support channels that work well for public as well as private API programs.

<ul>
  <li>
    <p><strong>Forums:</strong> Provide a localized, SaaS, or wider community forum for API consumers to have their questions answered by the platform or by other users within the ecosystem.
  </li>
  <li>
    <p><strong>FAQ:</strong> Publish a list of common questions broken down by category, allowing API consumers to quickly find the most common questions that get asked. Regularly update the FAQ listing based on questions gathered using the platform feedback loop(s).
  </li>
  <li>
    <p><strong>Stack Overflow:</strong> Leverage the question and answer site Stack Overflow to respond to inquiries and allow API consumers to publish their questions, as well as answers to questions posed by other members of the community network.
  </li>
</ul>

<p>Indirect, self-service support will be essential to scaling API operations and allowing the API team to do more with less. Outsourcing, automating, and standardizing how support is offered through the platform can make API support available 24 hours per day, turning it into an always-available option for motivated developers to find the answers they are looking for.

<h4 id="resources">Resources</h4>

<p>Beyond the documentation and communication, it is helpful to provide other resources to assist API consumers in onboarding and to strengthen their understanding of what is offered via the platform. There are a handful of common resources API providers make available to their consumers, helping to bring attention to the platform and drive usage and adoption of APIs.

<ul>
  <li>
    <p><strong>Guides:</strong> Providing step by step guides covering the entire platform, or individual sections of the platform, helps consumers understand how to use the API to solve common challenges they face.
  </li>
  <li>
    <p><strong>Case studies:</strong> Examples such as real-world case studies of how companies, organizations, institutions, and other government agencies have put APIs to work in their web, mobile, device, and network applications can help demonstrate the variety of functions that an API platform can perform.
  </li>
  <li>
    <p><strong>Videos:</strong> Make video content available on YouTube, and other platforms. Providing video walkthroughs of how the APIs work and the best way to integrate features into existing applications can demystify the process of onboarding with API technologies.
  </li>
  <li>
    <p><strong>Webinars:</strong> While webinars can be a helpful source of information to API consumers trying to understand specific concepts, maintaining and publishing an archive of webinars can serve as a historic catalog of such searches, which can provide targeted Q&amp;A for how to put API platforms to work.
  </li>
  <li>
    <p><strong>Presentations:</strong> Provide access to all the presentations that have been used in talks about the platform, allowing consumers to search, browse, and learn from presentations that have been given at past conferences, meetups, and other gatherings.
  </li>
  <li>
    <p><strong>Training:</strong> It can be immensely helpful to invest in formal curriculum and training materials to help educate API consumers about what the platform does. This provides a structured approach to learn more about the APIs and gives developers access to comprehensive training materials users can tap into on their own schedule.
  </li>
</ul>

<p>Like other areas of this recommendation, these resource areas should only be invested in if an organization has the resources available to develop, deliver, and maintain them over time. Providing additional resources like guides, case studies, presentations, and other materials help further extend the reach of the API platform, allowing the API team behind operations do more with less, as well as reach more consumers with well constructed, self-service resources that are easy to discover.

<h4 id="observability">Observability</h4>

<p>One important attribute of API platforms that successfully balance attracting new users and creating long term relationships is platform observability. Being able to understand the overall health, availability, and reliability of an API platform allows API consumers to stay informed regarding the services they are incorporating into their applications. There are several key areas that contribute to the observability of an API platform.

<ul>
  <li>
    <p><strong>Roadmap:</strong> A simple list of what is being planned for the future of a platform, one that provides as much detail and ranges as far into the future as possible.
  </li>
  <li>
    <p><strong>Issues:</strong> A document of any open issues that exist, allowing API consumers to quickly understand if there are any open issues that might impact their applications.
  </li>
  <li>
    <p><strong>Status:</strong> A dashboard that describes the health of the overall platform, as well as the status of each individual API being made available via the platform.
  </li>
  <li>
    <p><strong>Change log:</strong> A simple list of what has changed on a platform, taking the roadmap and issues that have been satisfied and rolling it into a historical registry that consumers can use to understand what has occurred.
  </li>
  <li>
    <p><strong>Security:</strong> Share information about platform security practices and the strategies used to secure platform resources, and share the expectations held of developers when it comes to application security practices.
  </li>
  <li>
    <p><strong>Breaches:</strong> Be proactive and communicative around any breaches that occur, providing immediate notification of the breach and a common place to find information regarding current and historic breaches on the platform.
  </li>
</ul>

<p>Observability helps build trust with API consumers. In order to develop this trust, platform providers have to invest in APIs in ways that make consumers feel like the platform is stable and reliable. The less transparent that the elements of the platform are, the less likely that API consumers are going to expand and increase their usage of services.

<h4 id="real-world-presence">Real-world presence</h4>

<p>The final set of recommendations centers on maintaining a real-world presence for the platform. It is important to ensure that the platform does not just have a wide online presence, but is also engaging with API consumers in a face-to-face capacity. There are a handful of ways that leading API providers get their platforms face-time with their community.

<ul>
  <li>
    <p><strong>Meetups:</strong> Speaking at and attending meetup events in relevant markets.
  </li>
  <li>
    <p><strong>Hackathons:</strong> Throwing, participating in, and attending hackathon events.
  </li>
  <li>
    <p><strong>Conferences:</strong> Speaking, exhibiting, and attending conferences in relevant areas.
  </li>
  <li>
    <p><strong>Workshops:</strong> Conducting workshops within the enterprise, with partners, and the public.
  </li>
</ul>

<p>These four areas help extend and strengthen the relationship between the API platform provider and consumers.

<h2 id="how-api-developer-sandboxes-are-used-to-drive-adoption">How API developer sandboxes are used to drive adoption</h2>

<p>One of the more interesting and forward-thinking aspects of this research is around the delivery of sandbox development, labs, and virtualized environments. Providing a non-production area of a platform where developers can play with API resources in a much safer environment than a live production area can encourage creativity and innovation as well as exploration of the API’s resources.

<h3 id="what-we-learned-3">What we learned</h3>

<p>Some of the API providers we interviewed for this proposal had sandbox environments. Their insights into the merits of these environments provided us with some ideas to reduce friction for new developers when onboarding, as well as to help certify applications as they mature with their integrations. Here is what we learned about sandbox environments from the API providers we talked to.

<ul>
  <li>
    <p><strong>Sandboxes are used:</strong> Sandbox environments are used, and they do provide value to the API integration process, making them something all API providers should be considering.
  </li>
  <li>
    <p><strong>Sandboxes are not default:</strong> Sandboxes are not a default feature of all APIs but have become more critical when PII (personally identifying information) and other sensitive resources are available.
  </li>
  <li>
    <p><strong>Data is virtualized:</strong> It was enlightening to see how many of the API providers we talked to provided virtualized data sets and complete database downloads to help drive the sandbox experience.
  </li>
  <li>
    <p><strong>Doing sandboxes well is difficult:</strong> We learned that providing sandbox environments that reflect the production environment is, quite simply, hard. It takes significant investment and support to make it something that will work for developers.
  </li>
  <li>
    <p><strong>Safe onboarding:</strong> Sandbox environments allow for the safe onboarding of developers and their applications. This helps ensure that only high-quality applications enter into a production environment, which protects the interests of the platform as well as the security and privacy of end-users.
  </li>
  <li>
    <p><strong>Integrated with training:</strong> We learned how sandbox environments should also be integrated with other content and training materials. This facilitates access for API consumers to test out the training materials they need while also directly learning about the API.
  </li>
  <li>
    <p><strong>Leverage API management:</strong> It was interesting to learn the role that API management plays in being able to deliver both sandbox and production environments. API gateways and management solutions are used to help mock and deliver virtualized solutions, but also to manage the transition of applications from development to a production environment.
  </li>
</ul>

<p>Talking to API providers, it was clear that sandboxes provided value to their operations. It was also clear that they aren’t essential for every API implementation and took a considerable investment to do right. API virtualization is a valuable tool when it comes to engaging with API consumers, and it is something that should be considered by most API providers, but it should be approached pragmatically and realistically, with an awareness of both the costs as well as the benefits of moving from transition to production environments.

<h3 id="what-our-thoughts-are-3">What our thoughts are</h3>

<p>Sandboxes, labs, and virtualized environments are commonplace across the API sector but are not as ubiquitous they should be. We commend the presence of virtualized building blocks for any API that is dealing with sensitive data. A sandbox should be a default aspect of all API platforms, but should be especially applied to help ensure quality control as part of the API journey from development to production. Here are some of the building blocks we recommend when looking at structuring a sandbox environment for a platform.

<ul>
  <li>
    <p><strong>Virtualize APIs:</strong> Provide virtualized instance of APIs and a sandbox catalog of API resources.
  </li>
  <li>
    <p><strong>Virtualize data:</strong> Provide virtualized and synthesized data along with APIs in order to create as realistic an experience as possible.
  </li>
  <li>
    <p><strong>Virtualized instances:</strong> Consider the availability of virtualized instances of an API as computable instances on major cloud platforms, allowing for the deployment of sandboxes anywhere.
  </li>
  <li>
    <p><strong>Virtualized containers:</strong> Consider the availability of virtualized instances of an API as containers, allowing API sandboxes to be created locally, in the cloud, and anywhere containers run.
  </li>
  <li>
    <p><strong>Bake into onboarding:</strong> Make a sandbox environment a default requirement in the API onboarding process, providing a place for all developers to learn about what a platform offers and pushing applications to prove their value, security, and privacy before entering a production environment.
  </li>
  <li>
    <p><strong>Applications:</strong> Center the sandbox experience on the application by certifying it meets all the platform requirements before allowing it to move forward. All developers and their applications get access to the sandbox, but only some applications will be given production access.
  </li>
  <li>
    <p><strong>Certification:</strong> Provide certification for both developers and applications. Establishing a minimum bar for what is expected of applications before they can move into production helps developers understand what it takes to move an application from sandbox to distribution, which ensures a high-quality application experience at scale.
  </li>
  <li>
    <p><strong>Showcase:</strong> Always provide a showcase for certified applications as well as certified developers. Allow for the browsing and searching of applications and developers, while also highlighting them in communications and other platform resources.
  </li>
</ul>

<p>When it comes to API resources that contain sensitive data, virtualized APIs, data, and environments are essential. They should be a default part of ensuring that developers push only high-quality applications to production by forcing them to prove themselves in a sandbox environment first.

<h2 id="types-of-metrics-for-measuring-adoption-and-making-decisions">Types of metrics for measuring adoption and making decisions</h2>

<p>This area of our research overlaps somewhat with the earlier section on measuring success, but here, we provide a more precise look at what can be measured to help quantify success while also ensuring that findings are used as part of the decision-making process throughout the API journey.

<h3 id="what-we-learned-4">What we learned</h3>

<p>Our interviews reminded us that it is useful to consider that not all API providers have a fully fleshed out strategy for measuring activity across their platforms. However, we did come away with some interesting lessons from those providers that were using metrics to drive API decision making.

<ul>
  <li>
    <p><strong>Look at it as a funnel:</strong> Treat API outreach and engagement as a sales and marketing funnel. Attract as many new users as you can, but then profile the target demographic to try to understand who they are and what their working objectives comprise. From there, devote efforts to incentivizing users to “move down the funnel” through the sandbox environment and eventually to production status. In short, treat API operations like a business and platform users like they are customers.
  </li>
  <li>
    <p><strong>Do not have formal metrics:</strong> It was illuminating to also learn that some providers felt that having an overly formal metrics strategy might constrain developer outreach and engagement. Providing words of caution when it comes to measuring too much, as well as only examining data when it comes to making critical decisions, can keep outreach efforts more constructively interacting with API consumers regarding their needs.
  </li>
  <li>
    <p><strong>API keys registered:</strong> For data accessibility purposes, it is worthwhile to ensure that all developers have an application and API key before they can access any API resources. Requiring all internal, partner, and eternal developers to pass along their API key with each API call allows all platform activity to be measured, tracked, and used as part of the wider platform decision-making process.
  </li>
  <li>
    <p><strong>Small percentage of users:</strong> We also heard that it is common for a small percentage of the overall platform users to generate the majority of API calls to the platform. This makes it important to measure activity on the platform in terms of which users are the most salient (and thereby driving the majority of value on a platform).
  </li>
  <li>
    <p><strong>Amount of investment:</strong> Importantly, the usage rates of a platform’s resources can provide a strong justification for investing more resources into the platform’s success, making tracking that data of paramount importance. This transforms investment into a data-driven decision that responds to the actual needs of the platform.
  </li>
</ul>

<p>The interview portion of our research provided a valuable look at how API providers are measuring activity across their platforms. Data and metrics are not only being used to define success, but are also used as part of the ongoing decision making process around the direction API providers take their platform, particularly when it comes to measuring adoption of API resources across a platform.

<h3 id="what-our-thoughts-are-4">What our thoughts are</h3>

<p>When it comes to measuring adoption and understanding how API consumers are putting resources to work, we recommend starting small. Activity tracking is something that will change and evolve as the organization develops a better understanding of platform resources and the interests of internal stakeholders, partners, and 3rd party consumers. These are just a handful of areas we recommend collecting data on to begin with.

<ul>
  <li>
    <p><strong>Traffic:</strong> Measure and understand traffic (network and otherwise) across the platform developer portal.
  </li>
  <li>
    <p><strong>New accounts:</strong> Track and profile all new accounts signing up for API access.
  </li>
  <li>
    <p><strong>New applications:</strong> Track and profile all new applications registered for access.
  </li>
  <li>
    <p><strong>Active applications:</strong> Measure and track the usage of the active platform applications.
  </li>
  <li>
    <p><strong>Number of API calls:</strong> Understand how many APIs are being called in different dimensions.
  </li>
  <li>
    <p><strong>Conversation:</strong> Measure all conversation happening across the platform and use these estimates to develop awareness.
  </li>
  <li>
    <p><strong>Support:</strong> Measure all support activity in order to pinpoint the needs of API consumers.
  </li>
  <li>
    <p><strong>Personas:</strong> Quantify the different types of consumers who are putting a platform to use.
  </li>
</ul>

<p>Begin here when it comes to tracking: develop an awareness of the community and get to know what is important. Then expand from there. Add and remove the metrics that make sense for your organization without tracking metrics for no reason (i.e. all tracking should add value to the API platform or its decision making process).

<h2 id="structuring-and-staffing-outreach-programs">Structuring and staffing outreach programs</h2>

<p>Each platform will have its own mandate for how API programs should be staffed, but there are some common patterns that exist across the space.

<h3 id="what-we-learned-5">What we learned</h3>

<p>The API providers we talked to had a lot to share about what it takes to staff their operations, including their structures (or in a few cases lack thereof!).

<ul>
  <li>
    <p><strong>Dedicated evangelist:</strong> Make sure there is a dedicated evangelist to help talk-up and support the platform.
  </li>
  <li>
    <p><strong>Include marketing:</strong> Include the marketing team in conversations around amplifying the platform and its presence.
  </li>
  <li>
    <p><strong>Provide support:</strong> Invest in the resources to support all aspects of the platform effectively, not just those that are consumer-facing or particularly visible.
  </li>
  <li>
    <p><strong>Conduct regular calls:</strong> Conduct regular calls with internal, partner, and external stakeholders in order to bring everyone together to discuss the progress of the platform.
  </li>
  <li>
    <p><strong>Use a CRM for automation:</strong> Put a CRM to use when it comes to tracking and automating outreach efforts for the platform. Do not reinvent the wheel; leverage an existing service to track all of the details that can be automated.
  </li>
  <li>
    <p><strong>Include other 
        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/09/28/api-developer-outreach-research-for-the-department-of-veterans-affairs/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/09/20/having-the-dedication-to-lead-an-api-effort-forward-within-a-large-enterprise/">Having The Dedication To Lead An API Effort Forward Within A Large Enterprise</a></h3>
        <span class="post-date">20 Sep 2018</span>
        <p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/97_193_800_500_0_max_0_-5_-1.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I work with a lot of folks who work in large enterprise organizations, institutions, and government agencies who are moving the API conversation forward within their groups. I’m all too familiar with what it takes to move forward the API conversation within large, well established enterprise organizations. However, I am the first to admit that while I have a deep understanding of what it involves, I do not have the fortitude to actually lead an effort for the sustained amount of time it takes to actually make change. I just do not have the patience and the personality for it, and I’m eternally grateful for those that do.

<p>There are regular streams of emails in my inbox from people embedded within enterprise organizations, looking for guidance, counseling, and assistance in moving forward the API conversation at their organizations. I am happy to provide assistance in an advisory capacity, and consulting with groups to help them develop their strategies. A significant portion of my income comes from conducting 1-3 day workshops within the enterprise, helping teams work through what they need to. There is one thing I cannot contribute to any of these teams, the dedication and perseverance it will need to actually make it happen.

<p>It takes a huge amount of organization knowledge to move things forward at a large organization. You have to know who the decision makers are, and who are the gatekeepers for all of the important resources–this is knowledge you have to acquire by being embedded, and working within an organization for a very long time. You just can’t walk in the door and be able to make sense of things within days, or weeks. You have to be able to work around schedules, and personalities–getting to know people, and truly begin to understand their motivations, and willingness to contribute, or whether they’ll actually decide to work against you. The culture of any enterprise organization will be the most important area of concern for you as you craft and evolve your API strategy.

<p>I often wish I had the fortitude to work in a sustained capacity within a large organization. I’ve tried. It just doesn’t fit my view of the world. However, I am super thankful for those of you who are. I’m super happy to help you in your journey. I’m happy to help you think through what you are experiencing as part of my storytelling here on my blog–just email me your questions, thoughts, and concerns. I’m happy to anonymize as I work through my responses here on the blog, about 60% of the stories you read here are the anonymized result of emails I receive from y’all. I’m happy to vent for you, and use you as my muse. I’m also happy to help out in a more dedicated capacity, and provide my consulting assistance to your organization–it is what I do, and how I pay the bills. Let me know how I can help.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/09/20/having-the-dedication-to-lead-an-api-effort-forward-within-a-large-enterprise/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/09/18/understanding-the-eventdriven-api-infrastructure-opportunity-that-exists/">Understanding The Event-Driven API Infrastructure Opportunity That Exists</a></h3>
        <span class="post-date">18 Sep 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/kong/kong-summit-2018.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I am at <a href="https://konghq.com/kong-summit/">the Kong Summit in San Francisco</a> all day tomorrow. I’m going to be speaking about research into the event-driven architectural layers I’ve been mapping out across the API space. Looking for the opportunity to augment existing APIs with push technology like webhooks, and streaming technology like SSE, as well as pipe data in an out of Kafka, fill data lakes, and train machine learning models. I’ll be sharing what I’m finding from some of the more mature API providers when it comes to their investment in event-driven infrastructure, focusing in on Twilio, SendGrid, Stripe, Slack, and GitHub.

<p>As I am profiling APIs for inclusion in my <a href="http://theapistack.com">API Stack</a> research, and in <a href="http://api.gallery.streamdata.io/">the API Gallery</a>, I create an <a href="http://apisjson.org">APIs.json</a>, <a href="http://openapis.org">OpenAPI</a>, <a href="https://www.postman.com/docs/v6/postman/collections/creating_collections">Postman Collection(s)</a>, and sometimes an <a href="https://www.asyncapi.com/">AsyncAPI definition</a> for each API. All of my API catalogs, and API discovery collections use APIs.json + OpenAPI by default. One of the things I profile in each of my APIs.json, is the usage of webhooks as part of API operations. <a href="http://webhook.implementations.api.gallery.streamdata.io/">You can see collections of them that I’ve published to the API Gallery</a>, aggregating many different approaches in what I consider to be the 101 of event-driven architecture, built on top of existing request and response HTTP API infrastructure. Allowing me to better understand how people are doing webhooks, and beginning to sketch out plans for a more event-driven approach to delivering resources, and managing activity on any platform that is scaling.

<p>While studying APIs at this level you begin to see patterns across how providers are doing what they are doing, even amidst a lack of standards for things like webhooks. API providers emulate each other, it is how much of the API space has evolved in the last decade. You see patterns like how leading API providers are defining their event types. Naming, describing, and allowing API consumers to subscribe to a variety of events, and receive webhook pings or pushes of data, as well as other types of notifications. Helping establish a vocabulary for defining the most meaningful events that are occurring across an API platform, and then providing an even-driven framework for subscribing to push data out when something occurs, as well as sustained API connections in the form of server-sent event (SSE), HTTP long polling, and other long running HTTP connections.

<p>As I said, webhooks is the 101 of event-driven technology, and once API providers evolve in their journey you begin to see investment in the 201 level solutions like SSE, WebSub, and more formal approaches to delivering resources as real time streams and publish / subscribe solutions. Then you see platforms begin to mature and evolve into other 301 and beyond courses, with AMQP, Kafka, and often times other Apache Projects. Sure, some API providers begin their journey here, but for many API providers, they are having to ease into the world of event-driven architecture, getting their feet wet with managing their request and response API infrastructure, and slowly evolving with webhooks. Then as API operations harden, mature, and become more easily managed, API providers can confidently begin evolving into using more sophisticated approaches to delivering data where it needs to be, when it is needed.

<p>From what I’ve gathered, the more mature API providers, who are further along in their API journey have invested in some key areas, which has allowed them to continue investing in some other key ways:

<ul>
  <li><strong>Defined Resources</strong> - These API providers have their APIs well defined, with master planned designs for their suite of services, possessing machine readable definitions like OpenAPI, Postman Collections, and AsyncAPI.<img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-event-driven-steering.png" style="padding: 15px;" align="right" width="25%" /></li>
  <li><strong>Request / Response</strong> - Who have fined tuned their approach to delivering their HTTP based request and response structure, along with their infrastructure being so well defined.</li>
  <li><strong>Known Event Types</strong> - Which has resulted in having a handle on what is changing, and what the most important events are for API providers, as well as API consumers.</li>
  <li><strong>Push Technology</strong> - Having begun investing in webhooks, and other push technology to make sure their API infrastructure is a two-way street, and they can easily push data out based upon any event.</li>
  <li><strong>Query Language</strong> - Understanding the value of investment in a coherent querying strategy for their infrastructure that can work seamlessly with the defining, triggering, and overall management of event driven infrastructure.</li>
  <li><strong>Stream Technology</strong> - Having a solid understanding of what data changes most frequently, as well as the topics people are most interested, and augmenting push technology with streaming subscriptions that consumers can tap into.</li>
</ul>

<p>At this point in most API providers journey, they are successfully operating a full suite of event-driven solutions that can be tapped internally, and externally with partners, and other 3rd party developers. They probably are already investing in Kafka, and other Apache projects, an getting more sophisticated with their event-driven API orchestration. Request and response API infrastructure is well documented with OpenAPI, and groups are looking at event-driven specifications like AsyncAPI to continue to ensure all resources, messages, events, topics, and other moving parts are well defined.

<p>I’ll be showcasing the event-driven approaches of Twilio, SendGrid, Stripe, Slack, and GitHub at the Kong Summit tomorrow. I’ll also be looking at streaming approaches by Twitter, Slack, SalesForce, and Xignite. Which is just the tip of the event-driven API architecture opportunity I’m seeing across the existing API landscape. After mapping out several hundred API providers, and over 30K API paths using OpenAPI, and then augmenting and extending what is possible using AsyncAPI, you begin to see the event-driven opportunity that already exists out there. When you look at how API pioneers are investing in their event-driven approaches, it is easy to get a glimpse at what all API providers will be doing in 3-5 years, once they are further along in their API journey, and have continued to mature their approach to moving their valuable bits an bytes around using the web.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/09/18/understanding-the-eventdriven-api-infrastructure-opportunity-that-exists/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/09/18/talking-healthcare-apis-with-the-cms-blue-button-api-team-at-apistrat-in/">Talking Healthcare APIs With The CMS Blue Button API Team At #APIStrat In</a></h3>
        <span class="post-date">18 Sep 2018</span>
        <p><a href="https://bluebutton.cms.gov/"><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/blue-button/blue-button-api-docs.png" width="45%" align="right" style="padding: 15px;" /></a>
<p>We have the API evangelist from one of the most significant APIs out there today at #APIStrat in Nashville next week. Mark Scrimshire (<a href="https://twitter.com/ekivemark">@ekivemark</a>), Blue Button Innovator and Developer Evangelist from NewWave Telecoms and Technologies will be on the main stage next Tuesday, September 25th 2018. Mark will be bringing his experience helping stand up <a href="https://bluebutton.cms.gov/">the Blue Button API with the Centers for Medicare and Medicaid Services (CMS)</a>, and sharing the stories from the trenches while delivering this critical piece of health API infrastructure within the United States.

<p>I consider the Blue Button API to be one of the most significant APIs out there right now for several key factors:

<ul>
  <li><strong>API Reach</strong> - An API that has potential to reach 44 million Medicare beneficiaries, which is 15 percent of the U.S. population–that is a pretty significant audience to reach when it comes to the overall API conversation.</li>
  <li><strong>Fast Healthcare Interoperability Resources (FHIR)</strong> - The Blue Button API supports <a href="https://www.hl7.org/fhir/overview.html">Hl7 / FHIR</a>, pushing the specification forward in the overall healthcare API interoperability discussion, making it extremely relevant to APIStrat and the OpenAPI Initiative (OAI).</li>
  <li><strong>Government API Blueprint</strong> - The way in which the Blue Button API team at CMS and USDS is delivering the API is providing a potential blueprint that other federal and stage level agencies can follow when rolling out their own Medicare related APIs, but also any other critical infrastructure that this country depends on.</li>
</ul>

<p>This is why I am always happy to support the Blue Button API team in any way I can, and I am very stoked to have them at APIStrat in Nashville next week. I’ve spent a lot of time working with, and studying what the Blue Button API team is up to, <a href="https://events.linuxfoundation.org/events/apistrat-2018/program/schedule/"><img src="https://s3.amazonaws.com/kinlane-productions2/events/apistrat/apistrat-nashville.png" align="right" width="40%" style="padding: 15px;" /></a>and <a href="http://apievangelist.com/2018/08/07/i-am-speaking-in-washington-dc-at-the-blue-button-20-developer-conference-on-the-api-lifecycle-this-monday/">I spoke at their developer conference hosted at the White House last month</a>. They have some serious wisdom to share when it comes to delivering public APIs at this scale, making the keynote with Mark something you will not want to miss.

<p><a href="https://events.linuxfoundation.org/events/apistrat-2018/program/schedule/">You can check out the schedule for APIStrat next week on the website</a>. There are also <a href="https://events.linuxfoundation.org/events/apistrat-2018/attend/register/">still tickets available</a> if you want to join in the conversation going on there Monday, Tuesday, and Wednesday next week. <a href="https://www.openapis.org/">APIStrat is operated by the OpenAPI Initiative (OA)</a>, making it the place where you will be having high level API conversation like this one. When it comes to APIs, and industry changing API specifications like FHIR, APIStrat is the place to be. I’ll see you all in Nashville next week, and I look forward to talking APIs with all y’all in the halls, and around town for APIStrat 2018.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/09/18/talking-healthcare-apis-with-the-cms-blue-button-api-team-at-apistrat-in/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/09/17/sadly-stack-exchange-blocks-api-calls-being-made-from-any-of-amazons-ip-block/">"Sadly Stack Exchange Blocks API Calls Being Made From Any Of Amazons IP Block"</a></h3>
        <span class="post-date">17 Sep 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/stack-exchange/stack-exchange-api.png" width="45%" align="right" style="padding: 15px;" />
<p>I am developing an authentication and access layer for <a href="http://api.gallery.streamdata.io/">the API Gallery</a> that I am building for <a href="http://streamdata.io">Streamdata.io</a>, while also federating it for usage as part of <a href="http://theapistack.com">my API Stack research.</a> In addition to building out these catalogs for API discovery purposes, I’m also developing a suite of tools that allow users to subscribe to different topics from popular sources like <a href="http://subscribe.github.repository.search.streamdata.io/">GitHub</a>, <a href="http://subscribe.reddit.streamdata.io/">Reddit</a>, and <a href="http://subscribe.stack.exchange.search.streamdata.io/">Stack Overflow (Exchange)</a>. I’ve been busy adding one or two providers to my OAuth broker each week, until the other day I hit a snag with the Stack Exchange API.

<p>I thought my Stack Exchange API OAuth flow had been working, it’s been up for a few months, and I seem to remember authenticating against it before, but this weekend I began getting an error that my IP address was blocked. I was looking at log files trying to understand if I was making too many calls, or some other potential violation, but I couldn’t find anything. Eventually I emailed Stack Exchange to see what their guidance once, to which I got a prompt reply:

<p><em>“Yes, we block all of Amazon’s AWS IP addresses due to the large amount of abuse that comes from their services. Unfortunately we cannot unblock those addresses at this time.”</em>

<p>Ok then. I guess that is that. I really don’t feel like setting up another server with another provider just so I can run an OAuth server from there. Or, maybe I guess I might have to if I expect to offer a service that provides OAuth integration with Stack Exchange. It’s a pretty unfortunate situation that doesn’t make a whole lot of sense. I can understand adding another layer of white listing for developers, pushing them to add their IP address to their Stack Exchange API application, and push us to justify that our app should have access, but blacklisting an entire cloud provider from accessing your API is just dumb.

<p>I am going to weigh my options, and explore what it will take to setup another server elsewhere. Maybe I will start setting up individual subdomains for each OAuth provider I add to the stack, so I can decouple them, and host them on another platform, in another region. This is one of those road blocks you encounter doing APIs that just doesn’t make a whole lot of sense, and yet you still have to find a work around–you can’t just give in, despite API providers being so heavy handed, and not considering the impact of the moves on their consumers. I’m guessing in the end, the Stack Exchange API doesn’t fit into their wider business model, which is something that allows blind spots like this to form, and continue.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/09/17/sadly-stack-exchange-blocks-api-calls-being-made-from-any-of-amazons-ip-block/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/09/17/justifying-my-existence-in-your-api-sales-and-marketing-funnel/">Justifying My Existence In Your API Sales And Marketing Funnel</a></h3>
        <span class="post-date">17 Sep 2018</span>
        <p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/32_39_600_400_0_avg_1_1_1.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I feel like I’m regularly having to advocate for my existence, and the existence of developers who are like me, within the sales and marketing funnel for many APIs. I sign up for a lot of APIs, and have the pleasure of enjoy a wide variety of on-boarding processes for APIs. Many APIs I have no problem signing up, on-boarding, and beginning to make calls, while others I have to just my existence within their API sales and marketing funnel. Don’t get me wrong, I’m not saying that I shouldn’t be expected to justify my existence, it is just that many API providers are setup to immediately discourage, create friction for, and dismiss my class of API integrator–that doesn’t fit neatly into the shiny big money integration you have defined at the bottom of your funnel.

<p>I get that we all need to make money. I have to. I’m actually in the business of helping you make money. I’m just saying that you are missing out on a significant amount of opportunity if you only focus on what comes out the other side of your funnel, and discount the nutrients developers like me can bring to your funnel ecosystem. I’m guessing that my little domain apievangelist.com does return the deal size scope you are looking for, but I think you are putting too much trust into the numbers provided to you by your business intelligence provider. I get that you are hyper focused on making the big deals, but you might be leaving a big deal on the table by shutting out small fish, who might have oversized influence within their organization, government agency, or within an industry. Your business intelligence is focusing on the knowns, and doesn’t seem very open to considering the unknowns.

<p>As the API Evangelist I have an audience. I’ve been in business since 2010, so I’ve built up an audience of enterprise folks who read what I write, and listen to “some” of what I say. I know people like me within the federal government, within city government, and across the enterprise. Over half the people I know who work within the enterprise, helping influence API decisions, are also kicking the tires of APIs at night. Developers like us do not always have a straightforward project, we are just learning, understanding, and connecting the dots. We don’t always have the ready to go deal in the pipeline, and are usually doing some homework so that we can go sell the concept to decision makers. Make sure your funnel doesn’t keep us out, run us away, or remove channels for our voice to be heard.

<p>In a world where we focus only on the big deals, and focus on scaling and automating the operation of platforms, we run the risk of losing ourselves. If you are only interested in landing those big customers, and achieving the exit you desire, I understand. I am not your target audience. I will move. It also means that I won’t be telling any stories about what you are doing, building any prototypes, and generally amplifying what you are doing on social media, and across the media landscape. Providing many of the nutrients you will need to land some of the details you are looking to get, generating the internal and external buzz needed to influence the decision makers. Providing real world use cases of why your API-driven solution is the one an enterprise group should be investing in. Make sure you aren’t locking us out of your platform, and you are investing the energy into getting to know your API consumers, more about what their intentions are, and how it might fit into your larger API strategy–if you have one.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/09/17/justifying-my-existence-in-your-api-sales-and-marketing-funnel/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/09/17/i-am-needing-some-evidence-of-how-apis-can-make-an-impact-in-government/">I Am Needing Some Evidence Of How APIs Can Make An Impact In Government</a></h3>
        <span class="post-date">17 Sep 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/capital-battle_blue_circuit.jpg" width="45%" align="right" style="padding: 15px;" />
<p>Eric Horesnyi (<a href="https://twitter.com/EricHoresnyi">@EricHoresnyi</a>), the CEO of <a href="http://streamdata.io">Streamata.io</a> and I were on a call with a group of people who are moving forward the API conversation across Europe, with the assistance of the EU. The project has asked us to assist them in the discovery of more data and evidence of how APIs are making an impact in how government operates within the European Union, but also elsewhere in the world. Aggregating as much evidence as possible to help influence the EU API strategy, and learn from what is already being done. I’m heading to Italy next month to present to the group, and participate in conversations with other API practitioners and evangelists, so I wanted to start my usual amount of storytelling here on the blog to solicit contributions from my audience about what they are seeing.

<p>I am looking for some help from my readers who work at city, county, state, and federal agencies, or at the private entities who help them with their API efforts. I am looking for official, validated, on the record examples of APIs making a positive impact on how government serves its constituents. Quantifiable examples of how a government agency have published a private, partner, or public API, and it helped the agency better meet its mission. I’m looking for anything mundane, as well as the unique and interesting, with tangible evidence to back it all up. Like number of developers, partners, apps, cost saving, efficiencies, or any other positive effect. Demonstrating that APIs when done right can move the conversation forward at a government agency. For this round, I’m going to need first hand accounts, because I will need to help organize the data, and work with this group to submit it to the European Union as part of their wider effort.

<p>This is something I’ve been doing loosely since 2012, but I need to start getting more official about how I gather the stories, and pull together actual evidence, going beyond just my commentary from the outside in. I’ll be reaching out to all my people in government, asking for examples. If you know of anything, please email me at <a href="/cdn-cgi/l/email-protection" class="__cf_email__" data-cfemail="8fe4e6e1cfeeffe6eaf9eee1e8eae3e6fcfba1ece0e2">[email&#160;protected]</a> with your thoughts. We have an opportunity to influence the regulatory stance in Europe when it comes to government putting APIs to work, which will be something that washes back upon the shores of the United States during each wave of API regulations to come out of the EU. My casual storytelling about how government APIs are making change on my blog has worked for the last five years, but moving forward we are going to need to better at gathering, documenting, and sharing examples how APIs are working across government. Helping establish more concrete blueprints for how to do all of this properly, and ensuring that we aren’t reinventing the wheel when it comes to API in government.

<p>If you know someone working on APIs in at any level of government, feel free to share a link to my story, or send an introduction via <a href="/cdn-cgi/l/email-protection" class="__cf_email__" data-cfemail="1f7476715f7e6f767a697e71787a73766c6b317c707231">[email&#160;protected]</a> I’d love to help share the story, and evidence of the impact they are making with APIs. I appreciate all your support in making this happen–it is something I’ll put back out to the community once we’ve assembled it and talked through it in Italy next month.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/09/17/i-am-needing-some-evidence-of-how-apis-can-make-an-impact-in-government/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/09/17/being-open-is-more-about-being-open-so-someone-can-extract-value-than-open/">Being Open Is More About Being Open So Someone Can Extract Value Than Open</a></h3>
        <span class="post-date">17 Sep 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/adam-smith_blue_circuit_5.jpg" width="45%" align="right" style="padding: 15px;" />
<p>One of the most important lessons I’ve learned in the last eight years, is that when people are insistent about things being open, in both accessibility, and cost, it is often more about things remaining open for them to freely (license-free) to extract value from it, that it is ever about any shared or reciprocal value being generated. I’ve fought many a battle on the front lines of “open”, leaving me pretty skeptical when anyone is advocating for open, and forcing me to be even critical of my own positions as the API Evangelist, and the bullshit I peddle.

<p>In my opinion, ANYONE wielding the term open should be scrutinized for insights into their motivations–me included. I’ve spend eight years operating on the front line of both the open data, and the open API movements, and unless you are coming at it from the position of a government entity, or from a social justice frame of mind, you are probably wanting open so that you can extract value from whatever is being opened. With many different shades of intent  existing when it comes to actually contributing any value back, and supporting the ecosystem around whatever is actually being opened.

<p>I ran with the open data dogs from 2008 through 2015 (still howl and bark), pushing for city, county, state, and federal government open up data. I’ve witnessed how everyone wants it opened, sustained, maintained, and supported, but do not want to give anything back. Google doesn’t care about the health of local transit, as long as the data gets updated in Google Maps. Almost every open data activist, and data focused startup I’ve worked with has a high expectation for what government should be required to do, and want very low expectations regarding what should be expected of them when it comes to pay for commercial access, sharing enhancements and enrichments, providing access to usage analytics, and be observable and open to sharing access to end-users of this open data. Libertarian capitalism is well designed to take, and not give back–yet be actively encouraging open.

<p>I deal with companies, organizations, and institutions every day who want me to be more open with my work. Are more than happy to go along for the ride when it comes to the momentum built up from open in-person gatherings, Meetups, and conference. Always be open to syndicating data, content, and research. All while working as hard as possible to extract as much value, and not give anything back. There are many, many, many companies who have benefitted from the open API work that I, and other evangelist in the space do on a regular basis, without ever considering if they should support them, or give back. I regularly witness partnerships scenarios in all of the API platforms I monitor, where the larger more proprietary and successful partner extracts value from the smaller, more open and less proven partner. I get that some of this is just the way things are, but much of it is about larger, well-resourced, and more closed groups just taking advantage of smaller, less-resourced, and more open groups.

<p>I have visibility into a number of API platforms that are the targets of many unscrupulous API consumers who sign up for multiple accounts, do not actively communicate with platform owners, and are just looking for a free hand out at every turn. Making it very difficult to be open, and often times something that can also be very costly to maintain, sustain, and support. Open isn’t FREE! Publicly available data, content, media, and other resources cost money to operate. The anti-competitive practices of large tech giants setting the price so low for common digital resources have set the bar so low, for so long, it has change behaviors and set unrealistic expectations as the default. Resulting in some very badly behaved API ecosystem players, and ecosystems that encourage and incentivize bad behavior within specific API communities, but also is something that spreads from provider to provider. Giving APIs a bad name.

<p>When I come across people being vocal about some digital resource being open, I immediately begin conducting a little due diligence on who they are. Their motivations will vary depending on where the come from, and while there are no constants, I can usually tell a lot about someone whether they come from a startup ecosystem, the enterprise, government, venture capital, or other dimensions of ur reality that the web has reached into recently. My self-appointed role isn’t just about teaching people to be more “open” with their digital assets, it is more about teaching people to be more aware and in control over their digital assets. Because there are a lot of wolves in sheeps clothing out there, trying to convince you that “open” is an essential part of your “digital transformation”, and showcasing all the amazing things that will happen when you are more “open”. When in reality they are just interested in you being more open so that they can get their grubby hands on your digital resources, then move on down the road to the next sucker who will fall for their “open” promises.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/09/17/being-open-is-more-about-being-open-so-someone-can-extract-value-than-open/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/09/13/providing-minimum-viable-api-documentation-blueprints-to-help-guide-your-api/">Providing Minimum Viable API Documentation Blueprints To Help Guide Your API</a></h3>
        <span class="post-date">13 Sep 2018</span>
        <p><a href="https://va-working.github.io/api-documentation/"><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/va/va-definitions-support.png" width="45%" align="right" style="padding: 15px;" /></a>
<p>I was taking a look at the Department of Veterans Affairs (VA) API documentation for the VA Facilities API, and intending on providing some feedback on the API implementation. The API itself is pretty sound, and I don’t have any feedback without having actually integrated it into an application, but following on the heals of <a href="http://apievangelist.com/2018/08/27/how-do-we-get-developers-to-follow-the-minimum-viable-api-documentation-guidance/">my previous story about how we get API developers to follow minimum viable API documentation guidance</a>, I had lots of feedback on the overall deliver of the documentation for <a href="https://developer.va.gov/explore/facilities/docs/facilities">the VA Facilities API</a>, helping improve on what they have there.

<p><strong>Provide A Working Example of Minimum Viable API Documentation</strong><br />
One of the ways that you help incentivize your API developers to deliver minimum viable API documentation across their API implementations is you do as much of the work for them as you can, and provide them with a forkable, downloadable, clonable API documentation that meets the minimum viable requirements. <a href="https://va-working.github.io/api-documentation/">To help illustrate what I’m talking about I created a base GitHub blueprint for what I’d suggest as a minimum viable API documentation at the VA</a>. Providing something the VA can consider, and borrow from as they are developing their own strategy for ensuring all APIs are consistently documented.

<p><strong>Covering The Bare Essentials That Should Exist For All APIs</strong><br />
I wanted to make sure each API had the bare essentials, so I took what the VA has already done over at <a href="https://developer.va.gov/">developer.va.gov</a>, and republished it as a static single page application that runs 100% on GitHub pages, and hosted in a GitHub repository–providing the following essential building blocks for APIs at the VA:

<ul>
  <li><strong>Landing Page</strong> - Giving any API a single landing page that contains everything you need to know about working with an API. The landing page can be hosted as its own repo, and subdomain, and the linked up with other APIs using a facade page, or it could be published with many other APIs in a single repository.</li>
  <li><strong>Interactive Documentation</strong> - Providing interactive, OpenAPI-driven API documentation using Swagger UI. Providing a usable, and up to date version of the documentation that developers can use to understand what the API does.</li>
  <li><strong>OpenAPI Definition</strong> - Making sure the OpenAPI behind the documentation is front and center, and easily downloaded for use in other tools and services.</li>
  <li><strong>Postman Collection</strong> - Providing a Postman Collection for the API, and offering it as more of a transactional alternative to the OpenAPI.</li>
</ul>

<p>That covers the bases for the documentation that EVERY API should have. Making API documentation available at a single URL to a human viewable landing page, complete with documentation. While also making sure that there are two machine readable API definitions available for an API, allowing the API documentation to be more portable, and useable in other tooling and services–letting developers use the API definitions as part of other stops along the API lifecycle.

<p><strong>Bringing In Some Other Essential API Documentation Elements</strong><br />
Beyond the landing page, interactive documentation, OpenAPI, and Postman Collection, I wanted to suggest some other building blocks that would really make sure API developers at the VA are properly documenting, communicating, as well as supporting their APIs. To go beyond the bare bones API documentation, I wanted to suggest a handful of other elements, as well as incorporate some building blocks the VA already had on the API documentation landing page for the VA Facilities API.

<ul>
  <li><strong>Authentication</strong> - Providing an overview of authenticating with the API using the header apikey.</li>
  <li><strong>Response Formats</strong> - They already had a listing of media types available for the API.</li>
  <li><strong>Support</strong> - Ensuring that an API has at least one support channel, if not multiple channels.</li>
  <li><strong>Road Map</strong> - Making sure there is a road map providing insights into what is planned for an API.</li>
  <li><strong>References</strong> - They already had a listing of references, which I expanded upon here.</li>
</ul>

<p>I tried not to go to town adding all the building blocks I consider to be essential, and just contribute couple of other basic items. I feel support and road map are essential and cannot be ignored, and should always be part of the minimum viable API documentation requirements. My biggest frustrations with APIs are 1) Up to date documentation, 2) No support, and 3) Not knowing what the future holds. I’d say that I’m also increasingly frustrated when I can’t get at the OpenAPI for an API, or at least find a Postman Collection for the API. Machine readable definitions moved into the essential category for me a couple years ago–even though I know some folks don’t feel the same.

<p><strong>A Self Contained API Documentation Blueprint For Reuse</strong><br />
To create the minimum viable API documentation blueprint demo for the VA, I took the HTML template from <a href="https://developer.va.gov/">developer.va.gov</a>, and deployed as a static Jekyll website that runs on GitHub Pages. The landing page for the documentation is a single index.html page in the root of the site, leverage Jekyll for the user interface, but <a href="https://github.com/va-working/api-documentation/blob/master/_config.yml">driving all the content on the page from the central config.yml for the API project</a>. Providing a YAML checklist that API developers can follow when publishing their own documentation, helping do a lot of the heavy lifting for developers. All they have to do is update the OpenAPI for the API and add their own data and content to the config.yml to update the landing page for the API. Providing a self-contained set of API documentation that developers can fork, download, and reuse as part of their work, delivering consistent API documentation across teams.

<p>The demo API documentation blueprint could use some more polishing and comments. I will keep adding to it, and evolving it as I have time. I just wanted to share more of my thoughts about the approach the VA could take to provide function API documentation guidance, as a functional demo. Providing them with something they could fork, evolve, and polish on their own, turning it into a more solid, viable solution for documentation at the federal agency. Helping evolve how they deliver API documentation across the agency, and ensuring that they can properly scale the delivery of APIs across teams and vendors. While also helping maximize how they leverage GitHub as part of their API lifecycle, setting the base for API documentation in a way that ensures it can also be used as part of a build pipeline to deploy APIs, as well as manage, testing, secure, and helping deliver along almost every stop along a modern API lifecycle.

<p>The website for this project is available at: https://va-working.github.io/api-documentation/
You can access the GitHub repository at: https://github.com/va-working/api-documentation


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/09/13/providing-minimum-viable-api-documentation-blueprints-to-help-guide-your-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/09/13/please-refer-the-engineer-from-your-api-team-to-this-story/">Please Refer The Engineer From Your API Team To This Story</a></h3>
        <span class="post-date">13 Sep 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/mosaic-face_blue_circuit.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I reach out to API providers on a regular basis, asking them if they have an <a href="https://www.openapis.org/">OpenAPI</a> or <a href="https://www.postman.com/docs/v6/postman/collections/creating_collections">Postman Collection</a> available behind the scenes. I am adding these machine readable API definitions to my index of APIs that I monitor, while also publishing them out to <a href="http://theapistack.com">my API Stack research</a>, the <a href="http://api.gallery.streamdata.io/">API Gallery</a>, <a href="http://apis.io">APIs.io</a>, work to get them published in <a href="https://www.postman.com/api-network/">the Postman Network</a>, and syndicated as part of my wider work as <a href="https://www.openapis.org/membership/members">an OpenAPI member</a>. However, even beyond my own personal needs for API providers to have a machine readable definition of their API, and helping them get more syndication and exposure for their API, having an definition present significantly reduces friction when on-boarding with their APIs at almost every stop along a developer’s API integration journey.

<p>One of the API providers I reached out to recently responded with this, <em>“I spoke with one of our engineers and he asked me to refer you to https://developer.[company].com/”</em>. Ok. First, I spend over 30 minutes there just the other day. Learning about what you do, reading through documentation, and thinking about what was possible–which I referenced in my email. At this point I’m guessing that the engineer in question doesn’t know what an OpenAPI or Postman Collection is, they do not understand the impact these specifications are having on the wider API ecosystem, and lastly, I’m guessing they don’t have any idea who I am(ego taking control). All of which provides me with the signals I need to make an assessment of where any API is in their overall journey. Demonstrating to me that they have a long ways to go when it comes to understanding the wider API landscape in which they are operating in, and they are too busy to really come out of their engineering box and help their API consumers truly be successful in integrating with their platform.

<p>I see this a lot. It isn’t that I expect everyone to understand what OpenAPI and Postman Collections are, or even know who I am. However, I do expect people doing APIs to come out of their boxes a little bit, and be willing to maybe Google a topic before responding to question, or maybe Google the name of the person they are responding to. I don’t use a gmail.com address to communicate, I am using apievangelist.com, and if you are using a solution like <a href="https://clearbit.com/">Clearbit</a>, or other business intelligence solution, you should always be retrieving some basic details about who you are communicating with, before you ever respond. That is, you do all of this kind of stuff if you are truly serious about operating your API, helping your API consumers be more successful, and taking the time to provide them with the resources they need  along the way–things like an OpenAPI, or Postman Collections.

<p><strong>Ok, so why was this response so inadequate?</strong>

<ul>
  <li><strong>No API Team Present</strong> - It shows me that your company doesn’t have any humans their to support the humans that will be using your API. My email went from general support, to a backend engineer who doesn’t care about who I am, and about what I need. This is a sign of what the future will hold if I actually bake their API into my applications–I don’t need my questions lost between support and engineering, with no dedicated API team to talk to.</li>
  <li><strong>No Business Intelligence</strong> - It shows me that your company has put zero thought into the API business model, on-boarding, and support process. Which means you do not have a feedback loop established for your platform, and your API will always be deficient of the nutrients it needs to grow. Always make sure you conduct a lookup based upon on the domain, or Twitter handle or your consumers to get the context you need to understand who you are talking to.</li>
  <li><strong>Stuck In Your Bubble</strong> - You aren’t aware of the wider API community, and the impact OpenAPI, and Postman are having on the on-boarding, documentation, and other stops along the API lifecycle. Which means you probably aren’t going to keep your platform evolving with where things are headed.</li>
</ul>

<p><strong>Ok, so why should you have an OpenAPI and Postman Collection?</strong>

<ul>
  <li><strong>Reduce Onboarding Friction</strong> - As a developer I won’t always have the time to spend absorbing your documentation. Let me import your OpenAPI or Postman Collection into my client tooling of choice, register for a key and begin making API calls in seconds, or minutes. Make learning about your API a hands on experience, something I’m not going to get from your static documentation.</li>
  <li><strong>Interactive API Documentation</strong> - Having a machine readable definition for your API allows you to easily keep your documentation up to date, and make it a more interactive experience. Rather than just reading your API documentation, I should be able to make calls, see responses, errors, and other elements I will need to truly understand what you do. There are plenty of open source interactive API documentation solutions that are driven by OpenAPI and Postman, but you’d know this if you were aware of the wider landscape.</li>
  <li><strong>Generate SDKs, and Other Code</strong> - Please do not make me hand code the integration with each of your API endpoints, crafting each request and response manually. Allow me to autogenerate the most mundane aspects of integration, allowing OpenAPI and Postman Collection to act as the integration contract.</li>
  <li><strong>Discovery</strong> - Please don’t expect your potential consumers to always know about your company, and regularly return to your developer.[company].com portal. Please make your APIs portable so that they can be published in any directory, catalog, gallery, marketplace, and platform that I’m already using, and frequent as part of my daily activities. If you are in my Postman Client, I’m more likely to remember that you exist in my busy world.</li>
</ul>

<p>These are just a few of the basics of why this type of response to my question was inadequate, and why you’d want to have OpenAPI and Postman Collections available. My experience on-boarding will be similar to that of other developers, it just happens that the application I’m developing are out of the normal range of web and mobile applications you have probably been thinking about when publishing your API. But this is why we do APIs, to reach the long tail users, and encourage innovate around our platforms. I just stepped up and gave 30 minutes of my time (now 60 minutes with this story) to learning about your platform, and pointing me to your developer.[company].com page was all you could muster in return?

<p>Just like other developers will, if I can’t onboard with your API without friction, and I can’t tell if there is anyone home, and willing to give me the time of day when I have questions, I’m going to move on. There are other platforms that will accommodate me. The other downside of your response, and me moving on to another platform, is that now I’m not going to write about your API on my blog. Oh well? After eight years of blogging on APIs, and getting 5-10K page views per day, I can write about a topic or industry, and usually dominate the SEO landscape for that API search term(s) (ego still has control). But…I am moving on, no story to be told here. The best part of my job is there are always stories to be told somewhere else, and I get to just move on, and avoid the friction wherever possible when learning how to put APIs to work.

<p>I just needed this single link to provide in response to my email response, before I moved on!


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/09/13/please-refer-the-engineer-from-your-api-team-to-this-story/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/09/10/stack-exchange-has-an-api-that-returns-the-details-for-all-of-your-access/">Stack Exchange Has An API That Returns The Details For All Of Your Access</a></h3>
        <span class="post-date">10 Sep 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/stack-exchange/stack-exchange-access-tokens-api.png" width="45%" align="right" style="padding: 15px;" />
<p>I’m a big fan of helpful authentication features, where API providers make it easier to manage our increasingly hellish environment, application, token, and other management duties of the average API integrator. To help me better manage my API apps, and the OAuth tokens I have in play, I am trying to document all the sensible approaches I come across while putting different APIs to work, and scouring the API landscape for stories.

<p>One example of this in action is out of the Stack Exchange API, <a href="https://api.stackexchange.com/docs/read-access-tokens">where you can find an API endpoint for accessing the details of your OAuth tokens, and invalidate, and de-authorize them</a>. A pretty useful API endpoint when you are integrating with APIs, and find yourself having to manage many tokens across many APIs, apps, and users. Helping you check in on the overall health and activity of your tokens, revoking, renewing, and making sure they work when you need them the most.

<p>It is helpful for me to write about the helpful authentication practices I come across while using APIs. It helps me aggregate them into a nice list of features API providers should consider supporting. If I don’t write about it here on the blog, then it doesn’t exist in my research, and my future storytelling. My goal is to help spread the knowledge about what is working across the sector, so that more API providers will adopt along the way. You know what is better than Stack Exchange providing an API to manage your access tokens? All API providers providing you with an API to manage your access tokens!

<p>These stories, and any other relevant links I’ve curated will be published to <a href="http://authentication.apievangelist.com/">my API authentication research</a>. Eventually I’ll roll all the features I’ve aggregated into either a long form blog post, or white paper I’ll publish and put out with the assistance of one of my partners. I’m interested in the authentication portion of this, but also I’m looking to begin defining processes for helping us better manage our API integration environments, application ids, secrets, tokens, and other goodies we depend on to secure our consumption of APIs across many different providers. It is something that will continue to expand, multiply, and grow more complex with each additional API we add to our growing list of dependencies.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/09/10/stack-exchange-has-an-api-that-returns-the-details-for-all-of-your-access/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/09/10/some-ideas-for-api-discovery-collections-that-students-can-use/">Some Ideas For API Discovery Collections That Students Can Use</a></h3>
        <span class="post-date">10 Sep 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist/priorities/university-of-api.png" width="45%" align="right" style="padding: 15px;" />
<p>This is a topic I’ve wanted to set in motion for some time now. I had a new university professor city my work again as part of one of their courses recently, something that floated this concept to the top of the pile again–API discovery collections meant for just for students. Helping k-12, community college, and university students quickly understand where to find the most relevant APIs to whatever they are working on. Providing human, but also machine readable collections that can help jumpstart their API education.

<p>I use the API discovery format <a href="http://apisjson.org">APIs.json</a> to profile individual, as well as collections of APIs. I’m going to kickstart a couple of project repos, helping me flesh out a handful of interesting collections that might help students better understand the world of APIs:

<ul>
  <li><strong>Social</strong> - The popular social APIs like Twitter, Facebook, Instagram, and others.</li>
  <li><strong>Messaging</strong> - The main messaging APIs like Slack, Facebook, Twitter, Telegram, and others.</li>
  <li><strong>Rock Star</strong> - The cool APIs like Twitter, Stripe, Twilio, YouTube, and others.</li>
  <li><strong>Amazon Stack</strong> - The core AWS Stack like EC2, S3, RDS, DynamoDB, Lambda, and others.</li>
  <li><strong>Backend Stack</strong> - The essential App stack like AWS S3, Twilio, Flickr, YouTube, and others.</li>
</ul>

<p>I am going to start there. I am trying to provide some simple, usable collections or relevant APIs for students are just getting started If there are any other categories, or stacks of APIs you think would be relevant for students to learn from I’d love to hear your thoughts. I’ve done a lot of writing about educational and university based APIs, but I’ve only lightly touched upon what APIs should students be learning about in the classroom.

<p>Providing ready to go API collections will be an important aspect of the implementation of any API training and curriculum effort. Having the technical details of the API readily available, as well as the less technical aspects like signing up, pricing, terms of service, privacy policies, and other relevant building blocks should also be front and center. I’ll get to work on these five API discovery collections for students. Get the title, description, and list of each API stack published as a README, then I’ll get to work on publishing the machine, and human readable details for the technology, business, and politics of using APIs.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/09/10/some-ideas-for-api-discovery-collections-that-students-can-use/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/09/07/the-path-to-production-for-department-of-veteran-affairs-va-api-applications/">The Path To Production For Department of Veteran Affairs (VA) API Applications</a></h3>
        <span class="post-date">07 Sep 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/los-angeles-downtow-freeway_blue_circuit_5.jpg" width="45%" align="right" style="padding: 15px;" />
<p>This post is part of my ongoing <a href="http://apievangelist.com/2018/08/28/reviewing-the-department-of-veterans-affairs-va-new-developer-portal/">review of the Department of Veteran Affairs (VA) developer portal and API presence</a>, moving on to where I take a closer look at their path to production process, and provide some feedback on how the agency can continue to refine the information they provide to their new developers. Helping map out the on-boarding process for any new developer, ensuring they are fully informed about what it will take to develop an application on top of VA APIs, and move those application(s) from a developer state to a production environment, and actually serving veterans.

<p>Beginning with <a href="https://github.com/department-of-veterans-affairs/vets-api-clients/blob/master/Alpha-Path-to-Production.md">the VA’s base path to production template on GitHub</a>, then pulling in some elements I found across the other APIs they have published to developer.va.gov, and finishing off with some ideas of my own, I shifted the outline for the path to production to look something like this:

<ul>
  <li><strong>Background</strong> - Keeping the background of the VA API program.</li>
  <li><strong>[API Overview]</strong> - Any information relevant to specific API(s).</li>
  <li><strong>Applications</strong> - One right now, but eventually several applications, SDK, and samples.</li>
  <li><strong>Documentation</strong> - The link, or embedded access to the API documentation, OpenAPI definition, and Postman Collection.</li>
  <li><strong>Authentication</strong> - Overview of how to authenticate with VA APIs.</li>
  <li><strong>Development Access</strong> - Provide an overview of signing up for development access.</li>
  <li><strong>Developer Review</strong> - What is needed to become a developer.
    <ul>
      <li><strong>Individual</strong> - Name, email, and phone.</li>
      <li><strong>Business</strong> - Name, URL.</li>
      <li><strong>Location</strong> - In country, city, and state.</li>
    </ul>
  </li>
  <li><strong>Application Review</strong> - What is needed to have an application(s).
    <ul>
      <li><strong>Terms of Service</strong> - In alignment with platform TOS.</li>
      <li><strong>Privacy Policy</strong> - In alignment with platform TOS.</li>
      <li><strong>Rate Limits</strong> - Aware of the rate limits that are imposed.</li>
    </ul>
  </li>
  <li><strong>Production Access</strong> - What happens once you have production access.</li>
  <li><strong>Support &amp; Engagement</strong> - Using support, and expected levels of engagement.</li>
  <li><strong>Service Level Agreement</strong> - Platform working to meet an SLA governing engagement.</li>
  <li><strong>Monthly Review</strong> - Providing monthly reviews of access and usage on platform.</li>
  <li><strong>Annual Audits</strong> - Annual audits of access, and usage, with developer and application reviews.</li>
</ul>

<p>I wanted to keep much of the content that the VA already had up there, but I also wanted to reorganize things a little bit, and make some suggestions for what might be next. Resulting in a path production section that might look a little more like this.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/09/07/the-path-to-production-for-department-of-veteran-affairs-va-api-applications/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/09/07/an-api-value-generation-funnel-with-metrics/">An API Value Generation Funnel With Metrics</a></h3>
        <span class="post-date">07 Sep 2018</span>
        <p>I’ve had several folks asking me to articulate my vision of an API centric “sales” funnel, which technically is out of my wheelhouse in the sales and marketing area, but since I do have lots opinions on what a funnel should look like for an API platform, I thought I’d take a crack at it. To help articulate what is in my head I wanted to craft a narrative, as well as a visual to accompany how I like to imagine a value generation funnel for any API platform.

<p>I envision a API-driven value generation funnel that can be tipped upside down, over and over, like an hour glass, generating value is it repeatedly pushes API activity through center, driven by a healthy ecosystem of developers, applications, and end-users putting applications to work / use. Providing a way to generate awareness and engagement with any API platform, while also ensuring a safe, reliable, and secure ecosystem of applications that encourage end-user adoption, engagement, and loyalty–further expanding on the potential for developers to continue developing new applications, and enhancing their applications to better serve end-users.

<p>I am seeing things in ten separate layers right now, something I’ll keep shifting and adjusting in future iterations, but I just wanted to get a draft funnel out the door:

<ul>
  <li><strong>Layer One</strong> - Getting folks in the top of the funnel.
    <ul>
      <li><strong>Awareness</strong> - Making people aware of the APIs that are available.</li>
      <li><strong>Engagement</strong> - Getting people engaged with the platform in some way.</li>
      <li><strong>Conversation</strong> - Encouraging folks to be part of the conversation.</li>
      <li><strong>Participation</strong> - Getting developers participating on regular basis.</li>
    </ul>
  </li>
  <li><strong>Layer Two</strong>
    <ul>
      <li><strong>Developers</strong> - Getting developers signing up and creating accounts.</li>
      <li><strong>Applications</strong> - Getting developers signing up and creating applications.<a href="https://s3.amazonaws.com/kinlane-productions2/api-value-generation-funnel.png" target="_blank"><img src="https://s3.amazonaws.com/kinlane-productions2/api-value-generation-funnel.png" align="right" width="75%" style="padding: 15px;" /></a></li>
    </ul>
  </li>
  <li><strong>Layer Three</strong>
    <ul>
      <li><strong>Sandbox Activity</strong> - Developers being active within the sandbox environment.</li>
    </ul>
  </li>
  <li><strong>Layer Four</strong>
    <ul>
      <li><strong>Certifed Developers</strong> - Certifying developers in some way to know who they are.</li>
      <li><strong>Certified Application</strong> - Certifying applications in some way to ensure quality.</li>
    </ul>
  </li>
  <li><strong>Layer Five</strong>
    <ul>
      <li><strong>Production Activity</strong> - Incentivizing production applications to be as active as possible.</li>
    </ul>
  </li>
  <li><strong>Layer Six</strong>
    <ul>
      <li><strong>Value Generation (IN / OUT)</strong> - Driving the intended behavior from all applications.</li>
    </ul>
  </li>
  <li><strong>Layer Seven</strong>
    <ul>
      <li><strong>Operational Activity</strong> - Doing what it takes internally to properly support applications.</li>
    </ul>
  </li>
  <li><strong>Layer Eight</strong>
    <ul>
      <li><strong>Audit Developers</strong> - Make sure there is always a known developer behind the application.</li>
      <li><strong>Audit Applications</strong> - Ensure the quality of each application with regular audits.</li>
    </ul>
  </li>
  <li><strong>Layer Nine</strong>
    <ul>
      <li><strong>Showcase Developers</strong> - Showcase developers as part of your wider partner strategy.</li>
      <li><strong>Showcase Applications</strong> - Showcase and push for application usage across an organization.</li>
    </ul>
  </li>
  <li><strong>Layer Ten</strong>
    <ul>
      <li><strong>Loyalty</strong> - Develop loyal users by delivering the applications that user are needing.</li>
      <li><strong>End-Users</strong> - Drive end-user growth by providing the applications end-users need.</li>
      <li><strong>Engagement</strong> - Push for deeper engagement with end-users, and the applications they use.</li>
      <li><strong>End-Usage</strong> - Incentivize the publishing and consumption of all platform resources.</li>
    </ul>
  </li>
</ul>

<p>I’m envisioning a funnel that you can turn on its head over and over and generate momentum, and kinetic energy, with the right amount of investment–the narrative for this will work in either direction. Resulting in a two-sided funnel both working in concert to generate value in the middle of the two-sided funnel.

<p>To go along with this API value generation funnel, I’m picturing the following metrics being applied to quantify what is going on across the platform, and the eleven layers:

<ul>
  <li><strong>Layer One</strong> - Unique visitors, page views, RSS subscribers, blog comments, tweets, GitHub follows, forks, and likes.</li>
  <li><strong>Layer Two</strong> - New developers who are signing up, and adding new applications to the platform.</li>
  <li><strong>Layer Three</strong> - API calls on sandbox API resources, and overall activity in the development environment.</li>
  <li><strong>Layer Four</strong> - New certified developers and applications that have been reviewed and given production access.</li>
  <li><strong>Layer Five</strong> - API calls for production API resources, understanding the overall activity across the platform.</li>
  <li><strong>Layer Six</strong> - GET, POST, PUT, DELETE on different types of resources, in different types of service plans, at different rates.</li>
  <li><strong>Layer Seven</strong> - Support requests, communication, and other new resources that have occurred in support of operations.</li>
  <li><strong>Layer Eight</strong> - Number of developers and applications audited on a regular basis ensuring quality of application catalog.</li>
  <li><strong>Layer Nine</strong> - Number of new and existing developers and applications being showcased as part of platform operations.</li>
  <li><strong>Layer Ten</strong> - Number of end-users, sessions, page views, and other activity across the applications being delivered.</li>
</ul>

<p>Providing a stack of metrics you can use to understand how well you are doing within each layer, understanding not just the metrics for a single area of your operations, but how well you are doing at building momentum, and increasing value generation. I hesitate to call this a sales funnel, because sales isn’t my jam. It is also because I do not see APIs as something you always sell–sometimes you want people contributing data and content into a platform, and not always just consuming resources. A well balanced API platform is generating value, not just selling API calls.

<p>I am not entirely happy with this API value generation funnel outline and diagram, but it is a good start, and gets me closer to what I’m seeing in my head. I’ll let it simmer for a few weeks, engage in some conversations with folks, and then take another pass at refining it. Along the way I’ll think about how I would apply to my own API platform, and actually take some actions in each area, and begin fleshing out my own funnel. I’m also going to be working on a version of it with the CMO at Streadmata.io, and a handful of other groups I’m working with on their API strategy. The more input I can get from a variety of API platforms, the more refined I can make this outline and visual of my suggested API value generation funnel.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/09/07/an-api-value-generation-funnel-with-metrics/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/09/06/my-api-storytelling-depends-on-the-momentum-from-regular-exercise-and-practice/">My API Storytelling Depends On The Momentum From Regular Exercise And Practice</a></h3>
        <span class="post-date">06 Sep 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/kin-lane/141-Post+Con+2018-Speakers.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I’ve been falling short of my normal storytelling quotas recently. I like to have at least 3 posts on API Evangelist, and <a href="https://streamdata.io/blog/">two posts on Streamdata.io each day</a>. I have been letting it slip because it was summer, but I will be getting back to my regular levels as we head into the fall. Whenever I put more coal in the writing furnace, I’m  reminded of just how much momentum all of this takes, as well as the regular exercise and practice involved, allowing me to keep pace in the storytelling marathon across my blog(s).

<p>The more stories I tell, the more stories I can tell. After eight years of doing this, I’m still surprised abut what it takes to pick things back up, and regain my normal levels of storytelling. If you make storytelling a default aspect of doing work each day, finding a way to narrate your regular work with it, it is possible to achieve high volumes of storytelling going out the door, generating search engine and social media traffic. Also, if you root your storytelling in the regular work you are already doing each day, the chances it will be meaningful enough for people to tune in only increases.

<p>My storytelling on API Evangelist is important because it helps me think through what I’m working on. It helps me become publicly accessible by generating more attention to my work, firing up new conversations, and reenforces the existing ones I’m already having. When the storytelling slows, it means I’m either doing a unhealthy amount of coding or other work, or my productivity levels are suffering overall. This makes my API storytelling a heartbeat of my operations, and a regular stream of storytelling reflects how healthy my heartbeat is from regular exercise, and usage of my words (instead of code).

<p>I know plenty of individuals, and API related operations that have trouble finding their storytelling voice. Expressing that they just don’t have the time or resources to do it properly. Regular storytelling on your blog is hard to maintain, even with the amount of experience I have. Regardless, it is something you just have to do, and you will have mandate that storytelling just becomes a default aspect of your work each day. If you work on it regularly, eventually you’ll find your voice. However, there will always be times where you lose it, and have to work to regain it again. It is just the fight you will have to fight, but ultimately if you continue, it will be totally worth it. I’m very thankful I’ve managed to keep it going for over eight years now, resulting in a pretty solid platform that enables me to do what I do.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/09/06/my-api-storytelling-depends-on-the-momentum-from-regular-exercise-and-practice/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/09/06/allowing-users-to-get-their-own-oauth-tokens-for-accessing-an-api-without-the/">Allowing Users To Get Their Own OAuth Tokens For Accessing An API Without The</a></h3>
        <span class="post-date">06 Sep 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/github/github-personal-access-token.png" width="45%" align="right" style="padding: 15px;" />
<p>I run a lot of different applications that depend on GitHub, and use GitHub authentication as the identity and access management layer for these apps. One of the things I like the most about GitHub and how I feel it handles it’s OAuth more thoroughly than most other platforms, is how they let you get you own OAuth token under your settings &gt; developer settings &gt;personal access tokens. You don’t need to setup an application, and do the whole OAuth dance, you just get a token that you can use to pass along with each API call.

<p>I operate my own OAuth server which allows me to authenticate using OAuth with many leading APIs, so generating an OAuth token, and setting up a new provider isn’t too hard. However, it is always much easier to go under my account settings, create a new personal access token for a specific purpose, and get to work playing with an API. I wish that ALL API providers did this. At first glance, it looks like <a href="https://docs.gitlab.com/ee/user/profile/personal_access_tokens.html">GitLab</a>, <a href="https://help.getharvest.com/api-v2/authentication-api/authentication/authentication/">Harvest</a>, <a href="https://developer.typeform.com/get-started/personal-access-token/">TypeForm</a>, and <a href="https://www.contentful.com/r/knowledgebase/personal-access-tokens/">ContentFul</a> all provide personal access tokens as a first option for on-boarding with their APIs. Demonstrating this is more of a pattern, than just a GitHub feature.

<p>One of these days I’m going to have to do another story documenting the entire GitHub OAuth system, because they have a lot of interesting bells and whistles that make using their platform much more secure, and just a more frictionless experience than other API providers I use on a regular basis. GitHub has ground down a lot of the sharp corners on the whole authentication experience when it comes to OAuth. It would make a nice blueprint to share, and work to convince other API providers it is a pattern worth following. Reducing the cognitive load around OAuth management for any API integration, and standardizing how API providers support their API consumers, and end-users.

<p>I have 3 separate Twitter Apps setup for specific purposes, but I wanted to have a separate personal application just for managing my person @kinlane account. I submitted a Twitter application for review, but haven’t heard back after almost a month. As a individual user of any platform, I should be able to instantly issue a personal access token that let’s me, or someone I sanction, to access my data and content on the platform. Personal access tokens should be a default feature for any consumer focused platform, putting API access more within the control of each end-user, and the platform power users.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/09/06/allowing-users-to-get-their-own-oauth-tokens-for-accessing-an-api-without-the/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/09/04/what-have-you-done-for-us-lately-api-partner-edition/">What Have You Done For Us Lately (API Partner Edition)</a></h3>
        <span class="post-date">04 Sep 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/streamdata/streamdata-api-partners-philosophy.png" width="45%" align="right" style="padding: 15px;" />
<p>I’ve been working on developing and evolving the Streamdata.io partner program, trying to move forward conversations with other service providers in the space that have existed long before I started working on things, as well as other newer relationships that I’ve helped bring in. I’m fascinated by how partner programs work, or do not work, and have <a href="http://apievangelist.com/2018/04/09/creating-a-productive-api-industry-partner-program/">invested a lot of time trying to optimize and improve how I do my own operations</a>, and assist my partners and clients in evolving and delivering on their own partner vision.

<p>It is difficult to establish, and continue meaningful and balanced partnerships between technology service and tooling providers. Sometimes providers have enough compatibility and synergy, that they are able to hit the ground running with meaningful activities that strengthen, and build partnership momentum. We are trying to establish a meaningful, yet effective way of measuring partner activity, and understanding the value that is being generated, and where reciprocity exists. Looking at the following activities produced by Streamdata.io and it’s partners:

<ul>
  <li><strong>Partner Page</strong> - Being published to both of our partner pages.</li>
  <li><strong>Testimonials</strong> - Providing quotes for each other about our services.</li>
  <li><strong>Blog Posts</strong> - Publishing blog posts about partnership and each others services.</li>
  <li><strong>White Papers</strong> - Publishing white papers or guides about partnership and each others services.</li>
  <li><strong>Press Releases</strong> - Working on join press releases about partnership and each others services.</li>
  <li><strong>Integrations</strong> - Publishing open source repositories demonstrating integration and usage of each others services.</li>
  <li><strong>Workshops</strong> - Conduct workshops for each others customers, helping deliver each others services within our ecosystems.</li>
  <li><strong>Business</strong> - Actually provide business referrals from our customers, and conversations occurring across both companies.</li>
</ul>

<p>There are other activities we like to see happening, but these eight areas represent the most common exchanges we encourage amongst our partners. The trick is always pushing for reciprocity across all these areas, help deliver on a balanced partnership, and make sure there is equal value being generated for both sides of the partnership. Each of our partners look at this list of activities differently, requiring different levels of participation, and having expectations of results set at different levels.

<p>There are some “potential partners” who don’t want to event talk about any of these items until we have that first business deal. While other partners are more than happy when we engage in these activities, but are hesitant about reciprocating on their side. We are more than happy to take the lead on many of these activities, but increasingly we are tracking on the activity on both sides of the track, to help quantify each partnership, guide our conversations, and our marketing, development, and evangelism efforts. Leaving us to ask regularly of our partners, what have you done for us lately? While also asking ourselves the same question about what we have done for our partners.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/09/04/what-have-you-done-for-us-lately-api-partner-edition/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/09/04/the-federal-agencies-who-use-their-developer-domain-gov-subdomain/">The Federal Agencies Who Use Their developer.[domain].gov Subdomain</a></h3>
        <span class="post-date">04 Sep 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/federal-goverment-portals.png" width="45%" align="right" style="padding: 15px;" />
<p>I was reviewing the new developer portal for the Department of Veterans Affairs (VA), and one of things I took notice of, was their use of the developer.va.gov subdomain. In my experience, the API efforts that invest in a dedicated subdomain, and specifically a developer dot subdomain, tend to more invested in what they are doing than efforts that publish to a subfolder, or subsection of their website. As I was writing this post, I had a question in arise in my mind, regarding how many other federal agencies use a dedicated subdomain for their developer programs–something I wanted to pick up later, and understand the landscape a little more.

<p>I took a list of current federal agency domains from the GSA and wrote a little script to append developer. to each of the domains, and conduct an HTTP status code check to see whether or not these pages existed. Here are the dedicated developer areas I found for the US federal government:

<ul>
  <li><strong>Department of Veterans Affairs (VA)</strong> - https://developer.va.gov/</li>
  <li><strong>Department of Labor</strong> - https://developer.dol.gov</li>
  <li><strong>International Trade Administration (ITA)</strong> - https://developer.trade.gov &amp; https://developer.export.gov</li>
  <li><strong>United States Patent and Trademark Office</strong> - https://developer.uspto.gov</li>
  <li><strong>National Renewable Energy Laboratory</strong> - https://developer.nrel.gov</li>
  <li><strong>Centers for Medicare &amp; Medicaid Services</strong> - https://developer.cms.gov</li>
  <li><strong>The Advanced Distributed Learning Initiative</strong> - http://developer.adlnet.gov &amp; http://developers.adlnet.gov</li>
  <li><strong>United States Environmental Protection Agency</strong> - http://developer.epa.gov</li>
  <li><strong>USA Jobs</strong> - http://developer.usajobs.gov</li>
</ul>

<p>These nine agencies have decided to invest in a subdomain for their developer portals. I have to recognize two others who provide these subdomains, but then redirect to a subsection of their websites:

<ul>
  <li><strong>National Park Service</strong> - http://developer.nps.gov redirect to https://www.nps.gov/subjects/developer/index.htm</li>
  <li><strong>Data.gov</strong> - http://developer.data.gov redirects to https://www.data.gov/developers/</li>
</ul>

<p>Additionally, there is a single domain I noticed that used the plural version of the subdomain:

<ul>
  <li><strong>Code.gov</strong> - https://developers.code.gov (plural)</li>
</ul>

<p>Along the way, I also noticed that many agencies would redirect their subdomain, and I assume all subdomains to the root of their agency’s domain. Ideally, all federal agencies would have a Github account, and publish a developer portal using Github Pages, and publish the developer.[agencydomain].gov as the address for the portal. Even if they just provide access to the agency’s data inventory, it is important to lay down the foundation for a developer platform across data, APIs, and open source software out of all federal agencies, providing a common, well-known location develop upon the government platform.

<p>As part of my larger API discovery work I am going to keep lobbying that federal agencies work to publish a common developer.[agencydomain].gov portal. It would begin to transform how applications are built if you knew that you could automatically find a government agency’s data, APIs, and open source tooling at a single location. Especially if it was something that was default across ALL federal agencies, who were also actively publishing their public data assets, entire API catalog, and showcase of open source solutions they depend on and produce.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/09/04/the-federal-agencies-who-use-their-developer-domain-gov-subdomain/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/09/04/the-basics-of-the-va-api-feedback-loop/">The Basics Of The VA API Feedback Loop</a></h3>
        <span class="post-date">04 Sep 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/va/va-github-issue-production-api-access-request.png" width="45%" align="right" style="padding: 15px;" />
<p>I’m working to break down the moving parts of API efforts over at the VA, and work to provide as much relevant feedback as I possibly can. One of the components I’m wanting to think about more is the feedback loop for the VA API efforts. The feedback loop is one of the most essential aspects of doing an API, and is quickly can become one of the most debilitating, paralyzing, and nutrient starving aspects of operating an API platform if done wrong, or non-existent. However, the feedback loop is also one of the most valuable reasons for wanting to do APIs in the first place, providing the essential feedback you will need from consumers, and the entire API ecosystem to move the API forward in a meaningful way.

<p><strong>Current Seeds Of The VA API Feedback Loop</strong><br />
Current the VA is supporting the VA API developer portal using <a href="https://github.com/department-of-veterans-affairs/vets-api-clients/issues">GitHub Issues</a> and email. I mentioned in <a href="http://apievangelist.com/2018/08/28/reviewing-the-department-of-veterans-affairs-va-new-developer-portal/">my previous review of the VA API developer portal</a> that the personal email addresses provided for email support should be generalized, sharing the load when it comes to email support for the platform. Next, I’d like to address the usage of GitHub issues for support, along with email, and step back to look at how this contributes to, or could possibly take away from the overall feedback loop for the VAPI API effort. Defining what the basics of an API feedback loop for the VA might be.

<p><strong>Expanding Upon The VA Usage Of GitHub Issues</strong><br />
I support the usage of GitHub issues for public support of any API related project. It provides a simple, observable way for anyone to get support around the VA APIs. While I’m guessing it was accidental, I like the specialization of the current repo, and usage of GitHub issues, and that it being dedicated to VA API clients and their needs. I’d encourage this type of continued focus when it comes to establishing additional feedback loops, keeping them dedicated to a specific aspect of operating on the VA API platform. It is something that might seem a little messy at first, but could easily be managed with the proper strategy, and usage of GitHub APIs, which I’ll highlight below.

<p><strong>Makes API Operations More Public And Observable</strong><br />
One of the most important reasons for using GitHub as the cornerstone of the VA API feedback loop is that it allows for transparent, observable, auditable operation of the feedback loop across the VA API platform. One of the critical aspects of the overall health of the VA API platform in the future, will be feedback loops being as open as they possibly can. Of course, there are some feedback loops that should remain private, which GitHub issues can accommodate, but whenever possible the feedback loop for the VA API platform should be in the public domain, allowing all stakeholders, veterans, and the public to actively participate in the process. In a way that can ensures every aspect of API operations is documented, and auditable, providing as much accountability as possible across VA API operations.

<p><strong>Allowing For More Modular Organization Of Feedback Loops</strong><br />
Using GitHub Issues for the deployment, management, and organization of more modular feedback loops. Treating your feedback loops just as you would your APIs, making them small, meaningful, and doing one thing and doing it well. Any GitHub repository can have its own GitHub Issues, allowing for the deployment of specialized feedback loops based upon single project that are part of different organizational groups. Beyond the modularity available when you leverage GitHub repositories, and organize them within GitHub Organizations, Github Issues can also be tagged, allowing for even more meaningful organization of feedback as it comes in, tagging and bagging for consideration as part of the road map, and other decision making processes that will be feeding off the VA API platform’s feedback loop.

<p><strong>Enabling Feedback Loop Automation With The GitHub API</strong><br />
Another benefit of using GitHub Issues as an API feedback loop building block, is that they also have an API. Allowing for the automation of all aspects of the VA API platform feedback loop(s). The GitHub API can be used to aggregate, automate, audit, and work with the Github Issues for any GitHub organization and repo the VA has access to. Providing the ability to manage not just the Github Issues for a single GitHub repository, but for the orchestration of feedback loops across many different GitHub repositories, managed by many different GitHub organizations. Establishing a distributed feedback loop system in which VA API leadership can use to coordinate with different internal, agency, partner, vendor, or public stakeholder(s) at scale, across many different projects, and components of the VA API platform.

<p><strong>Augmenting Public Feedback With Private Github Repos</strong><br />
While it is critical that as many of the feedback loops across the VA API platform are publicly accessible, and observable by everyone as possible, it is also important that there are private channels for communication around some of the components of the platform. This is another reason why GitHub Issues can work as a building block for not just public feedback loops, but also being able to operate feedback loops as well. Taking advantage of private repositories when it comes to establishing modular, automated, and private conversations to occur around certain VA API platform projects. Balancing the public aspects of the platform, with feedback loops amongst trusted groups, while still leveraging GitHub for delivering the identity and access management aspects of governing private VA feedback loops.

<p><strong>Extending Private GitHub Repos With Email Support</strong><br />
Beyond the private GitHub repositories, and using their issues to facilitate private conversations, it always makes sense to have a generalized and dedicated email account as part of the feedback loop for any API platform. Providing another private, but also a vendor neutral way of supporting the platform. People just are familiar with email, and it makes sense to have a general account that is managed by many individuals who are coordinating around platform operations. Make it easy to provide feedback around the API the VA API operations, and support anyone participating within the VA API ecosystem.

<p><strong>Auditing, Documenting, And Reporting Upon The VA Feedback Loop</strong><br />
I suggested in <a href="http://apievangelist.com/2018/08/28/reviewing-the-department-of-veterans-affairs-va-new-developer-portal/">my review of the VA API platform</a> that email should be standardized and delivered via a dedicated email account, so that multiple stakeholders can participate in support of the platform from a VA operational perspective. This way emails can be tagged, organized, and archived in support of the larger VA API feedback loop. Making sure all questions get answered, and that they are also contributed to the evolution of the platform. Something that can also be done via the automation described earlier using the GitHub API. Allowing all threads, across any project and organization to be audited, documented, and reported upon across all VA API operations. Ensuring that their is transparency, observability, and accountability across the VA API platform feedback loop.

<p><strong>Have A Strategy In Place For The VA API Feedback Loop</strong><br />
GitHub Issues and email are the two basic building blocks of any API platform, and I support the VA starting their official journey here. I think GitHub makes for an essential building block of any API platform, when used right. It just helps to have a plan in place for when a repo’s GitHub is included in the overall feedback loop framework, and the organization and prioritization of the conversation going on there. GitHub Issues spread across many different GitHub repositories, without any real strategy to how they are organized, tagged, and engaged with can seem overwhelming, and become a mess. However, with a little planning, and the establishment of even the most basic approach to managing them, can help develop a pretty robust feedback loop across the VA API platform, that follows the lead of how open source software gets delivered.

<p><strong>Consider Other API Feedback Loop Building Blocks</strong><br />
I wanted to keep this post just about the basics of the feedback loop for the VA, or for any API platform–GitHub Issues, and email. However, I’d also like suggest the consideration of some other building blocks, to help augment GitHub Issues, providing some other direct, and indirect approaches to operating the VA API platform feedback loop:

<ul>
  <li><strong>FAQ</strong> - Providing a frequently asked question that is an aggregate of all the questions that get asked across the GitHub issues, and via email.</li>
  <li><strong>Newsletter</strong> - Providing a regular channel for updating platform stakeholders, via a structured email newsletter. Offering up private, and public editions, targeting different groups.</li>
  <li><strong>Road Map</strong> - Publishing a road map regarding what is getting built across all projects included within the VA API platform perimeter, aggregating GitHub Issues that evolve as part of the feedback loop and get tagged as milestones for adding to the road map.</li>
</ul>

<p>I’m always hesitant to make suggestions about where to go next, when an organization is just getting started on their API journey. However, I think the VA team knows when to ignore my advice, and when they can cherry pick the things they want to include in their strategy. I just want to make sure I provide as much constructive criticism about what is there, and constructive feedback around what can be invested in next.

<p>Hopefully this post provides a basic overview of the VA API platform feedback loop. Expands on what they are already doing, but shines a light on some of the positive aspects of using GitHub for the VA API platform feedback loop. I was the one who worked with the former VA CIO Marina Martin (<a href="https://twitter.com/MarinaNitze">@MarinaNitze</a>) to get the the VA GitHub organization setup back in 2013. So it makes me happy to see it being used as a cornerstone of the VA API platform. I am happy to give feedback on how they can continue to put the powerful platform to such good use.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/09/04/the-basics-of-the-va-api-feedback-loop/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/09/04/remembering-that-apis-are-used-to-reduce-everything-down-to-a-transaction/">Remembering That APIs Are Used To Reduce Everything Down To A Transaction</a></h3>
        <span class="post-date">04 Sep 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/reduce+everything+to+a+transaction_tr.png" width="45%" align="right" style="padding: 15px;" />
<p>This is our regular reminder that APIs are not good, nor bad, nor neutral. They are simply a tool in our technological toolbox, and something that is often used for very dark reasons, and occasionally for good. One of the most dangerous things I’ve found about APIs is just the general thought process that goes along with them, regarding how all roads lead to reducing, and distilling things down to a single transaction. APIs, REST, microservices, and other design patterns are all about taking something from our physical world, and redefining it as something that can be transmitted back and forth using the low cost request and response infrastructure of the web.

<p>No matter what you are designing your API for, your mission is to reduce everything to a simple transaction that can be exchanged between your server, and any other system, web, mobile, device, or network application. This digital resource could be a photo of your kids, a message to your mother, the balance of your bank account, your personal thoughts going into your notebook, the latest song you listened to, your DNA, your test results for cancer, or any other piece of relevant data, content, media, object, or other resource that is being sent or received online. APIs are all about reducing all of our meaningful digital bits to the smallest possible transaction, and then daisy chaining them together to produce some desired set of results.

<p>This API-ification of everything can be a good thing. It can make our lives better, but one of the negative side effects of this reducing of everything to a transaction, is that now that transaction can be bought and sold. The digitization of everything in our lives is rarely ever about making our lives better and whatever the reasons we are told up front, and almost always are about reducing that little piece of our lives to a transaction that can be quantified, have a value place on it, and then sold individually, or in bulk with millions of other transactions. As consumers of a digital reality, we rarely see the reasons why something around us are being digitized, and API-ified so that it can transacted online, resulting in something we’ve heard a lot–that we are the product.

<p>It’s easy to believe in the potential of APIs. It is easy to get caught up in the reducing of everyday things down to transactions. It takes discipline, and the ability to stop and consider the bigger picture on a regular basis to avoid being stuck in the strong under currents of the API economy. Making sure we are regularly asking ourselves if we want this piece of our reality digitized and reduced to a transaction, and what the potential negative consequences of this element of our existence being a transaction. Thinking a little more deeply about how we’d feel if someone was buying and selling the digital bits of our life, and are we only ok with this as long as it is someone else’s bits and bytes–demonstrating that APIs are winning, and humanity is losing in this game we’ve developed online.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/09/04/remembering-that-apis-are-used-to-reduce-everything-down-to-a-transaction/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/30/why-i-feel-the-department-of-veterans-affairs-api-effort-is-so-significant/">Why I Feel The Department Of Veterans Affairs API Effort Is So Significant</a></h3>
        <span class="post-date">30 Aug 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/va/va-developer-portal.png" width="45%" align="right" style="padding: 15px;" />
<p>I have been working on API and open data efforts at the Department of Veterans Affairs (VA) for five years now. I’m passionate about pushing forward the API conversation at the federal agency because I want to see the agency deliver on its mission to take care of veterans. My father, and my step-father were both vets, and I lived through the fallout from my step-fathers two tours in Vietnam, exposure to the VA healthcare and benefits bureaucracy, and ultimately his passing away from cancer which he acquired from to his exposure to Agent Orange. I truly want to see the VA streamline as many of its veteran facing programs as they possibly can.

<p>I’ve been evangelizing for API change and leadership at the VA since I worked there in 2013. I’m regularly investing unpaid time to craft stories that help influence people I know who are working at the VA, and who are potentially reading my work. Resulting in posts like <a href="https://apievangelist.com/2017/10/26/my-response-on-the-department-of-veterans-affairs-rfi-for-the-lighthouse-api-management-platform/">my response to the VA’s RFI for the Lighthouse API management platform</a>, which included <a href="https://apievangelist.com/2018/02/24/department-of-veterans-affairs-lighthouse-platform-rfi-round-two/">a round two response a few months later</a>. Influence through storytelling is the most powerful tool I got in my API evangelist toolbox.

<p><strong>This Is An Amazon Web Services Opportunity</strong><br />
The most popular story on my blog is, “<a href="https://apievangelist.com/2012/01/12/the-secret-to-amazons-success-internal-apis/">The Secret to Amazon’s Success–Internal APIs</a>”. Which tells a story of the mythical transition of Amazon from an online commerce website to the cloud giant, who is now powering a significant portion of the web. The story is mostly fiction, but continues to be the top performing story on my blog six years later. I’ve heard endless conference talks about this subject, I’ve seen my own story framed on the wall in enterprise organizations in Europe and Australia, and as a feature link on internal IT portals. This is one of the most important stories we have in the API sector, and what is happening at the VA right now will become similar to the AWS story when we are talking about delivering government services a decade from now.

<p><strong>The VA Is Going All In On An API Vision</strong><br />
One of the reasons the VA will obtain the intended results from their API initiative is because they are going all in on APIs across the agency. The API effort isn’t just some sideshow going on in a single department or group. This API movement is being led out of the VA’s Digital Innovation center, but is being adopted, picked up, and moved forward by many different groups across the large government agency. When I did <a href="https://apievangelist.com/2018/06/18/va-lighthouse-landscape-analysis-and-roadmapping-project-report/">my landscape analysis for them</a>, I scanned as much of their public digital presence as possible in a two week timeframe, and provided them with a list of targets to go after. I see the scope of the results obtained from VA landscape analysis present in the APIs I’m seeing published to their portal, and in development by different groups, revealing in the beginnings of an agency-wide API journey.

<p><strong>The Use Of developer.va.gov Demonstrates The Scope</strong><br />
One way you can tell that the VA is going all in on an API vision, is their usage of the <a href="http://developer.va.gov">developer.va.gov</a> subdomain. This may seem like it is a trivial thing, but after looking at thousands of API operations, and monitoring some of them for eight years, the companies, organizations, institutions, and government agencies to dedicate a subdomain to their API programs are always more committed to them, and invest the appropriate amount of resources needed to be successful. These API leaders always stand out from the organizations that publish their API efforts as an entry in their help center or knowledge-base, or make it just a footnote in their online presence. The use of the developer.va.gov subdomain demonstrates the scope of investment going on over at the VA in my experience.

<p><strong>The VA Is Properly Funding Their API Efforts</strong><br />
One of the most common challenges I see API teams face is the lack of resources to do what they need to do. API teams that can’t afford to deliver across multiple stops along the API lifecycle, cutting corners on testing, monitoring, security, documentation, and other common building blocks of a successful API platform. Properly funding an API initiative, and making it a first class citizen within the enterprise is essential to success. The number one response an API platform gets rendered ineffective is due to a lack of resources to properly deliver, evangelize, and scale API operations. This condition often leaves API programs unable to effectively spread across large organizations, properly reach out to  partners, and generate the attention a public API program will need to be successful. From what I’ve seen so far, the VA is properly funding the expansion of the API vision at the agency, and will continue to do so for the foreseeable future.

<p><strong>The VA Is Providing Access To Meaningful API Resources</strong><br />
I’ve seen thousands of APIs get launched. Large enterprise always start with the safest resources possible. Learning by delivering resources that won’t cause any waves across the organization, which can be a good thing, but after a while, it is important that the resources you put forth do cause waves, and make change across an organization. The VA started with simple APIs like the VA Facilities API, but is quickly shifting gears into benefits, veteran validation, health, and other APIs that are centered around the veteran. I’m seeing the development of APIs that provide a full stack of resources that touch on every aspect of the veterans engagement with the VA. In order to see the intended results from the VA API efforts, they need to be delivering meaningful API resources, that truly make an impact on the veteran. From what I’m seeing so far, the VA is getting right at the heart of it, and delivering the useful API resources that will be needed across web, mobile, and device based applications that are serving veterans today.

<p><strong>There Is Transparency And Storytelling</strong><br />
Every one of my engagements with the VA this year has ended up on my blog. One of the reasons I stopped working within the VA back in 2013 was there were too many questions about being able to publish stories on my blog. I haven’t seen such questions of my work this year, and I’m seeing the same tone being struck across other internal and vendor efforts. The current API movement at the VA understands the significance of transparency, observability, and of doing much of the API work the VA out in the open. Sure, there is still the privacy and security apparatus that exists across the federal government, but I can see into what is happening with this API movement from the outside-in. I’m seeing the right amount of storytelling occurring around this effort, which will continue to sell the API vision internally to other groups, laterally to other federal agencies, and outwards to software vendors and contractors, as well as sufficiently involving the public throughout the journey.

<p><strong>Evolving The Way Things Get Done With Micro-Procurement</strong><br />
Two of the projects I’ve done with the VA have been micro-procurement engagements: 1) <a href="https://apievangelist.com/2018/06/18/va-lighthouse-landscape-analysis-and-roadmapping-project-report/">VA API Landscape Analysis</a>, and 2) <a href="https://apievangelist.com/2018/07/19/api-governance-models-in-the-public-and-private-sector/">VA API Governance Models In The Public And Private Sector</a>. Both of these projects were openly published to GitHub, opening up the projects beyond the usual government pool of contractors, then were awarded and delivered within two week sprints for less than $10,000.00. Demonstrating that the VA is adopting an API approach to not just changing the technical side of delivering service, but also working to address the business side of the equation. While still a very small portion of what will be needed to get things done at the VA, it reflects an important change in how technical projects can be delivered at the VA. Working to decompose and decouple not just the technology of delivering APIs at the VA, but also the business, and potentially the internal and vendor politics of delivering services at scale across the VA.

<p><strong>The VA Has Been Doing Their API Homework</strong><br />
As of the last couple of months, the VA is shifting their efforts into high gear with <a href="https://www.fbo.gov/index.php?s=opportunity&amp;mode=form&amp;id=989a6748f86f4851e30b6585b22ab137&amp;tab=core&amp;_cview=0">an API management</a>, as well as an <a href="https://www.fbo.gov/index.php?s=opportunity&amp;mode=form&amp;id=f7a6f0c12e1a4801df53d7f3fa239d93&amp;tab=core&amp;_cview=1">API development and operations</a> solicitation(s) to help invest in, and build capacity across the agency. However, before these solicitations were crafted the VA has been doing some serious homework. You can see this reflected in the RFI effort which started in 2017, and continued in 2018. <a href="https://github.com/department-of-veterans-affairs/VA-Micropurchase-Repo">You can see this reflected in the micro-procurement contracts that have been executed</a>, and are in progress as we speak. I’ve seen a solid year of the VA doing their homework before moving forward, but once they’ve started moving forward, they’ve managed to be able to shift gears rapidly because of the homework they’ve done to date.

<p><strong>Investing In An API Lifecycle And Governance</strong><br />
<img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/43_113_800_500_0_max_0_-5_-5.jpg" width="45%" align="right" style="padding: 15px;" />
I am actively contributing to, and tuning into the API strategy building going on at the VA, and I’m seeing investment into a comprehensive approach to delivering all APIs in a consistent way across a modern API lifecycle. Addressing API design, mocking, deployment, orchestration, management, documentation, monitoring, and testing in a consistent way using an OpenAPI 3.0 contract. Something that is not just allowing them to reliably deliver APIs consistently across different groups and vendors, but is also allowing them to develop a comprehensive API governance strategy to measure, report upon, and evolve their API lifecycle and strategy over time. Dialing in how they deliver services across the VA, by leveraging the development, management, and operational level capacity they are bringing to the table with the solicitations referenced above. This approach demonstrates the scope in which the VA API leadership understands what will be necessary to transform the way the VA delivers services across the massive federal agency.

<p><strong>Providing An API Blueprint For Other Agencies</strong><br />
What the VA is doing is poised to change the way the VA meets its mission. However, because it is being done in such a transparent and observable way, with every stop along the lifecycle being so well documented and repeatable, they are also developing an API blueprint that other federal agencies can follow. There are other healthy API blueprints to follow across the federal government, out of Census, Labor, NASA, CFPB, FDA, and others, but there is not an agency-wide, API definition driven, full life cycle, complete with governance blueprint available at the moment. The VA API initiative has the potential to be the blueprint for API change across the federal government, becoming the Amazon Web Services story that we’ll be referencing for decades to come. All eyes are on the VA right now, because their API efforts reflect an important change at the agency, but also an important change for how the federal government delivers services to it’s people.

<p>I am all in when it comes to support APIs at the VA. As I mentioned earlier, my primary motivation is rooted in my own experiences with the VA system, and helping it better serve veterans. My secondary motivation is all about contributing to, and playing a role in the implementation of one of the significant API platforms out there, which if done right, will change how our federal government works. I’m not trying to be hyperbolic in my storytelling around the VA API platform, I truly believe that we can do this. As always, I am working to be as honest as I can about the challenges we face, and I know that the API journey is always filled with twists and turns, but with the right amount of <a href="https://apievangelist.com/2017/07/18/diagramming-the-components-of-api-observability/">observability</a>, I believe the VA API platform can deliver on the vision being set by leadership at the agency, and why I find this work to be so significant.


        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/30/why-i-feel-the-department-of-veterans-affairs-api-effort-is-so-significant/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  

<p align="center"><a href="http://apievangelist.com/archive/"><strong>View Previous Posts Via Archives</strong></a></p>

  </div>
</section>

              <footer>
	<hr>
	<div class="features">
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
	<hr>
	<p align="center">
		relevant work:
		<a href="http://apievangelist.com">apievangelist.com</a> |
		<a href="http://adopta.agency">adopta.agency</a>
	</p>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Homepage</a></li>
    <li><a href="/blog/">Blog</a></li>
    <li><a href="/about/">About</a></li>
  </ul>
</nav>

              <section>
	<div class="mini-posts">
		<header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/tyk/tyk-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.openapis.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/openapi.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.asyncapi.com/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/asyncapi/asyncapi-horiozontal.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://json-schema.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/json-schema.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
