<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
  <a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
  <ul class="icons">
    <li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
    <li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
    <li><a href="https://www.linkedin.com/organization/1500316/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
    <li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
  </ul>
</header>

    	        <section>
	<div class="content">

	<h3>The API Evangelist Blog</h3>
	<p>This blog is dedicated to understanding the world of APIs, exploring a wide range of topics from design to deprecation, and spanning the technology, business, and politics of APIs. <a href="https://github.com/kinlane/api-evangelist" target="_blank">All of this runs on Github, so if you see a mistake, you can either fix by submitting a pull request, or let us know by submitting a Github issue for the repository</a>.</p>
	<center><hr style="width: 75%;" /></center>
	
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/06/you-have-to-know-where-all-your-apis-are-before-you-can-deliver-on-api-governance/">You Have to Know Where All Your APIs Are Before You Can Deliver On API Governance</a></h3>
        <span class="post-date">06 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/64_185_800_500_0_max_0_1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I wrote an earlier article that <a href="http://apievangelist.com/2017/12/20/basic-api-design-guidelines-are-you-first-step-towards-api-governance/">basic API design guidelines are your first step towards API governance</a>, but I wanted to introduce another first step you should be taking even before basic API design guides–cataloging all of your APIs. I’m regularly surprised by the number of companies I’m talking with who don’t even know where all of their APIs are. Sometimes, but not always, there is some sort of API directory or catalog in place, but often times it is out of date, and people just aren’t registering their APIs, or following any common approach to delivering APIs within an organization–hence the need for API governance.</p>

<p>My recommendation is that even before you start thinking about what your governance will look like, or even mention the word to anyone, you take inventory of what is already happening. Develop an org chart, and begin having conversations. Identify EVERYONE who is developing APIs, and start tracking on how they are doing what they do. Sure, you want to get an inventory of all the APIs each individual or team is developing or operating, but you should also be documenting all the tooling, services, and processes they employ as part of their workflow. Ideally, there is some sort of continuous deployment workflow in place, but this isn’t a reality in many of the organization I work with, so mapping out how things get done is often the first order of business.</p>

<p>One of the biggest failures of API governance I see is that the strategy has no plan for how we get from where we are to where we ant to be, it simply focuses on where we want to be. This type of approach contributes significantly to pissing people off right out of the gate, making API governance a lot more difficult. Stop focusing on where you want to be for a moment, and focus on where you are. Build a map of where people are, tools, services, skills, best and worst practices. Develop a comprehensive map of where organization is today, and then sit down with all stakeholders to evaluate what can be improved upon, and streamlined. Beginning the hard work of building a bridge between your existing teams and what might end up being a future API governance strategy.</p>

<p>API design is definitely the first logical step of your API governance strategy, standardizing how you design your APIs, but this shouldn’t be developed from the outside-in. It should be developed from what already exists within your organization, and then begin mapping to healthy API design practices from across the industry. Make sure you are involving everyone you’ve reached out to as part of inventory of APIs, tools, services, and people. Make sure they have a voice in crafting that first draft of API design guidelines you bring to the table. Without buy-in from everyone involved, you are going to have a much harder time ever reaching the point where you can call what you are doing governance, let alone seeing the results you desire across your API operations.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/06/you-have-to-know-where-all-your-apis-are-before-you-can-deliver-on-api-governance/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/06/riot-games-regional-endpoints/">Riot Games Regional API Endpoints</a></h3>
        <span class="post-date">06 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/riot-games/riot-games-developer-api.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m slowly categorizing all the APIs I find who are offering up some sort regional availability as part of their operations. With the easy of deployment using leading cloud services, it is something I am beginning to see more frequently. However, there is still a wide variety of reasons why an API provider will invest in this aspect of their operations, and I’m looking to understand more about what these motivations are. Sometimes it is because they are serving a global audience, and latency kills the experience, but other times I’m seeing it is more about the maturity of the API provider, and they’ve have such a large user base that they are getting more requests to deliver resources closer to home.</p>

<p><a href="https://developer.riotgames.com/regional-endpoints.html">The most recent API provider I have come across who is offering regional API endpoints is from Riot Games</a>, the makers of League of Legends, who offers <a href="https://developer.riotgames.com/regional-endpoints.html">twelve separate regions for you to chose from</a>, broken down using a variety of regional subdomains. The Riot Games API provides a wealth of meta data around their games, and while they don’t state their reasons for providing regional APIs, I’m guessing it is to make sure the meta data is localized to whichever country their customers are playing in. Reducing an latency across networks, making the overall gaming and supporting application experience as smooth and seamless as possible. Pretty standard reasons for doing regional APIs, and providing a simple example of how you do this at the DNS level.</p>

<p><a href="https://developer.riotgames.com/api-status/">RIot Games also provides a regional breakdown of the availability of their regional endpoints on their API status page</a>, adding another dimension to the regional API delivery conversation. If you are providing regional APIs, you should be monitoring them, and communicating this to your consumers. This is all pretty standard stuff, but I’m working to document every example of regional APIs I come across as part of my research. I’m considering adding a separate research area to track on the different approaches so I can publish a guide, and supporting white papers when I have enough information organized. All part of my work to understand how the API business operates, and is expanding. Showcasing how the leaders are delivering resources via APIs in a scalable way.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/06/riot-games-regional-endpoints/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/06/consistentency-and-branding-across-api-portals/">Consistency in Branding Across API Portals</a></h3>
        <span class="post-date">06 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/subway/london-underground.png" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="http://www.bbc.co.uk/programmes/b0903ppd">I recently watched a BBC documentary about the history of the branding used as part of the London Underground</a>. I’m pretty absorbed lately with using public transit as an analogy for complex API implementations, and moving beyond just using subway maps, I thought the branding strategy for the London Underground provided other important lessons for API providers. The BBC documentary went into great detail regarding how much work was put into standardizing the font, branding, and presentation of information for each London Underground, to help reduce confusion, and help riders get where they needed, and making the city operate more efficiently.</p>

<p>As I continue to study the world of <a href="http://documentation.apievangelist.com">API documentation</a>, I think we have so much work ahead of us when it comes to standardizing how we present our API portals. Right now every API portal is different, even often times with multiple portals from the same company–see Amazon Web Services for example. I think we underestimate the damage this has to the overall API experience for consumers, and why we see API documentation like Swagger UI, Slate, and Read the Docs have such an impact. However this is just documentation, and we need this to occur as part of the wider API portal user experience. I’ve seen some standardized open source API portal solutions, and there are a handful of API portal services out there, but there really is no standard for how we deliver, brand, and operate the wider API experience.</p>

<p>I have <a href="https://apievangelist.com/2015/04/10/my-minimum-viable-api-footprint-definition/">my minimum viable API portal definition</a>, and have been tracking on the common building blocks of API operations for eight years now, but there are no “plug and play” solutions that users can implement, following any single approach. I have the data, and <a href="http://portal.minimum.apievangelist.com/">I even have a simple Twitter Bootstrap version of my definition (something I’m upgrading ASAP)</a>, but in my experience people get very, very, very hung up on the visual aspects of this conversation, want different visual elements, and quickly get lost on the functional details. I’m working with my partners APIMATIC to help standardize their portal offering, but honestly it is something that needs to be wider than just me, and any single provider. It is something that needs to emerge as a common API portal standard. If we can bring this into focus, I think we will see API adoption significantly increase, reducing much of the confusion we all face getting up and running with any new API.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/06/consistentency-and-branding-across-api-portals/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/06/keeping-api-schema-simple-for-wider-adoption/">Keeping API Schema Simple For Wider Adoption</a></h3>
        <span class="post-date">06 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-csv.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>One aspect of <a href="http://apievangelist.com/2018/02/03/api-is-not-just-rest/">my talk at APIDays Paris this last week</a>, included a slide about considering to allow API consumers to negotiate CSV responses from our API. Something that would probably NEVER occur to most API providers, and probably would make many even laugh at me. I’m used to it, and don’t care. While not something that every API provider should be considering, depending on the data you are serving up, and who your target API consumer ares, it is something that might make sense. Allowing for the negotiation of CSV responses represents lowering the bar for API consumption, and widening the audience who can put our APIs to work.</p>

<p>I was doing more work around public data recently, and was introduced to an interesting look at some lessons from developing open data standards. I’m doing a deep dive into municipal data lately as part of my partnership with <a href="http://apis.how/streamdata">Streamdata.io</a>, and I found <a href="http://www.opennorth.ca/2017/12/21/from-development-to-adoption-lessons-from-three-open-standards.html">the lessons they published interesting</a>, and something that reflects my stance on API content negotiation.</p>

<blockquote>
  <p><em>From the development and maintenance of the API, it quickly became clear that adjusting scripts after every election (and by-election) and website modification, was quickly becoming unsustainable. To address this issue, a simple CSV schema was developed to encourage standardisation of this data from the outset. The schema was designed to be as simple and easy to understand and implement as possible. Comprised of just 21 fields, 7 of which are recommended fields, the schema does not have hierarchical relationships between terms and can be implemented in a single CSV file. By making the standard this simple, we were able to get a number of adopters onboard and outputting their lists of elected representatives on their own open data portals.</em></p>
</blockquote>

<p>When it comes to APIs, simplicity rules. The simpler you can make your API, the more impact you will make. Allowing for the negotiation of CSV responses from your API when possible allows API consumers to go from API to a spreadsheet in just one or two clicks. This is huge when it comes to on boarding business users with the concepts of APIs, and what they do, and allows them to easily put valuable data resources to work in their native environment–the spreadsheet. This is something many API consumers won’t understand, but when it comes to seeking meaningful API adoption, it is something that expand the reach of any API beyond the developer class, putting it within reach of business users.</p>

<p>I am a big fan of pushing our APIs to allow for the negotiation of CSV. XML, and JSON by default, whenever possible. I’m also a fan of delivering richer experiences by allowing for the negotiation of hypermedia media types. While delivering hypermedia takes a significant amount of thought and investment, allowing for the negotiation of CSV, XML, and JSON doesn’t take a lot of work. When delivering your APIs, I highly recommend thinking about who your API consumers are, and whether offering CSV responses might shift the landscape even a little bit, making your valuable data resources a little more usable by business users who won’t necessarily be delivering web or mobile applications.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/06/keeping-api-schema-simple-for-wider-adoption/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/05/api-quota-api-webhooks-and-server-sent-events-sse/">API Quota API, Webhooks, and Server-Sent Events (SSE)</a></h3>
        <span class="post-date">05 Feb 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/goldsilverfalls/creativity/file-00_00_40_84.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am profiling market data APIs as part of my partnership with <a href="http://apis.how/streamdata">Streamdata.io</a>. It is a process I enjoy, because it provides me with a number of interesting stories I can tell here on API Evangelist. Many of the APIs I profile just frustrate me, but there are always the gems who are doing interesting things with their APIs, and understand providing APIs, as well as consuming APIs. One API that I’ve been profiling, and I am able to put to use in my work to build a gallery of real time data APIs, was <a href="https://1forge.com">1Forge</a>.</p>

<p><a href="https://1forge.com/forex-data-api/api-documentation">1Forge provides dead simple APIs for accessing market data</a>, and surprise!! – you can sign up for a key, and begin making API calls within minutes. It might not sound like that big of a deal, but after going through 25+ APIs, I only have about 5 API keys. I’m working on an OpenAPI definition for 1Forge, so I can begin to poll, and stream the data they make available, including it in the Streamdata.io API gallery I’m building. However, as I was getting up and running with the API, I noticed <a href="https://1forge.com/forex-data-api/api-documentation">their quota endpoint</a>, which allows me to check my usage quote with the 1Forge API–something that I thought was story worthy.</p>

<p>The idea of an endpoint to check my applications usage quota for an API seems like a pretty fundamental concept, but sadly it is something I do not see very often. It is something that should be default for ALL APIs, but additionally I’d like to see a webhook for, letting me know when my API consumption reaches different levels. Since I’m talking about Streamdata.io, it would also make sense to offer a Server-Sent Event (SSE) for the API quote endpoint, allowing me to bake the usage quota for all the APIs I depend on into a single API dashboard–streaming real time usage information across the APIs depend on, and maybe displaying things in RED when I reach certain levels.</p>

<p>An API quota API is useful for when you depend on a single API, but is something that becomes almost critical when it comes to depending on many APIs. These are one of those APIs that API providers are going to need to realize has to be present by default for their API platforms. It is something that can keep us humans in tune with our consumption, but more importantly can help us programmatically manage our API consumption, and adjust our polling frequency automatically as reach the limits of our API access tier, or even upgrade as we realize our rate limit constraints are too tight for a specific application. I’m going to add an API quota API to my list of default administrative APIs that API providers should be offering. Updating the default set of resources we should have available for ALL APIs we are operating.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/05/api-quota-api-webhooks-and-server-sent-events-sse/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/05/the-more-we-know-about-you-the-more-api-acces-you-get/">The More We Know About You The More API Access You Get</a></h3>
        <span class="post-date">05 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/65_144_800_500_0_max_0_1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="http://apievangelist.com/2018/01/24/where-am-i-in-the-sales-funnel-for-your-api/">I’ve been trash talking APIs that identify me as part of some sort of sales funnel</a>, and automate the decision around whether or not I get access to their API. My beef isn’t with API providers profiling me and making decisions about how much access I get, it is about them limiting profiles making it so I do not get access to their APIs at all. Their narrow definitions of the type of API consumers they are seeking does not include me, even though I have thousands of regular readers of my blog who do fit their profile. In the end, it is their loss, not mine, that they do not let me in, but the topic is still something I feel should be discussed out in the open, hopefully expanding the profile definitions for some API providers who may not have considered the bigger picture.</p>

<p>I’ve highlighted the limiting profiling of API consumers that prevent access to APIs, but now I want to talk about how profiling can be sensibly used to limit access to API resources. Healthy API management always has an entry level tier, but what tiers are available after that often depend on a variety of other data points. One thing I see API providers regularly doing is requiring API consumers to provide more detail about who they are and what they are doing with an API. I don’t have any problem with API providers doing this, making educated and informed decisions regarding who an API consumer is or isn’t. As the API Evangelist I am happy to share more data points about me to get more access. I don’t necessarily want to do this to sign up for your entry level access tier, just so I can kick the tires, but if I’m needing deeper access, I am happy to fill our a fuller profile of myself, and what I am working on.</p>

<p>Stay out of my way when it comes to getting started and test driving your APIs. However, it is perfectly acceptable to require me to disclose more information, require me to reach out an connect with your team, and other things that you feel are necessary before giving me wider access to your APIs, and provide me with looser rate limits. I encourage API providers to push on API consumers before you give away the keys to the farm. Developing tiered levels of access is how you do this. Make me round off the CRM entry for my personal profile, as well as my company. Push me to validate who I am, and that my intentions are truly honest. I encourage you to reach out to each one of your API consumers with an honest “hello” email after I sign up. Don’t require me to jump on the phone, or get pushy with sales. However, making sure I provide you with more information about myself, my project and company in exchange for higher levels of API access is a perfectly acceptable way of doing business with APIs.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/05/the-more-we-know-about-you-the-more-api-acces-you-get/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/05/headers-used-for-grpc-over-http2/">Learning About The Headers Used for gRPC over HTTP/2</a></h3>
        <span class="post-date">05 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-grpc.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am learning more about gRPC and HTTP/2, as part of the recent expansion of my API toolbox. I’m not a huge fan of Protocol Buffers, however I do get the performance gain they introduce, but I am very interested in learning more about how HTTP/2 is being used as a transport. While I’ve been studying how websockets, Kafka, MQTT, and other protocols have left the boundaries of HTTP and are embracing the performance gains available in the pure TCP realm, I’m more intrigued by the next generation of HTTP as a transport.</p>

<p>Part of my learning process is all about understanding the headers available to us in the HTTP/2 realm. I’ve been learning more about the next generation HTTP headers from the <a href="https://github.com/grpc/grpc/blob/master/doc/PROTOCOL-HTTP2.md">gRPC Github repository</a> which provides details on the request and response headers in play.</p>

<p><strong>HTTP/2 API Request Headers</strong></p>

<ul>
  <li>Request-Headers → Call-Definition *Custom-Metadata</li>
  <li>Call-Definition → Method Scheme Path TE [Authority] [Timeout] Content-Type [Message-Type] [Message-Encoding] [Message-Accept-Encoding] [User-Agent]</li>
  <li>Method → “:method POST”</li>
  <li>Scheme → “:scheme “ (“http” / “https”)</li>
  <li>Path → “:path” “/” Service-Name “/” {method name} # But see note below.</li>
  <li>Service-Name → {IDL-specific service name}</li>
  <li>Authority → “:authority” {virtual host name of authority}</li>
  <li>TE → “te” “trailers” # Used to detect incompatible proxies</li>
  <li>Timeout → “grpc-timeout” TimeoutValue TimeoutUnit</li>
  <li>TimeoutValue → {positive integer as ASCII string of at most 8 digits}</li>
  <li>TimeoutUnit → Hour / Minute / Second / Millisecond / Microsecond / Nanosecond</li>
  <li>Hour → “H”</li>
  <li>Minute → “M”</li>
  <li>Second → “S”</li>
  <li>Millisecond → “m”</li>
  <li>Microsecond → “u”</li>
  <li>Nanosecond → “n”</li>
  <li>Content-Type → “content-type” “application/grpc” [(“+proto” / “+json” / {custom})]</li>
  <li>Content-Coding → “identity” / “gzip” / “deflate” / “snappy” / {custom}</li>
  <li>Message-Encoding → “grpc-encoding” Content-Coding</li>
  <li>Message-Accept-Encoding → “grpc-accept-encoding” Content-Coding *(“,” Content-Coding)</li>
  <li>User-Agent → “user-agent” {structured user-agent string}</li>
  <li>Message-Type → “grpc-message-type” {type name for message schema}</li>
  <li>Custom-Metadata → Binary-Header / ASCII-Header</li>
  <li>Binary-Header → {Header-Name “-bin” } {base64 encoded value}</li>
  <li>ASCII-Header → Header-Name ASCII-Value</li>
  <li>Header-Name → 1*( %x30-39 / %x61-7A / “_” / “-“ / “.”) ; 0-9 a-z _ - .</li>
  <li>ASCII-Value → 1*( %x20-%x7E ) ; space and printable ASCII</li>
</ul>

<p><strong>HTTP/2 API Response Headers</strong></p>

<ul>
  <li>Response → (Response-Headers *Length-Prefixed-Message Trailers) / Trailers-Only
Response-Headers → HTTP-Status [Message-Encoding] [Message-Accept-Encoding] Content-Type *Custom-Metadata</li>
  <li>Trailers-Only → HTTP-Status Content-Type Trailers</li>
  <li>Trailers → Status [Status-Message] *Custom-Metadata</li>
  <li>HTTP-Status → “:status 200”</li>
  <li>Status → “grpc-status” 1*DIGIT ; 0-9</li>
  <li>Status-Message → “grpc-message” Percent-Encoded</li>
  <li>Percent-Encoded → 1*(Percent-Byte-Unencoded / Percent-Byte-Encoded)</li>
  <li>Percent-Byte-Unencoded → 1*( %x20-%x24 / %x26-%x7E ) ; space and VCHAR, except %</li>
  <li>Percent-Byte-Encoded → “%” 2HEXDIGIT ; 0-9 A-F</li>
</ul>

<p>I’m enjoying getting down to the nitty gritty details of how HTTP/2 works. I’m intrigued by the multi-directionality of it. Being able to use just like HTTP/1.1 with simple requests and responses, but also being able to introduce bi-directional API calls, where you can make many different API calls as you want. I don’t think I will get any time to play with in the near future. I have way too much work. However, I do like learning about how it is being used, and I think Google is the most forward thinking when it comes to HTTP/2 adoption in the API sector–providing multi-speed APIs in JSON using HTTP/1.1, or Protocol Buffers using HTTP/2.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/05/headers-used-for-grpc-over-http2/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/05/i-appreciate-you-wanting-to-jump-on-the-phone-but-i-have-other-apis-to-test-drive/">I Appreciate The Request To Jump On Phone But I Have Other APIs To Test Drive</a></h3>
        <span class="post-date">05 Feb 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/desertroad/clean_view/file-00_00_00_00.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="http://apis.how/streamdata">Streamdata.io</a> is investing in my <a href="http://theapistack.com">API Stack</a> work as we build out their API Gallery of valuable data streaming APIs. I’m powering through hundreds of APIs and using my approach to profiling APIs that I have been developing over the last eight years of operating API Evangelist. I have a large number of APIs to get through, so I don’t have a lot of time to spend on each API. I am quickly profiling and ranking them to quickly identify which one’s are worth my time. While there are many elements that get in the way of me actually being able to obtain an API key and begin using an API, one of the more frustrating elements when API providers require me to jump on the phone with them before I can test drive any APIs.</p>

<p>I’ve encountered numerous APIs that require me talk to a sales person before I can do anything. I know that y’all think this is savvy. This is how business is done these days, but it just isn’t the way you start conversations with API consumers. Sure, there should be support channels available when I need them, but it SHOULD NOT be the way you begin a conversation with us API consumers. I’ve heard all the reasons possible for why companies feel like they need to do this, and I guarantee that all of them are based upon out of date perspectives around what APIs are all about. Often times they are a bi-product of not having a modern API management solution in place, and a team that lacks a wider awareness of the API sector and how API operations works.</p>

<p>In 2018, I shouldn’t have to talk to you on the phone to understand what your API does, and how it fits into what I’m working on. Most of the time I do not even know what I’m working on. I’m just kicking the tires, seeing what is possible, and considering how it fits into my bigger picture. What good does it do for me to jump on the phone if I don’t even know what I’m working on? I can’t tell you much. You can’t share API responses with me. You will able to do less than if you just give me access to APIs, and allow me to make API calls. You don’t have to allow me to make too many calls, just a handful to get going. You don’t even have to give me access to ALL the APIs, just enough of them to wet my appetite and help me understand what it is that you do. This is done using modern API management solutions, and service composition. Giving you the control over exactly how mcuh of your resources I will have access to, until I prove myself worthy.</p>

<p>The APIs I come across that require me to jump on sales call will have to wait until later. I just won’t have the time to evaluate their value, and understand where they fit into my work. Which means they probably won’t ever make it into my project, or any of my storytelling around the work. Which means many of these APIs will not get the free exposure to my readers, helping them understand what is possible. It is just one of many self-inflicted wounds API providers make along the way when they leave their enterprise blinders on, and are too restrictive around their API resources. Sales still has a place in the API game, but the overall API strategy has significantly evolved in the last five years, and is something that is pretty easy to see if you spend time playing with other leading APIs on the market. Demonstrating that these providers probably haven’t done much due diligence about what is out there, which often is just yet another symptom of a poorly run API program, making passing on it probably a good idea.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/05/i-appreciate-you-wanting-to-jump-on-the-phone-but-i-have-other-apis-to-test-drive/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/03/api-is-not-just-rest/">API Is Not Just REST</a></h3>
        <span class="post-date">03 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-days-paris-2018-api-is-not-just-rest.png" align="right" width="40%" style="padding: 15px;" /></p>
<p><em>This is one of my talks from <a href="http://www.apidays.io/events/paris-2017">APIDays Paris 2018</a>. Here is the abstract: The modern API toolbox includes a variety of standards and methodologies, which centers around REST,  but also includes Hypermedia, GraphQL, real time streaming, event-driven architecture , and gRPC. API design has pushed beyond just basic access to data, and also can be about querying complex data structures, providing experience rich APIs, real-time data streams with Kafka and other standards, as well as also leveraging the latest algorithms and providing access to machine learning models. The biggest mistake any company, organization, or government agency can do is limit their API toolbox to be just about REST. Learn about a robust and modern API toolbox from the API Evangelist, Kin Lane.</em></p>

<p><strong>Diverse Toolbox</strong><br />
After eight years of evangelizing APIs, when I participate in many API conversations, some people still assume I’m exclusively talking about REST as the API Evangelist–when in reality I am simply talking about APIs that leverage the web. Sure, REST is a dominant design pattern I shine a light on, and has enjoyed a significant amount of the spotlight over the last decade, but in reality on the ground at companies, organizations, institutions, and government agencies of all shapes and sizes, I find a much more robust API toolbox is required to get the job done. REST is just one tool in my robust and diverse toolbox, and I wanted to share with you what I am using in 2018.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-1.png" /></p>

<p>The toolbox I’m referring tool isn’t just about what is needed to equip an API architect to build out the perfect vision of the future. This is a toolbox that is equipped to get us from present day into the future, acknowledging all of the technical debt that exists within most organizations which many are looking to evolve as part of their larger digital transformation. My toolbox is increasingly pushing the boundaries of what I’ve historically defined as an API, and I’m hoping that my experiences will also push the boundaries of what you define as an API, making you ready for what you will encounter on the ground within organizations you are delivering APis within.</p>

<p><strong>Application Programming Interface</strong><br />
API is an acronym standing for application programming interface. I do not limit the scope of application in the context to just be about web or mobile application. I don’t even limit it to the growing number of device-based applications I’m seeing emerge. For me, application is about applying the digital resources made available via an programmatic interface. I’m looking to take the data, content, media, and algorithms being made available via APIs and apply them anywhere they are needed on the web, within mobile and device applications, or on the desktop, via spreadsheets, digital signage, or anywhere else that is relevant, and sensible in 2018.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-2.png" /></p>

<p>API does not mean REST. I’m really unsure how it got this dogmatic association, nor do I care. It is an unproductive legacy of the API sector, and one I’m looking to move beyond. Application programming interfaces aren’t the solution to every digital problem we face. They are about understanding a variety of protocols, messaging formats, and trying to understand the best path forward depending on your application of the digital resources you are making accessible. My API toolbox reflects this view of the API landscape, and is something that has significantly evolved over the last decade of my career, and is something that will continue to evolve, and be defined by what I am seeing on the ground within the companies, organizations, institutions, and government agencies I am working with.</p>

<p><strong>SOAP</strong><br />
I have been working with databases since 1987, so I fully experienced the web services evolution of our industry. During the early years of the web, there was a significant amount of investment  into thinking about how we exchanged data across many industries, as well as within individual companies when it came to building out the infrastructure to deliver upon this vision. The web was new, but we did the hard work to understand how we could make data interoperability in a machine readable way, with an emphasis on the messages we were exchanging. Looking back I wish we had spent more time thinking about how we were using the web as a transport, as well as the influence of industry and investment interests, but maybe it wasn’t possible as the web was still so new.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-3.png" /></p>

<p>While web services provided a good foundation for delivering application programming interfaces, it may have underinvested in its usage of the web as a transport, and became a victim of the commercial success of the web. The need to deliver web applications more efficiently, and a desire to hastily use the low cost web as a transport quickly bastardized and cannibalized web services, into a variety of experiments and approaches that would get the job done with a lot less overhead and friction. Introducing efficiencies along the way, but also fragmenting our enterprise toolbox in a way which we are still putting back together.</p>

<p><strong>XML &amp; JSON RPC</strong><br />
One of the more fractious aspects of the web API evolution has been the pushback when API providers call their XML or JSON remote procedure call (RPC) APIs, RESTful, RESTish, or other mixing of philosophy and ideology, which has proven to be a dogma stimulating event. RESTafarians prefer that API providers properly define their approach, while many RPC providers could care less about labels, and are looking to just get the job done. Making XML and JSON RPC a very viable approach to doing APIs, something that still persists almost 20 years later.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-4.png" /></p>

<p>Amazon Web Services, Flickr, Slack, and other RPC APIs are doing just fine when it comes to getting the job done, despite the frustration, ranting, and shaming by the RESTafarians. It isn’t an ideal approach to delivering programmatic interfaces using the web, but it reflects its web roots, and gets the job done with low cost web infrastructure. RPC leaves a lot of room for improvement, but is a tool that has to remain in the toolbox. Not because I am designing new RPC APIs, but there is no doubt that at some point I will have to be integrating with an RPC API to do what you need to get done in my regular work.</p>

<p><strong>REST at Center</strong><br />
Roy Fielding’s dissertation on representational state transfer, often referred to as simple REST, is an amazing piece of work. It makes a lot of sense, and I feel is one of the most thorough looks at how to use the web for making data, content, media, and algorithms accessible in a machine readable way. I get why so many folks feel it is the RIGHT WAY to do things, and one of the reasons it is the default approach for many API designers and architects–myself included. However, REST is a philosophy, and much like microservices, provides us with a framework to think about how we put our API toolbox to work, but isn’t something that should blind us from the other tools we have within our reach.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-5.png" /></p>

<p>REST is where I begin most conversations about APIs, but it doesn’t entirely encompass what I mean when every time I use the phrase API. I feel REST has given me an excellent base for thinking about how I deliver APIs, but will slow my effectiveness if I leave my REST blinders on, and let dogma control the scope of my toolbox. REST has shown me the importance of the web when talking about APIs, and will continue to drive how I deliver APIs for many years. It has shown me how to structure, standardize, and simplify how I do APIs, and help my applications reach as wide as possible audience, using commonly understood infrastructure.</p>

<p><strong>Negotiating CSV</strong><br />
As the API Evangelist, I work with a lot of government, and business users. One thing I’ve learned working with this group is the power of using comma separated values (CSV) as a media type. I know that us developers and database folks enjoy a lot more structure in our lives, but I have found that allowing for the negotiation of CSV responses from APIs, can move mountains when it comes to helping onboard business users, and decision makers to the potential of APIs–even if the data format doesn’t represent the full potential of an API. CSV responses is the low bar I set for my APIs, making them accessible to a very wide business audience.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-6.png" /></p>

<p>CSV as a data format represents an anchor for the lowest common denominator for API access. As a developer, it won’t be the data format I personally will negotiate, but as a business user, it very well could mean the difference between using an API or not. Allowing me to take API responses and work with them in my native environment, the Excel spreadsheet, or Google Sheets environment. As I am designing my APIs, I’m always thinking about how I can make my resources available to the masses, and enabling the negotiation of CSV responses whenever possible, helps me achieve my wier objectives.</p>

<p><strong>Negotiating XML</strong><br />
I remember making the transition from XML to JSON in 2009. At first I was uncomfortable with the data format, and resisted using it over my more proven XML. However, I quickly saw the potential for the scrappy format while developing JavaScript applications, and when developing mobile applications. While JSON is my preferred, and default format for API design, I am still using XML on a regular basis while working with legacy APIs, as well as allowing for XML to be negotiated by the APIs I’m developing for wider consumption beyond the startup community. Some developers are just more comfortable using XML over JSON, and who knows, maybe by extending an XML olive branch, I might help developers begin to evolve in how they consume APIs.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-7.png" /></p>

<p>Similar to CSV, XML represents support for a wider audience. JSON has definitely shifted the landscape, but there are still many developers out there who haven’t made the shift. Whether we are consumers of their APIs, or providing APIs that target these developers, XML needs to be on the radar. Our toolbox needs to still allow for us to provide, consume, validate, and transform XML. If you aren’t working with XML at all in your job, consider yourself privileged, but also know that you exist within a siloed world of development, and you don’t receive much exposure to many systems that are the backbone of government and business.</p>

<p><strong>Negotiating JSON</strong><br />
I think about my career evolution, and the different data formats I’ve used in 30 years. It helps me see JSON as the default reality, not the default solution. It is what is working now, and reflects not just the technology, but also the business and politics of doing APIs in a mobile era, where JavaScript is widely used for delivering responsive solutions via multiple digital channels. JSON speaks to a wide number of developers, but we can’t forget that it is mostly comprised of developers who have entered the sector in the last decade.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-8.png" /></p>

<p>JSON is the default media type I use for any API I’m developing today. No matter what my backend data source is. However, it is just one of several data formats I will potentially open up for negotiation. I feel like plain JSON is lazy, and whenever possible I should be thinking about a wider audience by providing CSV and XML representations, but I should also be getting more structured and standardized in how I handle the requests and responses for my API. While I want my APIs to reach as wide as possible audience, I also want them to deliver rich results that best represents the data, content, media, algorithms, and other digital resources I’m serving up.</p>

<p><strong>Hypermedia Media Types</strong><br />
Taking the affordances present when humans engage with the web via browsers for granted is one of the most common mistakes I make as an API design, developer, and architect. This is a shortcoming I am regularly trying to make up for by getting more sophisticated in my usage of existing media types, and allowing for consumers to negotiate exactly the content they are looking for, and achieve a heightened experience consuming any API that I deliver. Hypermedia media types provide a wealth of ways to deliver consistent experiences, that help be deliver many of the affordances we expect as we make use of data, content, media, and algorithms via the web.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-9.png" /></p>

<p>Using media types like Hal, Siren, JSON API, Collection+JSON, and JSON-LD are allowing me to deliver a much more robust API experience, to a variety of API clients. Hypermedia reflects where I want to be when it comes to API design and architecture that leverages the web, but it is a reflection I have to often think deeply about as I still work to reach out to a wide audience, forcing me to make it one of several types of experience my consumers can negotiate. While I wish everyone saw the benefits, sometimes I need to make sure CSV, XML, and simpler JSON are also on the menu, ensuring I don’t leave anyone behind as I work to bridge where we are with where I’d like to go.</p>

<p><strong>API Query Layers</strong><br />
Knowing my API consumers is an important aspect of how I use my API toolbox. Depending on who I’m targeting with my APIs, I will make different decisions regarding the design pattern(s) I put to work. While I prefer investing resources into the design of my APIs, and crafting the URLs, requests, and responses my consumers will receive, in some situations my consumers might also need much more control over crafting the responses they are getting back. This is when I look to existing API query languages like Falcor or GraphQL to give my API consumers more of a voice in what their API responses will look like.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-10.png" /></p>

<p>API query layers are never a replacement for a more RESTful, or hypermedia approaches to delivering web APIs, but they can provide a very robust way to hand over control to consumers. API design is important for providers to understand, and define the resources they are making available, but a query language can be very powerful when it comes to making very complex data and content resources available via a single API URL. Of course, as with each tool present in this API toolbox, there are trade offs with deciding to use an API query language, but in some situations it can make the development of clients much more efficient and agile, depending on who your audience is, and the resources you are looking to make available.</p>

<p><strong>Webhooks</strong><br />
In my world APIs are rarely a one way street. My APIs don’t just allow API consumers to poll for data, content, and updates. I’m looking to define and respond to events, allowing data, and content to be pushed to consumers. I’m increasingly using Webhooks as a way to help my clients make their APIs a two-way street, and limit the amount of resources it takes to make digital assets available via APIs. Working with them to define the meaningful events that occur across the platform, and allow API consumers to subscribe to these events via Webhooks. Opening the door for API providers to deliver a more event-driven approach to doing APIs.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-11.png" /></p>

<p>Webhooks are the 101 level of event-driven API architecture for API providers. It is where you get started trying to understand the meaningful events that are occurring via any platform. Webhooks are how I am helping API providers understand what is possible, but also how I’m training API consumers in a variety of API communities about how they can deliver better experiences with their applications. I see webhooks alongside API design and management, as a way to help API providers and consumers better understand how API resources are being used, developing a wider awareness around which resources actually matter, and which ones do not.</p>

<p><strong>Websub</strong><br />
In 2018, I am investing more time in putting Websub, formerly known as the word which none of us could actually pronounce, PubSubHubbub. This approach to making content available by subscription as things change has finally matured into a standard, and reflects the evolution of how we deliver APIs in my opinion. I am using Websub to help me understand not just the event-driven nature of the APIs I’m delivering, but also that intersection of how we make API infrastructure more efficient and precise in doing what it does. Helping us develop meaningful subscriptions to data and content, that adds another dimension to the API design and even query conversation.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-12.png" /></p>

<p>Websub represents the many ways we can orchestrate our API implementations using a variety of content types, push and pull mechanisms, all leveraging web as the transport. I’m intrigued by the distributed aspect of API implementations using Websub, and the discovery that is built into the approach. The remaining pieces are pretty standard API stuff using GETs, POSTs, and content negotiation to get the job done. While not an approach I will be using by default, for specific use cases, delivering data and content to known consumers, I am beginning to put Websub to work alongside API query languages, and other event-driven architectural approaches. Now that Websub has matured as a standard, I’m even more interested in leveraging it as part of my diverse API toolbox.th</p>

<p><strong>Server Sent Events (SSE)</strong><br />
I consider webhooks to be the gateway drug for API event-driven architecture. Making API integrations a two street, while also making them more efficient, and potentially real time. After webhooks, the next tool in my toolbox for making API consumption more efficient and real time are server-sent events (SSE). Server-sent events (SSE) is a technology where a browser receives automatic updates from a server via a sustained HTTP connection, which has been standardized as part of HTML5 by the W3C. The approach is primarily used to established a sustained connection between a server, and the browser, but can just as easily be used server to server.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-13.png" /></p>

<p>Server-sent events (SSE) delivers one-way streaming APIs which can be used to send regular, and sustained updates, which can be more efficient than regular polling of an API. SSE is an efficient way to begin going beyond the basics of client-server request and response model and pushing the boundaries of what APIs can do. I am using SSE to make APIs much more real time, while also getting more precise with the delivery of data and content, leverage other standards like JSON Patch to only provide what has changed, rather than sending the same data out over the pipes again, making API communication much more efficient.</p>

<p><strong>Websockets</strong><br />
Shifting things further into real time, websockets is what I’m using to deliver two-way API streams that require data be both sent and received, providing full-duplex communication channels over a single TCP connection. WebSocket is a different TCP protocol from HTTP, but is designed to work over HTTP ports 80 and 443 as well as to support HTTP proxies and intermediaries, making it compatible with the HTTP protocol. To further achieve compatibility, the WebSocket handshake uses the HTTP Upgrade header to change from the HTTP protocol to the WebSocket protocol, pushing the boundaries of APIs beyond HTTP in a very seamless way.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-14.png" /></p>

<p>SSE is all about the one-way efficiency, and websockets is about two-way efficiency. I prefer keeping things within the realm of HTTP with SSE, unless I absolutely need the two-way, full-duplex communication channel. As you’ll see, I’m fine with pushing the definition of API out of the HTTP realm, but I’d prefer to keep things within bounds, as I feel it is best to embrace HTTP when doing business on the web. I can accomplish a number of objectives for data, content, media, and algorithmic access using the HTTP tools in my toolbox, leaving me to be pretty selective when I push things out of this context.</p>

<p><strong>gRPC Using HTTP/2</strong><br />
While I am forced to use Websockets for some existing integrations such as with Twitter, and other legacy implementations, it isn’t my choice for next generation projects. I’m opting to keep things within the HTTP realm, and embracing the next evolution of the protocol, and follow Google’s lead with gRPC. As with other RPC approaches, gRPC is based around the idea of defining a service, specifying the methods that can be called remotely with their parameters and return types. gRPC embraces HTTP/2 as its next generation transport protocol, and while also employing Protocol Buffers, Google’s open source mechanism for the serialization of structured data.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-15.png" /></p>

<p>At Google, I am seeing Protocol Buffers used in parallel with OpenAPI for defining JSON APIs, providing two speed APIs using HTTP/1.1 and HTTP/2. I am also seeing Protocol Buffers used with HTTP/1.1 as a transport, making it something I have had to integrated with alongside SOAP, and other web APIs. While I am integrating with APIs that use Protocol Buffers, I am most interested in the usage of HTTP/2 as a transport for APIs, and I am investing more time learning about the next generation headers in use, and the variety of approaches in which HTTP/2 is used as a transport for traditional APIs, as well as multi-directional, streaming APIs.</p>

<p><strong>Apache Kafka</strong><br />
Another shift I could not ignore across the API landscape in 2017 was the growth in adoption of Kafka as a distributed streaming API platform. Kafka focuses on enabling providers to read and write streams of data like a messaging system, and develop applications that react to events in real-time, and store data safely in a distributed, replicated, fault-tolerant cluster. Kafka was originally developed at LinkedIn, but is now an Apache open source product that is in use across a number of very interesting companies, many of which have been sharing their stories of how efficient it is for developing internal data pipelines. I’ve been studying Kafka throughout 2017, and I have added it to my toolbox, despite it pushing the boundaries of my definition of what is an API beyond the HTTP realm.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-16.png" /></p>

<p>Kafka has moved out of the realm of HTTP, using a binary protocol over TCP, defining all APIs as request response message pairs, using its own messaging format. Each client initiates a socket connection and then writes a sequence of request messages and reads back the corresponding response message–no handshake is required on connection or disconnection. TCP is much more efficient over HTTP because it allows you to maintain persistent connections used for many requests. Taking streaming APIs to new levels, providing a super fast set of open source tools you can use internally to deliver the big data pipeline you need to get the job done. My mission is to understand how these pipelines are changing the landscape and which tools in my toolbox can help augment Kafka and deliver the last mile of connectivity to partners, and public applications.</p>

<p><strong>Message Queuing Telemetry Transport (MQTT)</strong><br />
Continuing to round off my API toolbox in a way that pushes the definition of APIs beyond HTTP, and helping me understand how APIs are being used to drive Internet-connected devices, I’ve added Message Queuing Telemetry Transport (MQTT), an ISO standard for implementing publish-subscribe-based messaging protocol to my toolbox. The protocol works on top of the TCP/IP protocol, and is designed for connections with remote locations where a light footprint” is required because compute, storage, or network capacity is limited. Making MQTT optimal for considering when you are connecting devices to the Internet, and unsure of the reliability of your connection.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-17.png" /></p>

<p>Both Kafka, and MQTT have shown me in the last couple of years, the limitations of HTTP when it comes to the high and low volume aspects of moving data around using networks. I don’t see this as a threat to APIs that leverage HTTP as a transport, I just see them as additional tools in my toolbox, for projects that meet these requirements. This isn’t a failure of HTTP, this is simply a limitation, and when I’m working on API projects involving internet connected devices I’m going to weight the pros and cons of using simple HTTP APIs, alongside using MQTT, and being a little more considerate about the messages I’m sending back and forth between devices and the cloud over the network I have in place. MQTT reflects my robust and diverse API toolbox, as one that gives me a wide variety of tools I’m familiar with and can use in different environments.</p>

<p><strong>Mastering My Usage Of Headers</strong><br />
One thing I’ve learned over the years while building my API toolbox is the importance of headers, and they are something that have regularly been not just about HTTP headers, but the more general usage of network networks. I have to admit that I understood the role of headers in the API conversation, but had not fully understood the scope of their importance when it comes to taking control over how your APIs operate within a distributed environment. Knowing which headers are required to consume APIs is essential to delivering stable integrations, and providing clear guidance on headers from a provider standpoint is essential to APIs operating as expected on the open web.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-18.png" /></p>

<p>Content negotiation was the header doorway I walked through that demonstrated the importance of HTTP headers when it comes to deliver the meaningful API experiences. Being able to negotiate CSV, XML, and JSON message formats, as well as being able to engage with my digital resources in a deeper way using hypermedia media types. My headers mastery is allowing me to better orchestrate an event-driven experience via webhooks, and long running HTTP connections via Server-Sent Events. They are also taking me into the next generation of connectivity using HTTP/2, making them a critical aspect of my API toolbox. Historically, headers have often been hidden in the background of my API work, but increasingly they are front and center, and essential to me getting the results I’m looking for.</p>

<p><strong>Standardizing My Messaging</strong><br />
I have to admit I had taken the strength of message formats present in my web service days for granted. While I still think they are bloated and too complex, I feel like we threw out a lot of benefits when we made the switch to more RESTful APIs. Overall I think the benefits of the evolution were positive, and media types provide us with some strong ways to standardize the messages we pass back and forth. I’m fine operating in a chaotic world of message formats and schema that are developed in the moment, but I’m a big fan of all roads leading to standardization and reuse of meaningful formats, so that we can try to speak with each other via APIs in more common formats.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-19.png" /></p>

<p>I do not feel that there is one message format to rule them all, or that even one for each industry. I think innovation at the message layer is important, but I also feel like we should be leveraging JSON Schema to help tame things whenever possible, and standardize as media types. Whenever possible, reuse existing standards from day one is preferred, but I get that this isn’t always the reality, and in many cases we are handed the equivalent of a filing cabinet filled with handwritten notes. In my world, there will always be a mixed of known and unknown message formats, something that I will always work to tame, as well as be increasingly apply machine learning models to help me identify, evolve, and make sense of–standardizing things in any way I possibly can.</p>

<p><strong>Knowing (Potential) Clients</strong><br />
I am developing APIs for a wide variety of clients. Some are designed for web applications, others are mobile applications, and some are devices. They could be spreadsheets, widgets, documents, and machine learning models. The tables could be flipped, and the APIs exist on device, and the cloud becomes the client. Sometimes the clients are known, other times they are unknown, and I am looking to attract new types of clients I never envisioned. I am always working to understand what types of clients I am looking to serve with my APIs, but the most important aspect of this process is understanding when there will be unknown clients.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-20.png" /></p>

<p>When I have a tightly controlled group of target clients, my world is much easier. When I do not know who will be developing against an API, and I am looking to encourage wider participation, this is when my toolbox comes into action. This is when I keep the bar as low as possible regarding the design of my APIs, the protocols I use, and the types of data formats and messages I use. When I do not know my API consumers and the clients they will developing, I invest more in API design, and keep my default requests and responses as simple as possible. Then I also allow for the negotiation of more complex, higher speed, more control aspects of my APIs by consumers who are in the know, targeting more specific client scenarios.</p>

<p><strong>Using The Right Tools For The Job</strong><br />
API is not REST. It is one tool in my toolbox. API deployment and integration is about having the right tool for the job. It is a waste of my time to demand that everyone understand one way of doing APIs, or my way of doing APIs. Sure, I wish people would study and learn about common API patterns, but in reality, on the ground in companies, organizations, institutions, and government agencies, this is not the state of things. Of course, I’ll spend time educating and training folks wherever I can, but my role is always more about delivering APIs, and integrating with existing APIs, and my API toolbox reflects this reality. I do not shame API providers for their lack of knowledge and available resources, I roll up my sleeves, put my API toolbox on the table, and get to work improving any situation that I can.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-not-rest-21.png" /></p>

<p>My API toolbox is crafted for the world we have, as well as the world I’d like to see. I rarely get what I want on the ground deploying and integrating with APIs. I don’t let this stop me. I just keep refining my awareness and knowledge by watching, studying, and learning from what others are doing. I often find that when someone is in the business of shutting down a particular approach, or being dogmatic about a single approach, it is usually because they aren’t on the ground working with average businesses, organizations, and government agencies–they enjoy a pretty isolated, privileged existence. My toolbox is almost always open, constantly evolving, and perpetually being refined based upon the reality I experience on the ground, learning from people doing the hard work to keep critical services up and running, not simply dreaming about what should be.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/03/api-is-not-just-rest/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/02/03/a-regulatory-subway-map-for-psd2/">A Regulatory Subway Map For PSD2</a></h3>
        <span class="post-date">03 Feb 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/talks/api-days-paris-2018/api-days-paris-2018-a-regulatory-subway-map-for-psd2.png" align="right" width="40%" style="padding: 15px;" /></p>
<p><em>This is one of my talks from <a href="http://www.apidays.io/events/paris-2017">APIDays Paris 2018</a>. Here is the abstract: Understanding the PSD2 regulations unfolding for the banking industry is a daunting challenge for banks, aggregators, and regulators, let alone when you are an average user, journalist, or analyst trying to make sense of things. As the API Evangelist I have used a common approach to mapping out transit systems using the universally recognized subway map introduced by Harry Beck in London in the early 20th century. I’m taking this mapping technique and applying it to the PSD2 specification, helping provide a visual, and interactive way to navigating this new regulatory world unfolding in Europe. Join me for an exploration of API regulations in the banking industry, and how we can use a subway and transit map to help us quickly understand the complexities of PSD2, and learn what takes to get up and running with this new definition for the API economy.</em></p>

<p><strong>Using Transit As API Analogy</strong><br />
I’ve been fascinated with transit systems and subway maps for most of my adult life. Recently I’ve begin thinking about the parallels between complex municipal transit systems, and complex API infrastructure. Beginning in 2015, I started working to see if I could apply the concept of a subway map to the world of APIs, not just as a visualization of complex API systems, but as a working interface to discover, learn about, and even govern hundreds, or even thousands of APIs working in concert. While very much a physical world thing, transit systems still share many common elements with API infrastructure, spanning multiple disparate systems, operated by disparate authorities, possessing legacy as well as modern infrastructure, and are used by many people for work and in their personal lives.</p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/transit/paris-metro.png" width="100%" /></p>

<p>While the transit system isn’t a perfect analogy for modern API infrastructure, there is enough there to keep me working to find a way to apply the concept, and specifically the transit map to helping us make sense of our API-driven systems. I’ve called my work API Transit, leaning on both the noun definition of transit, “the carrying of people, goods, or materials from one place to another”, as well as the verb, “pass across or through (an area)”. The noun portion reflects the moving of our digital bits around the web using APIs, while the verb dimension helps us understand what is often called the API lifecycle, but more importantly the governance that each API should pass through as it matures, and continues to move our important digital bits around.</p>

<p><strong>The History of Subway Maps</strong><br />
The modern approach to mapping transit systems can be traced back to Henry Beck who created the now iconic map of the London Underground in 1933. The methodology was decoupled from earlier ways of mapping and communicating around transit resources, in a way that was focused on how the resources would be experienced by end-users, evolving beyond a focus on the transit resources themselves, and their location in our physical world. At the beginning of the 20th century, subway maps were still being plotted along roads and rivers, leaving them coupled to legacy elements, and ways of thought. By 1915, public transit engineers like Henry Beck were beginning to rethink how they mapped the transit infrastructure, in a way that helped them better communicate their increasingly complex infrastructure internally, but most importantly, in a way that helped them communicate externally to the public.</p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/talks/november-2015/Beck_Map_1933.jpg" width="100%" /></p>

<p>This is what I’m looking to do with applying 20th century transit infrastructure mapping while applying it to 21st century digital API transit infrastructure. We are needing to get people(s) bits, and digital goods and materials around the web, while also understanding how to develop, discover, operate and manage this infrastructure in a consistent, and intuitive way. However, API infrastructure is rapidly growing, and with the introduction of microservices becoming increasingly complex, and difficult to logically map out the increasingly evolving, shifting, and moving API landscape. I’m hoping to suspend reality a little bit as Henry Beck did, and take utilize the same visual cues to begin to visualize API infrastructure in a way that is familiar to not just developers, but hopefully regulators, and average consumers. Much like the transit system of any major city, initially an API transit system will seem overwhelming, and confusing, but with time and experience it will come into focus, and become a natural part of our daily lives.</p>

<p><strong>API Transit Applied To Governance</strong><br />
Over the last two years I have been actively working to apply the API transit model to the concept of API governance. Beginning with API design, I have been looking for a way to help us understand how to consistently craft APIs that meet a certain level of quality set by a team, company, or even an entire industry. I’ve pushed this definition to include many of the almost 100 stops along the API lifecycle I track on as part of my work as the API Evangelist. Allowing API governance transit maps to be crafted, which can be used to help understand API governance efforts, but also help be applied to actually executing against this governance framework. Delivering the verb dimension of API transit, to “pass across or through (an area)”, allowing each API or microservice to regularly pass through each area, line, or stop of each API Transit governance map.</p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/transit/api-governance-subway-map.png" width="100%" /></p>

<p>API Transit began as a way to define and visualize the API lifecycle for me, acknowledging that the lifecycle is rarely ever a straight line. Often times it begins as that, but then over time it can become a mashup of different areas, lines, and stops that will need to be realized on a regular basis, on a one time basis, or any other erratic combination our operations can conceive of. With API Transit applied as an API governance model, I wanted to push the analogy even further, and see if I can push it to accommodate an individual API definition, or even a complex set of microservices working in concert. I began playing with a variety of existing API implementations, looking for one that could be used to tell a bigger story of how the transit model could be use beyond mapping how buses, trains, and other transit vehicles operate.</p>

<p><strong>API Transit For PSD2 Landscape</strong><br />
To push the API transit concept further, I took the Payment Services Directive 2 (PSD2) governing banking APIs in Europe, to see what might be possible when it comes to mapping out API infrastructure in a impactful way. I was able to easily map out lines for some of the most common aspects of PSD2, visualizing accounts, transactions, customers, and then eventually every other stop I wanted to include. The trick now, is how to I articulate shared stops, transfer stations, and other common patterns that exist as part of the PSD2 specification. I didn’t want to just visualize the banking regulations, I want to create an interactive visualization that can be experienced by developers, regulators, and even end-users.</p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/transit/psd2-transit.png" width="100%" /></p>

<p>The HTML Canvas transit map solution I’m using allows me to plot each line, and stop, and give each stop a label as well as a link. This allows me to provide much more detail about each stop, allowing someone exploring the specification to obtain much more information about each aspect of how the PSD2 specification is put to use. The resulting API transit map for the PSD2 landscape is pretty crude, and not very attractive. It will take much more work to bring each line in alignment, and allowing for overlap and redundancy to exist across lines. The reasons behind each line and stop will be different than a physical transit map, but there are still existing constraints I need to consider as a craft each transit map. The process is very rewarding, and helps me better understand each intimate detail of the PSD2 specification, something I’m hoping will eventually be passed on visually, and experimentally to each map user.</p>

<p><strong>API Transit Is OpenAPI Defined</strong><br />
As part of my rendering of the API Transit map for the PSD2 landscape, I was able to generate the HTML5 Canvas using the OpenAPI for the PSD2 API. Each line was created by the tags applied across APIs, and each stop was created to reflect each individual API method available. Providing a machine readable set of instructions to quantify what the structure of the landscape should be. I feel that the tagging of each API method can be improved, and used as the key driver of the how each API transit map is rendered. Something that will be used to make sense of complex renderings, as well as deliver on much more simplified, distilled versions to help make the landscape more accessible to non-developers.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/psd2/openapi-psd2.png" width="75%" /></p>

<p>The details present as part of each PSD2 API path, method, parameters, responses, and schema can be rendered as part of the information provided with each stop. Providing a clear, but detailed representation of each stop within the API Transit system, from a machine readable core which will be used to make the API Transit map more interactive, as well as part of a larger continuous deployment and integration experience. While each stop along each PSD2 line will not always be linear in nature, this approach allows for stops to be organized in a meaningful order, driven by a set of common machine readable rules that can be versioned, evolved over time, and used throughout the API Transit system, and tooling that is deployed to keep the API Transit system operational.</p>

<p><strong>API Transit Has Hypermedia Engine</strong><br />
With the ability to map out an overall API governance system, as well as a set of microservices that meet a specific industry objective, I needed a way to put in all into motion, so it could work together as a complete API transit system. I chose hypermedia to be the engine for the interaction layer, and OpenAPI to be the definition for each individual API. Hypermedia is used as a scaffolding for each area, line, and stop along the transit system, allowing each API to be navigated, but in a way that each API could also navigate any overall API governance model. Introducing the concept that an API could be experienced by any human, and any API governance model could be experienced by an API–again, allowing it to “pass across or through (an area)”, thus API Transit.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/psd2/psd2-hypermeida-engine.png" width="75%" /></p>

<p>Each API has a machine readable OpenAPI definition allowing requests and responses to be made against the surface area of the API as it is explored via the API Transit experience, much like interactive documentation provides already. However, the same OpenAPI definition allows the API to also be validated against overall API governance practices, ensuring certain design standards are met. I’m also playing with the use a machine readable APIs.json index for each API, in addition to the OpenAPI definition, allowing governance to be easily expanded to other API governance lines like management, documentation, pricing, support, terms of service, and other aspects of API operations not defined in an OpenAPI, but could be indexed within an APIs.json document.</p>

<p><strong>API Transit Runs As Github Repository</strong><br />
To continue evolving on the API Transit experience, I’ve begun deploying the API Transit map, with OpenAPI core, and hypermedia engine to Github for continuous deployment, and hosting of the API Transit map using Github Pages. Github, combined with Github Pages, and the static CMS solution Jekyll introduces an interesting opportunity for bringing each API Transit map to life. Jekyll transforms the hypermedia engine, and each API definition into a collection of objects which can be referenced with the API Transit map using the Liquid syntax. This allows each area, line, and stop to be easily traveled, moving from stop to stop, transferring between lines, and consuming rich information all along the way.</p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/talks/apistrat-portland/jekyll-plus-github.png" width="100%" /></p>

<p>Running each API Transit map makes exploring an interactive experience, but it also makes the experience easily part of any continuous integration or deployment pipeline, using Git or the Github API to engage with, or evolve the API Transit definition. This makes each API definition seamlessly integrated with actual deployment workflows, while also making API governance also part of these workflows, helping realize API testing, monitoring, validation, and other elements of the build process. With seamless integration as part of existing continuous deployment and integration workflows, I can easily envision a future where API Transit maps aren’t just interactive, but they are continually changing, streaming, and visually updated in real time.</p>

<p><strong>Each Stop Filled With Information</strong><br />
Each stop along an API Transit line is defined by its OpenAPI path, and navigated to by the hypermedia engine, but that is just the beginning. The hypermedia object for each stop contains the basics like the name and description of the stop, as well as the reference to the path, method, parameters, responses, and schema in the OpenAPI, but it also can possess other fields, images, video, audio, and links that deliver rich information about each individual stop. The hypermedia engine allows us to define and evolve these over time without breaking the client, which is the API Transit map. Providing an huge opportunity to educate as well as implement governance at each stop along the API Transit line(s).</p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/transit/multi-modal-ddot.jpg" width="100%" /></p>

<p>Since a stop can be along an API governance line, or along an individual API line, the opportunity for education is enormous. We can educate developers about healthy API design practices, or we can educate a developer about a specific API method, where the healthy API design practices can be fully realized. With this approach to using Github Pages, Jekyll, and a hypermedia engine as an interactive CMS, there is a pretty significant opportunity to make the API Transit experience educational and informative for anyone exploring. Each developer, aggregator, regulator, and even end-users can take control over which aspects of the PSD2 specification they wish to engage with and learn about, allowing for the PSDS API Transit to serve as wide of an audience possible.</p>

<p><strong>Map Progress Of Individual Banks</strong><br />
While there is an OpenAPI, and now API Transit map for the overall PSD2 specification. I will be using the approach to track on the evolution of each individual bank in Europe being impacted by the regulation. Ideally each bank will actively maintain and publish their own OpenAPI definition, something I will be actively advocating for, but ultimately I know the way things end up working in the real world, and I’ll have to maintain an OpenAPI definition for many of the banks, scraping them from the documentation each bank provides, and SDKs available on Github. I will use this as a literal map of the progress for each individual bank, and how compliant they are with EU regulation.</p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/psd2/bank-single.png" width="100%" /></p>

<p>Each individual bank PSD2 API Transit map will allow me to navigate the APIs for each bank, including URLs, and other information that is unique to each provider. I will be using APIs.json to track index API operations, which I will work to bring out as visual elements through the API Transit map experience. In theory, each banks APIs will be an exact copy of the master PSD2 specification, but I’m guessing more often than not, when you overlay the master PSD2 API Transit with each individual bank’s API Transit map, you are going to see some differences. Some differences will be positive, but most will demonstrate deficiencies and deviation from the overall guidance provided by EU regulatory agencies.</p>

<p><strong>Compare Over PSD2 To Individual Banks</strong><br />
I am using the hypermedia engine for the API Transit map to crawl each individual banks implementation to deliver the overlap map I discussed. I’m working on a way to show the differences between the master PSD2 specification, and one or many individual PSD2 API implementations. I’m not just looking to create an overlay map for visual reference, I will actually crawl the OpenAPI for each individual bank and output a checklist of which paths, parameters, and schema they do not support, ensuring their definition matches the master PSD2 specification. The first approach to doing this will be about comparing OpenAPI definitions, with a second dimension actually running assertions against the API for those that I have access to.</p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/psd2/bank-comparison.png" width="100%" /></p>

<p>There will be too much information to show on a single API Transit map, especially if we are comparing multiple banks at once. There will need to be a report format, accompanying a PSD2 API Transit map overlay showing the differences at the overall line and stop level. The programmatic comparison between the master PSD2 API and each of the banks will be easy to accomplish using the hypermedia engine, as well as the detailed OpenAPI definition. What will prove to be more challenging, is to create a meaningful representation of that map that allows the differences to be visualized, as well as the detail that is missing or present to be navigated via an aggregate Github repository.</p>

<p><strong>Apply Governance To Individual Banks</strong><br />
In addition to comparing the OpenAPI definitions of individual banks against the master PSD2 OpenAPI definition, I’m using the API Transit hypermedia engine to compare each individual API against the overall API governance. At first the API governance will just be about the API design elements present in the OpenAPI definition for each bank’s API. I will be looking for overall design considerations that may or may not be present in the master PSD2 specification, as well as beginning to add in additional API governance areas like testing, monitoring, performance, documentation, and other areas I mentioned earlier, which will be tracked via an APIs.json index.</p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/transit/api-governance-subway-map.png" width="100%" /></p>

<p>There are a number of details I’d like to see banks provide that isn’t covered as part of the PSD2 guidance. Having an API is only the beginning of usable banking solution for 3rd party applications. There needs to be documentation, support, and other elements present, or a well designed, and PSD2 compliant doesn’t mean much. I’m using API Transit to take the governance of PSD2 implementations beyond what the EU has put into motion. Taking what I know from the wider API sector and getting to work to map out, validate, and rank banking APIs regarding how easy to use, responsive, and usable their PSD2 APIs are. Making sure 3rd party developers, aggregators, and regulators can get at the APIs that are supposed to be defining their compliance.</p>

<p><strong>Use to Map Out Countries &amp; Regions</strong><br />
Using API Transit, I will be aso zooming out and mapping out the PSD2 landscape for individual countries, and regions. Creating separate transit maps that allow each country to be navigated, using the API Transit hypermedia engine to crawl each banks API Transit system map, and eventually provide aggregate comparison tools to see how different banks compare. With a machine readable index of each country, as well as individual indexes of each bank, the entire PSD2 regulatory landscape will become explorable via a series of API transit maps, as well as programmatically using the hypermedia engine present for each individual bank API implementation.</p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/psd2/coountries.png" width="100%" /></p>

<p>API Transit is meant to help map out the complexity of individual API systems, as well as the overall API governance in place. It will also enable the mapping of the complexity of many implementations across cities, regions, and countries. Allowing for regulatory tooling to eventually be developed on top of the thousands of Github repositories that will be created, housing each individual banks API Transit maps. Leveraging the Github API, Git, and the hypermedia and OpenAPI core of each individual API Transit implementation. Allowing for searching, reporting, auditing, and other essential aspects of making sure PSD2 is working.</p>

<p><strong>Making Sense Of Complex API Landscapes</strong><br />
The goal of API Transit is to make sense of complex API landscape. Tracking the details of thousands of API operations across the EU countries who are required to be compliant in 2018. Providing machine readable blueprints of the regulation, as well as each individual bank that is required to be compliant, and each individual detail of what that compliance means.  Then tying it all together with a hypermedia engine that allows it to be explored by any system or tooling, with a familiar transit map on top of it all–providing a way for humans to explore and navigate this very complex banking API landscape.</p>

<p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/losangelescloudy/blue_circuit/file-00_00_35_50.jpg" width="100%" /></p>

<p>As I am evolving the process of creating API Transit maps, the API Transit hypermedia engine allows for the development of a gradual awareness of a very complex and technical landscape. It all feels the same as when I land in any new city and begin understand the local transit system. At first, I am overwhelmed, and confused, but the more time I spend exploring the transit maps, riding the subway and buses, developing a familiarity with each system, the easier it all becomes. There is a reason transit maps are used to provide access to complex municipal transit systems, as they help provide a comprehensive view of the landscape, that allows you to find your way around, and use as a reference as you develop an awareness–this is what I’m looking to build with API Transit for the banking sector.</p>

<p><strong>Tours For Individual Roles</strong><br />
One of the benefits of an API Transit map client that is built on a hypermedia engine is that you can take advantage of the ability to change the map, and experience depending on who you are targeting. The concept of the transit maps allows us to suspend realities of the physical world, and the hypermedia client for the API Transit map can suspend realities of the virtual world, allowing for the creation of unique tours and experiences depending on who the end user is. The complete API Transit map for each bank’s implementation will still exist, but the ability to suspend some details to distill down complexity into more simpler experiences will be possible.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-people.JPG" width="75%" /></p>

<p>The objective of individual API Transit tours and experiences will allow for onboarding new developers, business users, or possibly regulator oversight looking at just a specific dimension of the PSD2 guidance. Allowing for the ability to zoom in just at the customer experience layer, or possible just the transactional capabilities across many banking API providers or even a city, region, or country. Think of how a city might have separate transit maps for the subway, buses, and other systems, but still have the overall system map. We can do the same with API Transit, further reducing complexity, and helping users make sense of the complex banking API systems.</p>

<p><strong>Interactive Industry API Governance</strong><br />
For API governance to be effective it needs to be understood by everyone being impacted, and are required to participate. It has to be able to measured, quantified, tested, audited, and visualized in real time, to understand how it is working, and not working. Modern API operations excel at doing this at the company, organization, institutional, and government level, and we should be using API solutions and tooling to help us understand how governance is being applied. There should be machine readable definitions, and common media types, as well as API driven tooling for educating, implementing, and measuring API governance–completing the API loop.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/psd2/psd2-eu.png" width="60%" /></p>

<p>This is another reason why the transit analogy is working so well for me when it comes to making API governance an interactive experience. I’m able to develop not just linear experiences where participants can click next, next, next and walk through everything. They can choose their own paths, experience the lines they feel need attention, while allowing managers, auditors, and regulators to understand how well each implementation is responding, evolving, and interacting with regulations. Making API governance at the industry level interactive not just for the banks and the EU regulatory body, but also for every other participant in between trying to make sense of what is happening with PSD2 in 2018.</p>

<p><strong>Continuously Deployed And Integrated</strong><br />
As I mentioned before, API Transit runs 100% on Github, within a Github repository. The API Transit map client is unaware of each individual banks API implementation. The uniqueness for each implementation resides in a series of Siren hypermedia, OpenAPI, and APIs.json files, that provide a machine readable snapshot of the implementation. The combination of Github, and the machine readable YAML core of each API Transit instance makes it all able to be integrated into continuous integration and deployment pipelines. Evolving each API Transit instance, as each actual banks API implementation is built and released–hopefully ensuring they are kept in sync with the reality on the ground at each bank.</p>

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions/continuous/continuous-bw.png" width="75%" /></p>

<p>At this point, API Transit has become another application being developed upon the banking APIs that PSD2 are designed to expose. It ultimately will be an aggregated application that uses not just one banking API, but all the banking APIs. The difference from other aggregators is API Transit is not interested in the data each bank possesses. It is interested in continuously understanding how well each banks API is when it comes to complying with PSD2 regulations, while also continuously helping developers, aggregators, regulators, and anyone else make sense of the complexities of banking APIs and the PSD2 regulations.</p>

<p><strong>Mapping Technology, Business, &amp; Politics</strong><br />
While API Transit seems very technical at first look, the solution is meant to help make sense of the business and politics around the PSD2 rollout. The first part of the conversation might seem like it is about ensuring each bank has a compliant API, but it is more about ensuring they have common operational level components of API operations like a developer portal, documentation, support, and other business aspects of doing business with APIs. Next, it will be about ensuring all the technical aspects of PSD2 compliant APIs in place, validating each API path, as well as it’s responses, schema, and other components.</p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/tech-business-politics-03-2014.png" width="100%" /></p>

<p>While we need to make sure 100% of the PSD2 OpenAPI definition is represented with each bank, we also need to make sure that API is accessible, secure, and usable, otherwise none of this matters. If a bank plays politics by making their API unusable to aggregators and 3rd party developers, yet appear on the surface to have a functioning API that is compliant with the PSD2 specification, we need to be able to identify that this is the case, make sure we are testing for these scenarios on a recurring basis, and be able to visualize and report upon it to regulators. Properly addressing the technical, as well as the business, and politics of API operations.</p>

<p><strong>Making API Regulation More Familiar</strong><br />
The main reason I’m using the transit map approach is because it is familiar. Secondarily, because the concept continues to work after two years of pushing forward the concept. Everyone I have shown it to immediately responds with, “well the trick will be to not make things too overwhelming and complicated”. Which I response with, “do you remember the first time you used the transit system in your city? How did you feel?” Overwhelmed, and thinking this was complicated–because it is. However, because there is a familiar, universal map that helps you navigate this complexity, eventually you begin to learn more about the complexity of the city you live in, or are visiting. Helping bridge the complexity, making it all a little more familiar over time.</p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/transit/paris-metro.png" width="100%" /></p>

<p>The transit map concept is universal and familiar. It is in use across hundreds of transit systems around the globe. The transit map is familiar, but it also has the ability to help make a complex system more familiar with a little exploration, and over the course of time. This is why I’m extending the transit approach to the world of APIs, and using it to make complex API systems like the PSD2 banking API regulation more familiar. Banking APIs aren’t easy. Banking regulations aren’t easy. We need a map to help us navigate this world, and be able to make sense of the complexity, even as it continues to evolve over time–API Transit is that.</p>

<p><strong>Helping API Regulation Be More Consistent</strong><br />
API Transit is machine readable, driven by existing specifications, including Siren a hypermedia media type, OpenAPI, and APIs.json. The first machine readable definition I started with was the OpenAPI definition for the PSD2 regulation. This was the seed, then I generated the hypermedia engine from that, and took the API governance engine I had been working on for the last couple of years and built it into the existing API Transit core. The objective of all of this work is to use the framework to introduce more consistency into how I am mapping out the rollout of PSD2 across the European banking landscape. This is something that is too big to do manually, and something that requires a machine readable, API driven approach to get the job done properly.</p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/transit/map-of-moscow-metro-underground2.gif" width="100%" /></p>

<p>The PSD2 OpenAPI definition provides a way to consistently measure, and report on whether each banks APIs are in compliance with PSD2 regulations. The APIs.json is meant to make sure each bank’s APIs are in compliance with API Evangelists guidelines for API operations. The Siren hypermedia engine is meant to enable the consistent crawling and auditing of everything as it evolves over time by other systems and applications, and when combined with the HTML5 Canvas API Transit map, it allows humans to explore, navigate, and visual banking APIs, and the PSD2 regulations in a more consistent, and organized fashion.</p>

<p><strong>Seeing PSD2 In Motion In A Visual Way</strong><br />
Ultimately, API Transit is about being able to see API operations across an industry in a visual way, where you can see everything in motion. Applying it to the PSD2 landscape is meant to help  visualize and understand everything happening. It has taken me many hours of work to get intimate with the core OpenAPI definition for PSD2. Learning about all the paths and schema that make up the regulation. As I work to evaluate the compliance of hundreds of banks across France and UK, I’m quickly realizing that it will be beyond my capacity to be able to see everything that is going on. API Transit helps me see the PSD2 landscape in a way that I can be regularly updated, revisited, and experienced as the landscape evolves.</p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/subway/london_underground2.gif" width="100%" /></p>

<p>The PSD2 API Transit application is just getting started. The methodology has been proven, and I’ve begun profiling banks, and collecting OpenAPI definitions for any APIs that I find, and profiling the wider presence of their API operations using APIs.json. Then I will begin improving on the mapping capabilities of the API Transit map client, making for a more visually pleasing experience. Along the way I am working with partners to better monitor the availability and performance of each banks APIs. Within a couple of months I expect a clearer picture to begin to come into focus regarding how the PSD2 landscape is unfolding in 2018, and by the end of the year, it will become clear which banks have emerged as competitive leaders within the API economy.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/02/03/a-regulatory-subway-map-for-psd2/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/26/shifting-gears-between-the-technology-and-politics-of-apis/">Shifting Gears Between the Technology and Politics of APIs</a></h3>
        <span class="post-date">26 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/80_87_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’ve been working on two talks for <a href="http://www.apidays.io/events/paris-2017">API Days in Paris next week</a>. These talks are at two opposite ends of the API spectrum for me. The first one, API Is Not Just REST, is rooted in the technology of APIs, but then touches lightly on the business and politics of APIs. The second one, a regulatory subway map for PSD2, is all about the politics of APIs, but then touches lightly on the business and technology of APIs. I have the outlines for both talks done, and I’m working on the narrative and slides for each, along the way I’m really caught by how different each end of the spectrum are, and require me to use a different part of my brain–something I think really defines the yin and yang of APIs.</p>

<p>When I’m down at the protocol level of APIs, thinking about the details of HTTP, TCP, and the nuance of how headers are used, and the messages we pass back and forth, the politics of APIs do not matter to me. My developer and architect brain is absorbed with the technical details, and the human or political consequences of my decisions really do not matter all that much. As long as things are technically correct, and my responses and requests are doing what is expected, I am good. It is easy for me to be railroaded within this technical silo, and I wouldn’t ever need to be concerned for the business and politics of it all, if my work as API Evangelist didn’t force me out of my comfort zone.</p>

<p>Inversely, when I’m thinking about the politics of how this all works, and the intention and impact of regulatory guidance like PSD2 in Europe, the technical details of HTTP, headers, and what messages I’m using feel less important. Sure, they still matter to what I’m trying to do, but the strictness in which I define my protocols, headers, data formats, schema, and other gears of API operations takes a looser form. When you look at banks who have NO PUBLIC API, just getting them up an running seems much more important than ensuring each HTTP response status code is present, and each schema is perfectly represented. Eventually, I will need to make sure all my technical i’s are dotted, and t’s are crossed, but I have bigger battles to wage at the moment.</p>

<p>As I’m pulled back and forth between the technical and the politics of APIs, I find myself becoming very, very aware of the business of APIs, and how the complexity of both technology and the politics are wielded for business gain. Seeing how technology is used as a competitive advantage, as well as leveraging politics to get ahead in the game. You see this in the rhetoric around PSD2, where invoking the bogey man can be very telling of a company’s position, just as much as the company’s who are proactively jumping on the PSD2 bandwagon, and getting ahead of the game, or even being a leader when it comes to defining implementations. Competitive edges can be sharpened by embracing political shifts, as well as through the adoption of leading edge technologies present across the API space (Kafka, OpenAPI, etc.)–it really depends on your organizations approach to the API game, and how up to speed you are on how it is played.</p>

<p>While I don’t feel it should be everyone’s role to be exposed to the all the extremes of the API sector, I do feel like we do a poor job of exposing developers and architects to the business and politics of it all in a meaningful way. I also feel like we spend too much time either hiding from or protecting business users from the technical details. I find that I learn a lot being pulled back and forth to either end of the spectrum, something I’m hoping to share as part of my talks next week in France, as well as the conversations I have in the hallways at the conference, and meeting rooms before and after the event. I look forward to seeing you all in Paris, and Grenoble next week.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/26/shifting-gears-between-the-technology-and-politics-of-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/24/where-am-i-in-the-sales-funnel-for-your-api/">Where Am I In The Sales Funnel For Your API?</a></h3>
        <span class="post-date">24 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/sand-hand_light_dali.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m signing up for a large number of new APIs lately as part of a project I am working on. It is pretty normal for me to sign up for a couple new APIs a week, or 10-20 within a month, but right now I’m signing up for hundreds, setting up an application and getting keys for each service. I’ll share more about what I’m working on in future stories, but I wanted to talk more about the on-boarding practices of some of these APIs. It is pretty clear from the on-boarding processes that I don’t rank very high in some of these API provider’s sales funnels, making me not deserving of self-service access, or even a sales call–which is a separate topic I will talk about in a future post.</p>

<p>I’ve registered with a number of high value API providers who have more of an enterprise focus, but also have a seemingly self-service, public API available. After signing up for access, it becomes very clear that APIs are anything but self-service, and there is a sales funnel in play, and I’ve been ranked, tagged, and identified where I am in this sales funnel, and what value I bring as a potential small business–which is not much by usual measurements. I’m pretty well versed in how company’s set up their sales strategy, and have seen many companies think it is a good idea to translate these practies to their API operations. Only targeting the high value customers, and not really giving a shit about the rest of them. It just isn’t worth the resources to go after them, they don’t have the spending capacity of customers you want in your funnel.</p>

<p>I get it. You are right. I don’t have a lot of money to buy your services, and will not become a high value customers. However, I have many readers who are high value customers, and trust my opinion about which services are worthy paying attention to. And guess what? I’m not going to write about your service. I’m not going to include you in my prime time storytelling, and when I do reference your API as part of my research, it will be in the club of shame, and APIs that really aren’t worth your time playing with. My enterprise readers, growing startups, university IT leadership, and government project owners with big budgets won’t ever know about your API, all because you didn’t see me as being worth your time, and your API on-boarding practices are out dated.</p>

<p>In a self-service API world you don’t need to be high touch with the long tail of your API consumers. This is why we have API management in place, with sensibly priced access tiers, requiring monthly levels of access, all requiring credit cards on file. While it frustrates me when companies don’t have a free tier of access for me to kick the tires, I get it. What makes a situation untenable is when I put in a credit card, and I am willing to spend a couple hundred bucks to write a story, build a prototype, or publish a landscape guide or white paper, and I still can’t get access to your resources and understand what is going on. It’s ok. I’m guessing they probably weren’t worth sharing with my readers anyways, and you belong right where you should belong in my API research that gets read by business and IT leadership around the globe.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/24/where-am-i-in-the-sales-funnel-for-your-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/24/helping-understand-that-stream-line-data-io-is-more-than-just-real-time-streaming/">Helping Define Stream(Line)Data.io As More Than Just Real Time Streaming</a></h3>
        <span class="post-date">24 Jan 2018</span>
        <p><a href="http://apis.how/streamdata"><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/containership_dali_three.jpg" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>One aspect of my partnership with <a href="http://apis.how/streamdata">Streamdata.io</a> is about helping define what it is that Streamdata.io does–internally, and externally. When I use any API technology I always immerse myself in what it does, and understand every detail regarding the value it delivers, and I work to tell stories about this. This process helps me refine not just how I talk about the products and services, but also helps influence the road map for what the products and services deliver. As I get intimate with what Streamdata.io delivers, I’m beginning to push forward how I talk about the company.</p>

<p>The first thoughts you have when you hear the name Streamdata.io, and learn about how you can proxy any existing JSON API, and begin delivering responses via Server-Sent Events (SSE) and JSON Patch, are all about streaming and real time. While streaming of data from existing APIs is the dominant feature of the service, I’m increasingly finding that the conversations I’m having with clients, and would be clients are more about efficiencies, caching, and streamlining how companies are delivering data. Many API providers I talk to tell me they don’t need real time streaming, but at the same time they have rate limits in place to keep their consumers from polling their APIs too much, increasing friction in API consumption, and not at all about streamlining it.</p>

<p>These experiences are forcing me to shift how I open up conversations with API providers. Making real time and streaming secondary to streamlining how API providers are delivering data to their consumers. Real time streaming using Server-Sent Events (SSE) isn’t always about delivering financial and other data in real time. It is about delivering data using APIs efficiently, making sure only what what has been updated and needed is delivered when it is needed. The right time. This is why you’ll see me increasingly adding (line) to the Stream(line)data.io name, helping focus on the fact that we are helping streamline how companies, organizations, institutions, and government agencies are putting data to work–not just streaming data in real time.</p>

<p>I really enjoy this aspect of getting to know what a specific type of API technology delivers, combined with the storytelling that I engage in. I was feeling intimidated about talking about streaming APIs with providers who clearly didn’t need it. I’m not that kind of technologist. I just can’t do that. I have to be genuine in what I do, or I just can’t do it. So I was pleasantly surprised to find that conversations were quickly becoming about making things more efficient, over actually ever getting to the streaming real time portion of things. It makes what I do much easier, and something I can continue on a day to day basis, across many different industries.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/24/helping-understand-that-stream-line-data-io-is-more-than-just-real-time-streaming/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/24/i-want-to-be-able-to-post-and-put-and-receive-credits-on-my-api-bill/">I Want to Be Able to POST and PUT and Receive Credits on My API Bill</a></h3>
        <span class="post-date">24 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/69_120_800_500_0_max_0_-5_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="http://apievangelist.com/2018/01/17/api-management-and-the-measurement-of-value-exchange/">I’ve been thinking about the potential for measuring value exchange at the API management level a lot more lately</a>, and while I’m working on a project to profile banks that are needing to comply with the PSD2 regulations in Europe, I’m thinking about the missed opportunities for the API providers I’m using to fuel my research to leverage the value I’m generating. I’m using a variety of data enrichment APIs for helping add to the contact data, corporate profiles, images, documents, patents, and other valuable data to what I’m doing as part of my wider research into the API space. While these APIs have valuable services that I am paying for, all of the APIs are just using one HTTP verb–GET.</p>

<p>On a regular basis I come across incorrect, or incomplete data, and as part of my work I dive in and correct the data, and continue to connect the dots. I often find better copies of logos, add in relevant business profile data, and I always provide very detailed information regarding a company’s API–which I find to be some of the most telling aspects of what a company does, or doesn’t do. I’m thankful for the services that the 3rd party APIs I utilize, but I think they are missing out on a pretty big opportunity for trusted partners like me to be able to POST or PUT the data I am gathering back to their systems.</p>

<p>I would love to be able to POST and PUT back information to the APIs I am GETting my data from, and receive credits to my API bill for these contributions. It would benefit API providers by helping ensure the data they are providing is complete and acurate, and it would benefit me by helping me keep my API bills as low as possible. I understand that it would some work on the provider side to ensure I’m a trusted partner, and being able to verify the POST and PUT API calls I am making actually add value, but with a proper queue, and a little bit of human power, it wouldn’t take that much. At first it may seem like the investment would be more than the value, but all you’d have to do is find a handful of partners like me who would significantly contribute to the data you provide via your APIs.</p>

<p>When it comes to the value exchanged via APIs, and it always seems to be the heavily dominated user generated content platforms like Facebook and Twitter, or the GET only data providers, without a lot in between. Most providers I talk with are nervous about the quality of data, and the overhead with managing contributions. Which is definitely true if the flood gates are wide open, but with the proper approach to API management, and sensible access tiers for partners, you could easily identify who the most valuable API consumers were. Do not miss out on the opportunity to allow trusted partners like me to POST and PUT, and receive credits on my bill, making API management, and the value exchange that occurs via your platform a two-way street.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/24/i-want-to-be-able-to-post-and-put-and-receive-credits-on-my-api-bill/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/23/the-role-of-european-banking-authority-eba-in-regards-to-psd2/">The Role of European Banking Authority (EBA) When It Comes To PSD2</a></h3>
        <span class="post-date">23 Jan 2018</span>
        <p><a href="http://www.eba.europa.eu/"><img src="https://s3.amazonaws.com/kinlane-productions/psd2/european-banking-authority-eba.jpg" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>As part of my continued effort to break down <a href="https://ec.europa.eu/info/law/payment-services-psd-2-directive-eu-2015-2366_en">the Payment Services Directive 2 (PSD2) in Europe</a>, and develop my awareness of how the regulations are intended, as well as the reality on the ground within the industry, I am working to map out all of the players involved. This post is about understanding the role of the European Banking Authority (EBA), and clearly understanding when and where they come into the conversation.</p>

<p>First, what is the <a href="http://www.eba.europa.eu/">European Banking Authority (EBA)</a>? They are the regulatory agency for the European Union, who is in charge of conducting stress tests on European banks and increasing transparency in the European financial system and identifying weaknesses in banks’ capital structures. When it comes to PSD2, their role is to:</p>

<ul>
  <li>develop a publicly accessible central register of authorised payment institutions, which shall be kept up to date by the national authorities</li>
  <li>assist in resolving disputes between national authorities</li>
  <li>develop regulatory technical standards on strong customer authentication and secure communication channels with which all payment service providers must comply</li>
  <li>develop cooperation and information exchange between supervisory authorities</li>
</ul>

<p>The catalyst for this post was because I was looking for the <em>“central register of authorized payment institutions”</em>, and could not find it. Something that will be critical to this effort working, and evolving. I’m also on the hunt for more details regarding how they will be addressing authentication for all 3rd party API access, which will also be something that makes or breaks this effort. And, of course, as the API Evangelist I’m looking to help anyone in the position of helping <em>“develop cooperation and information exchange”</em>–it is what I do.</p>

<p>When it comes to PSD2, I have gotten to know <a href="https://github.com/api-evangelist/psd2/blob/master/_data/psd2/openapi.yaml">the API definition (OpenAPI)</a>, and I am making my way through the actual set of laws, but I’m still working to understand who all the players are. I’ll keep profiling every type participant in the PSD2 theater that is unfolding across Europe in 2018, until each of the actors makes sense in my head, and I can speak to all of them intelligently. Then I’m hoping to compare notes with my research regarding banking in the United States, and see how it all looks. I’ll be spending next week in France talking with bankers about PSD2, and giving a talk on API governance at this level. So for now, I’m going to be all about EU banking, but I’m engaged in several conversations here in the states with major banks as well, which will all make for some great financial API storytelling over the next couple of months.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/23/the-role-of-european-banking-authority-eba-in-regards-to-psd2/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/22/key-points-from-the-payment-services-directive-2-psd2/">Key Points From The Payment Services Directive 2 (PSD2)</a></h3>
        <span class="post-date">22 Jan 2018</span>
        <p><a href="https://ec.europa.eu/info/law/payment-services-psd-2-directive-eu-2015-2366_en"><img src="https://s3.amazonaws.com/kinlane-productions/psd2/psd2-eu.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>I’m immersed in studying <a href="https://ec.europa.eu/info/law/payment-services-psd-2-directive-eu-2015-2366_en">the Payment Services Directive 2 (PSD2) in Europe</a>, which includes an API definition to help enable the interoperability they are looking to achieve as part of the regulation. I’m working to break down the directive into bit such chunks to help be digest, and understand exactly what it does. The PSD2 laws seeks to improve the existing EU rules for electronic payments (hence the 2), and takes into account emerging approaches to payment services, such as Internet and mobile payments, with APIs at the hear.</p>

<p>The directive sets out rules concerning:</p>

<ul>
  <li>strict security requirements for electronic payments and the protection of consumers’ financial data, guaranteeing safe authentication and reducing the risk of fraud</li>
  <li>the transparency of conditions and information requirements for payment services</li>
  <li>the rights and obligations of users and providers of payment services</li>
</ul>

<p>Additionally, “the directive is complemented by <a href="http://eur-lex.europa.eu/legal-content/EN/TXT/?uri=celex:32015R0751">Regulation (EU) 2015/751</a> which puts a cap on interchange fees charged between banks for card-based transactions. This is expected to drive down the costs for merchants in accepting consumer debit and credit cards.” Which can be one of the most frustrating aspects of banking today, where you have no expectations regarding the fees you can be charged around every turn, as you are just trying to make ends meet.</p>

<p>You will be seeing a lot more posts about PSD2 as I work to absorb the regulations, and the technical guidance set forth regarding banking APIs. I’m playing around with the OpenAPI definition for PSD2, and crafting a version of my API Transit subway map to represent the technical guidance present. I’m also working to understand the business, and political aspects of PSD2, which involves me breaking down the directive into this small, digestible stories, here on API Evangelist.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/22/key-points-from-the-payment-services-directive-2-psd2/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/19/aws-api-gateway-openapi-vendor-extensions/">AWS API Gateway OpenAPI Vendor Extensions</a></h3>
        <span class="post-date">19 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/amazon/api-gateway-extensions-swagger.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I was doing some work on the AWS API Gateway, and as I was going through their API documentation I found some of <a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-swagger-extensions.html">the OpenAPI vendor extensions they use as part of operations</a>. These vendor extensions show up in the OpenAPI you export for any API, and reflect how AWS has extended the OpenAPI specification, making sure it does what they need it to do as part of AWS API Gateway operations.</p>

<p>AWS has 20 separate OpenAPI vendor extensions as part of the OpenAPI specification for any API you manage using their gateway solution:</p>

<ul>
  <li><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-swagger-extensions-any-method.html">x-amazon-apigateway-any-method</a> - Specifies the Swagger Operation Object for the API Gateway catch-all ANY method in a Swagger Path Item Object. This object can exist alongside other Operation objects and will catch any HTTP method that was not explicitly declared.</li>
  <li><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-swagger-extensions-api-key-source.html">x-amazon-apigateway-api-key-source</a> - Specify the source to receive an API key to throttle API methods that require a key. This API-level property is a String type.</li>
  <li><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-swagger-extensions-authorizer.html">x-amazon-apigateway-authorizer</a> - Defines a custom authorizer to be applied for authorization of method invocations in API Gateway. This object is an extended property of the Swagger Security Definitions object.</li>
  <li><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-swagger-extensions-authtype.html">x-amazon-apigateway-authtype</a> - Specify an optional customer-defined information describing a custom authorizer. It is used for API Gateway API import and export without functional impact.</li>
  <li><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-swagger-extensions-binary-media-types.html">x-amazon-apigateway-binary-media-types</a> - Specifies the list of binary media types to be supported by API Gateway, such as application/octet-stream, image/jpeg, etc. This extension is a JSON Array.</li>
  <li><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-swagger-extensions-documentation.html">x-amazon-apigateway-documentation</a> - Defines the documentation parts to be imported into API Gateway. This object is a JSON object containing an array of the DocumentationPart instances.</li>
  <li><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-swagger-extensions-gateway-responses.html">x-amazon-apigateway-gateway-responses</a> - Defines the gateway responses for an API as a string-to-GatewayResponse map of key-value pairs.</li>
  <li><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-swagger-extensions-gateway-responses.gatewayResponse.html">x-amazon-apigateway-gateway-responses.gatewayResponse</a> - Defines a gateway response of a given response type, including the status code, any applicable response parameters, or response templates.</li>
  <li><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-swagger-extensions-gateway-responses.responseParameters.html">x-amazon-apigateway-gateway-responses.responseParameters</a> - Defines a string-to-string map of key-value pairs to generate gateway response parameters from the incoming request parameters or using literal strings.</li>
  <li><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-swagger-extensions-gateway-responses.responseTemplates.html">x-amazon-apigateway-gateway-responses.responseTemplates</a> - Defines GatewayResponse mapping templates, as a string-to-string map of key-value pairs, for a given gateway response. For each key-value pair, the key is the content type; for example, “application/json”, and the value is a stringified mapping template for simple variable substitutions. A GatewayResponse mapping template is not processed by the Velocity Template Language (VTL) engine.</li>
  <li><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-swagger-extensions-integration.html">x-amazon-apigateway-integration</a> - Specifies details of the backend integration used for this method. This extension is an extended property of the Swagger Operation object. The result is an API Gateway integration object.</li>
  <li><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-swagger-extensions-integration-requestTemplates.html">x-amazon-apigateway-integration.requestTemplates</a> - Specifies mapping templates for a request payload of the specified MIME types.</li>
  <li><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-swagger-extensions-integration-requestParameters.html">x-amazon-apigateway-integration.requestParameters</a> - Specifies mappings from named method request parameters to integration request parameters. The method request parameters must be defined before being referenced.</li>
  <li><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-swagger-extensions-integration-responses.html">x-amazon-apigateway-integration.responses</a> - Defines the method’s responses and specifies parameter mappings or payload mappings from integration responses to method responses.</li>
  <li><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-swagger-extensions-integration-response.html">x-amazon-apigateway-integration.response</a> - Defines a response and specifies parameter mappings or payload mappings from the integration response to the method response.</li>
  <li><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-swagger-extensions-integration-responseTemplates.html">x-amazon-apigateway-integration.responseTemplates</a> - Specifies mapping templates for a response payload of the specified MIME types.</li>
  <li><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-swagger-extensions-integration-responseParameters.html">x-amazon-apigateway-integration.responseParameters</a> - Specifies mappings from integration method response parameters to method response parameters. Only the header and body types of the integration response parameters can be mapped to the header type of the method response.</li>
  <li><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-swagger-extensions-request-validator.html">x-amazon-apigateway-request-validator</a> - Specifies a request validator, by referencing a request_validator_name of the x-amazon-apigateway-request-validators Object map, to enable request validation on the containing API or a method. The value of this extension is a JSON string.</li>
  <li><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-swagger-extensions-request-validators.html">x-amazon-apigateway-request-validators</a> - Defines the supported request validators for the containing API as a map between a validator name and the associated request validation rules. This extension applies to an API.</li>
  <li><a href="https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-swagger-extensions-request-validators.requestValidator.html">x-amazon-apigateway-request-validators.requestValidator</a> - Specifies the validation rules of a request validator as part of the x-amazon-apigateway-request-validators Object map definition.</li>
</ul>

<p>I keep track of these vendor extensions as part of <a href="http://openapi.toolbox.apievangelist.com/">my OpenAPI toolbox</a>, but I also like to aggregate them, and learn from them. They tell an important story of what AWS is looking to do with the AWS API Gateway. They point to some interesting use cases for the OpenAPI specification including validation, and transforming or mapping API requests and responses, to name a few. There is always a lot to learn from API providers who are extending the OpenAPI specification.</p>

<p>I encounter a number of API designers and architects who don’t know they can extend the specification. It is important that teams realize they can not just extend the specification to fit their needs, but also that they should be learning from how other API proviers are doing this. A few signs of an API provider who is further along in their API journey are 1) actively maintaining and sharing and OpenAPI definition for their APIs, and 2) actively extending and sharing the vendor extensions they use to make OpenAPI do exactly what they need.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/19/aws-api-gateway-openapi-vendor-extensions/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/19/a-health-check-response-format-for-http-apis/">A Health Check Response Format for HTTP APIs</a></h3>
        <span class="post-date">19 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/irakli/health-check-response-format-for-http-apis.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>My friend Irakli Nadareishvili, <a href="https://inadarei.github.io/rfc-healthcheck/">has published a new health check response format for HTTP APIs</a> that I wanted to make sure was documented as part of my research. The way I do this is write a blog post, forever sealing this work in time, and adding it to the public record that is my API Evangelist brain. Since I use my blog as a reference when writing white papers, guides, blueprints, policies, and other aspects of my work, I need as many references to usable standards like this.</p>

<p>I am going to just share the introduction from Irakli’s draft, as it says it all:</p>

<blockquote>
  <p>The vast majority of modern APIs driving data to web and mobile applications use HTTP [RFC7230] as a transport protocol. The health and uptime of these APIs determine availability of the applications themselves. In distributed systems built with a number of APIs, understanding the health status of the APIs and making corresponding decisions, for failover or circuit-breaking, are essential for providing highly available solutions.
There exists a wide variety of operational software that relies on the ability to read health check response of APIs. There is currently no standard for the health check output response, however, so most applications either rely on the basic level of information included in HTTP status codes [RFC7231] or use task-specific formats.
Usage of task-specific or application-specific formats creates significant challenges, disallowing any meaningful interoperability across different implementations and between different tooling.
Standardizing a format for health checks can provide any of a number of benefits, including:</p>
  <ul>
    <li>Flexible deployment - since operational tooling and API clients can rely on rich, uniform format, they can be safely combined and substituted as needed.</li>
    <li>Evolvability - new APIs, conforming to the standard, can safely be introduced in any environment and ecosystem that also conforms to the same standard, without costly coordination and testing requirements.
This document defines a “health check” format using the JSON format [RFC7159] for APIs to use as a standard point for the health information they offer. Having a well-defined format for this purpose promotes good practice and tooling.</li>
  </ul>
</blockquote>

<p>Here is an example JSON response, showing the standard in action:</p>

<script src="https://gist.github.com/kinlane/5bea1128ccada3b26ab534b7e4bb138d.js"></script>

<p>I have seen a number of different approaches to providing health checks in APIs, from a single ping path, to proxying of the Docker Engine API for the microservices Docker container. It makes sense to have a standard for this, and I’ll reference Irakli’s important work from here on out as I’m advising on projects, or implementing my own.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/19/a-health-check-response-format-for-http-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/19/developing-a-microservice-to-orchestrate-long-running-background-server-sent-events/">Developing a Microservice to Orchestrate Long Running Background Server-Sent Events</a></h3>
        <span class="post-date">19 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/68_174_800_500_0_max_0_-5_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am working to understand the value that <a href="http://apis.how/streamdata">Streamdata.io</a> brings to the table, and one of the tools I am developing is a set of APIs to help me measure the difference in data received for normal API calls versus when they are proxied with Streamdata.io using Server-Sent Events (SSE) and JSON Patch. Creating an API to poll any 3rd party API I plug in is pretty easy and straightforward, but setting up a server setup to operate long running Server-Sent Events (SSE), managing for failure and keeping an eye on the results takes a little more consideration. Doing it browser side is easy, but server side removes the human aspect of the equation, which starts and stops the process.</p>

<p>This post is just meant to just outline what I’m looking to build, and act as a set of project requirements for what I’m going to develop–it isn’t a guide to building it. This is just my way of working through my projects, while also getting content published on the blog ;-). I just need to work out the details of what I will need to run many different Server-Sent Events (SSE) jobs for long periods of time, or even continuously, and make sure nothing breaks, or at least minimize the breakages. Half of my project will be polling hundreds of APIs, while the other half of it will be proxy those same APIs, and making sure I’m receiving those updates continuously.</p>

<p>I will need some basic APIs to operate each event stream I want to operate:</p>

<ul>
  <li><strong>Register</strong> - Register a new API URL I wish to run ongoing stream on.</li>
  <li><strong>Start</strong> - Kick off a new stream for any single API I’m tracking on.</li>
  <li><strong>Stop</strong> - Stop a stream from running for any single API I have streaming.</li>
</ul>

<p>Any API I deem worthy, and have successfully proxied with Streamdata.io will be registered, and operating as a long running background scripts via AWS EC2 instances I have deployed. This is the straightforward part of things. Next, I will need some APIs to monitor these long running scripts, to make sure they are doing what they should be doing.</p>

<ul>
  <li><strong>Status</strong> - Check the status of a long running script to make sure it is still running and doing what it is supposed to do.</li>
  <li><strong>Logs</strong> - View the logs of an event that has been running to see each time it has executed, and what the request and response were.</li>
  <li><strong>Notify</strong> - Adding a notification API to send a ping to either myself, or someone else response for a long running script to investigate further.</li>
</ul>

<p>I’m think that set of APIs should give me what I need to run these long running jobs. Each API will be executing command scripts that run in the background on Linux instances. Then I’m going to need a similar set of services to asses the payload, cache, and real time status of each API, keeping in line with <a href="http://apievangelist.com/2018/01/17/breaking-down-the-value-of-real-time-apis/">my efforts to break down the value of real time APIs</a>.</p>

<ul>
  <li><strong>Size</strong> - A service that processes each partial API response in the bucket and calculates the size of the response. If nothing changed, there was no JSON Patch response.</li>
  <li><strong>Change</strong> - A service that determines if a partial API response has changed from the previous response from 60 seconds before, identifying the frequency of change. If nothing changed, there was no JSON Patch response.</li>
</ul>

<p>I have three goals with long running script microservice. 1) Monitor the real time dimensions of a variety of APIs over time. 2) Understand the efficiencies gained with caching and streaming over polling APIs, and 3) Potentially store the results on Amazon S3, which I will write about in a separate post. I will build an application for each of these purposes on top of these APIs, keeping the microservice doing one thing–processing long run scripts that receive Server-Sent Events (SSE) deliver via Streamdata.io proxies I’ve sent for APIs I’ve targeted.</p>

<p>Next, I am going to get to work programming this service. I have a proof of concept in place that will run the long running scripts. I just need to shape it into a set of APIs that allow me to program against the scripts, and deliver these different use case applications I’m envisioning. Once I have done, I will run for a few months in beta, but then probably open it up as a Server-Sent (SSE) events as a service, that allows anyone to execute long running scripts on the server side. Others may not be interested in measuring the performance gains, but I am guessing they will be interested in storing the streams of response.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/19/developing-a-microservice-to-orchestrate-long-running-background-server-sent-events/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/18/docker-engine-api-has-openapi-download-at-top-of-their-api-docs/">Docker Engine API Has OpenAPI Download At Top Of Their API Docs</a></h3>
        <span class="post-date">18 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/docker/docker-engine-openapi-download.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am a big fan API providers taking ownership of their OpenAPI definition, which enables API consumers to download a complete OpenAPI then import into any client tooling like Postman, using it to generate client SDKs, and getting up to speed regarding the surface area of an API. This is why I like to showcase API providers I come across who do this well, and occasionally shame API providers who don’t do it, and demonstrate to their consumers that they don’t really understand what OpenAPI definitions are all about.</p>

<p>This week I am showcasing an API provider who does it well. I was on the hunt for an OpenAPI of the Docker Engine API, for use in a project I am consulting on, and was please to find that <a href="https://docs.docker.com/engine/api/v1.35/#">they have a button to download the OpenAPI for each version of the Docker Engine API right at the top of the page</a>. Making it dead simple for me, as an API consumer, to get up and running with the Docker API in my tooling. OpenAPI is about much more than just the API documentation, and something that should be a first class companion to ALL API documentation for EVERY API provider out there–whether or not you are a devout OpenAPI (fka Swgger) believer.</p>

<p>The Docker API team just saved me a significant amount of time in tracking down another OpenAPI, which most likely would be incomplete. Let alone the amount of work that would be required to hand-craft one for my project. I was able to take the existing OpenAPI and publish to the team Github Wiki for a project I’m advising on. The team will be able to use the OpenAPI to import into their Postman Client and begin to learn about the Docker API, which will be used to orchestrate the containers they are using to operate their own microservices. A subset of this team will also be crafting some APIs that proxy the Docker API, and allow for localized management of each microservice’s underlying engine.</p>

<p><a href="https://apievangelist.com/2018/01/08/i-created-an-openapi-for-the-hashicorp-consul-api/">I had to create the Consul OpenAPI for the team last week</a>, which took me a couple hours. I was pleased to see Docker taking ownership of their OpenAPI. This is a drum I will keep beating here on the blog, until EVERY API provider takes ownership of their OpenAPI definition, providing their consumers with a machine readable definition of their API. OpenAPI is much more than just API documentation, and is essential to making sense of what an API does, and then take that knowledge and quickly translate it into actual integration, in as short of time as possible. Don’t make integrating with your API difficult, reduce as much friction as possible, and publish an OpenAPI alongside your API documentation like Docker does.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/18/docker-engine-api-has-openapi-download-at-top-of-their-api-docs/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/17/five-apis-to-guide-you-on-your-way-to-the-data-dark-side/">Five APIs to Guide You on Your Way to the Data Dark Side</a></h3>
        <span class="post-date">17 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/76_135_800_500_0_max_0_1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I was integrating with <a href="https://dashboard.clearbit.com/docs#api-reference">the Clearbit API</a>, doing some enrichment of the API providers I track on, and I found their API stack pretty interesting. I’m just using the enrichment API, which allows me to pass it a URL, and it gives me back a bunch of intelligence on the organization behind. I’ve added a bookmarklet to my browser, which allows me to push it, and the enriched data goes directly into my CRM system. Delivering what it the title says it does–enrichment.</p>

<p>Next up, I’m going to be using the Clearbit Discovery API to find some potentially new companies who are doing APIs in specific industries. As I head over the to the docs for the API, I notice the other three APIs, and I feel like they reflect the five stages of transition to the data intelligence dark side.</p>

<ul>
  <li><strong>Enrichment API</strong> - The Enrichment API lets you look up person and company data based on an email or domain. For example, you could retrieve a person’s name, location and social handles from an email. Or you could lookup a company’s location, headcount or logo based on their domain name.</li>
  <li><strong>Discovery API</strong> - The Discovery API lets you search for companies via specific criteria. For example, you could search for all companies with a specific funding, that use a certain technology, or that are similar to your existing customers.</li>
  <li><strong>Prospector API</strong> - The Prospector API lets you fetch contacts and emails associated with a company, employment role, seniority, and job title.</li>
  <li><strong>Risk API</strong> - The Risk API takes an email and IP and calculates an associated risk score. This is especially useful for figuring out whether incoming signups to your service are spam or legitimate, or whether a payment has a high chargeback risk.</li>
  <li><strong>Reveal API</strong> - Reveal API takes an IP address, and returns the company associated with that IP. This is especially useful for de-anonymizing traffic on your website, analytics, and customizing landing pages for specific company verticals.</li>
</ul>

<p>Your journey to the dark side begins innocently enough. You just want to know more about a handful of companies, and the data provided is a real time saver! Then you begin discovering new things, finding some amazing new companies, products, services, and insights. You are addicted. You begin prospecting full time, and actively working to find your latest fix. Then you begin to get paranoid, worried you can’t trust anyone. I mean, if everyone is behaving like you, then you have to be on your guard. That visitor to your website might be your competitor, or worse! Who is it? I need to know everyone who comes to my site. Then in the darkest depths of your binges you are using the reveal API and surveilling all your users. You’ve crossed to the dark side. Your journey is complete.</p>

<p>Remember kids, this is all a very slippery slope. With great power comes great responsibility. One day you are a scrappy little startup, and the next your the fucking NSA. In all seriousness. I think their data intelligence stack is interesting. I do use the enrichment API, and will be using the discovery API. However, we do have to ask ourselves, do we want to be surveilling all our users and visitors. Do we want to be surveilled on every site we visit, and on every application we use? At some point we have to make sure and check how far towards the dark side we’ve gone, and ask ourselves, is this all really worth it?</p>

<p><em><strong>P.S.</strong> This story reminds me I totally flaked on delivering a white paper to Clearbit on the topic of risk. Last year was difficult for me, and I got swamped….sorry guys. Maybe I’ll pick up the topic and send something your way. It is an interesting one, and I hope to have time at some point.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/17/five-apis-to-guide-you-on-your-way-to-the-data-dark-side/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/17/api-management-and-the-measurement-of-value-exchange/">API Management and the Measurement of Value Exchanged</a></h3>
        <span class="post-date">17 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/42_16_600_500_0_max_1_1_2-0.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>The concept of API management <a href="https://apievangelist.com/2013/06/10/history-of-apis-mashery/">has been around for a decade now</a>, and is something that is now part of the fabric of the cloud with services like <a href="https://aws.amazon.com/api-gateway/">AWS API Gateway</a>. API management is about requiring all consumers of any API resource to sign up for any API access, obtain a set of keys that identify who they are, and pass these keys in with each API call they make from any application. Since every API call is logged, and every API call possesses these keys, it opens up the ability to understand exactly how your APIs are being used through reporting and analytics packages which come with all modern API management available today. This is the fundamentals of API management, allowing us to understand who is accessing our digital resources, and how they are putting them to use–in real time.</p>

<p>The security of API management comes in with this balance of opening up access, being aware of who is accessing what, and being able to throttle or shut down the access of bad actors–more than it is ever about authentication, and requiring keys for all API calls. If you want access to any digital resources from a company, organization, institution, or government agency in a machine readable format, for use in any other web, mobile, or device application–you use the API. This allows ALL digital assets to be made available internally, to trusted partners, and even to the public, while still maintaining control over who has access to what, and what types of applications they are able to use them in. APIs introduce more control over our digital resources, not less–which is a persistent myth when it comes to web APIs.</p>

<p>API management allows us to limit who has access to which APIs by putting each API into one or many “plans”. The concept of software as a service (SaaS) has dominated this discussion around plan access, establishing tiers of API access such as free, pro, and enterprise, or maybe bronze, silver, and platinum. Giving users different levels of access to APIs, often times depending on how much they are paying, or possibly depending upon the level of trust (ie. partners get access to more APIs, as well as higher rate limits). While this approach still persists, much of what we see lacks imagination, due to the rules of the road dictated by many API startup VC investors pulling the strings behind the scenes. When crafting API access plans you want to incentivize consumer behavior, but honestly startup culture, and VC dominance has limited API provider’s vision, and stagnated many of the conversations around what is possible with API management after a decade.</p>

<p>When you are trying to scale a startup fast you need a narrow offering of API products. If you are operating a “real” business, or organization, institution, or government agency, your API plans will look much different, and you shouldn’t be emulating the startup world. We should be more concerned with value exchange between internal groups, with our partners, and potentially with 3rd party public developers. We should create API plans that incentivize, but not limit access, encouraging developers to innovate, while still generating sensible revenue around our digital resources we are making available. The reason many existing companies are struggling with their API programs is they are emulating startups, and not thinking bigger than the table Silicon Valley has set for us. API management is about measuring and rewarding value exchange, not rapidly growing your company so you can inflate your numbers, and sell your business to the highest bidder. That is so 2012!</p>

<p>When you see API management being used to measure value exchange to its fullest you see all the HTTP verbs being used, and measured. Providers aren’t just providing GETs, measuring and charging for access. They are allowing for POST, PUT, and DELETE, and measuring that as well. If they are really progressive, they reward internal groups, partners, and 3rd party developers for the POSTs, PUTs, and DELETEs made. Incentivizing and then rewarding for desired behavior around valuable resources. POSTed a blog post? We’ll pay you $50.00. PUT 100 records, cleaning up addresses, we’ll pay you 50 cents per update. All you do is GET, GET, GET, well we’ll charge you accordingly, but if you also POST, and PUT, we’ll charge you a lower rate for your GETs, as well as apply credits to your account. API management shouldn’t be about three plans, and generating revenue, it should be about measuring the value exchange around ALL the data, content, media, and algorithmic exchanges that occurs on a daily basis.</p>

<p>API management has done amazing things for allowing companies, organizations, institutions, and government agencies to develop an awareness around who is accessing their resources. Look at what the Census has done with their API, what Capital One is doing with their API program, and Oxford Dictionaries are doing with their API program. Noticed I didn’t mention any startups, or tech rockstars? API management isn’t just about revenue generation. Don’t let the limited imagination of the startup space dictate otherwise. Now that API management is part of the fabric of the cloud, let’s begin to realize its full potential. Let’s not restrict ourselves to just a handful of plans. Let’s use it to broadly measure the value exchange around all the digital resources we are publishing to the web, making them available internally, to our partners, and the public in a machine readable way, so that they can be used in any web, mobile, or device application.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/17/api-management-and-the-measurement-of-value-exchange/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/17/api-transit-basics-deprecation/">API Transit Basics: Deprecation</a></h3>
        <span class="post-date">17 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-deprecation-2.png" align="right" width="30%" style="padding: 15px;" /></p>

<p><em>This is a series of stories I’m doing as part of <a href="http://basics.apievangelist.com/">my API Transit work</a>, trying to map out a simple journey that some of my clients can take to rethink some of the basics of their API strategy. I’m using a subway map visual, and experience to help map out the journey, which I’m calling <a href="http://basics.apievangelist.com/">API transit</a>–leveraging the verb form of transit, to describe what every API should go through.</em></p>

<p>This is a simple one. All APIs will eventually need to be deprecated. This is how you avoid legacy systems that have been up for over decades. Make sure the life span of each service is discussed as part of its conception, and put some details out about the expected timeline for its existence. Even if this becomes an unknown, at least you thought about it, and hopefully discussed it with others.</p>

<p>Here are just a few of the common building blocks I’m seeing with API operations that respect their users enough to plan for API deprecation:</p>

<ul>
  <li><strong>Releases</strong> - Have a set release schedule, and think about what will be deprecated along with each release, allowing for future planning with push.</li>
  <li><strong>Schedule</strong> - Have a deprecation schedule set for each API. You can always extend, or keep versions of your API beyond the date, but at least set a minimum schedule.</li>
  <li><strong>Communication</strong> - Make sure you have a communication strategy around deprecations. Post to the blog, Tweet out notices, and send emails.</li>
  <li><a href="https://tools.ietf.org/id/draft-wilde-sunset-header-03.html"><strong>The Sunset HTTP Header</strong></a> - This specification defines the Sunset HTTP response header field, which indicates that a URI is likely to become unresponsive at a specified point in the future.</li>
</ul>

<p>Another valuable concept this process will introduce is the possibility that APIs can be ephemeral and maybe only exist for days, weeks, or months. With CI/CD cycles allowing for daily, weekly, and monthly code pushes, there is no reason that APIs can evolve rapidly, and deprecate just as fast. Make sure deprecation is always discussed, and thought about in context of other legacy systems, and technical debt that exists at the organization.</p>

<p>API deprecation is inevitable. We might as well start planning for it from day one. Every API definition upon inception should have an API deprecation target date, 12 months, 18 months, or whatever your time frame is. You may have future versions of the API in place, and in some cases extend the life of an API, but having a deprecation strategy shows you are thinking about the future, considering change, as well as considering the impact on your consumers.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/17/api-transit-basics-deprecation/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/17/breaking-down-the-value-of-real-time-apis/">Breaking Down The Value Of Real Time APIs</a></h3>
        <span class="post-date">17 Jan 2018</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/80_168_800_500_0_max_0_1_-5.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am working to evolve an algorithm for Streamdata.io that helps measure the benefits of their streaming service. There are a couple layers to what they offer as a company, but as I dive into this algorithm, there are also multiple dimensions to what we all perceive as real time, and adding more complexity to the discussion, it is something that can significantly shift from industry to industry. The Streamdata.io team was working to productize this algorithm for quantifying the value their service delivers, but I wanted to take some time to break it down, lay it out on the workbench and think about it before they moved to far down this path.</p>

<p>Ok. To help me get my brain going, I wanted to work my way through the dictionary sites, exploring what is real time?  Real time often seems to describe a human rather than a machine sense of time. It is about communicating, showing, or presenting something at the time it actually happens, where there is no notable delay between the action and its effect or consequence. All of this is relative to the human receiving the real time event, as well as defining exactly when something truly happens / happened. Real time in banking is different than real time in stock trading, and will be different than media. All requiring their own perception of what is real time, and what the effects, consequences, and perceptions are.</p>

<p>When it comes to the delivery or streaming of real time events, it isn’t just about the delivering of the event, message, or transaction. It is about doing it efficiently. The value of real time gets ruined pretty quickly when you have to wade through too much information, or you are given too many updates of events, messages, and transactions that are not relevant. Adding an efficient element to the concept of what is real time. Real time, streaming updates of EVERYTHING are not as meaningful as streaming updates of only what just happened, staying truer to the concept of real time, in my opinion. Making the caching, and JSON Patch aspect of what Streamdat.io relevant to delivering a true real time experience–you only get what has changed in real time, not everything else.</p>

<p>To help me break down the algorithm for measuring the value delivered by Streamdata.io, I’ve started with creating three simple APIs.</p>

<ul>
  <li><strong>Poll API</strong> - A service for polling any API I give it. I can adjust the settings, but the default is that it polls it every 60 seconds, until I tell it to stop. Storing every response on a private Amazon S3 bucket.</li>
  <li><strong>API Size</strong> - A service that processes each API response in the bucket and calculates the size of the response.</li>
  <li><strong>API Change</strong> - A service that determines if an API response has changed from the previous response from 60 seconds before, identifying the frequency of change.</li>
</ul>

<p>This gives me a baseline of information I need to set the stage for what is real time. I am trying to understand what changes, and potentially what the value is of precise updates, rather than sending everything over the wire with each API response. After I set this process into motion for each API, I have another set of APIs for turning on the Streamdata.io portion, which reflects the other side of the coin.</p>

<ul>
  <li><strong>Stream API</strong> - This service proxies an API with Streamdata.io and begins to send updates every 60 seconds. Similar to the previous set of services, I am storing the initial request, as well as every incremental update on Amazon S3.</li>
  <li><strong>API Size</strong> - A service that processes each partial API response in the bucket and calculates the size of the response. If nothing changed, there was no JSON Patch response.</li>
  <li><strong>API Change</strong> - A service that determines if a partial API response has changed from the previous response from 60 seconds before, identifying the frequency of change. If nothing changed, there was no JSON Patch response.</li>
</ul>

<p>This gives me all the raw data I need to calculate the value which Streamdata.io delivers for any single API. However, it also gives me the raw data I need to begin calculating what is real time, and the value of it. We are tagging APIs that we catalog, allowing us to break down by common areas like finance, banking, media, transit, etc. This will allow us to start looking at how often things change within different sectors, and begin to look at how we can measure the value brought to the table when events, messages, and transactions are efficiently delivered in real time.</p>

<p>I am going to build me a dashboard to help me work with this data. I need to look at it for a couple of months, and run a number of different APIs through until I will know what dimensions I want to add next. I’m guessing I’m going to want some sort of freshness score on this, to see if something really truly is a new event, message, or transaction, or possibly being recirculated, duplicated, or some other anti-pattern. IDK. I’m guessing there are a number of new questions I will have about this data before I will truly be able to feel comfortable that the algorithm defines a meaningful vision of real time. Right now the algorithm sets to compute a couple meaningful efficiency gains.</p>

<ul>
  <li><strong>Client Bandwidth (BW) Savings</strong> - What efficiencies are realized when working with data in the client.</li>
  <li><strong>Server Bandwidth (BW) Savings</strong> - What efficiencies are realized in bandwidth, as data is transmitted.</li>
  <li><strong>Server CPU Savings</strong> - What efficiencies are realized on the service in CPU savings.</li>
</ul>

<p>You can see this calculated for <a href="http://apievangelist.com/2017/12/11/cost-savings-analysis-for-washington-metropolitan-area-transit-authority-wmata-data-apis/">the Washington Metropolitan Area Transit Authority (WMATA) Data APIs in a story I wrote last year</a>. I want to be able to calculate these efficiency gains, but I want to be able to do it over time, and begin to try and understand the real time dimension of savings, not just what is introduced through caching. These three calculations speak to the caching aspect of what Streamdata.io delivers, not the real time benefits of the service. Something that won’t be as straightforward to quantify, but I want to give it a try regardless.</p>

<p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/builder/filtered/80_174_800_500_0_max_0_-5_-5.jpg" align="right" width="45%" style="padding: 15px;" /></p>

<p>I want the algorithm to measure these efficiency gains, but I want to be able to capture the real time value of an API, both in quantifying the real time value delivered by the APIs, as well as the real time value delivered by Streamdata.io–establishing a combined real time ranking. This moves the algorithm into territory where it isn’t just describing the value delivered by Streamdata.io to their clients, but also quantifying the value delivered by the combination of the clients API, and Streamdata.io working together. This is where I think things will start to get interesting, especially as we begin to move Streamdata.io services, and our algorithm into new industries, and adding new dimensions and perceptions to the discussion.</p>

<p>This is when things will start to get interesting I feel. By the time we get to this point, the tagging structure I will have applied to different APIs will have evolved, and become more precise as well. Allowing me to further refine the algorithm to apply a real time value ranking to specific streams within a single provider, or even in aggregate across providers. Allowing consumers to subscribe to the most precise real time streams of events, messages, and transactions, and cutting out the noise and redundancy. This is when we can move things beyond just large volumes of data in real time, but precise volumes of data in real time. Consuming only what is needed, training our machine learning models on exactly what is required, and keeping them updating in real time, allowing us to deliver real time artificial intelligence streams that are updated by the second, or minute, producing the most relevant models possible.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/17/breaking-down-the-value-of-real-time-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/16/api-transit-basics-training/">API Transit Basics: Training</a></h3>
        <span class="post-date">16 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-classroom-api.png" width="45%" align="right" style="padding: 15px;" /></p>

<p><em>This is a series of stories I’m doing as part of <a href="http://basics.apievangelist.com/">my API Transit work</a>, trying to map out a simple journey that some of my clients can take to rethink some of the basics of their API strategy. I’m using a subway map visual, and experience to help map out the journey, which I’m calling <a href="http://basics.apievangelist.com/">API transit</a>–leveraging the verb form of transit, to describe what every API should go through.</em></p>

<p>Think of support as a reactive area, while training will be the proactive area of the API life cycle. Ensuring there is a wealth of up to date material for API developers and consumers across all stops along the API life cycle. Investing in internal, and partner capacity when it comes to the fundamentals of APIs, as well as the finer details of each stop along the API life cycle, and CI/CD pipelines will pay off big time down the road.</p>

<p>Every API should be included in training materials, workshops, and potentially part of conference talks given my product owners. If your API delivers value within your organization, to partners, and 3r party developers you should be training folks on putting this value to use. Here are some of the training areas I’m seeing emerge within successful API operations:</p>

<ul>
  <li><strong>Workshops</strong> - Conduct more of the workshops that I conducted with external consultants like me, as well as make sure they are conducted internally by each group. The first day of the our workshop was a great example of this in action.</li>
  <li><strong>Curriculum</strong> - Establish common approaches to designing, developing, and evolving curriculum for teaching about the API lifecycle, as well as using each individual API. Provide forkable templates that developers can easily put to work as part of their work, and make support materials a pipeline asset that gets deployed along with documentation, and other assets.</li>
  <li><strong>Conferences</strong> - Make sure you are sending team members to the latest conferences. In my conversations during the workshop, this didn’t seem like a problem, but something I think should be included anyways.</li>
</ul>

<p>Like every other stop along this journey, API training can be a pipeline artifact and be developed, deployed, and evolved alongside all other code, documentation, and available solutions. Pushing everyone to not just attend trainings, but also work to develop and deliver curriculum as part of trainings helps make sure everyone is able to communicate what they do across the company. Everyone should be contributing to, executing, and participating in API training, no matter what their skill level, or ability to get up in front of people and speak.</p>

<p>API training will increase adoption, and save resources down the road. It will compliment platform communications, and strengthen support, while providing valuable feedback that can be included as part of the road map. My favorite part about doing API training is that it forces me as a developer to think through my ideas, consider how I can articulate and share them with others, which if you think about it, is an essential part of the API journey, and something we should bake into operations by default.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/16/api-transit-basics-training/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/16/transit-authorities-need-to-understand-that-api-management-means-more-control-and-revenue/">Transit Authorities Need to Understand that API Management Means More Control and Revenue</a></h3>
        <span class="post-date">16 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/32_161_800_500_0_max_0_-5_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>As I look through the developer, data, and API portals of transit authorities in cities around the United States, one thing is clear–they are all underfunded, understaffed, and not a priority. This is something that will have to change if transit authorities are expected to survive, let alone thrive in a digital world. I’m working on a series of white papers as part of <a href="http://apis.how/streamdata">my partnership with Streamdata.io</a>, on transit data, and the monetization of public data. I’ll be writing more on this subject as part of this work, but I needed to start working through my ideas, and begin crafting my narrative around how transit authorities can generate much needed revenue and compete in this digital world.</p>

<p>I know that many transit authorities often see data as a byproduct, and something that is occasionally useful, and that their main objective is to keep the trains running. However, every tech company, and developer out there understands the value of the data being generated by the transit schedule, the trains, and most importantly the ridership. The Googles, Ubers, and data brokers are mining this data, enriching their big data warehouses, and actively working to generate revenue from transit authorities most valuable assets–their riders. Ticket sales, and people riding the trains seems like the direct value generation, but in the era of big data, where those people are going, what they are doing, reading, thinking, and who they are doing it with is equally or more valuable than the price of the ticket to ride.</p>

<p>Most of the transit APIs I’m using require you to sign up for a key, so there is some sort of API management in place. However, there are still huge volumes of feeds and downloads that are not being managed, and there is no evidence that there is any sort of analysis, reporting, or other intelligence being gathered from API consumption. Demonstrating that API management is more about rate limiting, and keeping servers up, than it is ever realizing what API management is truly about in the mainstream API world–value and revenue generation. Transit authorities need to understand that API management isn’t just about restricting access, it is about encouraging access, and developing awareness regarding the value of the data resources they have in their possession.</p>

<p>I’m sure there is so much more data available behind the scenes beyond the schedules, vehicles, or even the real time information some transit authorities are making available. There is no reason that fare purchases, user demographics, neighborhood, sensors, ticket swipes, and other operational data can’t be made available, of course taking care of privacy and security concerns. Technology platforms, and application developers have an insatiable appetite for this type of data, and are willing to pay for it. The greatest lie the devil has told is that public data should always be free, even for commercial purposes, allowing tech companies to freely mine, and generate revenues while transit authorities and municipalities struggle to make ends meet.</p>

<p>I’m not saying that transit data shouldn’t be freely available, and accessible by everyone, but there needs to be more balance in the system, and API management is how you do this. You measure who takes from the system, how much, and you pay or charge accordingly. Right now, there are applications mining transit data schedules and rider details, and selling leads to Uber, Lyft, and other ride share, further cannibalizing our public transit system. Transit authorities need to realize the value they generate on a daily basis, and understand that API management is key to quantifying this value, and generating the revenue they need to stay in operation, and even grow and thrive, better serving their constituents at a time when they need them the most.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/16/transit-authorities-need-to-understand-that-api-management-means-more-control-and-revenue/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/16/the-value-of-historical-transit-data-when-it-comes-to-machine-learning/">The Value of Historical Transit Data When it Comes to Machine Learning</a></h3>
        <span class="post-date">16 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/52_175_800_500_0_max_0_1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m working through the different ways that transit authorities can generate more revenue from their data using APIs as part of my work with Streamdata.io. Making data streaming and truly more real time is the obvious goal of this research, but Streamdata.io is invested in transit authorities take more control over their data resources, and use APIs to generate revenue at a time when they need all the revenue they can possible get their hands on.</p>

<p>One overlap in the projects I’m working on with Streamdata.io is where transit data intersects with machine learning, and artificial intelligence. I’m not sure what transit authorities are doing with their historical data, but I know that it isn’t available via their APIs, and developer portals. I’m guessing they see historical data about schedules, vehicles, riderships, and other data points as a burden, and once they’ve generated the reports they need, don’t do anything else with it. This historical data is a goldmine of information when it comes to training machine learning models, which could then in turn be better used to understand ridership, make predictions, understand maintenance, scheduling, and other aspects of transit operations–let alone commerce, real estate, and other demographic data.</p>

<p>There is a dizzying amount of investment going into machine learning and artificial intelligence right now, and is something that could be routed to transit authorities to help boost revenue. If all historical data on transit operations was digitized and available via APIs, then metered using modern API management approaches, it could be an entirely new revenue opportunity for transit authorities. Transit systems are the heartbeat of the cities they operate within, and historical data is the record of everything that occurs, which can be used to develop machine learning models for the transit industry, as well as real estate, commerce, and other sectors that transit systems feed into on a daily basis, and have for years.</p>

<p>I do not know what data transit authorities possess. I don’t know how much historical data they keep around, and what is required by government regulators, but I do know whatever there is, it has value. I’ve studied how API management is being used by tech companies for almost 8 years now, and it is how value is created, and revenue is generated, something that transit authorities and leadership needs to realize applies to them in a digital age. They are sitting on a wealth of historical data that would be of value to tech companies who are already mining their existing schedules, and real time vehicle data. Historical transit data, and machine learning just represents one of many opportunities on the table for transit authorities to tap when it comes to looking for new revenue opportunities in the future.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/16/the-value-of-historical-transit-data-when-it-comes-to-machine-learning/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/16/moving-beyond-a-single-api-developer-portal/">Moving Beyond A Single API Developer Portal</a></h3>
        <span class="post-date">16 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/window_clean_view.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am working with a number of different groups who are using developer portals in some very different ways once they have moved beyond the concept that API developer portals are just for use in the public domain. With the introduction of the static site shift in the CMS landscape, and the introduction of Jekyll and Github Pages as part of the Github project workflow, the concept of the API developer portal is beginning to mean a lot of different things, depending on what the project objective is.</p>

<p>An API portal is becoming something that can reflect a specific project, or group, and isn’t something that always has a public URL. Here are just a few of the ways in which I’m seeing portals being wielded as part of API operations.</p>

<ul>
  <li><strong>Individual Portals</strong> - Considering how developers and business users can be leverage portals to push forward conversations around the APIs they own and are moving forward.</li>
  <li><strong>Team Portals</strong> - Thinking about how different groups and teams can have their own portals which aggregate APIs and other portals from across their project.</li>
  <li><strong>Partner Portals</strong> - Leveraging a single, or even partner specific portals that are public or private for engaging in API projects with trusted partners.</li>
  <li><strong>Public Portal</strong> - Begin the process of establishing a single Mutual of Omaha developer portal to provide a single point of entry for all public API efforts across the organization.</li>
  <li><strong>Pipeline Integration</strong> - How can BitBucket be leverage for deploying of individual, team, partner, and even the public portal, making portals another aspect of the continuous deployment pipeline.</li>
</ul>

<p>One of the most interesting shifts that I am seeing is the deployment of portals as part of continuous deployment and integration pipelines. Since you can host a portal on Github, why not be deploying it, managing and evolving it as its own pipeline, or as part of individual projects, and partner integrations. This is something that static CMSs have have a profound effect on, as well as the integration of YAML, JSON, and CSV static data formats, which can be used to deliver data, content, and configurations that can be used throughout project pipelines.</p>

<p>Like every other stop along the API life cycle, API portals are quickly becoming more modular, and often times more ephemeral, shifting from the days where we just had one single API portal. We are beginning to move beyond just the concept of a single portal, and seeing a mix of API centric destinations that are public or private, and reflect the changing objectives of different teams, partners, and event outside industry influences.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/16/moving-beyond-a-single-api-developer-portal/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/16/api-transit-basics-support/">API Transit Basics: Support</a></h3>
        <span class="post-date">16 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-support.png" align="right" width="45%" style="padding: 15px;" /></p>

<p><em>This is a series of stories I’m doing as part of <a href="http://basics.apievangelist.com/">my API Transit work</a>, trying to map out a simple journey that some of my clients can take to rethink some of the basics of their API strategy. I’m using a subway map visual, and experience to help map out the journey, which I’m calling <a href="http://basics.apievangelist.com/">API transit</a>–leveraging the verb form of transit, to describe what every API should go through.</em></p>

<p>Beyond communication, make sure there is adequate support across API teams, and the services, tooling, and processes involved with operations. If an API goes unsupported, it might as well not exist at all, making standardized, comprehensive support practices essential. Every API should have an owner, with a contact information published as part of all API definitions. Ideally, every API owner has a backup point of contact to go to if someone should leave a company, or is out sick.</p>

<p>Similar to the communications stop along this journey, there a handful of common support building blocks you see present within the leading API pioneers portals. These are the four I recommend baking in by default to all of your API portals, and anywhere API discovery and documentation exists.</p>

<ul>
  <li><strong>Email</strong> - Make sure there is support available via email channels, with a responsive individual on the other end–with accountability.</li>
  <li><strong>Tickets</strong> - Consider a ticketing system for submitting and supporting requests around API operations.</li>
  <li><strong>Dedicated</strong> - Identify one, or many individuals who can act as internal, partner, and public support when it makes sense.</li>
  <li><strong>Office Hours</strong> - Consider one time a week where there is a human being available in person, or online to answer direct question.</li>
</ul>

<p>Every single API should have a support element present as part of its operations. Each API definition allows for the inclusion of responsible point of contact via email, and other channels–make use of it. Support should be baked into each API’s definition, making it accessible across the API lifecycle, in a machine readable way. Ideally, every API developer provides support for their own work, bringing them closer to how their solutions are working, or not working for consumers.</p>

<p>API support isn’t rocket surgery, but will make or break your API operations if not done well. The APIs that perform the best, will have strong support behind them. The APIs that go dormant, or aren’t reliable will not have proper support. Developers do not think about support by default, which means it will often go overlooked unless a more senior developer, business user, or manager steps up. This is why API operations is a team sport, because there are different skill levels, and personalities at play, and while proper support won’t be everyone’s strength, it is something everyone should be responsible for.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/16/api-transit-basics-support/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/16/providing-a-guest-api-key/">Providing a Guest API Key</a></h3>
        <span class="post-date">16 Jan 2018</span>
        <p><a href="https://developer.wmata.com/Products"><img src="https://s3.amazonaws.com/kinlane-productions/washington-metro/wmata-guest-key.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>I’m spending time immersed in the world of transit data and APIs lately, and found a simple, yet useful approach to helping onboard developers over at the <a href="https://developer.wmata.com/">Washington Metropolitan Area Transit Authority (WMATA) API</a>. When you you click on <a href="https://developer.wmata.com/Products">their products page</a> (not sure why they use this name), you get a guest API key which allows you try out the API, and kick the tires. Of course, you can’t use the key in production applications, as it is rate limited and can change at any time, but the concept is simple, and provides an example which other API providers might want to consider.</p>

<p>In my days as the API Evangelist I’ve seen API providers do this in a variety of ways, by providing sample API URLs complete with an API key, and by embedding a key in the OpenAPI definition, so that the interactive documentation picks up the key and will allow developers to make live, interactive API calls–to name a few. No matter what your approach, providing a guest key for users to play around without signing up makes a lot of sense. Of course, you want to rotate this key regularly, or at least be monitoring it to see what IP addresses it is being called from, and maybe understand how its being used–you never know, it might reveal some interesting use cases.</p>

<p>Whatever your approach, get out of the way of your consumers. Don’t expect that everyone is going to want to sign up for an account so that they can learn about what your API does. Not everyone is interested in handing over their email address, and other information just so they can test drive. If you have API management in place (which you should), it really doesn’t take much to generate test users, applications, and keys, monitor them, and rotate them on a regular basis. I recommend using them like you would marketing tags, and strategically place them into different blog posts, documentation, widgets, and other resources–you never know what other insight you might learn about how people are putting your resources to use.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/16/providing-a-guest-api-key/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/15/api-transit-basics-communication/">API Transit Basics: Communication</a></h3>
        <span class="post-date">15 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-comment-bubbles.png" align="right" width="45%" style="padding: 15px;" /></p>

<p><em>This is a series of stories I’m doing as part of <a href="http://basics.apievangelist.com/">my API Transit work</a>, trying to map out a simple journey that some of my clients can take to rethink some of the basics of their API strategy. I’m using a subway map visual, and experience to help map out the journey, which I’m calling <a href="http://basics.apievangelist.com/">API transit</a>–leveraging the verb form of transit, to describe what every API should go through.</em></p>

<p>Moving further towards the human side of this API transit journey, I’d like to focus on one of the areas that I see cause the failure and stagnation of many API operations–basic communications. A lack of communication, and one way communication are the most common contributors to APIs not reaching their intended audience, and establishing much needed feedback loops that contribute to the API road map. This portion of the journey is not rocket science, it just take stepping back from the tech for a moment and thinking about the humans involved.</p>

<p>When you look at Twitter, Twilio, Slack, Amazon, SalesForce, and the other leading API pioneers you see a handful of communication building blocks present across all of them. These are just a few of the communication elements that should be present in both internal, as well as external or publicly available API operations.</p>

<ul>
  <li><strong>Blogs</strong> - Make blogs a default part of ALL portals, whether partner, public, or internal. They don’t have to be grand storytelling vehicles, but can be used as part of communicating around updates within teams, groups, and for projects.</li>
  <li><strong>Twitter</strong> - Not required for internally focused APIs, but definitely essential if you are running a publicly available API.</li>
  <li><strong>Github</strong> - Github enables all types of communication around repos, issues, wikis, and other aspects of managing code, definitions, and content on the social coding platform.</li>
  <li><strong>Slack</strong> - Leverage Slack for communicating around APIs throughout their life cycle, providing a history of what has occurred from start to finish.</li>
  <li><strong>API Path IDs</strong> - Establish common DNS + API path identifiers for creating threads around each API, allowing for discussions on BitBucket, Slack, and in emails when it comes to each API.</li>
</ul>

<p>I recommend making communication a default requirement for all API owners, stewards, and evangelist who work internally, as well as externally. Ensuring that there is communication around the existence, and life cycle of an API, and helping make sure there is awareness across teams, as well as up the management chain. It’s not rocket science, but it is essential to doing business around programmatic interfaces. You don’t have to be a poet, or prolific blogger, but you do have to care about keeping your API consumers informed.</p>

<p>Communication around API operations is easily overlooked, and difficult to recreate down the road. Just put it in place from the beginning, and don’t worry about activity levels. Be genuine in what you publish and share, and be responsive and open with your readers. Whenever possible make things a two-way street, allowing readers to share their thoughts. Track everything, and route it back into your road map, leveraging all communications as part of the API feedback loop.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/15/api-transit-basics-communication/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/15/can-i-resell-your-api/">Can I Resell Your API?</a></h3>
        <span class="post-date">15 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/docks_copper_circuit.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>Everyone wants their API to be used. We all suffer from “if we build it, they will come” syndrome in the world of APIs. If we craft a simple, useful API, developers will flock to it and integrate it into their applications. However, if you operate an API, you know that getting the attention of developers, and standing out amongst the growing number of APIs is easier said than done. Even if your API truly does bring the value you envision to the table, getting people to discover this value, and invest the time into integrating it into the platforms, products, and services takes a significant amount of work–requiring that you remove all possible obstacles and friction from any possible integration opportunity.</p>

<p>One way we can remove obstacles for possible integrations is by allowing for ALL types of applications–even other APIs. If you think about it, APIs are just another type of application, and one that many API providers I’ve talked with either haven’t thought about at all, or haven’t thought about very deeply and restrict this use case, as they see it as directly competing with their interests. Why would you want to prevent someone from reselling your API, if it brings you traffic, sales, and the other value your API brings to your company, organization, institution, or government agency? If a potential API consumer has an audience, and wants to private label your API, how does that hurt your business? If you have proper API management in place, and have a partner agreement in place with them, how is it different than any other application?</p>

<p>I’ve been profiling companies as part of my partnership with <a href="http://apis.how/streamdata">Streamdata.io</a>, looking for opportunities to deliver real time streaming APIs on top of existing web APIs. Ideally, API providers become a Streamdata.io customer, but we are also looking to enable other businesses to step up and resell existing APIs as a streaming version. However, in some of the conversations I’m having, people are concerned about whether or not API provider’s terms of service will allow this. These developers are worried that revenue generation through the reselling of an existing API as something that would ruffle the feathers of their API provide, and result in getting their API keys turned off. Which is a completely valid concern, and something that is spelled out in some terms of service, but I’d say is often left more as an unknown, resulting in this type of apprehension from developers.</p>

<p>Reselling APIs is something I’m exploring more as part of my API partner research. Which APIs encourage reselling, white and private labeling, and OEM partnerships? Which APIs forbid the reselling of their API? As well as which APIs have not discussed it all. I’d love to hear your thoughts as an API provider, or someone who is selling their services to the API space. What are your thoughts on reselling your API, and have you had conversations with potential providers on this subject. I am going to explore this with Streamdat.io, APIMATIC, and other companies I already work with, as well as reach out to some API providers I’d like to resell as a streaming API using Streadmata.io and see what they say. It’s an interesting conversation, which I think we’ll see more discussion around in 2018.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/15/can-i-resell-your-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/15/api-transit-basics-security/">API Transit Basics: Security</a></h3>
        <span class="post-date">15 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-padlock.png" align="right" width="25%" style="padding: 15px;" /></p>

<p><em>This is a series of stories I’m doing as part of <a href="http://basics.apievangelist.com/">my API Transit work</a>, trying to map out a simple journey that some of my clients can take to rethink some of the basics of their API strategy. I’m using a subway map visual, and experience to help map out the journey, which I’m calling <a href="http://basics.apievangelist.com/">API transit</a>–leveraging the verb form of transit, to describe what every API should go through.</em></p>

<p>Hopefully you already have your own security practices in place, with the ability to scan for vulnerabilities, and understand where security problems might exist. If you do, I’m guessing you probably already have procedures and protocols around reporting, and handling security problems across teams. Ideally, your API security practices are more about prevention than they are about responding to a crisis, but your overall strategy should have plans in place for addressing both ends of the spectrum.</p>

<p>Unfortunately in the wider API space, much of the conversation around API security has been slowed by many people feeling like their API management solutions were doing everything that is needed. Luckily, in 2017 we began to see this thaw a bit and some API security focused solutions began to appear on the market, as well as some existing players began tuning into to address the specific concerns of API security, beyond the desktop, web, and other common areas of concern.</p>

<ul>
  <li><a href="https://zaproxy.blogspot.com/2017/06/scanning-apis-with-zap.html"><strong>Scanning APIs with OWASP Zap</strong></a> - OWASP is the top place for understanding security vulnerabilities of web applications, and they are expanding their focus to include APIs.</li>
  <li><a href="https://www.42crunch.com/"><strong>42 Crunch</strong></a> - A new, OpenAPI driven API security solution for helping deliver policies across API operations.</li>
  <li><a href="https://www.owasp.org/index.php/REST_Security_Cheat_Sheet"><strong>OWASP REST Security Cheat Sheet</strong></a> - A checklist of considerations when it comes to API security out of OWASP.</li>
</ul>

<p>After crafting this stop along the API lifecycle I wanted to make sure and include API discovery in the conversation. API definitions like OpenAPI, and a solid API discovery strategy helps provide the details of the surface area of API operations, allowing for easier scanning and securing of existing infrastructure. Another area that significantly introduces security benefits is making logging a first class citizen, allowing the DNS, gateway, code, server, and database layers to analyzed for vulnerabilities.</p>

<p>I prefer keeping this security stop short and sweet, as I know from experience that not all my readers have a strategy in place, and I want to give them a handful of options to consider as they look to get started. Many groups have been focusing on web and mobile security, but are just getting started thinking about API security. As APIs move out of the shadows behind mobile applications, and the number of threats increase, companies, institutions, and government agencies are getting more nervous, increasing the need for more API security storytelling here on my site.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/15/api-transit-basics-security/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/15/an-organized-approach-to-openapi-vendor-extensions-across-api-teams/">An Organized Approach to OpenAPI Vendor Extensions Across API Teams</a></h3>
        <span class="post-date">15 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/containership_blue_circuit.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>One of aspects of a project I’ve been assisting with recently involves helping define, implement, and organize the usage of OpenAPI vendor extensions across a distributed microservice development team. When I first began advising this group, I introduced them to the concept of extending the OpenAPI definition using x-extension format, expanding the teams approach to how they use the OpenAPI specification. They hadn’t heard that you can extend OpenAPI beyond what the specification brings to the table, allowing them to make it deliver exactly what they needed.</p>

<p>Within this project each microservice exists in its own Github repository, with an OpenAPI definition available in the root, defining the surface area of the API. At the organizational level, I have a script that will loop through all Github repos, spider each OpenAPI looking for any x-extensions, and then it aggregates them into a single master list of OpenAPI vendor extensions that are used across all APIs. I then take the list, and add descriptions to each verified extension, and when I come across ones I haven’t seen before I reach out to microservice owners to understand what the vendor extension is used for, and ensure it is in alignment with overall API governance for the project.</p>

<p>I am looking to encourage API designers, developers, and architects to extend OpenAPI. I am also looking to help them be responsible with this power, and make sure they are doing it for good reasons. I am also looking to organize, then educate across teams regarding how different groups are using OpenAPI vendor extensions, and incentivize the reuse and standardization of these extensions. I see vendor extensions as an area for potential innovation when it comes to defining what an API is capable of, as well as the relationship each microservice has with its supporting architecture. Acting for a relief valve for the often perceived constraints of the OpenAPI specification.</p>

<p>This project reminded me that I need to make sure and add a reminder to pick my head up each week and spend time aggregating vendor extensions from across the API space. <a href="http://openapi.toolbox.apievangelist.com/">I have a section dedicated to them in my OpenAPI toolbox</a>, but I haven’t added anything to it recently. OpenAPI Vendor extensions are an important way to learn about how companies are extending the specification, and similar to what I’m doing for this individual project, it is important to aggregate and organize them so people in the community can learn from them, and reuse them whenever possible.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/15/an-organized-approach-to-openapi-vendor-extensions-across-api-teams/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/12/orchestrating-api-integration-consumption-and-collaboration-with-the-postman-api/">Orchestrating API Integration, Consumption, and Collaboration with the Postman API</a></h3>
        <span class="post-date">12 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/postman/the-postman-api.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>You hear me say it all the time–if you are selling services and tooling to the API sector, you should have an API. In support of this way of thinking I like to highlight the API service providers I work with who follow this philosophy, and today’s example is from (Postman](https://getpostman.com). If you aren’t familiar with Postman, I recommend getting acquainted. It is an indispensable tool for integrating, consuming, and collaborating around the APIs you depend on, and are developing. Postman is essential to working with APIs in 2018, no matter whether you are developing them, or integrating with 3rd party APIs.</p>

<p>Further amplifying the usefulness of Postman as a client tool, <a href="https://docs.api.getpostman.com">the Postman API</a> reflects the heart of what Postman does as not just a client, but a complete life cycle tool. The Postman API provides five separate APIs, allowing you orchestration your API integration, consumption, and collaboration environment.</p>

<ul>
  <li><a href="https://docs.api.getpostman.com/#8ca888b7-ef54-f3b4-312f-3f3e2e2cf04e"><strong>Collections</strong></a> - The /collections endpoint returns a list of all collections that are accessible by you. The list includes your own collections and the collections that you have subscribed to.</li>
  <li><a href="https://docs.api.getpostman.com/#a237ffbe-0444-b394-a2c4-b99f691931cf"><strong>Environments</strong></a> - The /environments endpoint returns a list of all environments that belong to you. The response contains an array of environments’ information containing the name, id, owner and uid of each environment.</li>
  <li><a href="https://docs.api.getpostman.com/#ef6bef63-0b8e-1a70-dd88-c7c1b94f8dab"><strong>Mocks</strong></a> - This endpoint fetches all the mocks that you have created.</li>
  <li><a href="https://docs.api.getpostman.com/#993648ea-7b28-9636-f532-1ef8a74ff093"><strong>Monitors</strong></a> - The /monitors endpoint returns a list of all monitors that are accessible by you. The response contains an array of monitors information containing the name, id, owner and uid of each monitor.</li>
  <li><a href="https://docs.api.getpostman.com/#1ddc0a3c-d5ff-b062-d0fb-9e6086be0536"><strong>User</strong></a> - The /me endpoint allows you to fetch relevant information pertaining to the API Key being used.</li>
</ul>

<p>The user, collections, and environments APIs reflect the heart of the Postman API client, where mocks and monitors reflects its move to be a full API life cycle solution. This stack of APIs, and the Postman as a client tool reflects how API development, as well as API operation should be conducted. You should be maintaining collections of APIs that exist within many environments, and you should always be mocking interfaces as you are defining, designing, and developing them. You should then also be monitoring all the APIs you depend–whether or not the APIs are yours. If you depend on APIs, you should be monitoring them.</p>

<p>I’ve long been advocating that someone development an API environment management solution for API developers, providing a single place we can define, store, and share the configuration, keys, and other aspects of integration with the APIs we depend on. The Postman collections and environment APIs is essentially this, plus you get all the added benefits of the services and tooling that already exist as part of the platform. Demonstrating why as an API service provider, you want to be following your own advice and having an API, because you never know when the core of your solution, or even one of the features could potentially become baked into other applications and services, and be the next killer feature developers can’t do without.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/12/orchestrating-api-integration-consumption-and-collaboration-with-the-postman-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/12/api-life-cycle-basics-testing/">API Life Cycle Basics: Testing</a></h3>
        <span class="post-date">12 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-testing.png" align="right" width="35%" style="padding: 15px" /></p>
<p>Every API should be tested to ensure it delivers what is expected of it. All code being deployed should meet required unit and code tests, but increasingly API testing is adding another layer of assurance to existing build processes, even going so far as halting CI/CD workflows if tests fail. API testing is another area where API definitions are delivering, allowing tests to be built from existing artifacts, and allowing detailed assertions to be associated with tests to add to and evolve the existing definitions.</p>

<p>API testing has grown over the last couple of years to include a variety of open source solutions, as well as cloud service providers. Most of the quality solutions allow you to import your OpenAPI, and automate the testing via APIs. Here are a few of the solutions I recommend considering as you think about how API testing can be introduced into your API operations.</p>

<ul>
  <li><a href="https://www.runscope.com/">Runscope</a> - An API testing service that uses OpenAPI for importing and exporting of API tests and assertions.</li>
  <li><a href="https://github.com/CacheControl/hippie-swagger">Hippie-Swagger</a> - An open source solution for testing your OpenAPI defined APIs.</li>
  <li><a href="https://cloud.spring.io/spring-cloud-contract/">Spring Cloud Contract</a> - Spring Cloud Contract is an umbrella project holding solutions that help users in successfully implementing the Consumer Driven Contracts approach.</li>
  <li><a href="https://www.getpostman.com/docs/postman/scripts/test_scripts">Postman Testing</a> - With Postman you can write and run tests for each request using the JavaScript language.</li>
  <li><a href="https://www.frisbyjs.com/">Frisby.js</a> - Frisby is a REST API testing framework built on Node.js and Jasmine that makes testing API endpoints easy, fast, and fun.</li>
</ul>

<p>There are numerous ways to augment API testing on top of your existing testing strategy. More of these providers are integrating with Jenkins and other CI/CD solutions, allowing API testing to deeply integrate with existing pipelines. My recommendation is that the artifacts from these tests and assertions also live alongside OpenAPI and other artifacts and are used as part of the overall definition strategy, widening the meaning of “contract” to apply across all stops along the lifecycle–not just testing.</p>

<p>While API testing may seem like common sense, I’d say that more than 50% of the organizations I’m talking with do not actively test all their APIs. Of the ones that do, I’d say less than half get very granular in their testing or follow test driven development philosophies. This is where service providers like Runscope deliver, helping bring the tools and expertise to the table, allowing you to get up and running in a cloud environment, building on a platform, rather than started from scratch when it comes to your API testing.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/12/api-life-cycle-basics-testing/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/12/api-life-cycle-basics-sdks/">API Transit Basics: SDKs	</a></h3>
        <span class="post-date">12 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-sdk.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>Software Development Kits (SDKs), and code libraries in a variety of programming languages have always been a hallmark of API operations. Some API pundits feel that SDKs aren’t worth the effort to maintain, and keep in development alongside the rest of API operations, while others have done well delivering robust SDKs that span very valuable API stacks–consider the AWS JavaScript SDK as an example. Amidst this debate, SDKs continue to maintain their presence, and even have been evolving to support a more continuous integration (CI) and continuous deployment (CD) approach to delivering APIs and the applications that depend on them.</p>

<p>Supporting SDKs in a variety of programming languages can be difficult for some API providers. Luckily there is tooling available that help auto-generate SDKs from API definitions, helping make the SDK part of the conversation a little smoother. Of course, it depends on the scope and complexity of your APIs, but increasingly auto-generated SDKs and code as part of a CI/CD process is becoming the normal way of getting things done, whether you are just making them available to your API consumers, or you are actually doing the consuming yourself.</p>

<ul>
  <li><a href="https://github.com/swagger-api/swagger-codegen">Swagger Codegen</a> - The leading open source effort for generating SDKs from OpenAPI.</li>
  <li><a href="https://apimatic.io/">APIMATIC</a> - The leading service for generating SDKs from OpenAPI, and including as part of existing CI/CD efforts.</li>
  <li><a href="https://restunited.com/">RESTUnited</a> - The easiest way to generate SDKs (REST API libraries):
PHP, Python, Ruby, ActionScript (Flash), C#, Android, Objective-C, Scala, Java</li>
</ul>

<p>Depending on your versioning and build processes, SDK generation can be done alongside all the other stops along this life cycle. When you iterate on an API, you simply auto-generate documentation, tests, SDKs, and other aspects of supporting your services. Not all providers I talk with are easily able to jump into the aspect of producing code, as their build processes aren’t as streamline, and some of their APIs are too large to expect auto-generated code to perform as expected. However, it is something they are working towards, along with other microservices, and decoupling efforts going on across their teams.</p>

<p>Once you realize an API definition driven approach to delivering APIs, the line between deployment and SDKs blurs–it is all about generating code from your definitions. Sometimes the code is providing resources, and other times it is consuming them. It just comes down to whether you are deploying code server or client side. Another significant shift I’m seeing in the landscape with SDKs, are things moving beyond just programming languages, and providing platform specific libraries for managing SalesForce, AWS, Docker, and other common components of our operations–further evolving the notion of what an SDK is and does in 2018.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/12/api-life-cycle-basics-sdks/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/12/aws-has-head-start-helping-navigate-regulatory-compliance-in-the-cloud/">AWS Has Head Start Helping Navigate Regulatory Compliance In The Cloud</a></h3>
        <span class="post-date">12 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/aws/aws-cloud-compliance.png" align="right" width="45%" style="padding 15px;" /></p>
<p>I’m providing API guidance on a project being delivered to a government agency, as part of <a href="https://skylight.digital/">my Skylight partnership</a>, and found myself spending more time looking around <a href="https://aws.amazon.com/compliance/">the AWS compliance department</a>. You can find details on certifications, regulations, laws, and frameworks ranging from HIPPA and FERPA to FedRAMP, so that it can be used by federal government agencies in the United States, and other countries. You can find <a href="https://aws.amazon.com/compliance/services-in-scope/">a list of services that are in scope</a>, and track on their progress when it comes to compliance across this complex web of compliance rules. I’ve been primarily tracking on the progress of the AWS API Gateway which is currently in progress when it comes to FedRAMP compliance.</p>

<p>When it comes to regulatory compliance, AWS has a significant leg up on its competitors, <a href="https://cloud.google.com/security/compliance">Google</a> and <a href="https://www.microsoft.com/en-us/trustcenter/compliance/default.aspx">Microsoft</a>. Both of these cloud platforms have existing regulatory efforts, but they aren’t as organized, or as far along as AWS’s approach to delivering in this area. Delivering cloud solutions that are compliant gives AWS a pretty significant advantage when it comes to first impressions with government agencies, and enterprise organizations operating within heavily regulated industries. Once this impression is made, and these groups have gotten a taste of AWS, and migrated systems, and data to their cloud, it will be hard to change their behavior.</p>

<p>As this whole Internet thing grows up, regulatory compliance is unavoidable. Many companies, organizations, institutions, and government agencies we are selling to are already needing to deliver when it comes to compliance, but even for the shiny new starts breaking new ground, at some point you will have to mature and deliver within regulatory constraints. Making AWS a pretty appealing place to be publishing databases, servers, and I’m hoping pretty soon, APIs using AWS API Gateway. If you are on the AWS API Gateway team, I’d love to get an update on the status, as I have a big government project I’d love to deploy using the API Gateway, instead of another industry provider gateway solution.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/12/aws-has-head-start-helping-navigate-regulatory-compliance-in-the-cloud/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/11/api-life-cycle-basics-clients/">API Life Cycle Basics: Clients</a></h3>
        <span class="post-date">11 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-client.png" align="right" width="30%" style="padding: 15px;" /></p>
<p>I broke this area of my research into a separate stops a couple years back, as I saw several new types of service providers emerging to provide a new type of web-based API client. These new tools allowed you to consume, collaborate, and put APIs to use without writing any code. I knew that this shift was going to be significant, even though it hasn’t played out as I expected, with most of the providers disappearing, or being acquired, and leaving just a handful of solutions that we see today.</p>

<p>These new web API clients allow for authentication, and the ability to quickly copy and paste API urls, or the importing of API definitions to begin making requests, and seeing responses for targeted aPIs. These clients were born out of earlier API explorers and interactive API documentation, but have matured into standalone services that are doing interesting things to how we consume APIs. Here are the three web API clients I recommend you consider as part of your API life cycle.</p>

<ul>
  <li><a href="https://www.getpostman.com/"><strong>Postman</strong></a> - A desktop and web client for working with and collaborating in a team environment around APIs.</li>
  <li><a href="https://paw.cloud/"><strong>PAW</strong></a> - Paw is a full-featured HTTP client that lets you test and describe the APIs you build or consume. It has a beautiful native macOS interface to compose requests, inspect server responses, generate client code and export API definitions.
-<a href="http://www.restfiddle.com/"><strong>RESTFddle</strong></a> - An easy-to-use platform to work with APIs. It simplifies everything from API exposure to API consumption. RESTFiddle is an Enterprise-grade API Management Platform for teams. It helps you to design, develop, test and release APIs.</li>
</ul>

<p>Using web API clients allows for APIs to be easily defined, mocked, collaborated around, and leveraged as part of an API definition driven life cycle. The approach to integration saves significant cycles by allowing APIs to be designed, developed, and integrated with before any code gets written. Plus, the team and collaboration features that many of them posses can significantly benefit the process of not just consuming APIs, but also developing them. Making API clients an essential part of any development team, no matter what you are building.</p>

<p>Using API clients, bundled with an API definition-driven approach, and a healthy API mocking setup, can save you significant time and money when it comes to crafting the right API. What used to take years of development to iterate around, can take days or weeks, allowing you to define, design, mock, consume, collaborate, communicate, and iterate until exactly the right API is delivered. This approach to API development is changing how we deliver APIs, making operations much more flexible, agile, and fast moving, over the historically rigid, brittle, and slow moving approach to delivering API resources.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/11/api-life-cycle-basics-clients/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/11/the-metropolitan-transportation-authority-bus-time-api-supports-service-interface-for-real-time-information-siri/">The Metropolitan Transportation Authority (MTA) Bus Time API Supports Service Interface for Real Time Information (SIRI)</a></h3>
        <span class="post-date">11 Jan 2018</span>
        <p><a href="http://bustime.mta.info/wiki/Developers/SIRIIntro"><img src="https://s3.amazonaws.com/kinlane-productions/mta/mta-bus-time-siri.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>The General Transit Feed Specification (GTFS) format for providing access to transit data has dominated the landscape for most of the time I have been researching transit data and APIs over the last couple of weeks. A dominance led by Google and their Google Maps, who is <a href="https://developers.google.com/transit/">the primary entity behind GTFS</a>. However the tech team at Streamdata.io brought it to my attention the other day that the Metropolitan Transportation Authority (MTA) Bus Time API Supports Service Interface for Real Time Information (SIRI), another standard out of Europe. <a href="http://bustime.mta.info/wiki/Developers/SIRIIntro">I think MTA’s introduction to Siri</a>, and the story behind their decision tells a significant tale about how standards are viewed.</p>

<p>According to the MTA, <em>“<a href="http://user47094.vs.easily.co.uk/siri/overview.htm">SIRI (Service Interface for Real Time Information)</a> is a standard covering a wide range of types of real-time information for public transportation.  This standard has been adopted by the European standards-setting body CEN, and is not owned by any one vendor, public transportation agency or operator.  It has been implemented in a growing number of different projects by vendors and agencies around Europe.”</em> I feel like their thoughts about SIRI not being owned by any one vendor is an important thing to take note of. While GTFS is an open standard, it is clearly a Google-led effort, and I’d say their decision to use Protocol Buffers reflects the technology, business, and politics of Google’s vision for the transit sector.</p>

<p>The MTA has evolved SIRI as part of their adoption, opting to deliver APIs as more of a RESTful interface as opposed to SOAP, and providing responses in JSON, which makes things much more accessible to a wider audience. While technologically sound decisions, I think using Protocol Buffers or even SOAP have political implications when you do not deeply consider your API consumers during the planning phases of your API. I feel like MTA has done this, and understands the need to lower the bar when it comes to the access of public transit data, ensuring that as of an audience as possible can put the real time transit data to use–web APIs, plus JSON, just equals an easier interface to work with for many developers.</p>

<p>I’m getting up to speed with GTFS and GTFS Realtime, and I am also getting intimate with SIRI, and learning how to translate from GTFS into SIRI. I’m looking to lower the bar when it comes to accessing real time transit data. Something simple web APIs excel at. I’ve been able to pull GTFS and GTFS Realtime data and convert into simpler JSON. Now that MTA has introduced me to SIRI, I’m going to get acquainted with this specification, and understand how I can translate GTFS into SIRI, and then <a href="http://apis.how/streamdata">stream using Server-Sent Events (SSE) and JSON Patch using Streamdata.io</a>. Truly making these feeds available in real time, using common web standards.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/11/the-metropolitan-transportation-authority-bus-time-api-supports-service-interface-for-real-time-information-siri/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/11/api-life-cycle-basics-documentation/">API Life Cycle Basics: Documentation</a></h3>
        <span class="post-date">11 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-documentation-interactive.png" width="30%" style="padding: 15px;" align="right" /></p>
<p>API documentation is the number one pain point for developers trying to understand what is going on with an API, as they work to get up and running consuming the resources they possess. From many discussions I’ve had with API providers it is also a pretty big pain point for many API developers when it comes to trying to keep up to date, and delivering value to consumers. Thankfully API documentation has been being driven by API definitions like OpenAPI for a while, helping keep things up date and in sync with changes going on behind the scenes. The challenge for many groups who are only doing OpenAPI to produce documentation, is that if the OpenAPI isn’t used across the API life cycle, it will often become forgotten, recreating that timeless challenge with API documentation.</p>

<p>Thankfully in the last year or so I’m beginning to see more API documentation solutions emerge getting us beyond the Swagger UI age of docs. Don’t get me wrong, I’m thankful for what Swagger UI has done, but the I’m finding it to be very difficult to get people beyond the idea that OpenAPI (fka Swagger) isn’t the same thing as Swagger UI, and that the only reason you generate API definitions is to get documentation. There are a number of API documentation solutions to choose from in 2018, but Swagger UI still remains a viable choice for making sure your APIs are properly documented for your consumers.</p>

<ul>
  <li><a href="https://swagger.io/swagger-ui/">Swagger UI</a> - Do not abandon Swagger UI, keep using it, but decouple it from existing code generation practices.</li>
  <li><a href="https://github.com/Rebilly/ReDoc/">Redoc</a> - Another OpenAPI driven documentation solution.</li>
  <li><a href="https://readthedocs.org/">Read the Docs</a> - Read the Docs hosts documentation, making it fully searchable and easy to find. You can import your docs using any major version control system, including Mercurial, Git, Subversion, and Bazaar.</li>
  <li><a href="https://readme.io/">ReadMe.io</a> - ReadMe is a developer hub for your startup or code. It’s a completely customizable and collaborative place for documentation, support, key generation and more.</li>
  <li><a href="http://openapi-specification-visual-documentation.apihandyman.io/">OpenAPI Specification Visual Documentation</a> - Thinking about how documentation can become visualized, not just text and data.</li>
</ul>

<p>API documentation should not be static. It should always be driven from OpenAPI, JSON Schema, and other pipeline artifacts. Documentation should be part of the CI/CD build process, and published as part of an API portal life cycle as mentioned above. API documentation should exist for ALL APIs that are deployed within an organization, and used to drive conversations across development as well as business groups–making sure the details of API design are always in as plain language as possible.</p>

<p>I added the visual documentation as a link because I’m beginning to see hints of API documentation move beyond the static, and even dynamic realm, and becoming something more visual. It is an area I’m investing in with my subway map work, trying to develop a consistent and familiar way to document complex systems and infrastructure. Documentation doesn’t have to be a chore, and when done right it can make a developers day brighter, and help them go from learning to integration with minimal friction. Take the time to invest in this stop along your API life cycle, as it will help both you, and your consumers make sense of the resources you are producing.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/11/api-life-cycle-basics-documentation/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/11/working-with-general-transit-feed-specification-gtfs-realtime-data/">Working With General Transit Feed Specification(GTFS) Realtime Data</a></h3>
        <span class="post-date">11 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/subway/subway-moving.gif" align="right" width="40%" style="padding: 15px;" /></p>
<p>I’ve been diving into the world of transit data, and learning more about <a href="https://developers.google.com/transit/">GTFS and GTFS Realtime</a>, two of the leading specifications for providing access to static and real time transit data. I’ve been able to take the static GTFS data and quickly render as APIs, using the zipped up CSV files provided. Next on my list I wanted to be able to work with GTFS Realtime data, as this is where the data is that changes much more often, and ultimately is more valuable in applications and to consumers.</p>

<p>Google has developed a nice suite of GTFS Realtime bindings in a variety of programming languages, including <a href="https://github.com/google/gtfs-realtime-bindings/blob/master/dotnet/README.md">.NET</a>, <a href="https://github.com/google/gtfs-realtime-bindings/blob/master/java/README.md">Java</a>, <a href="https://github.com/google/gtfs-realtime-bindings/blob/master/nodejs/README.md">JavaScript / Node.js</a>, <a href="https://github.com/google/gtfs-realtime-bindings-php">PHP</a>, <a href="https://github.com/google/gtfs-realtime-bindings/blob/master/python/README.md">Python</a>, <a href="https://github.com/google/gtfs-realtime-bindings/blob/master/ruby/README.md">Ruby</a>, and <a href="https://github.com/google/gtfs-realtime-bindings/blob/master/golang/README.md">Golang</a>. I went with the PHP bindings, which interestingly enough is the only one in its own Github repository. I’m using it because I still feel that PHP has the best opportunity for adoption within municipal organizations–something that is beginning to change, but still holds true in my experience.</p>

<p>The GTFS-realtime data is encoded and decoded using Protocol Buffers, which provides a compact binary representation designed for fast and efficient processing of the data. Even with the usage of Protocol Buffers, which is also used by gRPC via HTTP/2, all of the GFTS Realtime data feeds I am consuming are being delivered via regular HTTP/1.1. I’m doing all this work to be able to make GTFS Realtime feeds more accessible for use by <a href="http://apis.how/streamdata">Streamdata.io</a>, as the Protocol Buffers isn’t something the service currently supports. To make the data accessible for delivery via Server-Sent Events (SSE), and for partial updates to be delivered via JSON Patch, I need the Protocol Buffer format to be reduced to a simpler JSON format–which will be my next weeks worth of work on this project.</p>

<p>I was able to pretty quickly bind to the MTA subway GTFS Realtime feed here in NYC using the PHP bindings, and get at up to date “vehicle” and “alerts” via the transit authorities feeds. I’ve just dumped the data to the screen in no particular format, but was able to prove that I am able to connect to any GTFS feed, and easily convert to something I can translate into any format I desire. I’m opting to go with <a href="http://user47094.vs.easily.co.uk/siri/overview.htm">the Service Interface for Real Time Information (SIRI)</a>, which is more verbose than GTFS, but allows for availability in a JSON format. Now I just need to get more acquainted with the SIRI standard, and understands how it maps to the GTFS format.</p>

<p>I’m looking to have a solid approach to proxying an GTFS, and GTFS Realtime feed, and deploying as a SIRI compliant API that returns to JSON in coming weeks, so that I can quickly proxy using Streamata.io and deliver updates in true real time. Where transit vehicles are located at any particular moment, and details about alerts coming out of each transit authority are the most relevant, and real time aspect of transit operations. While the GTFS Realtime format is real time in name, it really isn’t in how its delivered. You still have to poll the feeds for changes, which is a burden on both the client and server, making Server-Sent Events, and JSON Patch a much more desirable, and cost effective way to get the job done.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/11/working-with-general-transit-feed-specification-gtfs-realtime-data/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/11/openapi-will-help-you-get-your-api-house-in-order/">OpenAPI Will Help You Get Your API House In Order</a></h3>
        <span class="post-date">11 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/hashicorp/hashicorp-consul-api-screenshot.png" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="/2018/01/10/we-are-not-supporting-open-api-fk-swagger-as-we-already-published-our-docs/">I wrote a piece previously about Consul not supporting Swagger documentation at this time</a>, and the API provider and consumer impact of this decision. I’m going to continue picking on Consul with another API definition story, because they are forcing me to hand-craft my own OpenAPI. If I just had to learn about the API, and load the OpenAPI (fka Swagger) definition into my postman, publish to the repo for the project, and import into other tooling I’m using, I wouldn’t be so critical. However, since I’m having to take their static, and often incomplete documentation, and generate an OpenAPI for my project, I’m going to vent some about their API and schema design, and they short-sighted view of OpenAPI (cough, cough fka Swagger).</p>

<p>I just finished an OpenAPI for <a href="https://www.consul.io/api/acl.html">the Consul ACLs API path</a>, and currently working on one for <a href="https://www.consul.io/api/agent.html">the Agent API path</a>. I have already distilled down their static documentation into something that I can easily parse and translate into OpenAPI, I just need to finish doing the manual work. It is something I normally do with a scrape script, but the difficulties in consistently parsing their docs, combined with the scope of the docs, made me go with hand-crafting after distilling down the documentation for easier handling. I am familiar with the entire surface area of the Consul API, and now I’m getting to it’s intimate details, which also includes it’s intimate lack of details.</p>

<p>The first area I begin stumbling on is with the design of the Consul API. While it is a web or HTTP API, it isn’t following most of the basics of REST, which would significantly help things be a little more intuitive. Simple examples of this in action would be with the LAN Coordinates for a node path, which is a PUT with the following path /coordinate/update – making for verb redundancy in the HTTP Verb and path. There also isn’t a consistent approach to ids and how they are used in the paths, which is a side effect of no real focus on resources, or use of REST principles. Ultimately the API design isn’t as bad as many APIs that I consume, but it’s inconsistencies does it make it difficult to learn about.</p>

<p>The next area I find myself stumbling with is when it comes to the schema and response structure for the Consul API. In the documentation, some APIs provide no insight on what schema is returned, while others show a sample response, and others actually providing detail on the fields, types, and description. Some paths will utilize the same schema, but only reference the need for part of it, and return only what it needs, with no consistency in how it does this. Again, a lack of coherency around any resource model, and just requesting and responding with what is perceived in the moment. The challenge is not all of us new developers are “in the moment”, and the lack of consistency makes it difficult to understand just exactly what is going on.</p>

<p><a href="/2018/01/10/we-are-not-supporting-open-api-fk-swagger-as-we-already-published-our-docs/">As I mentioned in my previous piece, OpenAPI (fka Swagger) is much, much more than just API documentation</a>. In this example, if present, it would act as a design scaffolding for Consul to think through the overall design patterns they use for each of their APIs and their paths, as well as consistently using the schema across the request and response structure of each API. The only reason I’m writing this story is become I needed a break from documenting the inconsistent details of the API. Piecing together a complete picture of the schema from one piece over here, and one piece over there, just so I can learn about the API and put it to use correctly in a project. I’ve read 100% of their API documentation, and hand-crafted an OpenAPI for about 30% of your API, and now I’m wishing the Consul had embarked on this journey, instead of me.</p>

<p>OpenAPI is much, much more than just documentation. It would give that strategic polish to a very tactically designed API. The dismissal of OpenAPI, because they’ve already done API documentation is just one symptom of a very tactically operated API. If Consul had used OpenAPI, it would have given them scaffolding to help them get their API house in order. It would have allowed them to think through the details of their API, which now as a consumer I’m having to do, and provide feedback on, which will hopefully would get included into the next version. With OpenAPI you have the opportunity to expedite this feedback loop, and see the missing details yourself, or potentially mock and provide the interface to a select group of users to provide feedback before you ever begin development.</p>

<p>My frustration with the Consul API isn’t entirely the design of it. It is mostly the incomplete design of it, and their unwillingness to pick up their head and look around, understanding what is good design, as well as what OpenAPI (fka Swagger) is, before responding that they don’t support it. I’m going to keep hand-crafting my Consul OpenAPI, because I see the value of the service, even if I don’t like their API, or the efforts that was put into the API. I’m hoping that they’ll see the light with OpenAPI, and maybe my hand-crafted edition will turn on this light. If nothing else, at least it will provide an OpenAPI that other developers can use, even if Hashicorp’s Consult team doesn’t see the value.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/11/openapi-will-help-you-get-your-api-house-in-order/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/10/how-do-we-keep-teams-from-hoarding-their-data-when-doing-apis/">How Do We Keep Teams From Being Defensive With Resources When Doing APIs?</a></h3>
        <span class="post-date">10 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/castle-on-hill-edinburgh_copper_circuit.JPG" align="right" width="45%" style="padding: 15px;" /></p>
<p>I was talking with the IRS about their internal API strategy before Christmas, reviewing the teams current proposal before they pitched in across teams. One of the topics that came up, which I thought was interesting, was about how to prevent some teams from taking up a defensive stances around their resources when you are trying to level the playing field across groups using APIs and microservices. They had expressed concern that some groups just didn’t see APIs as a benefit, and in some cases perceived them as a threat to their current position within the agency.</p>

<p>This is something I see at almost EVERY SINGLE organization I work with. Most technical groups who have established control over some valuable data, content, or other digital resource, have entrenched themselves, and become resistant to change. Often times these teams have a financial incentive to remain entrenched, and see API efforts as a threat to their budget and long term viability. This type of politics within large companies, organizations, institutions, and government agencies is the biggest threat to change than technology ever is.</p>

<p>So, what can you do about it. Well, the most obvious thing is you can get leadership on your team, and get them to mandate change. Often times this will involve personnel change, and can get pretty ugly in the end. Alternately, I recommend trying to build bridges, by understanding the team in question, and find ways you can do API things that might benefit them. Maybe more revenue and budget opportunities. Reuse of code through open source, or reusable code and applications that might benefit their operations. I recommend mapping out the groups structure and needs, and put together a robust plan regarding how you can make inroads, build relationships, and potentially change behavior, instead of taking an adversarial tone.</p>

<p>Another way forward is to ignore them. Focus on other teams. Find success. Demonstrate what APIs can do, and make the more entrenched team come to you. Of course, this depends on the type of resources they have. Depending on the situation, you may or may not be able to ignore them completely. Leading by example is the best way to take down entrenched groups. Get them to come out of their entrenched positions, and lower their walls a little bit, rather than trying to breach them. You are better off focusing doing APIs and investing in moving forward, rather than battling with groups who don’t see the benefits. I guarantee they can last longer than you probably think, and have developed some pretty crafty ways of staying in control over the years.</p>

<p>Anytime I encounter entrenched team stories within organizations I get sad for anyone who has to deal with these situations. I’ve had some pretty big battles over my career, which ended up in me leaving good jobs, so I don’t take them lightly. However, it makes me smile a little to hear one out of the IRS, especially internally. I know plenty of human beings who work at the IRS, but with their hard-ass reputation they have from the outside, you can’t help but smile just a bit thinking them facing the same challenges that the rest of us do. ;-) I think this is one of the most important lessons of microservices, and APIs, is that we can’t let teams ever get this big and entrenched again. Once we decoupled, let’s keep things in small enough teams, that this type of power can’t aggregate and be to big to evolve again.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/10/how-do-we-keep-teams-from-hoarding-their-data-when-doing-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/10/building-an-api-partner-program-for-streamdat-io/">Building An API Partner Program For Streamdata.io</a></h3>
        <span class="post-date">10 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/streamdata/streamdata-api-evangelist-partner.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am working with <a href="http://apis.how/streamdata">Streamdata.io</a> on a number of fronts when it comes to our partnership in 2018. One of the areas I’m helping them build, is the partner program around their API service. I’m taking what <a href="http://partners.apievangelist.com/">I’ve learned from studying the partner programs of other leading APIs</a>, and I am pulling it together into coherent strategy that Streamdata.io can put to work over time. Like any other area of operations, we are going to start small, and move forward incrementally, making sure we are doing this in a sensible, and pragmatic way.</p>

<p>First up, are the basics. What are we trying to accomplish with the Streamdata.io partner program. I want to have a simple and concise answer to what their partner program does, and is designed to accomplish.</p>

<p><em>The Streamdata.io partner program is designed to encourage deeper engagement with companies, organizations, institutions, and government agencies that are putting Streamdata.io solutions to work, or are already operating within industries where Streamdata.io services will compliment what they are already doing. This partner program is meant to encourage continued participation by our customers, through offering them exposure, storytelling opportunities, referrals, and even new revenue opportunities. The Streamdata.io partner program is open to the public, just reach out and we’ll let you know if there is a fit between what our organizations are doing.</em></p>

<p>It is a first draft, and not the official description, but it takes a crack at describing why we are doing it, and some about what it is. After looking through our existing partner list, and talking about the objectives of the Streamdata.io partner program we have settled in on a handful of types of partner opportunities available with the program.</p>

<ul>
  <li><strong>OEM</strong> - Our deeply integrated partners who offer a white label version Streamdata.io services to their customers.</li>
  <li><strong>Mutual Lead Referral</strong> - Partners who we are looking to generate business for each other, sending leads back and forth to help drive growth.</li>
  <li><strong>Co-Marketing</strong> - Partners who include Streamdata.io in their marketing, as well as participate in our partner marketing opportunities.</li>
  <li><strong>Reseller / Distribution</strong> - Streamdata.io customers who are authorized to resell and distribute Streamdata.io services alongside their work.</li>
  <li><strong>Marketplace</strong> - Making sure Streamdata.io is in leading application and API marketplaces, such as Amazon Marketplace.</li>
  <li><strong>System Integrators</strong> - Agencies who offer consulting services, and are up to speed on what Streamdata.io does, and can intelligently offer to their customers.</li>
</ul>

<p>We are currently going through our existing partner list, and preparing the website page that articulates what the partner program is, and who our existing partners are. Then we are looking to craft a road map for what the future of the Streamdata.io partner program will look like.</p>

<ul>
  <li><strong>Storytelling</strong> - Opportunities to include their products, services, and use cases in storytelling on the Streamdata.io blog, and via white papers, guides or other formats.</li>
  <li><strong>Newsletter</strong> - Including partners in the newsletter we are launching later this month, allowing for regular exposure via this channel.</li>
  <li><strong>Testimonials</strong> - Getting, and showcasing the testimonials of our partners, and also giving testimonials regarding their solutions.</li>
  <li><strong>Case Studies</strong> - Crafting of full case studies about how a partner has applied Streamdata.io solutions within their products, and services.</li>
</ul>

<p>We have other ideas, but this represents what we are capable of in the first six months of the Streamata.io partner program. It gets the program off the ground, and has us reaching out to existing partners, letting them know of the opportunities on the table. It begins showcasing the program, and existing partners on the showcase page via the website, and invites other customers, and potential customers to participate. Once we get the storytelling drumbeat going for existing partners, and start driving traffic and links their way, we can look at what it will take to attract and land new partners as part of the effort.</p>

<p>Adding another dimension to this conversation, the telling of this story is directly related to my partnership with Streamdata.io. Which means I am a Streamdata.io partner, and they are one of my partners, and this storytelling is part of the benefits of being an API Evangelist partner. As part of the work on the Streamdata.io partner program we are looking at not just how the storytelling can be executed on Streamdata.io, but also here on API Evangelist. We are also looking into how the reseller, distribution, and integrators tiers of partnership can coexist with the consulting, speaking, and workshops I’m doing as part of my work as the API Evangelist. We will be revisiting this topic each week, and when I have relevant additions, or movements, I will tell the story here on the blog, as I do with most of my work.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/10/building-an-api-partner-program-for-streamdat-io/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/10/api-life-cycle-bsics-portal/">API Life Cycle Basics: Portal</a></h3>
        <span class="post-date">10 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-portal.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>A coherent strategy to delivering and operating API portals is something that gets lost in a number of the API operations I am asked to review. It is also one of the more interesting aspects of the successful strategies I track on, something that when done right, can become a vibrant source of information, and when done wrong, can make an API a ghost town, and something people back away from when finding. As part of my research I think a lot about how API portals can be used as part of each APIs lifecycle, as well as at the aggregate levels across teams, within groups, between partners, and the public.</p>

<p>The most common form of the API portal is the classic public developer portal you find with Twitter, Twilio, Facebook, and other leading API pioneers. These portals provide a wealth of healthy patterns we can emulate, as well as some not so healthy ones. Beyond these public portals, I also se other patterns within the enterprise organizations I work with, that I think are worth sharing, showing how portals aren’t always just a single public destination, and can be much, much more.</p>

<ul>
  <li><strong>Individual Portals</strong> - Considering how developers and business users can be leverage portals to push forward conversations around the APIs they own and are moving forward.</li>
  <li><strong>Team Portals</strong> - Thinking about how different groups and teams can have their own portals which aggregate APIs and other portals from across their project.</li>
  <li><strong>Partner Portals</strong> - Leveraging a single, or even partner specific portals that are public or private for engaging in API projects with trusted partners.</li>
  <li><strong>Public Portal</strong> - Begin the process of establishing a single Mutual of Omaha developer portal to provide a single point of entry for all public API efforts across the organization.</li>
  <li><strong>Pipeline Integration</strong> - How can BitBucket be leverage for deploying of individual, team, partner, and even the public portal, making portals another aspect of the continuous deployment pipeline.</li>
</ul>

<p>Portals can be used as the storage for the central truth of OpenAPI, and their JSON schema. They can be where documentation, coding, tooling, and other stops along the life cycle live. They also provide for an opportunity for decentralization of API deployment, but done in a way that can be evolved alongside the existing CI/CD evolution occurring within many organization, as well as aggregated and made available as part of company wide public, partner, or private discovery portals. Portals, can be much more than just a landing page, and can act as a doorway to a vibrant ecosystem within an organization.</p>

<p>I admit, it can be tough to turn a landing page for a portal into an active source of information, but with the right investment over time, it can happen. I maintain almost 200 separate portals as part of my work as the API Evangelist. Not all of them are active and vibrant, but they all serve a purpose. Some are meant to be static and never changing, with others being more ephemeral and meant to eventually go away. While others, like the home page for each stop along my API life cycle research staying active for almost eight years now, providing a wealth of information on not just a single APIs, but an entire industry.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/10/api-life-cycle-bsics-portal/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/10/api-life-cycle-basics-dns/">API Life Cycle Basics: DNS</a></h3>
        <span class="post-date">10 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-dns.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>DNS is one of those shadowy things that tends to be managed by a select few wizards, and the rest of an organization doesn’t have much knowledge, awareness, or access at this level. APIs has shifted this reality for me, and is something I’m also seeing at organizations who are adopting a microservices, and devops approach to getting things done. DNS should be a first class citizen in the API toolbox, allowing for well planned deployments supporting a variety of services, but also allow for logging, orchestration, and most importantly security, at the frontline of our API operations.</p>

<p>There are some basics I wanted to introduce to my readers when it comes to DNS for their API operations, but I also wanted to shine a light on where the DNS space is headed because of APIs. Some DNS and cloud providers are taking things to the next level, and APIs are central to that. Like most other stops along the API life cycle DNS is not just about doing DNS for your APIs, it is also about doing APIs for your DNS.</p>

<ul>
  <li><strong>Dedicated API DNS</strong> - I’m not in the business of telling you how to name the domain, or subdomain for your API, but you should have a plan, and be also considering having multiple subdomains, separating concerns across operations.</li>
  <li><strong>API Control Over DNS</strong> - DNS is the frontline for your API infrastructure, even internally, and you should be able to programmatically configure, audit, orchestrate, and manage the DNS for your APIs using APIs.</li>
  <li><strong>Regional Consideration</strong> - Begin thinking about how you name and manage your DNS with multiple zones and regions in operations–even if you aren’t quite ready, you should be thinking in this way.</li>
  <li><a href="https://aws.amazon.com/about-aws/whats-new/2017/12/amazon-route-53-releases-auto-naming-api-name-service-management/"><strong>Amazon Route 53 Releases Auto Naming API for Service Name Management and Discovery</strong></a> - Thinking about how service addressing can be automated, as well as standardized as part of the life cycle.</li>
  <li><a href="https://www.cloudflare.com/"><strong>CloudFlare</strong></a> - You may not use them as a provider, but tune in and study the way CloudFlare does their DNS, as well as provides APIs for managing DNS.</li>
</ul>

<p>DNS should be a prominent part of API operations, even with internal APIs. It is the first line of defense when it comes to security, as well as discovery, and allowing developers and partners to put APIs to work. DNS shouldn’t be separate from the rest of the API life cycle, and should be reachable by all developers, with logging at this layer shipped to be included within API life cycle operations. DNS needs to come out of the shadows and be something your entire team is aware of, with transparency around configuration, as well as standard practices for usage  across services.</p>

<p>I can’t emphasize enough regarding how DNS providers like CloudFlare have shifted my view of DNS. Even if you aren’t using them for your primary DNS, I recommend setting up a domain and playing around with what they have to offer. At least tune into their blog and Twitter account, as they are pushing the conversation forward when it comes to DNS, and API access to this layer. DNS in 2018 is much more than just addressing for your APIs, it is about logging, security, and much, much more. Bring it out of the background, and take another look at how it can make a bigger impact on what you are looking to achieve with APIs.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/10/api-life-cycle-basics-dns/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/10/some-of-the-thinking-behind-the-protocols-used-by-kafka/">Some Of The Thinking Behind The Protocols Used By Kafka</a></h3>
        <span class="post-date">10 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/kafka/kafka-protocol-guide.png" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="http://apievangelist.com/2017/10/03/looking-at-the-37-apached-data-projects/">I’ve been studying the overall Apache Stack a lot lately</a>, with an emphasis on <a href="https://kafka.apache.org/">Kafka</a>. I’m trying to understand what the future of APIs will hold, and where the leading edge of real time, event-driven architecture is at these days. I’m going through the protocol page for Kafka, learning about exactly how they move data around, and found their answers behind the decisions they’ve made along the way in deciding what protocols they chose to use were very interesting.</p>

<p><a href="https://kafka.apache.org/protocol">All the way at the bottom of the Kafka protocol page you can find the following “Some Common Philosophical Questions”</a>, providing some interesting backstory on the decisions behind the very popular platform.</p>

<p><em>Some people have asked why we don’t use HTTP. There are a number of reasons, the best is that client implementors can make use of some of the more advanced TCP features–the ability to multiplex requests, the ability to simultaneously poll many connections, etc. We have also found HTTP libraries in many languages to be surprisingly shabby.</em></p>

<p><em>Others have asked if maybe we shouldn’t support many different protocols. Prior experience with this was that it makes it very hard to add and test new features if they have to be ported across many protocol implementations. Our feeling is that most users don’t really see multiple protocols as a feature, they just want a good reliable client in the language of their choice.</em></p>

<p><em>Another question is why we don’t adopt XMPP, STOMP, AMQP or an existing protocol. The answer to this varies by protocol, but in general the problem is that the protocol does determine large parts of the implementation and we couldn’t do what we are doing if we didn’t have control over the protocol. Our belief is that it is possible to do better than existing messaging systems have in providing a truly distributed messaging system, and to do this we need to build something that works differently.</em></p>

<p><em>A final question is why we don’t use a system like Protocol Buffers or Thrift to define our request messages. These packages excel at helping you to managing lots and lots of serialized messages. However we have only a few messages. Support across languages is somewhat spotty (depending on the package). Finally the mapping between binary log format and wire protocol is something we manage somewhat carefully and this would not be possible with these systems. Finally we prefer the style of versioning APIs explicitly and checking this to inferring new values as nulls as it allows more nuanced control of compatibility.</em></p>

<p>It paints an interesting story about the team, technology, and I think the other directions the API sector is taking, when it comes to which protocols they are using. I don’t know enough about how Kafka works to take any stance on their decisions. I’m just looking to just take a snapshot of their stance, so that I can come back to it at some point in the future, when I do.</p>

<p><a href="http://apievangelist.com/2018/01/02/my-evolving-definition-of-a-robust-and-diverse-api-toolbox/">I published my diverse toolbox diagram a couple weeks back</a>, which includes Kafka. As I continue to develop my understanding of the Apache Stack, and Kafka, I will further dial-in the story that my API toolbox visual tells. The answers above further muddy the water for me about where it fits into the bigger picture, but I’m hoping it is something that will clear up with more awarness of what Kafka delivers.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/10/some-of-the-thinking-behind-the-protocols-used-by-kafka/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/10/we-are-not-supporting-open-api-fk-swagger-as-we-already-published-our-docs/">We Are Not Supporting OpenAPI (fka Swagger) As We Already Published Our Docs</a></h3>
        <span class="post-date">10 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/hashicorp/hashicorp-consul-github-swagger-docs.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I was looking for an OpenAPI for <a href="https://www.consul.io/api/index.html">the Consul API</a> to use in a project I’m working on. I have a few tricks for finding OpenAPI out in the wild, which always starts with looking over at <a href="https://apis.guru/">APIs.guru</a>, then secondarily Githubbing it (are we to verb status yet?). From a search on Github <a href="https://github.com/hashicorp/consul/issues/555">I came across an issue on the Github repo for Hashicorp’s Consul</a>, which asked for “improved API documentation”, a Hashicorp employee ultimately responded with “we just finished a revamp of the API docs and we don’t have plans to support Swagger at this time.”. Highlighting the continued misconception of what is “OpenAPI”, what it is used for, and how important it can be to not just providing an API, but also consuming it.</p>

<p>First things first. Swagger is now <a href="https://github.com/OAI/OpenAPI-Specification">OpenAPI</a> (has been for a while), an API specification format that is in <a href="https://www.openapis.org/">the Open API Initiative (OAI)</a>, which is part of the Linux Foundation. Swagger is proprietary tooling for building with the OpenAPI specification. It’s an unfortunate and confusing situation that arose out of the move to the Open API Initiative, but it is one we need to move beyond, so you will find me correcting folks more often on this subject.</p>

<p>Next, let’s look at the consumer question, asking for “improved API documentation”. OpenAPI (fka Swagger) is much more than documentation. I understand this position as much of the value it delivers to the API consumer is often the things we associate with documentation delivering. It teaches us about the surface area of an API, detailing the authentication, request, and response structure. However, OpenAPI does this in a machine readable way that allows us to take the definition with us, load it up in other tooling like Postman, as well as use to autogenerate code, tests, monitors, and many other time saving elements when we are working to integrate with an API. Lesson for the API consumers here is that OpenAPI (fka Swagger) is much, much, more than just documentation.</p>

<p>Then, let’s look at it from the provider side. Looks like you just revamped your API documentation, without much review of the state of things when it comes to API documentation. Without being too snarky, after learning more about the design of your API, I’m guessing you didn’t look at the state of things when it comes to API design either. My objective is to not shame you for poor API design and documentation practices, just to point out you are not picking your head up and looking around much when you developed a public facing API, that many “other” people will be consuming. It is precisely the time you should be picking up your head and looking around. Lesson for the API provider her is that OpenAPI (fka Swagger) is much, much, more than just documentation.</p>

<p>OpenAPI (fka Swagger) is much, much, more than just documentation! Instead of me being able to fork an OpenAPI definition and share with my team members, allowing me to drive interactive documentation within our project portal, empower each team member to import the definition and getting up and running in Postman, I’m spending a couple of hours creating an OpenAPI definition for YOUR API. Once done I will have the benefits for my team that I’m seeking, but I shouldn’t have to do this. As an API provider, Consul should provide us consumers with a machine readable definition of the entire surface area of the API. Not just static documentation (that are incomplete). Please API providers, take the time to look up and study the space a little more when you are designing your APIs, and learn from others are doing when it come to delivering API resources. If you do, you’ll be much happier for it, and I’m guessing your API consumers will be as well!</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/10/we-are-not-supporting-open-api-fk-swagger-as-we-already-published-our-docs/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/09/api-life-cycle-basics-api-logging/">API Life Cycle Basics: API Logging</a></h3>
        <span class="post-date">09 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-logging-2.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>Logging has always been in the background of other stops along the API lifecycle, most notably the API management layer. However increasingly I am recommending pulling logging out of API management, and making it a first-class citizen, ensuring that the logging of all systems across the API lifecycle are aggregated, and accessible, allowing them to be accessed alongside other resources. Almost every stop in this basics of an API life cycle series will have its own logging layer, providing an opportunity to better understand each stop, but also side by side as part of the bigger picture.</p>

<p>There are some clear leaders when it comes to logging, searching, and analyzing large volumes of data generated across API operations. This is one area you should not be reinventing the wheel in, and you need to be leveraging the experience of the open source tooling providers, as well as the cloud providers who have emerged across the landscape. Here is a snapshot of a few providers who will help you make logging a first class citizen in your API life cycle.</p>

<p><a href="https://www.elastic.co/products">Elastic Stack</a> - Formerly known as the Elk Stack, the evolved approach to logging, search, and analysis out of Elastic. I recommend incorporating it into all aspects of operations, and deploying APIs to make them first class citizens.
<a href="https://logmatic.io/">Logmatic</a> - Whatever the language or stack, staging or production, front or back, Logmatic.io centralizes all your logs and metrics right into your browser.
<a href="https://www.nagios.org/">Nagio</a> - Nagios Log Server greatly simplifies the process of searching your log data. Set up alerts to notify you when potential threats arise, or simply query your log data to quickly audit any system.
<a href="https://cloud.google.com/stackdriver/">Google Stackdriver</a> - Google Stackdriver provides powerful monitoring, logging, and diagnostics.
<a href="https://aws.amazon.com/cloudwatch/">AWS CloudWatch</a> - Amazon CloudWatch is a monitoring service for AWS cloud resources and the applications you run on AWS.</p>

<p>I recommend cracking open logging from EVERY layer, and shipping them into a central system like Elastic for making them accessible. While each stop along the API lifecycle will come with its own logging and analysis solutions, depending on the services and tooling used, logs should also be shipped as part of a central system for analysis at the bigger picture level. Each stop along the API life cycle will have its own tooling and service, which will most likely come with its own logging and analysis services. Use these solutions. However, don’t stop there, and consider the benefits from looking at log data side by side, and what the big picture might hold.</p>

<p>Logging will significantly overlap with the security stop along the API life cycle. The more logging you are doing, and the more accessible these logs are, the more comprehensive your API security will become. You’ll find this becomes true at other stops along the API life cycle, and you will be able to better deliver o discovery, testing, define, and deliver in other ways, with a more comprehensive logging strategy. Remember, logging isn’t just about providing a logging layer, it is also about having APIs for your logging, providing a programmatic layer to understand how things are working, or not.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/09/api-life-cycle-basics-api-logging/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/09/a-blueprint-for-an-augmented-transit-api/">A Blueprint For An Augmented Transit API</a></h3>
        <span class="post-date">09 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/metrohero/metro-hero-dashboard.png" align="right" width="45%" style="padding: 15px" /></p>
<p>I’m working through research on the world of <a href="http://transit.apievangelist.com">transit APIs</a> as part of my partnership with <a href="http://apis.how/streamdata">Streamdata.io</a>. From what I’ve gathered so far, the world of transit data and APIs is quite a mess, and there is a pretty significant opportunity to improve upon what already exists. In the course of my research, I stumbled across <a href="https://dcmetrohero.com/">MetroHero</a>, which is an application and API provider that operates on top of the <a href="https://wmata.com/">Washington Metropolitan Area Transit Authority</a> data and API feeds.</p>

<p>I’m still working my way through their website, services, APIs, as well as talking with their team, but I’m fascinated with what they are doing, and wanted to think a little more about it before I talk with them this week. While their approach to improving upon WMATA applications is interesting, I think applying this way of thought to a government API is more interesting (surprise). <a href="https://www.dcmetrohero.com/apis">The MetroHero API is what I’d consider to be an augmented API</a>, operating on top of the WMATA API, and improving upon the data and services they make available about the Washington DC transit system.</p>

<p>The MetroHero API, taken directly from their developer portal, “are available for free. In return, we require the following”:</p>

<ul>
  <li>You must abide by WMATA’s Transit Data Terms of Use; by using our APIs, you agree to these terms of service.</li>
  <li>Any data returned by or derived from data returned by our APIs must be freely available to all users of your application. Any paywalled application that utilizes our APIs must also provide a free tier with access to the same data returned by or derived from our APIs.</li>
  <li>Any data returned by or derived from data returned by our APIs must be prominently credited back to MetroHero. For example, if this data is being displayed to users on a website or in an application, MetroHero must always be visually credited wherever and whenever the data appears or is used.</li>
</ul>

<p>The MetroHero API is not sanctioned by WMATA. The MetroHero team doesn’t charge for their API, while also being very passionate about improving upon the WMATA APIs. I’ve been tuned into what MetroHero does since <a href="https://apievangelist.com/2017/11/30/licensing-over-dc-transit-data/">I first wrote about WMATA’s terms of service changes impacting them a while back</a>, and I have been intrigued by an API that augments, and improve upon a government agency’s API. This is a topic I’ve been thinking about since <a href="http://apievangelist.com/2013/10/17/shutdown-of-government-open-data-and-apis-is-not-government-services-business-as-usual/">my earlier frustrations with the federal government APIs I had been working on getting shutdown during the fall of 2013</a>. An experience that has pushed me to think more about ways in which we can improve upon existing government services using APIs.</p>

<p>I’m looking to craft a blueprint that reflects what MetroHero is already doing. Something that is forkable, and executable. I would like to see MetroHero be the default in communities. While transit is first in line, I’m envisioning a model that goes well beyond just transit, and into other 511 information like automobile traffic and incidents, then move into 311, and 911. I will be adding this blueprint to my <a href="http://adopta.agency/">Adopta.Agency</a> project, and focusing it on the local level, rather than at the federal level. The sub-domain for the project will Transit.Adopta.Agency (not setup), then I’ll add 511,911,311, and others later. I still have a lot of work to do on the transit portal blueprint, so first things first.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/09/a-blueprint-for-an-augmented-transit-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/09/api-life-cycle-basics-api-management/">API Life Cycle Basics: API Management</a></h3>
        <span class="post-date">09 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-tools.png" align="right" width="30%" style="padding: 15px;" /></p>
<p>The need to manage APIs is one of the older aspects of doing business with web APIs. Beginning around 2006, then maturing, and being baked into the cloud and markets by 2016. Whether it is through an management gateway that proxies existing APIs, natively as part of the gateway that is used to deploy the APIs themselves, or as a connective layer within the code, API management is all about authenticating, metering, logging, analyzing, reporting, and even billing against API consumption. This landscape has significantly shifted lately, with the bottom end of the market becoming more competitive, but luckily there are enough open source and cloud solutions available to get the job done.</p>

<p>Over the last decade API management providers have collectively defined some common approaches to getting business done using web APIs. While still very technical, API management is all about the business of APIs, and managing the value generated from providing access to data, content, algorithms, and other digital resources using the web. Here are the handful of common aspects of API management, which are being baked into the cloud, and made available across a number of open source solution providers catering to the API space:</p>

<ul>
  <li><strong>Authentication</strong> - Requiring all developers to register, obtain keys, and provide unique identification with API request they make.</li>
  <li><strong>Service Composition</strong> - Allowing for the organizing and breaking down of APIs into meaningful lines of business, and allowing for different times of access to these products.</li>
  <li><strong>Rate Limiting</strong> - Limiting, and protecting the value of digital resources, only allowing access to those who have been approved.</li>
  <li><strong>Metering</strong> - Measuring each call that is made to APIs, and applying service composition, and pricing to all API traffic, quantifying the value of business being conducted.</li>
  <li><strong>Reporting</strong> - Providing analysis and reporting on all activity, enabling API providers to develop awareness, and drill down regarding how resources are being used.</li>
  <li><strong>Invoicing</strong> - Accounting for all API traffic and invoicing, charging, and crediting for API consumption, to generate revenue and incentivize the desire behavior among consumers.</li>
</ul>

<p>APIs use the web, and API management allows companies, organizations, institutions, and government agency to provide secure access to valuable resources in this environment. API management is about developing an awareness of who has access to resources, understanding how they are using them, and charging or compensating for the value consumed or generated via API access. While much of the conversation in the tech sector is focused on revenue generation at this layer, in reality it is about understanding value generation and exchange around valuable resources–with revenue generation being one aspect of doing business using APIs on the web.</p>

<p>When it comes to selecting an API management solution my recommendation is always keep the relationship small, modular, and decoupled from other stops along the API lifecycle, keeping the business engagement limited as well, allowing you to grow and evolve without lengthy contracts. Every stop along the API life cycle should reflect the API philosophy, and kept small, decoupled, and doing one thing well. This goes for the technical, as well as the business of doing APIs. Increasingly my storytelling about API management is absent of vendors, and more focused on the nuts and bolts of managing APIs, reflecting the maturing and weaving in of API management into the fabric of the cloud.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/09/api-life-cycle-basics-api-management/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/09/the-data-behind-the-washington-post-story-on-police-shootings-in-2017/">The Data Behind The Washington Post Story On Police Shootings in 2017</a></h3>
        <span class="post-date">09 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/washington-post/washington-post-fatal-force-story.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I was getting ready to write my usual, “wish there was actual data behind this story about a database” story, while reading <a href="https://www.washingtonpost.com/graphics/national/police-shootings-2017/">the Fatal Force story in the Washington Post</a>, and then I saw the link! Fatal Force, 987 people have been shot and killed by police in 2017. <a href="https://www.washingtonpost.com/national/how-the-washington-post-is-examining-police-shootings-in-the-united-states/2016/07/07/d9c52238-43ad-11e6-8856-f26de2537a9d_story.html">Read about our methodology</a>. <a href="https://github.com/washingtonpost/data-police-shootings">Download the data</a>. I am so very happy to see this. An actual prominent link to a machine readable version of the data, published on Github–this should be the default for ALL data journalism in this era.</p>

<p>I see story after story reference the data behind, without providing any links to the data. As a database professional this practice drives me insane. Every single story that provides data driven visualizations, statistics, analysis, tables, or any other derivative from data journalism, should provide a link to the Github repository which contains at least CSV representations of the data, if not JSON. This is the minimum for ALL data journalism going forward. If you do not meet this bar, your work should be in question. Other analysts, researchers, and journalists should be able to come in behind your work and audit, verify, validate, and even build upon and augment your work, for it to be considered relevant in this time period.</p>

<p>Github is free. Google Sheets is free. There is no excuse for you not to be publishing the data behind your work in a machine readable format. <a href="https://github.com/washingtonpost">It makes me happy to see the Washington Post using Github like this, especially when they do not have an active API or developer program</a>. I’m going to spend some time looking through the other repositories in their Github organization, and also begin tracking on which news agencies are actively using Github. Hopefully, in the near future, I can stop ranting about reputable news outlets not sharing their data behind stories in machine readable formats, because the rest of the industry will help police this, and only the real data-driven journalists will be left. #ShowYourWork</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/09/the-data-behind-the-washington-post-story-on-police-shootings-in-2017/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/09/generating-revenue-from-the-support-of-public-data-using-apis/">Generating Revenue From The Support Of Public Data Using APIs</a></h3>
        <span class="post-date">09 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/machine-road_blue_circuit_4.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m exploring the different ways that public data access via APIs can be invested in, even with the opportunity for generating revenue, but without charging access to the data itself. I want public data to remain accessible, but in reality it costs money to provide access to public data, to refine, and evolve it. This is meant to be an exploration of how public data is invested in, not how you lock up public data, so please read everything before commenting, as every time I write about this subject, there are folks who blindly declare that ALL public data has to be free, no matter what. I agree (mostly), but there has to be commercial monetization opportunities around public data, otherwise it will never evolve, improve, be enriched, and in some cases available at all.</p>

<p>I am looking for opportunities for public data stewards / owners to generated much needed revenue, as well as commercial interests to come in and augment, and build upon what is already available–going beyond what cash and resource strapped data stewards / owners might be able to do on their own. Here are a couple of areas I’m documenting a little more right now.</p>

<ul>
  <li><strong>Wholesale APIs</strong> - Deploying, managing, and providing access to public data via APIs that are designed specifically for individual consumers. This could be deploying an API on AWS, Google, Azure, or other infrastructure provider, and providing private, or even I guess public access to the data. Delivering a personalized, customized, and performant public data API experience.</li>
  <li><strong>Real Time APIs</strong> - Providing a proxied stream of data from one or many public data sources, going beyond what the data steward / owner is capable of delivering. Charging for the technology, not access to the data itself.</li>
  <li><strong>Cached</strong> - Delivering a cached experience, so that when the primary source goes down, all API consumers are still able to get at historical, or other relevant data without interruption.</li>
  <li><strong>Transformation</strong> - Providing access to the data but in a different format than the source can provide. This is where the line begins to get blurry because technically you would be charging for access to the data here, albeit in a different format. Maybe it could be done as a wholesale API, where you charge for the service, not the data?</li>
  <li><strong>Enriched</strong> - Enriching public data, adding in additional data points, and other relevant data and content that makes it better. Another area things start to get blurry, because again, you are beginning to charge for access to the data. Maybe you would need to have a way to separate public, from the enrichment, I guess?</li>
</ul>

<p>Those are just a couple of examples I’m looking thinking about. Most public data sources restrict the ability to charge for access to their data by any 3rd party–makes sense. In this particular exercise I’m looking at transit data, and how the overall data and API access experience can be improved upon. There are augmented APIs built upon public transit feeds, like we see from <a href="https://dcmetrohero.com/">MetroHero</a>, who is built upon <a href="https://wmata.com/">WMATA</a>. They clearly state that they are not sanctioned by WMATA, and could go away at any point, and make it known the data is freely available, and should remain that way downstream. I’m also looking at building upon <a href="http://datamine.mta.info/list-of-feeds">MTA transit feeds in NYC</a>, making the GTFS Realtime feeds available as SIRI feeds, but it is something that will cost me money, and I can’t afford to just provide for access free, subsidizing other applications without some return.</p>

<p>I’d like to improve upon MTA transit feeds. Make them more usable, and available in a streaming format using <a href="http://apis.how/streamdata">Streamdata.io</a>. I can’t do this in their current format. I can easily proxy their feeds, and transform into a simpler SIRI JSON feed, but then I’m expected to just make them freely available. I’m looking to see how I can do this as a wholesale API, as well as make available in a paid streaming format. I’m not charging for the data, I’m charging for the serving of it in real time? IDK. I am just exploring these thoughts right now. I get where all the concerns come into the picture around making money off public data, and limiting who gets access, however I really want public data to remain available, and improve–this is something that takes money, investment, and the ability to generate revenue.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/09/generating-revenue-from-the-support-of-public-data-using-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/09/the-child-welfare-digital-services-certification-approval-and-licensing-services-api/">The Child Welfare Digital Services (CWDS) Certification, Approval, and Licensing Services (CALS) API</a></h3>
        <span class="post-date">09 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/child-welfare-digital-services-california/child-welfare-digital-services.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>My partner Chris Cairns(<a href="https://twitter.com/cscairns">@cscairns</a>) over at <a href="https://skylight.digital/">Skylight</a> sent me a link to <a href="https://github.com/ca-cwds">the Child Welfare Digital Services (CWDS) Certification, Approval, and Licensing Services (CALS) API on Github</a> the other day. The API isn’t your traditional public API, but shows what is possible when it comes to APIs at government agencies. The group behind the API has published their <a href="https://cwds.ca.gov/digital_service_design_standards">Digital Service Development Standards</a>, and is actively using <a href="https://github.com/ca-cwds/cals-api/wiki">a Github Wiki to layout the API strategy for the organization</a>.</p>

<p>To give some backround, the Child Welfare Digital Services (CWDS) is for <em>“state and county workers who ensure that safe and quality licensed facilities and approved homes are available for the children and nonminor dependents who need them, the CALS Digital Service Team will facilitate activities related to ensuring that licensed facilities, approved homes and associated adults meet and maintain required standards.”</em> It makes me happy to see that they are investing so heavily in API, in support of such a worthy cause.</p>

<p>Looking around their wiki I found a handful of APIs:</p>

<ul>
  <li><a href="https://github.com/ca-cwds/cals-api/wiki/Facility-Resource"><strong>Facility</strong></a></li>
  <li><a href="https://github.com/ca-cwds/cals-api/wiki/Facility-Children-Resource-v1"><strong>Facility Children</strong></a></li>
  <li><a href="https://github.com/ca-cwds/cals-api/wiki/Facility-Complaints-Resource"><strong>Facility Complaint</strong></a></li>
  <li><a href="https://github.com/ca-cwds/cals-api/wiki/Facility-Inspections-Resource-v1"><strong>Facility Inspections</strong></a></li>
</ul>

<p>You can also find more about their development process, data model, and approach to security on the Github Wiki for the organization. After looking around more at their Github organization, I found a handful of other operational APIs:</p>

<ul>
  <li><a href="https://github.com/ca-cwds/API"><strong>Modules API</strong></a></li>
  <li><a href="https://github.com/ca-cwds/intake_api"><strong>Intake API</strong></a></li>
  <li><a href="https://github.com/ca-cwds/case-management-api"><strong>Case Management API</strong></a></li>
  <li><a href="https://github.com/ca-cwds/dms-api"><strong>Document Management API</strong></a></li>
  <li><a href="https://github.com/ca-cwds/forms-api"><strong>Forms API</strong></a></li>
  <li><a href="https://github.com/ca-cwds/geo-services-api"><strong>Geo Services API</strong></a></li>
</ul>

<p>There was also a <a href="https://github.com/ca-cwds/api-core">Core API</a> which they use as a base across all API projects, standardizing how they do things. Smart! I also found their testing strategy worthy of noting, just so I can add to my research.</p>

<ul>
  <li><strong>Integration Testing</strong> - To run Integration tests set property cals.api.url to point to environment host. Use gradle integrationTest task. In this case token will be generated for default test user, so it’s possible to test environment with Perry running in dev mode.</li>
  <li><strong>Smoke Testing</strong> - Smoke test suite is part of integration tests. Set cals.api.url, use gradle smokeTestSuite task. Smoke test endpoint is not protected by Perry.</li>
</ul>

<p>I like the style of the Child Welfare Digital Services (CWDS) team. They are investing heavily in APIs, and aren’t afraid of doing it out in the open. As it should be. It is important that ALL government agencies do this, so that other agencies can come along and build on their work, making government more efficient, and cost effective when getting business done. All of the APIs above can, and should be forked, and put to use across other child welfare organizations. I notice they are also using OpenAPI (fka Swagger), but haven’t published them as part of some of their projects. I will keep an eye on and update when they do–reuse of API definitions, is even more than reuse of code.</p>

<p>If you know of any government agency who is this progressive, and publishing their API strategy, processes, definitions, and code on Github–please let me know! This type of behavior needs showcasing, and I like to have a wealth of references on my blog which I can cite as I’m traveling around speaking, and consulting with government agencies. This type of API efforts should be default across all city, county, state, and federal government agencies.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/09/the-child-welfare-digital-services-certification-approval-and-licensing-services-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/08/seeing-reflections-from-the-past-rippling-in-the-api-pool-when-translated-wsdl-to-openapi/">Seeing Reflections From The Past Rippling In The API Pool When I Translated WSDL To OpenAPI</a></h3>
        <span class="post-date">08 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/matrix-neo.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I was looking for <a href="http://user47094.vs.easily.co.uk/siri/schema/schemas.htm">the API definition and schema for the Service Interface for Real Time Information (SIRI) standard</a>, but all they have were WSDL and XSDs. I am working with <a href="http://bustime.mta.info/wiki/Developers/SIRIIntro">the Metropolitan Transit Authorities (MTA) SIRI feed</a>, which returns JSON, and I wanted to have a JSON schema reflecting the responses I was working with. <a href="https://apimatic.io/transformer">So I took the WSDLs and converted them to OpenAPI using the API Transformer</a>, which kind of felt like that scene in the matrix where the world around him is beginning to turn to liquid as he pokes at it, right before he exits the matrix.</p>

<p>I regularly get the emails and tweets from folks telling me “we’ve done all this before”, when I talk about OpenAPI. I’m fully aware that we have, and have even written a few stories about it. However, translating my first WSDL into an OpenAPI was a new experience for me, and made me think deeply about where we are at in 2018 a little bit more. I was reminded once again of how much we’ve left behind, and how much of this we are slowly recreating as part of the OpenAPI specification, and the other definitions and tooling we are developing. I don’t think all of this is bad, but I do think we’ve never been able to have an honest conversation about all of this.</p>

<p>Over the last decade I feel like there has been two camps, those still committed to web services, and those that had invested in this new paradigm. Web service folks have mostly dug in their heals, and proclaimed, “we’ve already done all this”. While us web API folks moved forward with this new realm, somewhat in denial about where we’ve been. Most discussions have been pretty black or white, meaning it was all new, or all old, and we couldn’t really ever talk about things at a granular level, and exploring the grey layers in between. There was a lot of good in the web services model, and I don’t think we have ever processed what that good was–we just moved on.</p>

<p>When I work with WSDL and XSD it is clear that this stuff wasn’t meant for humans. Something I think web APIs, and using YAML for describing interfaces and schema has significantly improved upon. I think an API design first approach has also helped out significantly, making the interfaces, and schema much softer, and easier to sit down and understand WTF is going on. However I think a lot of the scaffolding and structure that existed in the web services world is being recreated, and we are doing a lot of the same work all over again. I feel like the same polarizing conversation we see around hypermedia, GraphQL, microservices, and other areas of evolution in the space reflect the same damaging effects of technological dogma that keeps us from being sensible about this stuff.</p>

<p>Anyhoo, I don’t think there is any solutions here. I’m just going to develop a robust toolbox for helping me map web services to a fresher web API implementation. I’m not a fan of translating SOAP to REST, but I do like having tools that help me make sense of what was, and create a scaffolding for what can be. Then I just dive in and clean up, polish, and move forward as I see fit. I just had to share my moment of anxiety as I translated that WSDL into OpenAPI. For a moment I felt like Neo in the Matrix, and wasn’t entirely sure whether this was real, or a simulation. I’m betting this is all real, we are just humans, and enjoy being our own worst enemies most of the time, and doign work over and over and over–rarely ever learning from the past. ;-(</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/08/seeing-reflections-from-the-past-rippling-in-the-api-pool-when-translated-wsdl-to-openapi/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/08/api-life-cycle-basics-gateway/">API Life Cycle Basics: Gateway</a></h3>
        <span class="post-date">08 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-gateway.png" align="right" width="35%" style="padding: 15px;" /></p>
<p>API gateways have long played a role in providing access to backend resources via web services and APIs. This is how web services have historically been deployed, but it is also how modern web APIs are being managed. Providing a gateway that you can stand up in front of existing web APIs, and proxy them through a single gateway that authenticates, logs, and manages the traffic that comes in and out. There are many management characteristics of API gateways, but I want to provide a stop along the API lifecycle that allows us to think about the API deployment, as well as the API management aspects of delivering APIs.</p>

<p>I wanted to separate out the API gateway discussion from deploy and manage, focusing specifically on the opportunities to deploy one or many gateways, while also looking at it separately as a pattern in service of microservices. While code generation for API deployment is common, gateways are making a resurgence across the sector when it comes to working with a variety of backend systems, on-premise and in the cloud. There are many API gateway solutions available on the market, but I wanted to focus in on a handful that help span deployment and management, as well as allowing for new types of routing, and transformation patterns to emerge. Here are a couple of the gateway solutions I’m studying more these days:</p>

<ul>
  <li><a href="https://aws.amazon.com/api-gateway/"><strong>AWS API Gateway</strong></a> - The Amazon API Gateway allows for the ingestion of OpenAPIs (Swagger) and the deployment of APIs that connect to a variety of backend services define as part of the AWS infrastructure.</li>
  <li><a href="https://konghq.com/"><strong>Kong</strong></a> - Quickly build API-centric applications. Leverage the latest microservice and container design patterns. And tie it all together with the Kong microservice API gateway.</li>
  <li><a href="https://github.com/Netflix/zuul"><strong>Zuul</strong></a> - I’m putting Zuul here, because it has some routing characteristics with makes it a deployment, as well as management solution. One you begin routing, you start to do some of the heavy lifting of design and deployment of resources.</li>
  <li><a href="http://microservices.io/patterns/apigateway.html"><strong>API Gateway Pattern</strong></a> - A pattern for delivering microservices that use an API gateway, and support a variety of applications.</li>
  <li><a href="https://www.nginx.com/blog/building-microservices-using-an-api-gateway/"><strong>Building Microservices Using an API Gateway</strong></a> - Designing, building, and deploying microservices introduced the Microservices Architecture pattern.</li>
</ul>

<p>You’ll notice there is a mix of several concepts here. API gateway as a pattern, in service of delivering microservices, as well as deploying and managing your APIs. I’m doing this on purpose, to try and show how the API gateway landscape is shifting, and evolving with the microservices evolution, as well as in service of devices, and the Internet of Things (IoT). I think Netflix’s approach to using Zuul reflect the shifting middleware roots of the gateway, while working hard to establish the right set of patterns to meet the demanding needs of a growing variety of clients who are consuming our APIs. You see this landscape with new providers like Kong, as well as other leading web server platforms like NGINX outlining how we navigate this new world.</p>

<p>The API Gateway was always a single, monolithic point of entry in my mind. However, as I use AWS API Gateway in a variety of geographic regions and client accounts, and do more deploying of Kong wherever I need it, I’m beginning to change my tune. I’m working with more enterprise groups who have multiple API gateway solutions in play–the result of many disparate teams, as well as acquisitions, and incongruous evolution along the way. Sometimes this is seen as a bad thing, but when it is embraced as part of a larger API life cycle strategy, and driven my an API definition approach to doing APIs, you begin to see a method to the madness. Something that you can even begin to govern, and orchestrate at scale across many different groups, and thousands of APIs.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/08/api-life-cycle-basics-gateway/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/08/looking-for-the-answer-instead-of-developing-an-understanding-of-good-api-design/">Looking For THE Answer Instead Of Developing An Understanding Of Good API Design</a></h3>
        <span class="post-date">08 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/status-berlin_matrix.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I recently worked with a large team on a microservices and API governance training a couple months back, where I saw a repeating pattern that I’ve experienced with other large enterprise groups. They seemed to want the right answers to doing APIs and microservices, instead of developing an understanding of good (or bad) API design, and determining the right way for themselves. The biggest challenge with this perception amongst development groups, is that there is no right answer, or one way of doing APIs and microservices–you need to find the right way forward for your team, and for each project.</p>

<p>I get this a lot within large organizations who are just beginning their API journey–just show us the right way of doing it! To which I usually reply with, let’s roll up our sleeves and get to work on one of your services, and we will start showing you have to craft a simple, sensible, yet robust API that meets the needs of the project. I can’t show you the “right way”, until I understand what the particular need are, and zeroing in on what is the “right way”, takes work, refinement, and crafting f a robust definition for the service.</p>

<p>API design isn’t easy. It takes time to understand what schema is involved, and what role that schema will play in the request and response of an API. It takes work to understand whether the complexity should be spread horizontally across each API paths, or vertically within a single path, using parameters, the body, and other aspects of the surface area of the API. Until I understand the client needs I can’t fully articulate whether we should stick with a simple JSON response, or a more sophisticated hypermedia media type, or possibly even go with something like GraphQL.</p>

<p>Ultimately, I need you to go on this journey with me. I can help ask hard questions, and provide relevant answers about best practices I see across the API space, but I do not know the THE answer. Sometimes there might be multiple right answer, or we won’t know fully until we mock the interface and get to work playing with it. I know you want quick and direct solutions to how you should be doing this, but ultimately it is up to you to do the hard work of learning about what good API design is, and how we can apply it to this particular project. Each microservice within this project might have specific needs, and we’ll have to come up with a variety of solutions that we can apply consistently across the entire project.</p>

<p>I am spending more time helping train teams using OpenAPI as a scaffolding for walking through practical API design, in the context of each service. Going through the paths, verbs, parameters, body, responses, status codes, schema, and fields for each service, one step at a time. It is providing a great way to teach them good API design, in the context of each service they are looking to deliver. Helping them understand that there is no one right answer to all of this. That we just have to invest in their overall API design toolbox. Understand the pros and cons of each decision they make, and develop a healthy understanding of what API design is, both good, and bad–over time.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/08/looking-for-the-answer-instead-of-developing-an-understanding-of-good-api-design/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/08/api-life-cycle-basics-deployment/">API Life Cycle Basics: Deployment</a></h3>
        <span class="post-date">08 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-api-deployment.png" align="right" width="30%" style="padding: 15px;" /></p>
<p>There are many ways to deploy an API, making this another confusing stop along the API life cycle for some of my readers. My goal in having this be a separate stop from design, or possibly management, is to help API providers think about where and how they deploy APIs. From my perspective, API deployment might be about which framework and language you choose to deploy in, spanning all the way to where you might deploy it, either on-premise, on-device, or in the cloud. The how and why of deploying your API will play a significant role in determining how stable and consistent you are able to deploy API resources, impacting almost every other stop along the API life cycle.</p>

<p>Many API providers still think of API deployment in the context of their internal operations, as opposed to thinking about how they will be put to use. The providers I’m seeing enjoy more flexibility and agility when it comes to API consumption are able to deploy APIs in a variety of languages, supporting a variety of existing platforms, and in any environment where they are needed. There are several concepts that are beginning to define API deployment in this new generation of compute in the cloud, here are just a handful of them.</p>

<ul>
  <li><strong>Polyglot Deployment</strong> - The ability to deploy APIs in a variety of programming languages.</li>
  <li><strong>Multi-Platform</strong> - The ability to deploy APIs in a variety of platforms, and using existing system.</li>
  <li><strong>Multi-Cloud</strong> - The ability to deploy APIs within Amazon, Azure, Heroku, and Google environments.</li>
  <li><strong>Frameworks</strong> - Leverage a variety of open source API frameworks for deploying APIs.</li>
</ul>

<p>I normally would put API gateways here as well, but because of renewed energy around gateway solutions actually deploying APIs instead of just managing and securing them, I’m breaking out gateway into its own stop along the API lifecycle. Gateway spans API deployment and management in my opinion, and while it should be considered alongside these elements, it is increasingly becoming its own stop. Ideally, teams are able to use a combination of gateway, as well as hand-rolled, and auto-generated approaches to deploying APIs, with the diversity I mentioned above.</p>

<p>Most groups I work with have one way to deploy APIs, using a single programming language. This results in many of them thinking about API consumption on the same terms. When you allow for, and support a variety of languages, platforms, and cloud environments, you open up a new world of possibilities when it comes to scaling, migrating, and hiring talent as part of your API operations. API deployment will be a new concept to many of my readers, and something not all will be ready for, but being able to think outside the API box you’ve been operating within until now is one of the basic aspects of the API life cycle you should be looking at evolving.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/08/api-life-cycle-basics-deployment/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/08/i-created-an-openapi-for-the-hashicorp-consul-api/">I Created An OpenAPI For The Hashicorp Consul API</a></h3>
        <span class="post-date">08 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/hashicorp/hashicorp-consul-api-openapi-githbu.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I was needing an OpenAPI (fka Swagger) definition for <a href="https://www.consul.io/api/index.html">the Hashicorp Consul API,</a> so that I could use in a federal government project I’m advising on. We are using the solution for the microservices discovery layer, and I wanted to be able to automate using the Consul API, publish documentation within our project Github, import into Postman across the team, as well as several other aspects of API operations. I’m working to assemble at least a first draft OpenAPI for the entire technology stack we’ve opted to use for this project.</p>

<p>First thing I did was Google, “Consul API OpenAPI”, then “Consul API Swagger”, which didn’t yield any results. Then I Githubbed “Consul API Swagger”, and came across a Github Issue where a user had asked for “<a href="https://github.com/hashicorp/consul/issues/555">improved API documentation</a>”. The resulting response from Hashicorp was, “we just finished a revamp of the API docs and we don’t have plans to support Swagger at this time.” Demonstrating they really don’t understand what OpenAPI (fka Swagger) is, something I’ll write about in future stories this week.</p>

<p>One of the users on the thread had created <a href="https://consul.docs.apiary.io/">an API Blueprint for the Consul API, and published the resulting documentation to Apiary</a>. Since I wanted an OpenAPI, instead of an API Blueprint, I headed over to <a href="https://apimatic.io/transformer">APIMATIC API Transformer</a> to see if I could get the job done. After trying to transform the API Blueprint to OpenAPI 2.0 I got some errors, which forced to me to spend some time this weekend trying to hand-craft / scrape the static API docs and publish my own OpenAPI. The process was so frustrating I ended up pausing the work, and writing two blog posts about my experiences, and then this morning I received an email from the APIMATIC team that they caught the errors, updated the API Blueprint, allowing me to continue transforming it into an OpenAPI definition. Benefits of being the API Evangelist? No, benefits of using APIMATIC!</p>

<p>Anyways, <a href="https://github.com/api-stack/hashicorp-consul-api">you can find the resulting OpenAPI on Github</a>. I will be refining it as I use in my project. Ideally, Hashicorp would take ownership of their own OpenAPI, providing a machine readable API definition that consumers could use in tooling, and other services. However, they are stuck where many other API service providers, API providers, and API consumers are–thinking OpenAPI is still Swagger, which is just about API documentation. ;-( . I try not to let this frustrate me, and will write about it each time I come across, until things change. OpenAPI (fka Swagger) is so much more than just API documentation, and is such an enabler for me as an API consumer when I’m getting up and running with a project. If you are doing APIs, please take the time to understand what it is, it is something that could be the difference between me using our API, or moving on to find another solution. It is that much of a timesaver for me.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/08/i-created-an-openapi-for-the-hashicorp-consul-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/08/understanding-events-across-your-api-platform-in-real-time/">Understanding Events Across Your API Platform In Real Time</a></h3>
        <span class="post-date">08 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/stripe/stripe-the-event-object.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I spend a lot of time trying to understand and define what is API. With <a href="http://apis.how/streamdata">my new partnership with Streamdata.io</a> I’m pushing that work into understanding APIs in real time, and as part of event-driven architecture. As the Streamdata.io team and I work to identify interesting APIs out there that would benefit from streaming using the service, a picture of the real time nature of API platforms begins to emerge. I’m beginning to see all of this as a maturity aspect of API platforms, and those who are further along in their journey, have a better understanding the meaningful events that are occurring via their operations.</p>

<p>As part of this research I’ve been studying the Stripe API, looking for aspects of the platform that you could make more real time, and streaming. Immediately I come across <a href="https://stripe.com/docs/api#events">the Stripe Events API</a>, which is a “way of letting you know when something interesting happens in your account”. Using the Stripe Events API, “you can retrieve an individual event or a list of events from the API. We also have a separate system for sending the event objects directly to an endpoint on your server, called webhooks.” This is the heartbeat of the Stripe platform, and represents the “events” that API providers and consumers want to know about, and understand across platform usage.</p>

<p>I think about the awareness API management has brought to the table in the form of metrics and analytics. Then I consider the blueprint the more mature platforms like Stripe have established when it comes to codifying this awareness in a way that can be accessed via API, and begin to make more real time, or at least asynchronous using webhooks. Then I think about what Streamdata.io provides with Server-Sent Events, and JSON Patch, providing a stream of these meaningful events in real time, as soon as these events happen–no polling necessary. This is what I find interesting about what they do, and why I’ve signed up to partner with them. Well, that combined with them supporting me financially. ;-)</p>

<p>Even before working with Streamdata.io, <a href="http://apievangelist.com/2017/09/27/the-value-of-api-driven-events/">I have been studying the value of API driven events</a>, and working to identify the API platforms who are mature enough to be mapping this value exchange out. This has led to me to desire a better understanding of event-driven architecture, not necessarily because it is the next thing with APIs, but because there is some substance in there. There is a reason why events matter. They represent the meaningful exchanges that are occurring via platforms. The valuable ones. The percentage of transactions we should be tuning into. I want to better understand this realm, and continue collecting a wealth of blueprints regarding how companies like Stripe are maximizing these exchanges.</p>

<p><em><strong>Disclosure:</strong> In case it isn’t clear, <a href="http://apis.how/streamdata">Streamdata.io</a> is my primary partner, paying me to do research in this area, and helping support me as the API Evangelist.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/08/understanding-events-across-your-api-platform-in-real-time/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/04/api-discovery-can-just-be-about-you-sharing-stories-about-the-apis-you-depend-on/">API Discovery Is Mostly About You Sharing Stories About The APIs You Use</a></h3>
        <span class="post-date">04 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/photos/man-on-moon-flag.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I do a lot of thinking about <a href="http://discovery.apievangelist.com">API discovery</a>, and how I can help people find the APIs they need. As part of this thinking I’m always curious why API discovery hasn’t evolved much in the last decade. You know, no Google for APIs. No magical AI, ML, AR, VR, or Blockchain for distributed API mining. As I’m thinking, I ask myself, <em>“how is it that the API Evangelist finds most of his APIs?”</em> Well, word of mouth. Storytelling. People talking about the APIs they are using to solve a real world business problem.</p>

<p>That is it! API storytelling is API discovery. If people aren’t talking about your API, it is unlikely it will be found. Sure people still need to be able to Google for solutions, but really that is just Googling, not API discovery. <a href="http://apievangelist.com/2017/12/20/api-discovery-will-be-about-finding-companies-who-do-what-yu-need-and-api-is-assumed/">It is likely they are just looking for a company that does what they need, and the API is a given</a>. We really aren’t going to discover new APIs. I don’t know many people who spend time looking for new APIs (except me, and I have a problem). People are going to discover new APIs by hearing about what other people are using, through storytelling on the web and in person.</p>

<p>In my experience as the API Evangelist I see three forms of this in action:</p>

<p>1) APIs talking about their API use cases on their blog
2) Companies telling stories about their infrastructure on their blog
3) Individuals telling stories about the APIs they use in job, side projects, and elsewhere.</p>

<p>This represent the majority of ways in which I discover new APIs. Sure, as the API Evangelist I will discover new APIs occasionally by scouring Github, Googling, and harvesting social media, but I am an analyst. These three ways will be how the average person discovers new APIs. Which means, if you want your API to be discovered, you need to be telling stories about it. If you want the APIs you depend on to be successful and find new users, you need to be telling stories about it.</p>

<p>Sometimes in all of this techno hustle, good old fashioned storytelling is the most important tool in our toolbox. I’m sure we’ll keep seeing waves of API directories, search engines, and brain wave neural networks emerge to help us find APIs over the next couple of years. However, I’m predicting that API discovery will continue to be defined by human beings talking to each other, telling stories on their blogs, via social media, and occasionally through brain interfaces.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/04/api-discovery-can-just-be-about-you-sharing-stories-about-the-apis-you-depend-on/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/04/api-life-cycle-basics-mocking/">API Transit Basics: Mocking</a></h3>
        <span class="post-date">04 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-mock-interface.png" align="right" width="30%" style="padding: 15px;" /></p>

<p><em>This is a series of stories I’m doing as part of <a href="http://basics.apievangelist.com/">my API Transit work</a>, trying to map out a simple journey that some of my clients can take to rethink some of the basics of their API strategy. I’m using a subway map visual, and experience to help map out the journey, which I’m calling <a href="http://basics.apievangelist.com/">API transit</a>–leveraging the verb form of transit, to describe what every API should go through.</em></p>

<p>One key deficiency I see in organizations that I work with on a regular basis, is the absence of the ability to quickly deploy a mock version of an API. Meaning, the ability to deliver a virtualized instance of the surface area of an API, that will accept requests, and return responses, without writing or generating any existing backend code. Mocking APIs require an API definition, and with many groups still producing these definitions from code, the ability to mock an API is lost in the shuffle. Leaving out the ability to play with an API before it ever gets built–which if you think about it, goes against much of why we design APIs in the first place.</p>

<p>Mocking of an API goes hand in hand with a design first approach. Being able to define, design, mock, and then receive feedback from potential consumers, then repeat until the desired API is delivered is significantly more efficient than writing code, deploying an API, and iterating on it over a longer time frame. Over the last couple of years, a growing number of services and tooling have emerged to help us mock our APIs, as well as the schema that are used as part of their requests and responses, giving birth to this entirely new stop along the API life cycle.</p>

<ul>
  <li><a href="https://www.mockable.io/"><strong>Mockable</strong></a> - A simple service for mocking web and SOAP APIs</li>
  <li><a href="https://getsandbox.com/"><strong>Sandbox</strong></a> - A simple service for generating sandboxes using a variety of formats.</li>
  <li><a href="https://github.com/stoplightio/prism"><strong>Stoplight Prism</strong></a> - An open source tool for mocking and transforming from OpenAPIs.</li>
  <li><a href="https://github.com/tomakehurst/wiremock"><strong>Wiremock</strong></a> - An open source tool for mocking APIs.</li>
  <li><a href="https://www.getpostman.com/docs/postman/mock_servers/setting_up_mock"><strong>Postman Mock Server</strong></a>You can setup mock servers from within the Postman environment.</li>
</ul>

<p>Mocking of API is something that organizations who have not adopted an API definition approach to delivering APIs cannot ever fully realize. When you have a robust, machine readable definition of the surface area of your API it allows you to quickly generate sandboxes, mocks, and virtualized instance of an API. These interfaces can then be consumed, and played with, and allow for API definitions to be adjusted, tweaked, and polished until it meets the needs of consumers. Shortening the feedback loop between each iteration, and version of an API–saving both time and money.</p>

<p>The API developers I’ve seen who have become proficient in defining and designing their APIs, and delivering mock APIs, have also begun to be more agile when it comes to mocking and virtualizing of data that gets returned as part of mock API responses. Further pushing mocking and virtualization into testing, security, and other critical aspects of API operations. Being able to mock API interfaces is a sign that API operations is maturing, allowing for costly mistakes to be eliminated, or identified long before anything goes into production, making sure APIs meet the needs of both providers and consumers long before anything gets set into stone.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/04/api-life-cycle-basics-mocking/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/03/keeping-things-in-the-club-by-drowning-api-complexity/">Keeping Things In The Club By Drowning Everyone In API Complexity</a></h3>
        <span class="post-date">03 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/16_38_600_500_0_max_1_1_2-0.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>After seven years of doing API Evangelist I have learned a lot about the realities of the technology sector, versus my own beliefs. One of the things that attracted me to web APIs in the first place was the ability to simplify the access to data, content, algorithms, and other resources using a web url. I could get a list of news articles, post a picture, launch a server in the cloud, and many other common business tasks using a simple URL. To me good API design is more about simplicity, than it was ever about REST, or any other dogmatic approach to doing APIs. However, after seven years of doing this, I’m pretty convinced that most folks have very little interest in truly making things simple for anyone.</p>

<p>As API space continues to move forward with efforts to address technical debt, and the cultural issues involved with the technology we are using within large enterprises as a part of the microservices movement–we are simultaneously see other fronts where leading edge practitioners are embracing technical complexity in service of scope, volume, and satisfying the requests of developers down in the weeds, and not taking time to consider the big picture. You see this with trends like Kafka, GraphQL, ad other areas, where we are moving forward with technology that isn’t entirely embracing the web, and introducing some pretty complex approaches to getting the job done.</p>

<p>I get it. The problems being solved are big. There is a lot of data. Complex delivery models. Robust, and highly functional applications. Simple web APIs can’t always deliver at the scope, scale, and satisfaction of the very technical folks involved. I’m not knocking things moving forward, but I am asking if everyone involved is thinking seriously about the big picture, and assessing the costs down the road–as well as those who get left behind. Not everyone will have the resources, knowledge, and ability to keep up, and I actually question if this pace and the complexity going on is actually required–then I get the feeling that maybe it is actually intentional. Survival of the fittest, meritocracy, unicorns, and all the competitiveness that the tech sector loves about itself.</p>

<p>My assessment is that not everyone is intentionally choosing complexity over simplicity. It is just that the current environment doesn’t afford taking a breathe to think about it, and considering whether or not operating at this scope makes sense, and all this data is actually needed, or if down the road this will all truly pencil out. For me, this is when we get to each point in time where we have to stop and ask, how did we accumulate all this technical debt? How did things get so bloated and complex? Oh yeah, we didn’t ever stop and ask ourselves the right questions along the way, and the folks who made the decision have long moved on, and are enjoying greener pastures. Nobody is ever really ensuring there is accountability for any of this, but I guess that is how it all works, right? Moving forward, at all costs. It will be someone else’s problem down the road.</p>

<p>My guess is that folks who are in the business of mastering or developing the latest technology, and always moving on to the next big thing will not see eye to eye with me. However, when you are someone like me who has been doing this 30+ years, and come into many large organizations to help clean up legacy messes, you might agree with me a little more. I’m guessing this is why some high tech companies who are selling the next thing to the enterprise prefer hiring young whipper snappers, who like to move fast and break things. There will always be endless waves of these techpreneurs to hire out of college, as well as companies to sell the latest technological solution to that will magically fix all our legacy technical debt challenges. Circle of life, and all that. Keeping things in the club, by drowning everyone in complexity, and getting rich along the way.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/03/keeping-things-in-the-club-by-drowning-api-complexity/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/03/api-life-cycle-basics-database/">API Life Cycle Basics: Database</a></h3>
        <span class="post-date">03 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-database-new.png" align="right" width="25%" style="padding: 15px" /></p>

<p><em>This is a series of stories I’m doing as part of <a href="http://basics.apievangelist.com/">my API Transit work</a>, trying to map out a simple journey that some of my clients can take to rethink some of the basics of their API strategy. I’m using a subway map visual, and experience to help map out the journey, which I’m calling <a href="http://basics.apievangelist.com/">API transit</a>–leveraging the verb form of transit, to describe what every API should go through.</em></p>

<p>Deploying an API from a database is the most common approach to delivering APIs today. Most of the data resources we are making available to partners and 3rd party developers via APIs lives in a database behind our firewall(s). While we have seen database platform providers begin to take notice of the need to make data available using the web, most APIs get deployed through custom frameworks, as well as gateways that expose backend systems as web APIs.</p>

<p>If you are deploying APIs from a centralized legacy database, there will be significantly more security, performance, and other operational concerns than if your database is dedicated to providing a backend to your API. There are a growing number of open source tools for helping broker the relationship between your API and the database, as well as evolving services, and entire database platforms that are API-centric. Here are just a handful of what I’m seeing out there to support the database stop along an API journey.</p>

<ul>
  <li><a href="https://github.com/GSA/DataBeam">DataBeam</a> - Generic RESTful Interface for databases.</li>
  <li><a href="https://github.com/alixaxel/ArrestDB/">Arrest-MySQL</a> - A plug-n-play RESTful API for your MySQL database.</li>
  <li><a href="https://github.com/begriffs/postgrest/">Postgrest</a> - REST API for any Postgres database.</li>
  <li><a href="https://github.com/SoftInstigate/restheart">Restheart</a> - RESTHeart, the automatic REST API Server for MongoDB.</li>
  <li><a href="https://github.com/ealeksandrov/NodeAPI">NodeAPI</a> - Simple RESTful API implementation on Node.js + MongoDB.</li>
  <li><a href="https://github.com/mevdschee/php-crud-api">PHP CRUID API</a> - Single file PHP script that adds a REST API to a SQL database</li>
  <li><a href="https://cloud.google.com/spanner/">Google Cloud Spanner</a> - loud Spanner is the first and only relational database service that is both strongly consistent and horizontally scalable.</li>
</ul>

<p>There are many database to API tools and services available out there. There are also many cloud-native solutions available to help you generate APIs from your preferred cloud provider. Amazon, Azure, and Google all provide API deployment, and management solutions directly from their database solutions. The most difficult part about helping folks thinking about this stop along the API journey, is the many different scenarios for how data is stored, and the limitations on how that data can be made available via APIs.</p>

<p>Ideally you are starting from scratch with your API, and you can deploy a new database, with a brand new API layer exposing your data store within. If you are deploying from a legacy database which serves other systems and applications, I recommend thinking about replicating the database and creating read only instances for accessing via the API, or if if you need read / write capabilities, then take a look at many of the gateway solutions available today. Beyond that, if you have the skills to securely connect directly to your database, there are many more options on the table to help you get the job done in todays web-centric, data-driven world.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/03/api-life-cycle-basics-database/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/03/api-life-cycle-basics-api-design/">API Transit Basics: API Design</a></h3>
        <span class="post-date">03 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-design.png" align="right" width="25%" style="padding: 15px;" /></p>

<p><em>This is a series of stories I’m doing as part of <a href="http://basics.apievangelist.com/">my API Transit work</a>, trying to map out a simple journey that some of my clients can take to rethink some of the basics of their API strategy. I’m using a subway map visual, and experience to help map out the journey, which I’m calling <a href="http://basics.apievangelist.com/">API transit</a>–leveraging the verb form of transit, to describe what every API should go through.</em></p>

<p>API design is not just about REST. Sure, a great deal of the focus within this stop along the API journey will be focused on REST, but this is because it is the dominant methodology at this moment in time. API design is about establishing a framework for how you will consistently craft your APIs across teams, whether they are REST, GraphQL, Microservices, or even gRPC. Your API design strategy might be dominated by RESTful practices, especially early on in your journey, but API design should not be considered to be only REST methodologies.</p>

<p>In the last five years API design has matured into its own discipline, focusing on a define and design first approach to developing APIs, shifting away from a code then document approach we’ve seen dominate for the last decade, and is still common place at many organizations. There are a handful of tooling, and websites that have emerged to help API providers, architects, developers, and designers get a handle on this stop along the API journey–here are just a few.</p>

<ul>
  <li><a href="http://editor.swagger.io"><strong>Swagger Editor</strong></a> - Leverage the Swagger editor for manually working with OpenAPI definitions.</li>
  <li><a href="http://www.apicur.io/"><strong>Apicurio</strong></a> - A robust, and beautiful open source tool for designing APIs.</li>
  <li><strong>API Design Guide</strong> - Continue to establish, evolve, and disseminate the organizational API design guide, providing guidance for all teams–make sure there is a feedback loop involved with its development.</li>
  <li><a href="http://apistylebook.com/"><strong>API Stylebook</strong></a> - Learning and extracting from other companies API design guides.</li>
  <li><a href="https://www.infoq.com/articles/hypermedia-api-tutorial-part-one"><strong>Designing and Implementing Hypermedia APIs</strong></a> - A thoughtful post on how to design hypermedia APIs.</li>
  <li><a href="http://graphql.org/learn/best-practices/"><strong>GraphQL Design</strong></a> - The best practices for designing a GraphQL API.</li>
  <li><a href="https://grpc.io/docs/guides/"><strong>GRPC API Design</strong></a> - A guide to designing gRPC APIs that leverage HTTP/2 and Protocol Buffers.</li>
</ul>

<p>API design is as much of a discipline as it is a methodology rooted in a specific standard, protocol, or history. It is about having the discipline to document current practices for designing APIs that are in production, then standardize, communicate, and evolve those practices in a formal way. While also studying and learning from other leading API providers and practitioners regarding how they are designing their APIs. Which is why I include the API Stylebook in this stop along the API journey–everyone should be learning from each other, which also includes sharing your API design guide when it is ready.</p>

<p>We need to move beyond API design meaning REST in the API community. This is something that has caused significant damage to the health of many API operations, and is a dogmatic approach that has replicated itself in hypermedia, and GraphQL–it needs to stop. API design is about defining a common framework for designing your APIs, no matter which approach you adopt internally. Ideally, your API design philosophy is multi-approach, allowing you to apply the right pattern where is needed, and not viewing API design as a one size fits all set of rules. When it comes to API design within your organization, start small, keep things loose, learn from others, and begin documenting your approach in a guide, that can eventually grow into a wider set of API governance practices that will allow your operations to grow in the way you envision.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/03/api-life-cycle-basics-api-design/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/03/the-transit-feed-api-is-a-nice-blueprint-for-your-home-grown-api-project/">The Transit Feed API Is A Nice Blueprint For Your Home Grown API Project</a></h3>
        <span class="post-date">03 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/transit-feeds-api/transit-feeds-api-home-page.png" align="right" width="45%" style="padding:15px;" /></p>
<p>I look at a lot of APIs. When I land on the home page of an API portal, more often than not I am lost, confused, and unsure of what I need to do to get started. Us developers are very good at complexifying things, and making our APIs implementations as messy as our backends, and the API ideas in our heads. I suffer from this still, and I know what it takes to deliver a simple, useful API experience. It just takes time, resources, as well as knowledge to it properly, and simply. Oh, and caring. You have to care.</p>

<p>I am always on the hunt for good examples of simple API implementations that people can emulate, that aren’t the API rockstars like Twilio and Stripe who have crazy amounts of resources at their disposal. One good example of a simple, useful, well presented API can be found with <a href="http://transitfeeds.com/">the Transit Feeds API, which aggregates the feeds of many different transit providers around the world</a>. When I land on the home page of Transit Feeds, I immediately know what is going on, and I go from home page to making my first API call in under 60 seconds–pretty impressive stuff, for a home grown API project.</p>

<p>While there are still some rough edges, Transit Feeds has all the hallmarks of a quality API implementation. Simple UI, with a clear message about what it does on the home, but most importantly an API that does one thing, and does it well–providing access to transit feeds. The site uses Github OAuth to allow me to instantly sign up and get my API key–which is how ALL APIs should work. You land on the portal, you immediately know what they do, and you have your keys in hand, making an API call, all without having to create yet another API developer account.</p>

<p>The Transit Feed API <a href="http://transitfeeds.com/api/transitfeeds-api.yaml">provides an OpenAPI for their API</a>, and uses it to drive <a href="http://transitfeeds.com/api/swagger/">their Swagger UI API documentation</a>. I wish the API documentation was embedded onto the docs page, but I’m just thankful they are using OpenAPI, and provide detailed interactive API documentations. Additionally, they have a great <a href="http://transitfeeds.com/news">updates page</a>, providing recent site, feed, and data updates across the project. To provide support they wisely <a href="https://transitfeeds.com/issues">use Github Issues to help provide a feedback loop</a> with all their API consumers.</p>

<p>It isn’t rocket surgery. Transit Feed makes it look easy. They provide a pretty simple blueprint that the rest of us can follow. They have all the essential building blocks, in an easy to understand, easy to get up and running format. They leverage OpenAPI and Github, which should be the default for any public API. I’d love to see some POST and PUT methods for the API, encouraging for more engagement with users, but as I said earlier, I’m pretty happy with what is there, and just hope that the project owners keep investing in the Transit Feed API. It provides a great example for me to use when working with transit data, but also gives me a home grown example of an API project that any of my readers could emulate.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/03/the-transit-feed-api-is-a-nice-blueprint-for-your-home-grown-api-project/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/03/alexa-voice-skills-is-the-poster-child-for-your-enterprise-api-efforts/">Alexa Voice Skills Are The Poster Child For Your Enterprise API Efforts</a></h3>
        <span class="post-date">03 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/alexa/alexa-skills.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I was sitting in an IT architectural planning meeting for a large enterprise organization the other day, and one of the presentation from one of the executives contained a complex diagram of their IT infrastructure, with a column to the right showing a simple five step Alexa conversation, asking a specific question from customer. Each question posed as part of the Alexa conversation theoretically accessed a different system, weaving a pretty complex web of IT connections, to enable this simple conversation.</p>

<p>This presentation reflects why I feel that Alexa Skills development poses some interesting questions in the API world, and why the platform becomes interesting to so many business users. It reflects the end goal of why we are doing all of this (in theory), but then quickly illustrates how complicated we’ve actually made all of this, demonstrating how challenging delivering conversational interfaces will be in reality. There are many conversational challenges in enabling our system to be able to talk with humans, but I think many of the most daunting challenges companies will face in coming years will be to actually get at the right data to provide a relevant answer to questions poised in voice, bot, and other conversationally-enabled solutions.</p>

<p>Being able to quickly respond to information requests is why many companies, organizations, institutions, and government agencies are doing APIs. Being able to respond to them in real time conversations is definitely a question of doing APIs, but I’m finding in most organizations it is more about solving human and political questions, than it is just a technical one. Sure, you can envision the most beautiful stack of microservices reaching into every aspect of your organization(s), and even develop a robust conversational layer for answering questions posed across that stack, but delivering it all consistently, at scale, across multiple teams of human beings will never be easy, or quick.</p>

<p>I think that conversational interfaces provide an excellent exercise for companies, to help them map out the complexities of their backend systems, and try to understand how to deliver more real time solutions. Personally, I’m not a big fan of bot or voice-enablement, but I know others are. I’m more interested in them because of the technical challenges in delivering, and the business and cultural hurdles they put in front of development teams. It isn’t easy to deliver meaningful, relevant, and intelligent conversations via these new mediums, and I think the Alexa Skills framework provides a useful way for us to hang these conversations regarding our IT resources on.</p>

<p>While the majority of APIs are still about delivering data and content to the web and mobile applications, I think conversational interfaces are showing the future of where things are headed. I don’t think we’ll get there as fast as we would like, or as quickly as the vendors are promising us, but I do think we will make movements towards delivering more meaningful conversational interfaces in coming years. Mostly it will be due to the availability of API resources. If we can get at the data and content, we can usually answer questions regarding that data content. The problem will be that not everything is digitized, and easily accessible. Despite the promises of artificial intelligence, and voice-enabled platforms like Alexa, humans will prove to be the biggest obstacle to realizing the visions of business leaders to answer even the most basic questions we are looking to answer.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/03/alexa-voice-skills-is-the-poster-child-for-your-enterprise-api-efforts/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/03/the-api-transit-basics-api-definitions/">API Transit Basics: API Definitions</a></h3>
        <span class="post-date">03 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/bw-icons/bw-definition.png" align="right" width="40%" style="padding: 15px;" /></p>

<p><em>This is a series of stories I’m doing as part of <a href="http://basics.apievangelist.com/">my API Transit work</a>, trying to map out a simple journey that some of my clients can take to rethink some of the basics of their API strategy. I’m using a subway map visual, and experience to help map out the journey, which I’m calling <a href="http://basics.apievangelist.com/">API transit</a>–leveraging the verb form of transit, to describe what every API should go through.</em></p>

<p>Defining an API is the first stop along any API journey. When I say definitions, I’m not just talking about OpenAPI (fka Swagger), and specifically definitions for the surface area of your API. I’m talking about defining your idea, your goals, and the standard aspects of doing business with APIs. By API definitions, I mean having a robust toolbox of definitions for everything that is going into your API operations, from standardized dates and currencies, to common data schema, and yes to making sure there is an active OpenAPI definition for every single one of your APIs.</p>

<p>I’d say that 75% of the companies, organizations, institutions, and government agencies I’m talking with about APIs begin API development by coding. A very costly, and rigid approach to defining a solution to a problem. Many of the groups I know who are using OpenAPI in their operations still rely on it being generated from systems and code, and do not actually hand-define, or hand-craft the definitions for their APIs, which should be being applied across API operations, not just for delivering documentation. When it comes to establishing a robust API definition strategy for operations, I recommend starting with a handful of tools and concepts.</p>

<ul>
  <li><a href="https://www.openapis.org/">OpenAPI</a> - Ensuring there are OpenAPI definitions for ALL APIs / microservices.</li>
  <li><a href="http://json-schema.org/">JSON Schema</a> - Ensuring there are robust JSON schema for all data in use.</li>
  <li><a href="https://www.getpostman.com/docs/postman/collections/creating_collections">Postman Collections</a> - Postman’s proprietary format for defining APIs, which can be translated to and from OpenAPI.</li>
  <li><a href="https://apimatic.io/transformer">API Transformer</a> - Opening up the ability to transform APIs across formats.</li>
  <li>Multi-Format - Being able to speak XML, JSON, and YAML fluently and seamlessly across groups.</li>
</ul>

<p>There are many other tools to assist you in crafting, generating, managing, and evolving the definitions as part of your API operations. API definitions isn’t just one stop along this API journey, and I will be exploring ideas for how API definitions can be applied to each stop along this API journey, in a separate line of thought that runs parallel to what I consider to be my API Transit basics. These five areas represent what I think are the basics of API definitions for ANY API operations, and should be where any API provider begins their journey–by defining the moving parts of each API, and what it will do from define to deprecation.</p>

<p>The biggest threat to properly defining APIs is too much automation, and thinking that they only apply to one stop of the API Transit process. OpenAPI is not just about generating documentation. JSON Schema is not just about completing your OpenAPI definition. Not all APIs are purely JSON, and teams should be multi-lingual when it comes to the definitions they use across API operations. API definitions are essential to not just delivering your APIs, but also communicating and supporting them, and evolving them as part of your road map. API definitions are essential to establishing healthy API operations, and without them, things will easily break down for a single API, and be near impossible to deliver APIs consistently at scale across any organization.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/03/the-api-transit-basics-api-definitions/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/02/my-evolving-definition-of-a-robust-and-diverse-api-toolbox/">My Evolving Definition Of A Robust And Diverse API Toolbox</a></h3>
        <span class="post-date">02 Jan 2018</span>
        <p>It is always telling when folks assume I mean REST when I say API. While the web dominates my definition of API, and REST is definitely a leading architectural style, these assumptions always define the people who bring them to the table, more than they ever do me. I’m in the business of studying how people are applying programmatic interfaces using the web. To reflect my research I’ve been evolving a diagram of my toolbox that I’ve been publishing as part of workshops, presentations, and some talks I’m preparing for 2018. It reflects what I’m seeing as the evolving API toolbox that I’m seeing companies working with, and a diversity in which I’m encouraging others to think about more, as we choose to ignore the polarizing forces in the API sector.</p>

<p><img src="https://s3.amazonaws.com/kinlane-productions/api-toolbox/API+Toolbox.png" width="100%" style="padding: 15px;" /></p>

<p>To set the tone for any API conversation I am participating in, I prefer to introduce the concept of the API toolbox including more tools than just REST, acknowledging that there are a growing number of tools in our API infrastructure toolbox which can be applied to different APIs, to solve a variety of problems and challenges we face. Also we need to be more honest about the fact that there are many legacy solutions still in use across large organizations, even as we consider adopting the latest in leading edge approaches to API deployment in newer projects.</p>

<ul>
  <li>HTTP - Leverage the web, and the HTTP standard across ALL API efforts.</li>
  <li>SOAP - Acknowledging there are still a number of SOAP services in use.</li>
  <li>RPC - Understand how and why RPC APIs still might be viable in production.</li>
  <li>REST - Making REST, and a resource-centered approach the focus of the operations.</li>
  <li>Microservices - Emphasis on independently deployable and module API services.</li>
  <li>Verbs - Knowing, and putting to use HTTP verbs across API implementations.</li>
  <li>Content-Type - Understanding the negotiation between XML, JSON, and other types.</li>
  <li>Hypermedia - Considering how hypermedia design, and content types play a role.</li>
  <li>GraphQL - Thinking about GraphQL when it comes to data intensive API projects.</li>
  <li>HTTP/2 - Understanding and embracing the evolution of the HTTP standard.</li>
  <li>gRPC - Considering two-speed APIs, and using gRPC for higher volume API implementations.</li>
  <li>Webhooks - Seeing APIs as a two-way street, and pushing data to APIs as well as receiving.</li>
  <li>Server-Sent Events (SSE) - Leveraging HTTP push technology to make things real time.</li>
  <li>Websockets - Opening up two streams that allow for bi-directional API interactions.</li>
  <li>PubSubHubbub - Considering a distributed publish-subscribe approach to API interactions.</li>
  <li>Apache - Being aware of the Apache stack which includes Spark, Kafka, and other real time data solutions.</li>
</ul>

<p>While HTTP and REST are definitely the focal point of many API conversations I am in, SOAP and RPC are legacy realities we must accept are still getting the job done in many environments, and we shouldn’t be shaming the folks who own this infrastructure. At the same time I’m helping folks unwind this legacy infrastructure, I also find myself participating in discussions around event-driven architecture, streaming, and HTTP/2 which represent where API architecture is headed. I’m needing a toolbox that reflects this spectrum of API tooling, as well as where we’ve been, and find ourselves still supporting in 2018.</p>

<p>I’m still evaluating the Apache stack, as well as GraphQL and gRPC, to better understand how they fit in my definition. This work, as well as part new partnership with <a href="http://streamdata.io">Streamdata.io</a> is pushing me to re-evaluate exactly what is real time and streaming using webhooks and server-sent events, alongside a more event-driven approach I am seeing emerge within many leading organizers. People love to say that APIs are done. I wish I could show how silly this way of thinking makes y’all look. The idea that using the web to exchange data, content, and algorithms in a machine readable formats is going anywhere is laughable. My objective is to keep tracking on the tools people are using to get this job done, and help folks ensure their toolbox is as robust and diverse as possible, not traffic in silly dogmatic fantasies about API trends and religions.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/02/my-evolving-definition-of-a-robust-and-diverse-api-toolbox/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/02/treating-all-apis-like-they-are-public/">Treating All APIs Like They Are Public</a></h3>
        <span class="post-date">02 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/45_78_800_500_0_max_0_1_-5.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I was talking with the Internal Revenue Service (IRS) about their internal API strategy the week before Christmas, sharing my thoughts on the strategy that they were pitching internally when it comes to the next phase of their API journey. One topic that kept coming up is the firm line of separate between public and private APIs, which you kind of get at an organization like the IRS. It isn’t really the type of organization you want to be vague about this line, making sure everyone understands where an API should be consumed, and where it should not be consumed.</p>

<p>Even with that reality, I still made the suggestion that they should be treating ALL APIs like they are public. I clarified by saying you shouldn’t be getting rid of the hard line dictating whether or not an API is internal or external, but if you treat them all like they are public, and act like they are all under threat, you will be better off for it. This peaked their interest, was something they did not expect to hear from me, and was something they would be adding to their recommendations for the next version of their API strategy.</p>

<p>The first benefit of treating your internal APIs like they are public is when it come to security, logging, and overall API management. You have the tools in place to catch any threats, and develop awareness regarding how an API is being used, both good and bad. While the threats might be minimized internally, developing the same awareness, and having the tools to identify who is using what, and respond accordingly will benefit operations. API security isn’t just about firewalls, it is about an awareness of who is using what.</p>

<p>The next benefit is about the future of your APIs. If you treat APIs like they are public, and you ever want to make it public, you will be in much better shape. You will have proper authentication, management, logging, and security controls already in place. You can cross the line between internal and external with much less friction. When you are ready to work with partners on a project, the time to make resources available can be significantly reduced, making things more efficient and agile when it comes to working with partners.</p>

<p>I get the hard line between internal and external. However I don’t get having two separate API strategies. Have one strategy. Treat everything like they are public, but then be very strict, and explicit about who has access to an API, and monitor, audit, analyze, and report on who has access to API resources in real time. These are web APIs. Let’s treat them all the same, and expect that there will be threats and misuse of varying degrees. Let’s treat all APIs equal, and reduce the chance people will become complacent with API management and security just because it is an “internal” API.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/02/treating-all-apis-like-they-are-public/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/02/the-national-transit-database-ntd-needs-to-be-an-api/">The National Transit Database (NTD) Needs To Be An API</a></h3>
        <span class="post-date">02 Jan 2018</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/government/federal-transit-agency/the-national-transit-database.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’ve been looking for sources of transit data as part of some research I’m doing with Streamdata.io. Like most industries I study as part of my API research, it is a mess. There is no single source of truth, lack of robust open source solutions, government PDFs acting as databases, and tech companies extracting as much value as they can, and giving as little in return as they possibly can. Todays frustration centers around the unfortunately common federal government PDF database, or more specifically, <a href="https://www.transit.dot.gov/ntd">the National Transit Database (NTD)</a>.</p>

<p>In 2017, when you publish something to the web as a “database”, it should be machine readable. There is some valuable data in the agency profile reports for the 800+ transit agencies available in the database, but this information is locked up in PDFs. You can find machine readable, historic versions of this data up to 2015 in data.gov, but for 2016, and 2017, the data is only available in individual PDFs for each agency profile. To make things more difficult, the listing of transit agencies uses some Ajax voodoo for its pagination and detail pages, making it even harder to scrape, on top of rendering each agencies detail useless by storing it as a PDF.</p>

<p>I understand why government is stuck in this mode. The systems they use only provide them with PDF as a their primary output. Staff hasn’t been trained on the importance of making data available in machine readable formats. People just don’t understand the negative impact they are making on the life of their data, and how it restricts people putting it to work. In some cases, people are fully aware of this, and want to limit how the data gets used, interpreted, keeping them as the definitive source of truth. I’m not saying this is what the Federal Transit Agency (FTA) is up to, but I’m saying it is the effect of their actions, which is having a chilling effect on folks like me using the valuable data to help communities served by these transit agencies.</p>

<p>I emailed the FTA asking if they have a machine readable copy of the database. This information should be published by default as CSV to the agencies Github account. I’m sure the data is available in a spreadsheet somewhere, before it becomes a PDF. It wouldn’t be very hard to save this data as CSV and publish to Github, which could then be easily converted into JSON, or other machine readable formats. I’m happy with CSV. I’m just not happy with PDF being called a database. Database implies that I can put the data to work, and in its current format the National Transit Database (NTD) is’t usable as data–hence not a database. It is just too much work to get out of the PDFs and make usable again, forcing me to step away from my project to understand how communities are investing in transit–I am hoping I can find the data some other place.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/02/the-national-transit-database-ntd-needs-to-be-an-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/01/01/api-transit-the-basics/">API Transit - The Basics</a></h3>
        <span class="post-date">01 Jan 2018</span>
        <p align="center"><a href="http://basics.apievangelist.com/"><img src="https://s3.amazonaws.com/kinlane-productions/api-transit/api-transt-subway.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>I have been evolving my approach to mapping out all the stops along my API research, using a subway map approach lately. It has been something I’ve been <a href="http://apievangelist.com/2014/12/01/my-turkey-holiday-project-a-subway-map-api/">working on since 2014</a>, and had <a href="https://apievangelist.com/2015/11/29/the-api-lifecycle-my-talk-from-defrag-and-apistrat/">developed as a keynote talk in 2015</a>. My goal is to be able to lay out simple, as well as increasingly complex aspects of consistently operating an API. Something I’ve historically called the API life cycle, but <a href="http://apievangelist.com/2017/08/17/testing-out-the-concept-of-api-transit-instead-of-api-lifecycle/">will work to call API transit in the future</a>.</p>

<p>Right now, I have two main approaches to delivering the API Transit maps. 1) API Life Cycle, and 2) API Documentation. The first is about applying consistent practices to API operations, and the second is about understanding API operations as they happen. In my mind, both these types of API Transit maps will eventually work in sync, but I have to work my way up to that. Right now, I’m focusing on the API Life Cycle version, which is becoming more about API governance, but I’m going to try and rebrand as API Transit. I’m using transit as a verb, “pass across or through” a standard, and consistent way of doing APIs. What some might consider API design, or governance, but I’m considering more holistically.</p>

<p>To support a couple of my consulting projects I am working on at the moment, <a href="http://basics.apievangelist.com/">I have published a simple API Transit project to help navigate some API teams through what I’d consider to be the basics they should be considering as they look to standardize how they deliver APIs across teams</a>. It’s a basic single line, 19 stop API Transit map. It is something I will keep adding stops to, and expand many into their own lines, serving up much more detail, but for this first project I wanted to keep simple, and speaking to a specific enterprise audience. I don’t want to overwhelm them with information as they are just getting started on their API journey. They still have so much work to do in these 19 areas, I don’t them to get distracted with other areas, or feel like they are drowning in information.</p>

<p align="center"><a href="http://basics.apievangelist.com/"><img src="https://s3.amazonaws.com/kinlane-productions/api-transit/api-transit-basics.png" align="center" width="90%" style="padding: 5px;" /></a></p>

<p>My API Transit maps all run on Github, using Jekyll as the client. Each transit line, and stop is stored as Siren hypermedia stored in a Jekyll Collection. The resulting transit map, and details of each stop is just a simple HTML client which uses Liquid to render the data. This allows me to add stops, and lines as I need, expanding the API journey for each API Transit implementation. I still have routing challenges for the lines on the map. I have an editor for helping me plot where each line should go, but there are no easy answers when it comes to transit map layout, and is something that is proving to be more art than science, so I’m refraining from automating too much at the moment. I’m working on a routing algorithm, but just don’t have the time to perfect it at the moment.</p>

<p>Next, I’m working on more complex iterations of existing APIs, so more about documentation than governance, life cycle, or transit. <a href="http://working.laneworks.net/psd2/psd2-v1.html">I’m doing this with PSD2 as an exercise</a>. Once I’ve done some more complex transit and specific API maps, I will work on combining the two, and applying the governance that exists in the transit map to a specific API, or set of APIs. Not sure where all of this is going, it is just a work in progress right now. It has been for almost three years, and I expect it will continue for many more years. If you are interested in having an API Transit map created for an existing API, or for a specific API governance process, feel free to reach out. I’m looking for more paid work to help push this work forward. Otherwise, it will just move along at whatever pace I can on my own steam!</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/01/01/api-transit-the-basics/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/20/api-discovery-will-be-about-finding-companies-who-do-what-yu-need-and-api-is-assumed/">API Discovery Will Be About Finding Companies Who Do What You Need And API Is Assumed</a></h3>
        <span class="post-date">20 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/27_127_800_500_0_max_0_-5_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>While I’m still investing in defining the API discovery space, and I’m seeing some improvements from other API service and tooling providers when it comes to finding, sharing, indexing, and publishing API definitions, I honestly don’t think in the end API discovery will ever be a top-level concern. While API design, deployment, management, and even testing and monitoring have floated to the top as primary discussion areas for API providers, and consumers, the area of API discovery never has quite become a priority. There is always lots of talk about API discovery, mostly about what is broken, rarely about what is needed to fix, with regular waves of directories, marketplaces, and search solutions emerging to attempting to fix the problem, but always falling short.</p>

<p>As I watch more mainstream businesses on-board with the world of APIs, and banks, healthcare, insurance, automobile, and other staple industries work to find their way forward, I’m thinking that the mainstreamification of APIs will surpass API discovery. Meaning that people will be looking for companies who do the thing that they want, and that API is just assumed. Every business will need to have an API, just like every business is assumed to have an website. Sure there will be search engines, directories, and marketplaces to help us find what we are looking for, but when we just won’t always be looking for APIs, we will be looking for solutions. The presence of an API be will be assumed, and if it doesn’t exist we will move on looking for other companies, organizations, institutions, and agencies who do what we need.</p>

<p>I feel like this is one of the reasons API discovery really became a thing. It doesn’t need to be. If you are selling products and services online you need a website, and as the web has matured, you need the same data, content, media, and algorithms available in a machine readable format so they can be distributed to other websites, used within a variety of mobile applications, and available in voice, bot, device, and other applications. This is just how things will work. Developers won’t be searching for APIs, they’ll be searching for the solution to their problem, and the API is just one of the features that have to be present for them to actually become a customer. I’ll keep working to evolve my APIs.json discovery format, and incentivize the development of client, IDE, CI/CD, and other tooling, but I think these things will always be enablers, and not ever a primary concern in the API lifecycle.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/20/api-discovery-will-be-about-finding-companies-who-do-what-yu-need-and-api-is-assumed/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/20/basic-api-design-guidelines-are-you-first-step-towards-api-governance/">Basic API Design Guidelines Are Your First Step Towards API Governance</a></h3>
        <span class="post-date">20 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/54_32_800_500_0_max_0_-5_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am working with a group that has begun defining their API governance strategy. We’ve discussed a full spectrum of API lifecycle capabilities that need to be integrated into their development practices, and CI/CD workflow, as well as eventually their API governance documentation. However, they are just getting going with the concept of API governance, and I want to make sure they don’t get ahead of themselves and start piling in too much into their API governance documentation, before they can get buy in, and participation from other groups.</p>

<p>We are approaching the first draft of an API governance document for the organization, and while it has lofty aspirations, the first draft is really nothing more than some basic API design guidelines. It is basically a two-page document that explains why REST is good, provides guidance on naming paths, using your verbs, and a handful of other API design practices. While I have a much longer list of items I want to see added to the document, I feel it is much more important to get the basic first draft up, circulated amongst groups, and establishing feedback loops, than making sure the API governance document is comprehensive. Without buy-in from all groups, any API governance strategy will be ignored, and ultimately suffocated by teams who feel like they don’t have any ownership in the process.</p>

<p>I am lobbying that the API governance strategy be versioned and evolved much like any other artifact, code, or documentation applied across API operations. This is v1 of the API governance, and before we can iterate towards v2, we need to get feedback, accept issues, comments, and allow for pull requests on the strategy before it moves forward. It is critical that ALL teams feel like they have been part of the conversation from day one, otherwise it can be weakened as a strategy, and any team looking to implement, coach, advise, report on, and enforce will be hobbled. API governance advocates always wish for things to move forward at a faster speed, but the reality within large organizations will require more consensus, or at least involvement, which will move forward at a variety of speeds depending on the size of the organization.</p>

<p>This process has been a reminder for me, and hopefully for my readers who are looking to get started on their API governance strategy. Always start small. Get your first draft up. Start with the basics of how you define and design your APIs, and then begin to flesh out the finer details of design, deployment, management, testing, and the other stops along your lifecycle. Just get your basic version documentation and guidance published. Maybe even consider calling it something other than governance from day one. Come up with a much more friendly name, that might not turn your various teams off, and then once it matures you can call it what it is, after everyone is participating, and has buy-in regarding the overall API governance strategy for your platform.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/20/basic-api-design-guidelines-are-you-first-step-towards-api-governance/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/20/i-am-now-realizing-that-streamdata-io-is-not-just-for-api-providers/">I Am Now Realizing That Streamdata.io Is Not Just For API Providers</a></h3>
        <span class="post-date">20 Dec 2017</span>
        <p><a href="https://streamdata.io"><img src="https://s3.amazonaws.com/kinlane-productions/streamdata/streamdata-push.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>When I first started diving into what <a href="https://streamdata.io">Streamdata.io</a> does, and thinking of their role in the wider API landscape, I was pretty exclusively focused API providers. Meaning, if you are an API provider, depending on the resources you are serving up, you should consider augmenting it with a real time stream using Streamdata.io. This still holds true, but after using Streamdata.io more as a developer, it is becoming clear of Streamdata.io’s value in my toolbox as an API consumer, and thinking about how I can make my applications more efficient, real time, and event-driven.</p>

<p>Right now, I’m just taking a wide variety of existing web APIs and running through the Streamdata.io proxy, and seeing what comes out the other end. I’m in the phase where I’m just understanding what Server-Sent Events (SSE) combined with JSON Patch does to existing web APIs, and their resources. This process is helping me understand the possibilities with streaming existing web APIs, but as I fire up each API I’m seeing it also reveal a new layer of events that exist in between providing APIs, and consuming APIs. I feel like this layer isn’t always evident to API providers, who haven’t made it very far in their API journey.</p>

<p>While I study how the bleeding, and leading edge developers are deploying event-driven architecture, mining for the event value that exists within big data, I’m thinking there is also a pretty interesting opportunity in mining the event layer for existing web APIs. Once I turn on streaming for a web API, the immediate value you see is when a new resource is added. However, this really isn’t that amazing beyond just subscribing to a webhook, or polling an API. I feel like the valuable events we don’t fully see without Server-Sent Events (SSE) is the changes. When a price changes. When a link is modified. When content is refreshed. The subtle events that occur that might not be noticed in regular operations.</p>

<p>I’ve had this conversation with Nicolas Rigaud, the VP Marketing &amp; Partners for Streamdata.io several times recently. That there is unrealized value in these changes to any system. The more they are known, recognized, and responded to, the more value they will possess. I feel like this is potentially the value that is driving the wider event-driven architecture movement at the moment. Understanding the subtle, but important changes that exist across systems and the data that is generated. Not just individual events, but also aggregate events at scale, which equal something much, much bigger. While I feel like “hoovering” up all the data you can find, and dialing in Kafka, or some other event-driven, streaming solution, is how you mine this value at scale, I think there is an equally great opportunity to tune into web APIs, and the unrealized events that happen via everyday platforms.</p>

<p>I’m working on a target list of around 100 APIs to proxy with Streamdata.io so that I can get a handle on the types of events that are occurring within some of the most used APIs out there. I’m guessing that the API providers who have the resources and skills on staff are already jumping at this opportunity, but I’m guessing there are many other APIs that have a significant amount of untapped potential for defining the event layer. This is where I see the potential for Streamdata.io as a tool in the hands of API consumers, and the average developer. To step in from an external vantage point and identify the most meaningful events that are occurring, and make them accessible to other systems, and within applications. Depending on the industry, I’m guessing there will become a growing number of monetization opportunities to emerge from these newfound events as we discover them in the real time streams.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/20/i-am-now-realizing-that-streamdata-io-is-not-just-for-api-providers/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/20/understanding-server-sent-events-sse-as-part-of-the-api-landscape/">Understanding Server-Sent Events (SSE) As Part Of The API Landscape</a></h3>
        <span class="post-date">20 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/server-racks-clouds_clean_view.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m continuing to break down the technology stack as I get to know <a href="http://streamdata.io">my new partner Streamdata.io</a>. Yesterday <a href="http://apievangelist.com/2017/12/19/javascript-object-notation-json-patch/">I wrote about their use of JSON Patch for returning partial responses of changes made to an API that has been proxied through the service</a>, and today I want to focus on understanding <a href="https://en.wikipedia.org/wiki/Server-sent_events">Server-Sent Events (SSE)</a>, which Streamdata.io uses to stream those events in real time to any consumer. In my experience, SSE is a lesser known of the real time technologies out there, but is one that holds a lot of potential, so I wanted to spend some time covering it here on the blog.</p>

<p>As opposed to technology that delivers a two-way stream, Server-sent events (SSE) is all about a client receiving automatic updates from a server via HTTP connection. The technology is a standard, with the Server-sent events (SSE) EventSource API being standardized as part of the HTML5 specification out of the W3C. Similar to Streamdata.io’s usage of JSON Patch, SSE is all about efficiency. Making web APIs real time isn’t always about having a two-way connection, and SEE is a great way to make things streaming in a one-way direction, only sending you what has changed in real-time using JSON Patch. Efficiency in direction, delivery, and in message.</p>

<p>Server-sent events (SSE) definitely shines when you look at how it can be used to constantly push and refresh data in any web UI using JavaScript. It’s HTML5 roots makes it a first-class citizen in the browser, but I also think there are a huge number of scenarios to play with when it comes to system integration, and reducing polling on APIs. I think the news, currency, stock, and other financial data scenarios are the low hanging fruit, but I feel like Streamdata.io as a rapid deploy proxy that developers can throw in between any API and a system integration is where the killer use cases of Server-sent events (SSE) could be.</p>

<p>To help me validate this theory I will keep playing with <a href="http://streamdata.io">Streamdata.io</a> and proxying any API I can get my hand on to see what is possible when you replace basic web API requests and responses with Server-sent events (SSE), and begin streaming only what changes after that initial request. I’m guessing that a whole new world of events will begin to emerge, allowing us to think about look at common web API resources differently. I feel like there is a lot of opportunity in deploying real-time, event-driven solutions like Kafka, and other Apache solutions, but I feel like there will be even more opportunity when it comes to getting intimate with the events that are already occurring across existing web APIs, even if the providers are fully tuned into what is going on, or have the resources to tackle event-driven architecture yet.</p>

<p><em><strong>Disclosure:</strong> <a href="http://streamdata.io">Streamdata.io</a> is an API Evangelist partner.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/20/understanding-server-sent-events-sse-as-part-of-the-api-landscape/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/19/javascript-object-notation-json-patch/">JavaScript Object Notation (JSON) Patch</a></h3>
        <span class="post-date">19 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/rfc/6902/javascript-object-notation-json-patch.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m continuing my studying into what <a href="http://streamdata.io">my new partner in crime Streamdata.io does</a>, and part of this research is understanding the details of their technology stack. Today’s work involves understanding their usage of <a href="https://tools.ietf.org/html/rfc6902">JavaScript Object Notation (JSON) Patch</a>. When you <a href="http://streamdata.io">proxy any existing web API using Streamdata.io</a>, the first thing you get back is a complete JSON representation of the response, but then with each change you just get back a JSON Patch response with only the details of what has changed. JSON Patch is used for expressing a sequence of operations to apply to a any JSON object or document and you’ll find used with the HTTP PATCH method.</p>

<p>The introduction for <a href="https://tools.ietf.org/html/rfc6902">JSON Patch from RFC [RFC4627]</a> describes it this way:</p>

<blockquote>
  <p>JavaScript Object Notation (JSON) [RFC4627] is a common format for the exchange and storage of structured data.  HTTP PATCH [RFC5789] extends the Hypertext Transfer Protocol (HTTP) [RFC2616] with a method to perform partial modifications to resources.
JSON Patch is a format (identified by the media type “application/json-patch+json”) for expressing a sequence of operations to apply to a target JSON document; it is suitable for use with the HTTP PATCH method.
This format is also potentially useful in other cases in which it is necessary to make partial updates to a JSON document or to a data structure that has similar constraints (i.e., they can be serialized as an object or an array using the JSON grammar).</p>
</blockquote>

<p>JSON Patch is an efficient way to only get the details from an API regarding only what has changed, instead of sending everything over the pipes each time. It makes sense that Streamdata.io has used it in conjunction with Server-Sent Events (SSE) to efficiently cache and stream data from existing web APIs. I have to admit I never put this together with the PATCH method for API responses. Most of the APIs I’ve seen that use PATCH, do not actually implement JSON PATCH, so this was a learning moment for me. Something I’m always thankful for, constantly reminding me just how much I do not know in the space, even with my experience studying APIs.</p>

<p>Next, I’m going to invest more time understanding how to write code that navigates and applies JSON Patch in real-world situations. I’ve got my handful of Streamdata.io enabled APIs streaming me data, but I don’t have the experiencing applying the changes to a UI, or existing system integration as the data flows in. It all makes a lot more sense to me now, and provides me with a efficient model for managing changes that occur across APIs. It is also yet ANOTHER reminder for me of how important it is that I study up on RFCs, and be knowledgable regarding existing patterns like this that exist, so that I’m not reinventing the wheel when it comes to my API design.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/19/javascript-object-notation-json-patch/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/19/from-ci-cd-to-a-continous-everything-ce-workflow/">From CI/CD To A Continuous Everything (CE) Workflow</a></h3>
        <span class="post-date">19 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/27_93_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I am evaluating an existing continuous integration and deployment workflow to make recommendations regarding how they can evolve to service their growing API lifecycle. This is an area of my research that spans multiple areas of my work, but I tend to house under what I call <a href="http://orchestration.apievangelist.com/">API orchestration</a>. I try to always step back and look at an evolving area of the tech space as part of the big picture, and attempt to look beyond any individual company, or even the wider industry hype in place that is moving something forward. I see the clear technical benefits of CI/CD, and I see the business benefits of it as well, but I haven’t always been convinced of it as a standalone thing, and have spent the last couple of years trying understand how it fits into the bigger picture.</p>

<p>As I’ve been consulting with several enterprise groups working to adopt a CI/CD mindset, and having similar conversations with government agencies, I’m beginning to see the bigger picture of “continuous”, and starting to decouple it from just deployment and even integration. The first thing that is assumed, not always evident for newbies, but is always a default–is testing. You alway test before you integrate or deploy, right? As I watch groups adopt I’m seeing them struggle with making sure there are other things I feel are an obvious part of the API lifecycle, but aren’t default in a CI/CD mindset, but quickly are being plugged in–things like security, licensing, documentation, discovery, support, communications, etc. In the end, I think us technologists are good at focusing on the tech innovations, but often move right past many of the other things that are essential for the business world. I see this happening with containers, microservices, Kubernetes, Kafka, and other fast moving trends.</p>

<p>I guess the point I want to make is that there is more to a pipeline than just deployment, integration, and testing. We need to make sure that documentation, discovery, security, and other essentials are baked in by default. Otherwise us techies might be continuously forgetting about these aspects, and the newbies might be continuously frustrated that these things aren’t present. We need to make sure we are continuously documenting, continuously securing, and continuously communicating around training, and our continuously evolving (and sharing) our road maps. I’m sure what I’m saying isn’t anything new for the CI/CD veterans, but I’m trying to onboard new folks with the concept, and as with most areas of the tech sector I find the naming and on-boarding materials fairly deficient in possessing all the concepts large organizations are needing to make the shift.</p>

<p>I’m thinking I’m going to be merging my API orchestration (CI/CD) research with my overall API lifecycle research, thinking deeply about how everything from definition to deprecation fits into the pipeline. I feel like CI/CD has been highly focused on the technology of evolving how we deploy and integrate (rightfully so) for some time now, and with adoption expanding we need to zoom out and think about everything else organizations will need to be successful. I see CI/CD as being essential to decoupling the monolith, and changing culture at some of the large organizations I’m working with. I want these folks to be successful, and not fall into the trappings of only thinking about the tech, but also consider the business and political implications involved with being able to move from annual or quarterly deployments and integrations, to where they can do things in weeks, or even days.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/19/from-ci-cd-to-a-continous-everything-ce-workflow/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/19/no-more-scraping-of-banking-data-in-europe-according-to-psd2/">No More Scraping Of Banking Data In Europe According to PSD2, Only APIs</a></h3>
        <span class="post-date">19 Dec 2017</span>
        <p><a href="http://europa.eu/rapid/press-release_MEMO-17-4961_en.htm"><img src="https://s3.amazonaws.com/kinlane-productions/psd2/european-commission-press-release-psd2-scraping.png" align="right" width="45%" style="padding: 15px" /></a></p>
<p>Part of my partnership with <a href="Streamdata.io">http://streamdata.io</a> centers around me investing more time into studying the banking industry, starting with the rollout of PSD2 in Europe next month. I’ll be working through each aspect of the regulations for the banking industry when it comes to APIs, but I wanted to highlight a recent change regarding scraping that is pretty monumental. In <a href="http://europa.eu/rapid/press-release_MEMO-17-4961_en.htm">a recent press release from the European Commission they further clarified guidance for third party payment services providers (TPPs)</a>, and whether or not they can be scraping data from bank still, instead of using the APIs being mandated by the commission.</p>

<p>Here is the section from the press release specifically addressing “what data can TPPs access and use via screen scraping”:</p>

<blockquote>
  <p>PSD2 prohibits TPPs from accessing any other data from the customer payment account beyond those explicitly authorised by the customer. Customers will have to agree on the access, use and processing of these data.
With these new rules, it will no longer be allowed to access the customer’s data through the use of the techniques of “screen scraping”. Screen scraping means accessing the data through the customer interface with the use of the customer’s security credentials. Through screen scraping, TPPs can access customer data without any further identification vis-à-vis the banks.
Banks will have to put in place a communication channel that allows TPPs to access the data that they need in accordance with PSD2. The channel will also be used to enable banks and TPPs to identify each other when accessing these data. It will also allow them to communicate through secure messaging at all times.
Banks may establish this communication channel by adapting their customer online banking interface. They may also create a new dedicated interface that will include all necessary information for the relevant payment service providers.
The RTS specifies the contingency safeguards that banks shall put in place if they decide to develop a dedicated interface. This will ensure fair competition and business continuity for TPPs.</p>
</blockquote>

<p>Banks will have to provide APIs for aggregators to access data. Aggregators will not be allowed to scrape, and are being forced to use the APIs. While there will be a rolling out period, and I’m sure there will still be the bad actors on both sides of the equation, it is the groundwork for a much more sensible and secure approach to providing access to banking customers data–cleaning up the current mess. It is an important step for the banking sector, as well as a significant precedent for the API space when it comes to requiring API access to users data, allowing them to take advantage of valuable 3rd party services.</p>

<p>I’m seeing hints of similar language out of the CFPB regarding banking in the United States, but we are still years behind this kind of regulations. While I would like optimistic that the EU regulations will have an impact on the US when it comes to banks who do business overseas, I’m not holding my breathe. Where I’m going to be placing bets is when it comes to forward thinking banks like Capital One leading the charge when it comes to access to customer data via APIs because it makes sense, not because the government is mandating it. I’m not a big fan of government dictating that industries do APIs, I’m more about companies doing APIs because they make sense.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/19/no-more-scraping-of-banking-data-in-europe-according-to-psd2/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/19/robust-public-storytelling-around-your-ap-process-is-sign-of-maturity/">Robust Public Storytelling Around Your API Process Is Sign Of Maturity</a></h3>
        <span class="post-date">19 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/68_113_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>Sharing stories around your API is something you hear me talk about a lot. Many of my readers like to let me know how they are serious API people, and my storytelling emphasis is silly. Just do APIs. Storytelling is unnecessary fluff. When in reality, storytelling has real, direct benefits on your business bottom line, but also have many other indirect aspects, and its presence is a sign of the overall health of an organization from my vantage point. When you are actively telling stories about your operations, in my experience, it is a sign of the overall maturity of your API process.</p>

<p>I’m working through my storytelling around what Capital One is up to with their DevExchange, studying their approach to API governance, as well as the wider role they are playing in the banking, and even API regulation game here in the United States. I can find stories about each of the topics I’m looking for on their public blog(s), and out in the open. This type of storytelling isn’t accidental, it is an intentional part of a maturing internal and external API strategy. Sure, they have a lot of work ahead of them, but based upon my internal conversations with them, and their external storytelling, I’m aware of how far along they are in their API journey–compared to other banks I’m talking to.</p>

<p>Take a look at <a href="https://medium.com/capital-one-developers">Capital One’s storytelling on Medium</a>. Tune into <a href="https://developer.capitalone.com/community/">the storytelling within the DevExchange community</a>. This isn’t Capital One just being confident in what they do, and are able to tell their story publicly. This is part of what you do to work through your API processes and break down the monolith. Check out <a href="https://dzone.com/guides/microservices-breaking-down-the-monolith">the Breaking Down the Monolith guide from my friend Irakli Nadareishvili over at DZone</a>. When you know your stuff, you are able tell the story of how you are unwinding the enterprise mess publicly like this. You aren’t embarrassed to tell these stories publicly. You are able to get it pass legal. Everyone involved benefits from your storytelling. This is how you do APIs. This is how you begin to shift behavior internally, and set your organization up as a leader in the public sphere. Through storytelling. Sharing your API journey in real time, in a very public way.</p>

<p>The reason you are unable to tell stories at your organization in this way is the reason your API efforts aren’t seeing the success you envisioned. Sure you can blame this on legal, but that is just a symptom of the greater illness. Sure, you can say that you don’t have the skills to write these types of stories, but that is also a symptom of the greater illness. Many folks just don’t see the benefits or value of storytelling, which also means you will never see the benefits or value in doing APIs properly. All of this goes hand in hand, and enables your organization to play nicely with other organizations, and able to share data without friction, and rapidly develop new products and applications. Robust storytelling around your API processes is a sign of the overall maturity of your efforts, something I see play out over and over across the API space.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/19/robust-public-storytelling-around-your-ap-process-is-sign-of-maturity/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/18/definition-driven-api-lifecycle-instead-of-code-driven-apis/">Definition-Driven API Lifecycle Instead Of Code-Driven API Deployment</a></h3>
        <span class="post-date">18 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/beach-rocks-currents_internet_numbers.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>You hear a lot about being API design first out of the API echo chamber these days. I’m finding that concept to be challenging for many groups I’m working with due to some of uninformed perceptions around REST, leaving many unable to move towards a design first approach because they are worried if they are doing it correctly. I shifted my own thinking a while back to be more about define-first, requiring that I thoroughly define each API project before I begin moving it along whatever API lifecycle I’ve quantified for a project.</p>

<p>One thing I’m finding pretty common across the enterprise groups who have adopted OpenAPI (fka Swagger) as part of their operations is that many aren’t truly using the API specification format to its full potential as an API definition, let alone applying across multiple stops along the API lifecycle. Over and over I see groups “using Swagger”, but when you dig deeper you see the documentation being autogenerated as part of existing development, out of .JAR files, and (thankfully) evolving continuous deployment workflows. While this is progress, it’s not the definition-driven API lifecycle that organizations should be investing in, is is just code-driven APIs—not actually using OpenAPI to its full potential as a driver across all stops along the API lifecycle.</p>

<p>Getting your hands dirty in the defining, designing, and crafting of OpenAPI definitions is where rubber really begins to meet the road in implementing the API specification. Sure, you can still be autogenerating the specifications from your services and tooling, but you should be actively polishing, and rounding off the rough edges in an API design tool, and putting your definitions to work in an API client like Restlet or Postman to truly define what your API does, or what it doesn’t do. Then taking your definitions and generating server side code, importing into your API gateway, building SDKs, and publishing your documentation and tests. If you skip over getting your hands in there, and actually getting intimate with the requests, responses, and schema of an API, you really aren’t definition-driven, and really just code-driven, which is a much more costly, and inflexible way of doing business with APIs.</p>

<p>One of the biggest challenges in achieving a definition-driven approach to APIs I’m seeing is that many groups still think OpenAPI (fka Swagger) is SwaggerUI (aka Documentation). Most don’t even see the definition behind what they are autogenerating, and have never loaded it into an API design editor like Swagger Editor or Apicurio, let alone within a client tool like Postman. This is something that is only exacerbated with the confusion between what is Swagger and OpenAPI, now that the specification is in the OAI. Code-driven APIs are much more costly, and rigid than definition-driven APIs. While you can still use the resulting API definition throughout the API lifecycle, each iteration, and change will be significantly more costly than if you are working directly from a definition, then mocking, and iterating using a client tool like Postman. Something it will take large enterprises a while to fully realize, as they struggle to realize why their API efforts are returning the results they envisioned.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/18/definition-driven-api-lifecycle-instead-of-code-driven-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/18/what-you-can-expect-as-client-from-soap-to-grprc/">What You Can Expect As A Client From SOAP To gRPC</a></h3>
        <span class="post-date">18 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/57_64_800_500_0_max_0_-5_-5.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m working hard on what I consider to be my definition of a robust API deployment toolbox, and was enjoying the 100K perspective. As I explore, I wanted to share some of my thoughts about by you might expect to receive as a client in each of these scenarios.</p>

<ul>
  <li><strong>SOAP</strong>: You get what the vendor says we can send to you in very structured way.</li>
  <li><strong>REST</strong>: Is this what you want? Let us know if it wasn’t via StackOverflow.</li>
  <li><strong>Hypermedia</strong>: We are prepared to send you whatever we want at any point in the future.</li>
  <li><strong>Microservices</strong>: You are just going get a little bit of this one thing.</li>
  <li><strong>GraphQL</strong>: You get exactly what you want, you better know what to ask for!</li>
  <li><strong>Websockets</strong>: Here you get that, and this, and that, and that…</li>
  <li><strong>PubSub</strong>: You get only the topic you wish to subscribe to.</li>
  <li><strong>Webhooks</strong>: Here you asked us to send this to you–here you go.</li>
  <li><strong>Event Architecture</strong>: You get something whenever that something happens.</li>
  <li><strong>gRPC</strong>: You get what we want really fast, and can accept what we want really fast!!</li>
</ul>

<p>It is fun to step back and think about the motivations, ideology, and pros/cons of each of these API deployment scenarios. I’d love to hear your additions or perspective on what you think the client view of the conversation might be. As I see the API universe continue to expand, I’m curious to see how others are seeing it.</p>

<p>In coming months you’ll hear me write more about event driven architecture, gRPC, and how the pace of things are picking up when it comes to API consumption. I’m working with <a href="https://streamdata.io">Streamdata.io</a> to help try and map out this landscape, as well as some of the usual areas I focus on as the API Evangelist.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/18/what-you-can-expect-as-client-from-soap-to-grprc/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/18/reducing-polling-of-your-existing-api-using-streamdata-io/">Reducing Polling Of Your Existing API Using Streamdata.io</a></h3>
        <span class="post-date">18 Dec 2017</span>
        <p><a href="https://streamdata.io"><img src="https://s3.amazonaws.com/kinlane-productions/streamdata/cloud-architecture-1024x519.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p><a href="https://streamdata.io">I’ve partnered with Streamdata.io</a>, resulting in me getting more acquainted with their API solutions, and telling the story of that process here on API Evangelist. I figured I would dive right in and start with the basics of what Streamdata.io does–turning your existing web API into a real-time stream. Streamdata.io acts as a reverse proxy that translates REST API polling into a stream of data. Instead of constantly polling your API for changes, your API clients will poll Streamdata.io and get a <a href="https://tools.ietf.org/html/rfc6902">JSON Patch update</a> if anything has changed, and reducing the impact of the requests your clients will make to your API.</p>

<p>When thinking about what Streamdata.io does it is easy to get caught up on the real time and streaming nature of what they do, but the most immediate value they bring to the table is about making your relationship with your API clients more efficient. Streamdata.io reduces the costs associated with operating your API, stepping in between you and your demanding clients, and act as a buffer that will reduce the load on your servers. Eliminating one of the biggest headaches for API providers, and reigning in the behavior by our most demanding, and demanding clients.</p>

<p>I’m always surprised by the answers I get from API providers when I ask them why they rate limit their APIs. I’d say that 80% of the time it is based upon reducing the overhead and impact on backend systems, and dealing with the bad behavior of API consumers. Streamdata.io provides a pretty compelling solution to help alleviate this reality of operating APIs for most API providers. It isn’t just about making things real-time, it is more about cost savings, and minimizing the impact of API consumption on our back-end solutions. Making rate limiting irrelevant, unless you have some other specific business needs behind your decision.</p>

<p>There are numerous other benefits Streamdata.io brings to the table, but reducing the load on your APIs probably the most relevant to ALL of my readers who operate APIs. We can always do better when it comes to making our APIs more efficient, and <a href="https://streamdata.io">Streamdata.io</a> is a way we can do this with minimal costs, in minutes, not days, weeks, or months. Which is one of the primary reasons I am partnering with Streamdata.io. It is a service I find easy to push as part of my API storytelling here on the blog, and happy to have become part of the team.</p>

<p><em><strong>Disclosure:</strong> <a href="https://streamdata.io">Streamdata.io</a> is the primary partner for the API Evangelist website.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/18/reducing-polling-of-your-existing-api-using-streamdata-io/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/17/streaming-data-from-the-google-sheet-json-api-and-streamdata-io/">Streaming Data From The Google Sheet JSON API And Streamdata.io</a></h3>
        <span class="post-date">17 Dec 2017</span>
        <p><a href="https://streamdata.io"><img src="https://s3.amazonaws.com/kinlane-productions/streamdata/streamdata-google-sheet.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>I am playing with Streamdata.io as I learn how to use my new partner’s service. Streamdata.io proxies any API, and uses <a href="https://streamdata.io/blog/server-sent-events/">Server-Sent Event (SSE)</a> to push updates using <a href="https://tools.ietf.org/html/rfc6902">JSON Patch</a>. I am playing with making a variety of APIs real time using their service, and in my style, I wanted to share the story of what I’m working on, here on the blog. I was making updates to some data in a Google Sheet that I use to drive some data across a couple of my websites, and thought…can I make this spreadsheet streaming using Streamdata.io? Yes. Yes, I can.</p>

<p>To test out my theory I went and created a basic Google Sheet with two columns, one for product name, and one for price. Simulating a potential product pricing list that maybe I’d want to stream across multiple website, or possibly within client and partner portals. Then I published the Google Sheet to the web, making the data publicly available, so I didn’t have to deal with any sort of authentication–something you will only want to do with publicly available data. I’ll play around with an authenticated edition at some point in the future, showing more secure examples.</p>

<p>Once I made the sheet public I grabbed the unique key for the sheet, which you can find in the URL, and placed into this URL: https://spreadsheets.google.com/feeds/list/[sheet key]/od6/public/basic?alt=json. The Google Sheet key takes a little bit to identify in the URL, but it is the long GUID in the URL, which is the longest part of the URL when editing the sheet. Once you put the key in the URL, you can take the URL and paste in the browser–giving you a JSON representation of your sheet, instead of HTML, basically giving you a public API for your Google Sheet. The JSON for Google Sheets is a little verbose and complicated, but once you study a bit it doesn’t take long for it to come into focus, showing eaching of the columns and rows.</p>

<p>Next, I created a Streamdata.io account, verified my email, logged in and created a new app. Something that took me about 2 minutes. I take the new URL for my Google Sheet and publish as the target URL in my Streamdata.io account. The UI then generates a curl statement for calling the API through the Streamdata.io proxy. Before it will work, you will have to replace the second question mark with an ampersand (&amp;), as Streamdata.io assumes you do not have any parameters in the URL. Once replaced, you can open up your command line, paste in the command and run. Using Server-Sent Event (SSE) you’ll see the script running, checking for changes. When you make any changes to your Google Sheet, you will see a JSON Patch response returned with any changes in real time. Providing a real-time stream of your Google Sheet which can be displayed in any application.</p>

<p>Next, I’m going to make a simple JavaScript web page that will take the results and render to the page, showing how to navigate the Google Sheets API response structure, as well as the JSON Patch using the Streamdata.io JavaScript SDK. All together this took me about 5 minutes to make happen, from creating the Google Sheet, to firing up a new Streamdata.io account, and executing the curl command. Sure, you’d still have to make it display anywhere, but it was quicker than I expected to make a Google Sheet real-time. I’ll spend a little more time thinking about the possibilities for using Google Sheets in this way, and publishing some UI examples to Github, providing a forkable use case that anyone can follow when making it all work for them.</p>

<p><em><strong>Disclosure:</strong> <a href="https://streamdata.io">Streamdata.io</a> is an API Evangelist partner, and sponsors this site.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/17/streaming-data-from-the-google-sheet-json-api-and-streamdata-io/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/11/cost-savings-analysis-for-washington-metropolitan-area-transit-authority-wmata-data-apis/">Cost Saving Analysis For Washington Metropolitan Area Transit Authority (WMATA) Data APIs</a></h3>
        <span class="post-date">11 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/transit/washington-metropolitan-area-transit-authority-api.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>Even before I engaged with Streamdata.io on our current partnership, I was working with them to quantity the value they bring to the table with their service. As I was working on my story regarding <a href="http://apievangelist.com/2017/11/30/licensing-over-dc-transit-data/">the roubling terms of service changes From Washington Metropolitan Area Transit Authority (WMATA) data APIs</a>, the Streamdata.io team was running a cost savings analysis on the WMATA APIs. This is where they take their web API, and see what they could save if they used Streamdata.io, and turned it into a streaming API.</p>

<p>The Streamdata.io team took <a href="https://developer.wmata.com/docs/services/5476365e031f590f38092508/operations/5476365e031f5909e4fe331e">the WMATA Real-Time Bus PredictionsAPI</a>, and assessed the efficiency gains for WMATA when it comes to their most demanding API consumers. Here are the bandwidth and CPU savings:</p>

<ul>
  <li>Client Bandwidth (BW) Savings - 88%</li>
  <li>Server Bandwidth (BW) Savings - 99%</li>
  <li>Server CPU Savings - 87%</li>
</ul>

<p>Streamdata.io does this by being stood up in front of the WMATA web API and caching the results, then only showing changes to clients–in real-time. This isn’t just about making something real-time, it is about reducing the number of times API consumers need to be polling an API. When it comes to transit data you can imagine that the client is probably polling every second to see what has changed.</p>

<p>I’m learning about Streamdata.io’s process for calculating these savings, which is why I’m writing this story. I’m going to work to help apply this to many other APIs, as well as look at productizing the tool so that maybe it can become a self-service tool for other API providers to evaluate their own cost savings, if they went to a real-time way of doing things. To help me understand the savings beyond WMATA, I’m going to be doing benchmarks across all the other US transit provides, and see what kind of numbers I can generate.</p>

<p><em><strong>Disclosure:</strong> Streamdata.io is a paid API Evangelists partner.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/11/cost-savings-analysis-for-washington-metropolitan-area-transit-authority-wmata-data-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/11/api-evangelist-and-streamdata-io/">API Evangelist And Streamdata.io</a></h3>
        <span class="post-date">11 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/streamdata/083a1a91-3495-4fc8-ae2e-b5b6819548c6-original.jpeg" align="right" width="45%" style="padding: 15px;" /></p>
<p>Some of you in my backchannels know that I’ve been shopping around for a job lately. I’m looking to make a shift in API Evangelist, as I’ve written about some (and will write about more), and I’m also looking for a shift in how I fund my world. During a multi-week search I opened up conversations about a couple of different roles, and one particularly interesting partnership came my way from a company I’ve been working with for a while now. I’ve entered into a partnership with the <a href="https://streamdata.io/">Streamdata.io</a> team, to help them chart the course for their real-time, streaming, event-sourced future, and they’ll continue to invest in me being the API Evangelist. In the past I’ve had several partners at any one time, but moving forward I’m going to limit it to being with a single partner–changing the formula a little bit.</p>

<p>I’ll talk about why the older model wasn’t working in other posts on my personal blog, but moving forward API Evangelist will continue to be about <a href="http://apievangelist.com/api-lifecycle/">my research into the wider API space</a>, but it will increasingly have a focus about what I’m also doing with Streamdata.io. I’m interested in helping Streamdata.io understand that API sector, while I am helping my readers understand what Streamdata.io does. I will be actively telling stories about what I’m up to on a daily basis via API Evangelist, and I will continue to research the wider API space–this is valuable to Streamdata.io, and hopefully it is also valuable to you.  I will also be applying this knowledge to helping the Streamdata.io team help define their place in the API sector, and telling the stories in real-time on API Evangelist–this is valuable to Streamdata.io, and hopefully it is also valuable to you.</p>

<p>Streamdata.io is all about helping define the real-time, streaming layer of the API space–something I have been focused on for a number of years. However, they also invested into helping define the data streaming, event-sourced future of the API space, something I’ve lightly invested in with my webhooks research, but keen to further understand when it comes to Kafka, and other emerging trends. It fits well with what I”m seeing happen in the API space, as well as my interests when it comes to technology. Honestly, it also fits with my interests in having a stable income, and not having to chase stories in the name of page views, or other random projects just because they might bring in some money to pay the bills. I’m not too proud to say that I was ready for Streamdata.io’s help, and it is greatly appreciated.</p>

<p>This partnership wasn’t the next step I envisioned for API Evangelist, but it was the quickest, most natural, and positive offer to show itself–right when I needed it. To the API echo chamber I think what I offer via API Evangelist will change substantially, however to API providers, and API service providers, I’m guessing they probably won’t even notice a difference. I’m looking forward to partnering with Streamdata.io to help get the word out about the dead simple real-time streaming solutions they provide on top of existing APIs, and I’m also excited that they’ll continue to invest in me being the API Evangelist. Here’s to a great partnership Streamdata.io, and I look forward to the stories we can tell together in 2018. I’m looking forward to continuing to help define the API space, with a focus on the perspective that Streamdata.io brings to the table. Something that goes well beyond just streaming data–stay tuned!</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/11/api-evangelist-and-streamdata-io/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/10/warming-up-api-providers-we-are-targeting-for-using-streamdata/">Warming Up API Providers We Are Targeting For Using Streamdata.io With Storytelling</a></h3>
        <span class="post-date">10 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/streamdata/damian-stream-data.png" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="https://www.linkedin.com/in/damianodoemena/">My new partner in crime Damian Odoemena the technical account manager for Streamdata.io</a> has said he is ready to work with me to deliver on the road map for the real-time streaming API. I explained to him that I will help fill his head with my knowledge of the API space, as well as be completely transparent around our strategy through storytelling here on API Evangelist. This is one of the reasons I jumped at the opportunity to partner with the team, because of their willingness to let me share what I do with the team, but also tell the story in real-time, streaming (pun intended) our day to day activities here on API Evangelist. I’ll be working 50% of my time on website copy, white papers, as well as evangelism and platform strategy for them, but the other 50% of the time I will be telling the story of what I did for them, here on the blog for me readers to learn from.</p>

<p>This is essentially what I’ve been doing for API Evangelist for the last seven years, except I’ve  crafted my storytelling based upon rolling waves of many partners, as well as ongoing private conversations I’ve had with API providers, and service providers across the space. I’m going to keep this up, but now it will be centered around the work I’m doing for Streamdata.io, as well as talking to their customers to better understand what they need when it comes to delivering regular web APIs, as well as streaming, real-time APIs. I know that Damian nodded his head when I explained this, but I’m guessing he doesn’t fully get how he is going to be thrust into the center of my storytelling, as we work together to develop Streamdata.io’s evangelism strategy, build prototypes, and the other fun things we have planned–he will soon enough! ;-)</p>

<p>I’m working right now with Damian to create a list of API providers who would immediately benefit from Streamdata.io’s service. Even once I have the list established, I’m not a big fan of just cold calling people, or pinging them via social networks, even if I know them personally. It just isn’t my style. I’m a little more passive aggressive, then ever being directly sales aggressive. One way I work, is to warm them up a little bit by writing about them. Crafting one, or many stories about a potential target is a good way for me and Damian to better understand how the service might benefit them, develop the sales pitch we might be taking with them, while publishing and generating some search and social media exhaust that might benefit the platform. Who knows, maybe the target will even read it, and contact us–no sales outreach needed!</p>

<p>Any example of this can be found with an earlier story on <a href="http://apievangelist.com/2017/11/30/licensing-over-dc-transit-data/">the Washington Metropolitan Area Transit Authority (WMATA) terms of service change from a couple of weeks ago</a>, as well as <a href="http://apievangelist.com/2017/12/11/cost-savings-analysis-for-washington-metropolitan-area-transit-authority-wmata-data-apis.markdown">another one today on what WMATA would save if they augmented their web APIs using Streamdata.io</a>. WMATA is on our target list, and we want to be able to quantify how the service would benefit them, and develop an understanding of the transit API–no better way than crafting stories about them. Getting the attention of a transit authority isn’t easy, and these stories are no guarantee that they’ll be tuning in, but you never know. If nothing else, it will heighten our understanding of them, and generate some good SEO juice that maybe other transit groups will be tuning into. Let’s get to work on crafting stories for the rest of our target list Damian–you ready?</p>

<p><em><strong>Disclosure:</strong> Streamdata.io is a paid API Evangelists partner.</em></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/10/warming-up-api-providers-we-are-targeting-for-using-streamdata/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/06/will-apis-still-be-relevant/">Will APIs Still Be Relevant?</a></h3>
        <span class="post-date">06 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/32_119_800_500_0_max_0_-1_-1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I named my blog, company, as well assumed my own title as “API Evangelist” in 2010. Every year since making that decision I’ve questioned it, and wonder if the concept and acronym will fade away. First of all, I have to admit its a bullshit concept in the first place. Its an acronym. It’s a pretty wide umbrella that allows us (me) to assemble a wide variety of technological concepts underneath. However, I made an investment in it, I was going to continue. I found some meaning that I was able to articulate to others, that would make an impact on businesses, organizations, institutions, and government agencies. It works. I am going to run with it, and in 2017, I’m renewing that perspective, and keeping it as my brand, title, and the central message I’m peddling in the tech sector.</p>

<p>This all contributes to a significant under tow on my reality, pushing me to question reality on a regular basis. However, I’d say the strongest current I struggle with in this area is when it comes to endless waves of trends that crash on the shore. Maybe API is irrelevant because of microservices? Wait, maybe it is because of GraphQL? Sure, it will become irrelevant because of Kafka? Web APIs can’t do everything, and it something that will surely render them the wrong choice, just around the corner. There is truth in all of these statements. However, these statements are also just the nature of the game. Web APIs played these same cards when it came onto the scene. REST replaced SOAP, and JSON replaced XML. It is how the technology game is played.</p>

<p>The frontline of this sector will always be developing and evangelizing new tools. It is how it disrupts, and builds new markets. I’m complicit in this. However, the mainstream world won’t ever move as fast as the frontline. No matter how much we want it to. Change just doesn’t happen that quick at scale. I’m confident that my definition of APIs, using the web to make data, content, and algorithms available in a machine readable way isn’t going anywhere soon. I’m confident that the acronym API has enough mindshare, that it can occasionally rise above the trends. Although, the trends actually help refine, and further define the concept, even if they go by other names. It all moves forward at a glacial pace, even though often times it feels like everything always moving so fast–it does this by design.</p>

<p>After seven years of doing this, not a lot has changed. We are still doing web APIs. Most of the time pretty poorly. We are doing them smaller. Occasionally folks are doing them real time, as they have a lot going on. There are more of them, and more people doing them. Not much else is really that revolutionary. It just feels like everything is moving fast, changing at a break neck pace, and there are always new technologies threatening to make APIs irrelevant. If you are in the business of selling new things you believe things are. If you are in the business of buying new technology, you believe things are moving real fast. However, if you are just in the business of understanding what is happening, and getting business done, you see that things aren’t really moving that fast, and APIs will be relevant for some time to come.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/06/will-apis-still-be-relevant/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/06/the-shifting-api-landscape/">The Shifting API Landscape</a></h3>
        <span class="post-date">06 Dec 2017</span>
        <p><img src="http://kinlane-productions.s3.amazonaws.com/algorotoscope/canyon/yellow_collage/file-00_02_34_62.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’ve been watching, and trying to move forward the API conversation across all business sectors for seven years now. I’m not a startup. I’m not an API service provider. I’m not steering an enterprise group. I’m not an investor. I’m a software architect and storyteller who saw the potential for leveraging web infrastructure to deliver data, content, media, and algorithms across the web, to our mobile phones, as well as the seemingly endless number of devices we are connecting to the Internet in our personal, professional, and industrial worlds. I’m not studying the landscape so I sell to it. I am studying the landscape so I can understand it. While most of my readers will not grasp that difference, it gives me a fundamentally different view of what is going on across the space.</p>

<p>In the last seven years I’ve had a focus on helping individuals at SMB, SME, enterprise, organizations, institutions, and government agencies understand what APIs are, and why they should be doing them. In 2017, I feel that mission becoming irrelevant based upon the shifting API landscape. As I work on my third API-first strategy for a top level federal agency in response to an RFI in recent months, prepare for an all week API workshop at Mutual of Omaha in Nebraska, and bookmark the job postings for API architect at almost every major bank in the US and UK, my cute little mission to help understand people understand what APIs are clearly needs to be retired. While there are plenty of people who still need to be educated what APIs are, and that they should be doing them, I’m going to leave it to the waves of other pundits, advocates, evangelists, and analysts to help onboard them. I’ve done my time.</p>

<p>There are many changes on the horizon for API Evangelist which I’ll cover in future posts, but one significant one for me will be to lose “the mission”. As much as I’d like to think people care, in this investment fueled startup world, bundled with an endlessly uncritically belief in technology, I’m not convinced people do. I feel like I was bullshitting myself, right along with other entrepreneurs that I was doing the “good work”, and trying to make a difference in the world (it is a white dude condition). People like to rally around the little campfire I’ve built, but after seven years I can count on two hands the people who actually follow through on this vision. So, you’ll see this delusion disappear from my storytelling, and along with it the belief that everyone should be doing APIs. I’ll still hold on to a belief that EVERYONE should understand what APIs are, as they are an essential aspect of digital literacy in this online world we’ve assembled for ourselves, but I’ll be losing the social good aspect of my API Evangelism in the future.</p>

<p>The API landscape is shifting to be more mainstream. While API providers haven’t made all the choices I would have liked to see in the space when it comes to transparency, observability, privacy, security, and communication, I feel like I’ve had some influence on the space–even if it was just some better storytelling than the marketing that is pumped out of startup and enterprise factories on a daily basis, and the mouthpieces that are the tech blogs. While I will still be talking to the serious startups who are building real tools, and are willing to pay for my consulting services, all the other waves of predatory, exit-building startups that emerge will probably not even know who I am. That is just fine. I’m going to be shifting away from startup-land, in an effort to minimize my frustrations, depression, and to help eliminate the rantiness of my storytelling on the blog. While it may be amusing for some, it is a symptom of a larger illness that plagues not just me, but the wider sector–it is something I’ll be distancing myself from.</p>

<p>I’m working to keep API Evangelist alive and relevant after all these years, as well as pay my bills. I want to keep it up and running. I want to keep it telling relevant stories on a regular basis. This isn’t as easy as it sounds. After 3,060 blog posts I can say that finding the mojo to do it each week, and cover things that are valuable to readers isn’t straightforward. I’m going to shift things to be more relevant to organizations of all sizes to do API right. It will still be a blend of my focus on the technology, business, and politics, but it will be more grownup and mature. Less ranty. Less accessible. More professional. More about helping those doing APIs do them better. I’m just not convinced my helping folks “do APIs” was ever any good for anyone. It was my delusion. Sharing my skills and expertise as a professional might have value to some, but the mission thing was more about my ego, than it was about anyone else. The API space is shifting. It is expanding and growing. It is definitely going mainstream. I’m continuing to study it. Understand it. Report on it. And I am looking forward to my work shifting and evolving with it in 2018. I very curious where it will all lead, and how things will look from my perch.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/06/the-shifting-api-landscape/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/05/what-is-more-important-having-an-api-or-having-a-well-designed-api/">What Is More Important? Having An API? Or Having A Well-Designed API?</a></h3>
        <span class="post-date">05 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/16_38_600_500_0_avg_1_1_1.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I  got some expected flack this week for some stories on database to API deployments, and allowing folks to just auto-generate APIs from database structures. This approach is notorious for producing very badly designed APIs, which is something that just reflects whatever legacy infrastructure you have as a backend. It is something that drives many of API design, architects, and pundits crazy. Just do things properly!!! Follow good design practices! Put some thought into your API, and have some pride in this interface you are putting out there. All of this is easy for us to declare from our vantage point, but when your entrenched within an existing organization, battling for every movement forward, and often times just to not go backwards, this isn’t always the reality.</p>

<p>As technologists we are always looking forward, and have a really hard time empathizing with folks who are stuck in positions that aren’t as forward leaning as ours. I know we have a well of experience we want everyone to see eye to eye with, but that isn’t always the reality. You can’t convince someone who is just trying to stay afloat within an organization that they should be investing in all of these possibilities in a future they aren’t tuned into. Not everyone holds the privileged position that many of us enjoy in the technology space, and I feel we can do a better job empathizing with some of them. I’m not saying we should give up on leading, and telling stories of a better future, but we need to work to build bridges to many who are less fortunate than we are.</p>

<p>You know what is worse than being in an organization where you are battling for every bit of budget, resources, skills, and other things that help you stay afloat? Having people in more privileged positions making you feel stupid for what you do not understand, or have the time to learn. I wish folks at startups, and bigcos would spend more time investing in the knowledge transfer to smaller, more underserved organizations. Not teaching them to use their software, but actually investing in their staff becoming more web, and API literate. Instead, of making people feel like they don’t have the knowledge, skills, and resources to do things right. In my experience, most of these folks are well aware of this, and they don’t need to be reminded on it.</p>

<p>I’m investing in organizations just doing APIs. Sure, I would like them to do it as well as possible, but I’m more invested in people just doing them. Making their data, content, and other resources more accessible so they can be just a little bit more successful in what they are doing. There will be some pain to go along with this approach, but I feel like it will ultimately be worth it. I can’t shield data stewards, and other would-be API providers from all the pain of doing APIs. I feel it is more important to me that folks have an API, and be on their journey, than having a perfectly designed API. This is where the learning comes, and hopefully I can convince more technologists, startups, and bigcos to invest in this journey, rather than shame people for not being well-equipped when it comes to doing APIs, and quite possibly never even doing them at all. That is much worse, than a poorly designed API in my book.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/05/what-is-more-important-having-an-api-or-having-a-well-designed-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/05/the-picture-we-pain-with-each-api-release/">The Picture We Paint With The Stories We Tell Around Each API Version Release</a></h3>
        <span class="post-date">05 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/facebook/facebook-version-211-release.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I fell down the rabbit hole of the latest Facebook version release, trying to understand the deprecation of their User Insights API. The story of the deprecation of the API isn’t told accurately as part of the the regular release process, so I found myself thinking more deeply about how we tell stories (or don’t) around each step forward of our APIs. I have dedicated areas of my API research for the <a href="http://road-map.apievangelist.com/">road map</a>, <a href="http://issues.apievangelist.com/">issues</a>, and <a href="http://change-log.apievangelist.com/">change log</a> for API operations, because their presence tell a lot about the character of an API, and their usage I feel paints and accurate painting of each moment in time for an API.</p>

<p><a href="https://developers.facebook.com/docs/graph-api/changelog">Facebook has a dedicated change log for their API platform</a>, as well as an <a href="https://developers.facebook.com/status/dashboard/">active status</a> and <a href="https://developers.facebook.com/status/issues/">issues</a> pages, but they do not share much about what their road map looks like. They provide a handful of elements with each releases change log:</p>

<ul>
  <li><strong>New Features</strong> — New products or services, including new nodes, edges, and fields.</li>
  <li><strong>Changes</strong> — Changes to existing products or services (not including Deprecations).</li>
  <li><strong>Deprecations</strong> — Existing products or services that are being removed.</li>
  <li><strong>90-Day Breaking Changes</strong> — Changes and deprecations that will take effect 90 days after the version release date.</li>
</ul>

<p>The presence, or lack of presence, of a road map, change log, status and issue pages for an API paints a particular picture of a platform in my mind. Also, the stories they tell, or do not tell with each release paint an evolving picture of where a platform is headed, and whether or not we want to participating in the journey. Facebook does better than most platforms I track on when it comes to storytelling, by also releasing a blog post telling the story of each release, providing separate posts for <a href="https://developers.facebook.com/blog/post/2017/11/07/graphapi-v2.11/">the Graph API</a>, as well as <a href="https://developers.facebook.com/ads/blog/post/2017/11/07/marketing-api-v211/">the Marketing API</a>. It is too bad that <a href="https://developers.facebook.com/ads/blog/post/2017/11/07/marketing-api-v211/">they omitted the deprecation of the Audience Insight API</a>, which occurred at the time of this story.</p>

<p>While I consider the presence of building blocks like a change log, road map, issues and status page a positive sign for platforms. It still always requires reading between the lines, and staying in tune with each release to really get a feel for how well a platform puts these building blocks to work for the platform. Regardless, I think these building blocks do adequately paint a picture of the current state of a platform, it just usually happens to be the picture that platform wants you to see, not necessary the picture the platform consumers would like to see.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/05/the-picture-we-pain-with-each-api-release/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/05/api-deployment-templates-as-part-of-a-wider-api-governance-strategy/">API Deployment Templates As Part Of A Wider API Governance Strategy</a></h3>
        <span class="post-date">05 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/server-cloud1_internet_numbers.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>People have been asking me for more stories on API governance. Examples of how it is working, or not working at the companies, organizations, institutions, and government agencies I’m talking with. Some folks are looking for top down ways of controlling large teams of developers when it comes to delivering APIs consistently across large disparate organizations, while others are looking for bottom ways to educate and incentivize developers to operate APIs in sync, working together as a large, distributed engine.</p>

<p>I’m approach my research into API governance as I would any other area, not from the bottom up, or top down. I’m just assembling all the building blocks I come across, then began to assemble them into a coherent picture of what is working, and what is not. One example I’ve found of an approach to helping API providers across the federal government better implement consistent API patterns is out of the General Services Administration (GSA), with <a href="https://gsa.github.io/prototype-city-pairs-api-documentation/api-docs/">the Prototype City Pairs API</a>. The Github repository is a working API prototype, documentation and developer portal that is in alignment with the GSA API design guidelines, providing a working example that other API developers can reverse engineer.</p>

<p>The <a href="https://gsa.github.io/prototype-city-pairs-api-documentation/api-docs/">Prototype City Pairs API</a> is a forkable example of what you want developers to emulate in their work. It is a tool in the GSA’s API governance toolbox. It demonstrates what developers should be working towards in not just their API design, but also the supporting portal and documentation. The GSA leads by example. Providing a pretty compelling approach to model, and a building block any API provider could add to their toolbox. I would consider a working prototype to be both a bottom up approach because it is forkable, and usable, but also top down because it can reflect wider organizational API governance objectives.</p>

<p>I could see mature API governance operations having multiple API design and deployment templates like the GSA has done, providing a suite of forkable, reusable API templates that developers can put to use. While not all developers would use, in my experience many teams are actually made up of reverse engineers, who tend to emulate what they know. If they are exposed to bad API design, they tend to just emulate that, but if they are given robust, well-defined examples, they will just emulate healthy patterns. I’m adding API deployment templates to my API governance research, and will keep rounding off strategies for successful API governance, that can work at a wide variety of organizations, and platforms. As it stands, there are not very many examples out there, and I’m hoping to pull together any of the pieces I can find into a coherent set of approaches folks can choose from when crafting their own approach.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/05/api-deployment-templates-as-part-of-a-wider-api-governance-strategy/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/04/narrowing-in-on-my-api-governance-using-api-transit-to-map-out-psd2/">Narrowing In On My API Governance Strategy Using API Transit To Map Out PSD2</a></h3>
        <span class="post-date">04 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/talks/november-2015/subway-map-15.png" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="https://apievangelist.com/2017/08/17/testing-out-the-concept-of-api-transit-instead-of-api-lifecycle/">I’m still kicking around my API Transit strategy in my head</a>, trying to find a path forward with applying to API governance. <a href="https://apievangelist.com/2015/11/29/the-api-lifecycle-my-talk-from-defrag-and-apistrat/">I started moving it forward a couple years ago as a way to map out the API lifecycle</a>, but in my experience, managing APIs are rarely a linear lifecycle. I have been captivated by the potential of the subway map to help us map out, understand, and navigate complex infrastructure since I learned about <a href="https://en.wikipedia.org/wiki/Tube_map">Harry Beck’s approach to the London Tube map which has become the standard for quantifying transit around the globe</a>.</p>

<p>I am borrowing from Beck’s work, but augmenting for a digital world to try and map out the API practices I study in my research of the space in a way that allow them to be explored, but also implemented, measured, and reported upon by all stakeholders involved with API operations. While I’m still pushing forward this concept in the safe space of my own API projects, I’m beginning to dabble with applying at the industry level, by applying to PSD2 banking, and seeing if I can’t provide an interactive map that helps folks see, understand, and navigate what is going on when it comes to banking APIs.</p>

<p>An API Transit map for PSD2 would build upon the framework I have derived from my API research, applied specifically for quantifying the PSD2 world. Each of the areas of my research broken down into separate subway lines, that can be plotted along the map with relative stops along they way:</p>

<ul>
  <li><strong>Definition</strong> - Which definitions are used? Where are the OpenAPI, schema, and other relevant patterns.</li>
  <li><strong>Design</strong> - What design patterns are in play across the API definitions, and what is the meaning behind the design of all APIs.</li>
  <li><strong>Deployment</strong> - What does deployment look like on-premise, in the cloud, and from region to region.</li>
  <li><strong>Portals</strong> - What is the minimum viable standard for an API portal presence with any building blocks.</li>
  <li><strong>Management</strong> - Quantify the standard approaches to managing APIs from on-boarding to analysis and reporting.</li>
  <li><strong>Plans</strong> - How are access tiers and plans defined, providing 3rd party access to APIs, including that of aggregators and application developers.</li>
  <li><strong>Monitoring</strong> - What does monitoring of web APIs look like, and how is data aggregated and shared.</li>
  <li><strong>Testing</strong> - What does testing of web APIs look like, and how is data aggregated and shared.</li>
  <li><strong>Performance</strong> - What does performance evaluation of web APIs look like, and how is data aggregated and shared.</li>
  <li><strong>Security</strong> - What are the security practices in place for the entire API stack?</li>
  <li><strong>Breaches</strong> - When there is a breach, what is the protocol, and practices surrounding what should happen–where is the historical data as well.</li>
  <li><strong>Terms of Service</strong> - What does terms of service across many APIs look like?</li>
  <li><strong>Privacy Policy</strong> - How is privacy protected across API operations?</li>
  <li><strong>Support</strong> - What are all the expected support channels, and where are they located?</li>
  <li>Road Map - What is expected, and where do we find the road map and change log for the platform?</li>
</ul>

<p>These are just a handful of the lines I will be laying out as part of my subway map. I have others I want to add, but this provides a nice version of what I”d like to see as an API Transit map of the PSD2 universe. Each line would have numerous stops that would provide resources and potentially tooling to help educate, quantify, and walk people through each of these areas in detail, but in the context of PSD2, and the banking industry. This where I’m beginning to push the subway map context further to help make work in a virtualized world, and augmenting with some concepts I hope will add new dimensions to how we understand, and navigate our digital worlds, but using the subway map as a skeuomorph.</p>

<p>To help make the PSD2 landscape I’m mapping out more valuable I am playing with adding a “tour” layer, which allows me to craft tours that cover specific lines, hitting only the stops that matter, bridges multiple lines, and creates a meaningful tour for a specific audience. Here are a handful of the tours I’m planning for PSD2:</p>

<ul>
  <li><strong>Introduction</strong> - A simple introduction to the concepts at play when it comes to the PSD2 landscape.</li>
  <li><strong>Provider Training</strong> - A detailed training walk-through for anyone looking to provide a PSD2 compliant platform.</li>
  <li><strong>Provider Certification</strong> - A detailed walkthrough that gathers information and detail to map out, quantity, and assess a specific PSD2 API / platform.</li>
  <li><strong>Executive</strong> - A robust walk-through of the concepts at play for an executive from the 100K view, as well as those of their own companies PSD2 certified API, and possibly those of competitors.</li>
  <li><strong>Regulator</strong> - A comprehensive walk through the entire landscape, including what is required, as well as the certification of individual PSD2 API platforms, with real-time control dashboard.</li>
</ul>

<p>These are just a few of the areas I’m looking to provide tours through this quantified PSD2 API Transit landscape. I am using Github to deploy, and evolve my maps, which leverages Jekyll as a Hypermedia client to deliver the API Transit experience. While each line of the API Transit map has it’s own hypermedia flow for storing and experiencing each stop along the line, the tours also have its own hypermedia flows which can augment existing lines and stops, as well as inject their own text, images, audio, video, links and tooling along the way.</p>

<p>The result will be a single URL which anyone can land on for the PSD2 API Transit platform. You can choose from any of the pre-crafted tours, or just begin exploring each line, getting off at only the stops that interest you. Some stops will be destinations, while others will provide transfers to other lines. I’m going to be investing some cycles into my PSD2 API Transit platform over the holidays. If you have any questions, comments, input, or would like to invest in my work, please let me know. I’m always looking for feedback, as well as interested parties to help fund my work and ensure I can carve out the time to make them happen.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/04/narrowing-in-on-my-api-governance-using-api-transit-to-map-out-psd2/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/04/being-able-to-see-your-database-in-xml-json-and-csv/">Being Able To See Your Database In XML, JSON, and CSV</a></h3>
        <span class="post-date">04 Dec 2017</span>
        <p><a href="https://www.slashdb.com/documentation/api-documentation/"><img src="https://s3.amazonaws.com/kinlane-productions/slashdb/slashdb-content-negotiation.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p><em>This is a sponsored post by my friends over at <a href="https://www.slashdb.com/">SlashDB</a>. The topic is chosen by me, but the work is funded by SlasDB, making sure I keep doing what I do here at API Evangelist. Thank you <a href="https://www.slashdb.com/">SlashDB</a> for your support, and helping me educate my readers about what is going on in the API space.</em></p>

<p>I remember making the migration from XML to JSON. It was hard for me to understand that difference between the formats, and that you accomplish pretty much the same things in JSON that you could in XML. I’ve been seeing similarities in my migration to YAML from JSON. The parallels in each of these formats isn’t 100%, but this story is more about our perception of data formats, than it is about the technical details. CSV has long been a tool in my toolbox, but it was until this recent migration from JSON to YAML that I really started seeing the importance of CSV when it comes to helping onboard business users with the API possibilities.</p>

<p>In my experience API design plays a significant role in helping us understand our data. Half of this equation is understanding our schema, and what the dimensions, field names, and data types of the data we are moving around using APIs. As I was working through some stories on how my friends over at SlashDB are turning databases into APIs, I saw that they were translating database, tables, and field names into API design, and that <a href="https://www.slashdb.com/documentation/api-documentation/">they also help you handle content negotiation between JSON, XML, CSV</a>. Which I interpret as an excellent opportunity for learning more about the data we have in our databases, and getting to know the design aspects of the data schema.</p>

<p>In an earlier post about what SlashDB does I mentioned that many API designers cringe at translating database directly into a web API. While I agree that people should be investing into API design to get to know their data resources, the more time I spend with SlashDB’s approach to deploying APIs from a variety of databases, the more I see the potential for teaching API design skills along the way. I know many API developers who understand API design, but do not understand content negotiation between XML, JSON, and CSV. I see an opportunity for helping publish web APIs from a database, while having a conversation about what the API design should be, and also getting to know the underlying schema, then being able to actively negotiate between the different formats–all using an existing service.</p>

<p>While I want everyone to be as advanced as they possibly can with their API implementations, I also understand the reality on the ground at many organizations. I’m looking for any possible way to just get people doing APIs, and begin their journey, and I am not going to be to heavy handed when it comes to people being up to speed on modern API design concepts. The API journey is the perfect way to learn, and going from database to API, and kicking of the journey is more important than expecting everyone to be skilled from day one. This is why I’m partnering with companies like SlashDB, to help highlight tools that can help organizations take their existing legacy databases and translate them into web APIs, even if those APIs are just auto-translations of their database schema.</p>

<p>Being able to see your database as XML, JSON, and CSV is an important API literacy exercise for companies, organizations, institutions, and government agencies who are looking to make their data resources available to partners using the web. It is another important step in understanding what we have, and the naming and dimensions of what we are making available. I think the XML to JSON holds one particular set of lessons, but then CSV possesses a set of lessons all its own, helping keep the bar low for the average business user when it comes to making data available over the web. I’m feeling like there are a number of important lessons for companies looking to make their databases available via web APIs over at SlashDB, with automated XML, JSON, and CSV translation being just a notable one.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/04/being-able-to-see-your-database-in-xml-json-and-csv/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/04/facebook-quietly-deprecates-the-audience-insight-api-used-to-automate-targeting-during-the-election/">Facebook Quietly Deprecates The Audience Insight API Used To Automate Targeting During The Election</a></h3>
        <span class="post-date">04 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/facebook/audience-insights/facebook-audience-insights-api-affinity.png" align="right" width="45%" style="padding: 15px;" /></p>
<p><a href="http://www.adweek.com/digital/facebook-is-shutting-down-its-api-that-marketers-lean-on-for-research/#/">According to AdWeek, Facebook is quietly shutting down its Audience Insights API by the end of the year</a>. They have a statement from Facebook stating, “We have decided to focus marketers on our more broadly available Audience Insights tool, so we are winding down the Audience Insights API by end of year. We’ll continue testing different ways to provide valuable insights to advertisers and agencies through the tool and across other destinations on Facebook.” which I assume they got directly from Facebook, because I can find no other communication regarding the deprecation of the API through normal <a href="https://newsroom.fb.com/">newsroom</a>, or <a href="https://developers.facebook.com/docs/graph-api/changelog">API change log</a> channels. It could be that I’m missing it, but it is clear they are trying to minimize chatter around this.</p>

<p>According to <a href="https://www.facebook.com/business/help/304781119678235">the Facebook help page</a>, Audience Insights, “shows you data about your target audiences so that you can create more relevant advertisements for them”. The platform uses native Facebook data to show you audience features such as: Age and gender, Relationship status, Education level, Job role, Top categories, Page likes, Top cities, Top countries, Top languages, Frequency of activities, and Device users. Then using third-party data (data come from sources like Acxiom, Datalogix and Epsilon) they show you audience features such as: Lifestyle, Household income, Home ownership, Household size, Home market value, Spending methods, Retail spending, Online purchases, Purchase behavior, and whether they are in market for a vehicle. You can still get at this via <a href="https://www.facebook.com/ads/audience-insights/">the Facebook Audience Insights web interface</a>, but the APIs for automating this aspect of Facebook has mostly disappeared, or is in the process of disappearing.</p>

<p>There are three layers to the Faceook Audience Insights API deprecation. You can still access some insights for ads, pages, and other objects, as well as one audience insight still available:</p>

<ul>
  <li><a href="https://developers.facebook.com/docs/graph-api/reference/v2.11/insights"><strong>/{object-id}/insights</strong></a>  - Facebook Insights is a product available to all Pages and Apps on Facebook using the Insights dashboard.</li>
  <li><a href="https://developers.facebook.com/docs/graph-api/reference/audience-insights-rule/"><strong>Audience Insights Rule</strong></a> - Definition of an audience insight rule.</li>
</ul>

<p>Then there are a handful of API paths related to Audience Insights that are still there, but not listed off the main navigation:</p>

<ul>
  <li><a href="https://developers.facebook.com/docs/graph-api/reference/audience-insights-rule-component/"><strong>Audience Insights Rule Component</strong></a> - Rule component of a study rule.</li>
  <li><a href="https://developers.facebook.com/docs/graph-api/reference/audience-insights-post/"><strong>Audience Insights Post</strong></a> - Represents a sample post.</li>
  <li><a href="https://developers.facebook.com/docs/graph-api/reference/audience-insights-group-by-summary/"><strong>Audience Insights Group By Summary</strong></a> - Overall summary for audience insights query insights.</li>
  <li><a href="https://developers.facebook.com/docs/graph-api/reference/insights-value/"><strong>Insights Value</strong></a> - The value for one insights metric given a timestamp.</li>
  <li><a href="https://developers.facebook.com/docs/graph-api/reference/insights-result/"><strong>Insights Result</strong></a> - The result of an Insights query.</li>
</ul>

<p>Then there are the core Audince Insights APIs that are completely gone, with all documentation removed:</p>

<ul>
  <li><strong>Audience Insights Lifestyles</strong> (<a href="https://developers.facebook.com/docs/graph-api/reference/audience-insights-lifestyles/">URL</a>) (<a href="https://webcache.googleusercontent.com/search?q=cache:zcVvTrCjRTYJ:https://developers.facebook.com/docs/graph-api/reference/audience-insights-lifestyles/+&amp;cd=1&amp;hl=en&amp;ct=clnk&amp;gl=us">Cached URL</a>) - Insights about lifestyles for you audience.</li>
  <li><strong>Audience Insight Education Level</strong> (<a href="https://developers.facebook.com/docs/graph-api/reference/audience-insights-education-level/">URL</a> (<a href="https://webcache.googleusercontent.com/search?q=cache:AxUtpiQ0vuIJ:https://developers.facebook.com/docs/graph-api/reference/audience-insights-education-level/+&amp;cd=1&amp;hl=en&amp;ct=clnk&amp;gl=us">Cached URL</a>) - Information about the education level of your audience</li>
  <li><strong>Audience Insights Home Owners</strong> (<a href="https://developers.facebook.com/docs/graph-api/reference/audience-insights-home-owners/">URL</a> (<a href="https://webcache.googleusercontent.com/search?q=cache:8KLILtYG3KYJ:https://developers.facebook.com/docs/graph-api/reference/audience-insights-home-owners/+&amp;cd=1&amp;hl=en&amp;ct=clnk&amp;gl=us">Cached URL</a>) - Information about home ownership.</li>
  <li><strong>Audience Insights Household Income</strong> (<a href="https://developers.facebook.com/docs/graph-api/reference/audience-insights-household-income/">URL</a> (<a href="https://webcache.googleusercontent.com/search?q=cache:um_yOLJk-lYJ:https://developers.facebook.com/docs/graph-api/reference/audience-insights-household-incomes/+&amp;cd=1&amp;hl=en&amp;ct=clnk&amp;gl=us">Cached URL</a>) - Household incomes information about your audience.</li>
  <li><strong>Audience Insights Purchase Behaviors</strong> (<a href="https://developers.facebook.com/docs/graph-api/reference/audience-insights-purchase-behaviors/">URL</a> (<a href="https://webcache.googleusercontent.com/search?q=cache:7GPqnSgOYVIJ:https://developers.facebook.com/docs/graph-api/reference/audience-insights-purchase-behaviors/+&amp;cd=1&amp;hl=en&amp;ct=clnk&amp;gl=us">Cached URL</a>) - Purchase behaviors information for your audience.</li>
  <li><strong>Audience Insights Affinity</strong> (<a href="https://developers.facebook.com/docs/graph-api/reference/audience-insights">URL</a> (<a href="https://webcache.googleusercontent.com/search?q=cache:rhIkcFkbT7YJ:https://developers.facebook.com/docs/graph-api/reference/audience-insights-affinity/+&amp;cd=1&amp;hl=en&amp;ct=clnk&amp;gl=us">Cached URL</a>) - Information about the affinity for a given page.</li>
</ul>

<p>These are the six API paths that you would use to scale and automate any information, or disinformation campaign. This is how you develop, evolve, and act upon your models when it comes to publishing Facebook Pages, buying advertising and spreading video, photos, news, and other (dis)information that you are targeting your users with. I see hints of these insight API going away on <a href="https://developers.facebook.com/docs/graph-api/changelog/version2.11">the most recent November 7th update</a>, but there are <a href="https://developers.facebook.com/docs/graph-api/changelog/version2.10#mapi-deprecate">no marketing API deprecations in the last one in July that changed how you are able share links via the API</a>–something that was a response to the election backlash. The last cache of the missing documentation pages was on November 9th, showing they’ve been actively working in November to clean things up, and by the looks of things they are still working on this.</p>

<p>Ok, many might say that this is a good thing. Facebook is removing the tools that allow you to automate these types of campaigns. Limiting who has access to them. Sure. However, it doesn’t stop them from still providing access to partners, and other folks behind the scenes, further reducing any observability into the process, after <a href="https://newsroom.fb.com/news/2017/10/update-on-our-advertising-transparency-and-authenticity-efforts/">they’ve promised to be more transparent about all of this</a>. Also, the sneaky nature of the API deprecation, which isn’t unusual for Facebook reveals their true motivation. The deprecation is only published in AdWeek, and clearly is something other outlets are either unaware of, or unwilling to talk about due to retribution by Facebook, which might limit your exposure on the network. Facebook has many news outlets by the balls when it comes to platform exposure these days, potentially limiting who will be critical of the platform.</p>

<p>The Facebook Audience Insights API represents the conundrum of APIs for me. If APIs don’t exist we can’t see into the algorithms that are increasingly governing our lives. If they do exist then people with ill intentions get access to them, and can use them for shady things like we’ve been seeing as part of the election. The answer? They should exist, but then provide access by auditors, regulators, researchers, and journalists to see what is possible via platforms. Then, EVERYONE who has access to the tools should be observable and accountable. Not just the APIs, but also the web interface. If you are developing models that target a demographic, that demographic should know about it, and auditors, researchers, and journalists should have API access to all of this, so that they can assess and report on what is going on. The watchers should also be accountable. This is why I do APIs, not because I believe they are always good, but because they provide us with secure, managed, accountable observability into how platforms and algorithms work (or don’t).</p>

<p>Ideally, tools like this do not exist in the first place. My feeling is that we burn it down. However, I know this isn’t a reality. My next recommendation is that ALL advertising platforms possess APIs for ALL aspects of operations, with access tiers for auditors, regulators, researchers, and journalists. Observability into how these platforms are operating is the only way we can move this conversation forward in a way that protects the end-users of platforms from harm. It is clear that Facebook is not interested in true observability, and are playing the usual transparency games by acting like they are self-regulating, but then just pulling the curtains on what they are up to. In coming years, we’ll see more APIs be deprecated because of this, as the platforms realizing what is possible, and just commence more secretive about what they do. The cat is out of the bag. The technology exists to give us visibility into what is going on, the trick is going to be all about keeping the APIs that exist operational, and delivering 100% coverage of platform operations, and regulating that APIs be introduced where they do not exist already. Sorry platforms, you had plenty of time to be straight up about this stuff, and you chose not to.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/04/facebook-quietly-deprecates-the-audience-insight-api-used-to-automate-targeting-during-the-election/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/01/the-conversational-interface-appetite-for-data-via-apis/">The Conversational Interface Appetite For Data Via APIs</a></h3>
        <span class="post-date">01 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/16_77_800_500_0_max_0_1_-1.jpg" align="right" width="45%" style="padding: 15px" /></p>
<p><em>This is a sponsored post by my friends over at <a href="https://www.slashdb.com/">SlashDB</a>. The topic is chosen by me, but the work is funded by SlasDB, making sure I keep doing what I do here at API Evangelist. Thank you <a href="https://www.slashdb.com/">SlashDB</a> for your support, and helping me educate my readers about what is going on in the API space.</em></p>

<p>I spend a lot of time studying what is going on around bots on Twitter, Facebook, and Slack, as well as voice enablement like we see with Alexa, Google, and Siri. I lump these all under a research category called conversational interfaces. Conversational interfaces represent the next generation of API clients, with AWS Alexa being the most sophisticated example at how it will all work(eventually). While there are some interesting examples of conversational interfaces in action, for the most part they are still pretty simple, silly, and not providing much value. I’d say that any of the bots or voice implementations I’ve come across which are useful, are also pretty corporate, demonstrating the amount of resources you need to invest when crafting conversational interfaces.</p>

<p>From my vantage point I’m seeing three main areas slowing the growth of true usability of conversational interfaces, 1) desire, and people not wanting or caring to engage, 2) availability of data via APIs in format that is usable, and 3) the performance of APIs that do have relevant data, and their ability to deliver it as an answer to a question in reasonable amount of time. You can put me squarely into the first category of not really wanting to use conversational interfaces, but I do understand that there are people who are into doing it, which gets me somewhat involved when it comes to thinking about the 2nd, and 3rd challenge. APIs are what delivers answers in conversational interfaces, and since APIs are my jam, I’m tuning in.</p>

<p>One of the biggest challenges the conversational interface space will face in coming years is having the access to the answers or data they need to function as expected. It’s not that the data isn’t out there, it is that it isn’t available in accessible, usable API interfaces that developers can quickly wire up via platforms like Slack and Alexa. There is a wealth of sports data out there, but to make it available via bots and voice platforms you have to be able to get at via APIs. There is a wealth of movie data out there, but you have to be able to get at it via simple APIs. I can go on and on about the types of data we need, and even point out where you can find it, the problem is that it isn’t available via a simple web API so that a developer can quickly build a conversational interface on top of it.</p>

<p>This is why you’ll find me doing more research into data, and database to API implementations, partnering with folks like <a href="https://www.slashdb.com/">SlashDB</a>, who help make deploying web APIs from databases dead simple. We need more APIs, not thousands more, but millions more. We need the APIs to be simple, and authentication standardized, so that developers can quickly get their hands on what they need to develop valuable conversational interfaces. We don’t need API providers publishing APIs trying to be the next Twilio or SendGrid. We need API providers making ALL their valuable data available via APIs, and removing the friction for conversational interface developers to find what they need, so they can wire up the answers demanded by bots, voice, and other applications. If you want your valuable data available in conversational interfaces you need to be exposing it via web APIs.</p>

<p>Personally, I do not get excited by bot or voice enabled applications. I enjoy automation, but I’m more of a fan of the intimacy between my brain, my fingers, and the keyboard. However, like most of the tech space I understand that conversational interfaces will keep evolving, and want to contribute where I can to make them more usable. Another aspect of why I am getting on board with conversational interfaces, as with all the other API driven applications, is when it comes to surveillance and privacy. I want to play a role in helping define the backend layers of conversational interfaces, make them usable, valuable, while also protecting the privacy, security, and data ownership of individuals who are putting them to work. This is why you’ll find me chiming in more on the subject, not because I’m pro-conversational interface, it is because they are happening, and I want to make sure it works as well as possible for everyone involved.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/01/the-conversational-interface-appetite-for-data-via-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/01/how-do-you-ask-questions-of-data-using-apis/">How Do You Ask Questions Of Data Using APIs?</a></h3>
        <span class="post-date">01 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories-new/27_93_800_500_0_max_0_-5_-5.jpg" align="right" width="45%" style="padding: 15px;" /></p>
<p>I’m preparing to publish a bunch of transit related data as APIs, for us across a number of applications from visualizations to conversation interfaces like bots and voice-enablement. As I’m learning about the data, publishing it as unsophisticated CRUD APIs, I’m thinking deeply about how I would enable others to ask questions of this data using web APIs. I’m thinking about the hard work of deriving visual meaning from specific questions, all the way to how would you respond to an Alexa query regarding transit data in less than a second. Going well beyond what CRUD gives us when we publish our APIs and taking things to the next level.</p>

<p>Knowing the technology sector, the first response I’ll get is machine learning! You take all your data, and you train up some machine learning models, put some natural language process to work, and voila, you have your answer to how you provide answers. I think this is a sensible approach to many data sets, and for organizations who have the machine learning skills and resources at their disposal. There are also a growing number of SaaS solutions for helping put machine learning work to answer complex questions that might be asked of large databases. Machine learning is definitely part of the equation for me, but I’m not convinced it is the answer in all situations, and it might not always yield the correct answers we are always looking for.</p>

<p>After machine learning, and first on my list of solutions to this challenge is API design. How can I enable a domain expert to pull out the meaningful questions that will be asked of data, and expose as simple API paths, allowing consumers to easily get at the answers to questions. I’m a big fan of this approach because I feel like the chance we will get right answers to questions will be greater, and the APIs will help consumers understand what questions they might want to be asking, even when they are not domain experts. This approach might be more labor intensive than the magic of machine learning, but I feel like it will produce much higher quality results, and better serve the objectives I have for making data available for querying. Plus, this is a lower impact solution, allowing more people to implement, who might not have the machine learning skills or resources at their disposal. API design using low-cost web technology, makes for very accessible solutions.</p>

<p>Whether you go the machine learning or artisanal domain expert API design route, there has to be a feedback loop in place to help improve the questions being asked, as well as the answers being given. If there is no feedback loop, the process will never be improved. This is what APIs excel at when you do them properly. The savvy API platform providers have established feedback loops for API consumers, and their users to correct answers when they are wrong, learn how to ask new types of questions, and improve upon the entire question and answer life cycle. I don’t care whether you are going the machine learning route, or the API design route, you have to have a feedback loop in place to make this work as expected. Otherwise it is a closed loop system, and unlikely to give the answers people are looking for.</p>

<p>For now, I’m leaning heavily on the API design route to allow for my consumers to ask questions of the data I’m publishing as APIs. I’m convinced of my ability to ask some sensible questions of the data, and expose as simple URLs that anyone can query, and then evolve forward and improve upon as time passes. I just don’t have the time and resources to invest in the machine learning route at this point. As the leading machine learning platforms evolve, or as I generate more revenue to be able to invest in these solutions I may change my tune. However, for now I’ll just keep publishing data as simple web APIs, and crafting meaningful paths that allow people to ask questions of some of the data I’m coming across locked up in zip files, spreadsheets, and databases.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/01/how-do-you-ask-questions-of-data-using-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/12/01/how-to-say-you-might-charge-for-api-access-in-the-future-without-being-a-dick/">How To Say You Might Charge For API Access In The Future Without Being A Jerk</a></h3>
        <span class="post-date">01 Dec 2017</span>
        <p><img src="https://s3.amazonaws.com/kinlane-productions/algo-rotoscope/stories/statue-face-open-mouth_copper_circuit.png" align="right" width="45%" style="padding: 15px;" /></p>
<p>I get it. It takes money to operate APIs. I’m a big advocate for making sure API providers, even public data API providers can sensibly charge for access to their valuable resources. I’m also painfully aware at how unrealistic a libertarian driven view of the web being open and free makes it very difficult to begin charging for data that has been historically free. However, I’m also a fan of helping API providers understand how they can communicate that they might / will be charging for access to data at some point in the future without being complete jerks about it.</p>

<p>I see API providers regularly make the statement that they will begin charging for API access at some point in the future, but this particular story is driven from hearing it out of the <a href="https://technical.ly/dc/2017/11/22/developers-upset-wmatas-new-data-terms-use/">Washington Metropolitan Area Transit Authority (WMATA) making changes to their terms of service</a>, where one of the bullet points was that they would begin charging for access at some point. Making the announcement that you intend to begin charging for something that has been free is challenging in any API ecosystem, but especially so within public data API ecosystems like WMATA. In any of these environments you can’t just shoot across your community’s bow with a statement like this, and expect a positive response. Doing so, just shows how out of touch with your community you are.</p>

<p>First rule of communicating around the business side of your road map is don’t just say you’ll be charging at some point and leave things there. Give details of what this means. Demonstrate your knowledge around how API management and service composition works. Will ALL developers be charged? Will it just be commercial developers? Will it be developers over a certain level of consumption? Do not leave it to the communities imagination regarding what will happen, because this is where the powers of Internet speculation will take hold, and begin working against your API efforts. This is where your entire community will begin talking about how these changes will impact their business, and begin to prepare for the worst, even if the changes won’t even impact them. Creating a ripple effect across your API platform, and potentially hurting business beyond what will actually be reality.</p>

<p>Next, share some thoughts behind the reasoning behind these changes. Craft a blog post. Hold some office hours. Talk to your API consumers about why you will need to start charging for access to ALL or some of your API resources. Back up the details you provied with some actual insight into what went into the decision making process. Prove to your API consumers that you have their best interest in mind, and aren’t just looking to screw everyone over. A lack of visibility into the decision making process will only push your API consumers to assume the worst. Ideally, this isn’t just a one time event, and you publish a series of blog posts sharing the story behind the process of needing to generate more revenue, to cover rising costs, or whatever else might be the reason behind the need to charge for access at some point in the future. Don’t make this just a sudden thing, build up to it, and ease your community into the concept that APIs will move from free to paid.</p>

<p>After providing details on the API monetization strategy and plan, and sharing the story behind this shift in platform operations, lean on your API feedback loop as part of your shift in strategy. You have a strong feedback loop in place directly with your strongest API consumers, and at scale across the rest of your API consumers, right? You actively understand what your strongest platform consumers are thinking, and how the introduction of fees might impact their operations, right? I’m guessing if you are making vague statements about charging for access in the future and just walking away, that there is NO feedback loop, or the feedback loop is pernicious to say the least. You don’t really have much interest in what your API consumers are thinking, and how the shifts in a fee structure and monetization strategy will impact them. Otherwise, you’d fully understand the impacts of making statements about charging for API consumption at some date down the road.</p>

<p>Being an API provider isn’t easy. Balancing your platform concerns with those of your API consumers isn’t easy. Time and time again I see providers enter into the game without having put much thought into a monetization strategy, and have no coherent plan in place. Making changes down the road painful for everyone. Do yourself a favor, and spend the time learning about modern API management practices, and how API service composition works. Visit the API portals of leading API providers to see how they have structured their plans, and composed their service access tiers. Talk to people like me who study this stuff for a living, before you ever go public with your API. However, once you do, know that communication is essential, and that you won’t get away with being a jerk on this stuff, and just randomly telling people that at some point in the future you will be charging for access doesn’t fly in API-land, things don’t work like that.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/12/01/how-to-say-you-might-charge-for-api-access-in-the-future-without-being-a-dick/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2017/11/30/sql-statement-pass-through-using-web-apis/">SQL Statement Pass-Through Using Web APIs</a></h3>
        <span class="post-date">30 Nov 2017</span>
        <p><em>This is a sponsored post by my friends over at <a href="https://www.slashdb.com/">SlashDB</a>. The topic is chosen by me, but the work is funded by SlasDB, making sure I keep doing what I do here at API Evangelist. Thank you <a href="https://www.slashdb.com/">SlashDB</a> for your support, and helping me educate my readers about what is going on in the API space.</em></p>

<p><a href="https://www.slashdb.com/how-it-works/#sql-pass-thru"><img src="https://s3.amazonaws.com/kinlane-productions/slashdb/slashdb-sql-pass-through-mode.png" align="right" width="45%" style="padding: 15px;" /></a></p>
<p>I’m closely following the approach of GraphQL when it comes to making data resources more accessible by API consumers when developing applications. I think there is some serious value introduced when it comes empowering front-end developers with the ability to get exactly the data they need using a variety of querying structures. I enjoy studying up on different approaches to making different dimensions of a database to consumers and end-users, and found a pretty scrappy one from my friends over at SlashDB, with <a href="https://www.slashdb.com/how-it-works/#sql-pass-thru">their SQL statement pass through</a>. It’s not the most formal approach to query a database, but I think it’s scrappy and simple enough, that it might work for a wide variety of technical, as well as non-technical users.</p>

<p>Using the SlashDB mode, an administrator, or an application backend developer can define arbitrary SQL queries which once defined, can be executed as a smple URL. The example query they provide returns customers from London: http://demo.slashdb.com/query/customers-in-city/city/London.html. It is something that will make RESTafarians pull their hair (dreads?) out, but for business users looking to get their hands on some data to populate a spreadsheet, or share with a partner when developing an application–it will be a lifesaver. As the GraphQL folks like trumpet, REST isn’t the only way to get things done, and while I think we should be thinking critical about the long term impact of our API design choices, getting business done efficiently is an important aspect of doing APIs as well.</p>

<p>What I like about the SlashDB approach is it makes for an intuitive URL. Something business users can understand. I could see crafting these in bulk, and some becoming permanent, while others maybe being more of a temporary thing. Depending on the application you may want to standardize how you publish your URLs, using common patterns, and making sure queries aren’t changing, if they are being baked into applications. I think that simple URLs that retrieve data from a database will always trump a more complex, technical solution that developers often want. Developers are always going to want more robust solutions that they can tweak and play with, but business users just want what they need, and are looking for the quickest way to solve their business problem–SQL statement pass-through is this.</p>

<p>I’ve worked at companies that have an HTML Textarea on the dashboard of the internal portal where you can hand type SQL statements, or use from a pre-configured set of statements. Allowing business users to quickly query a database and dump to spreadsheet, CSV, and import into other applications. I can see SQL pass-through being a quick and dirty solution that reflects these other approaches I’ve seen in the past. I could see bookmarks, quick links, and other scrappy ways of using the web to query backend databases like this. When you couple this with some sort of API key or other identifier, you can also begin to develop an awareness of who is making these types of queries, and what types of applications they are putting them to use in. Taking SQL query pass-through to the next level and going beyond just API deployment, and moving into the realms of API management.</p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2017/11/30/sql-statement-pass-through-using-web-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  

<p align="center"><a href="http://apievangelist.com/archive/"><strong>View Previous Posts Via Archives</strong></a></p>

  </div>
</section>

              
<footer>
  <hr>
  <div class="features">
    
    
      
      <article>
        <div class="content">
          <p align="center"><a href="https://www.getpostman.com/post-con-2019/" target="_blank"><img src="https://apievangelist.com/images/300x250-postcon-2019.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
        </div>
      </article>
      
    
      
      <article>
        <div class="content">
          <p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://apievangelist.com/images/tyk-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
        </div>
      </article>
      
    
  </div>
  <hr>
  <p align="center">
    relevant work:
    <a href="http://apievangelist.com">apievangelist.com</a> |
    <a href="http://adopta.agency">adopta.agency</a>
  </p>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Homepage</a></li>
    <li><a href="http://101.apievangelist.com/">101</a></li>
    <li><a href="/blog/">Blog</a></li>
    <li><a href="http://history.apievangelist.com/">History of APIs</a></li>
    <li><a href="/#api-lifecycle">API Lifecycle</a></li>
    <li><a href="/search/">Search</a></li>
    <li><a href="/newsletters/">Newsletters</a></li>
    <li><a href="/images/">Images</a></li>
    <li><a href="/archive/">Archive</a></li>
  </ul>
</nav>

              <section>
  <div class="mini-posts">
    <header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
    
    
      
        <article style="display: inline;">
          <a href="https://www.getpostman.com/post-con-2019/" class="image"><img src="https://apievangelist.com/images/300x250-postcon-2019.png" alt="" width="50%" style="padding: 15px; border: 1px solid #000;" /></a>
        </article>
      
    
      
        <article style="display: inline;">
          <a href="https://tyk.io/" class="image"><img src="https://apievangelist.com/images/tyk-logo.png" alt="" width="50%" style="padding: 15px; border: 1px solid #000;" /></a>
        </article>
      
    
  </div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
