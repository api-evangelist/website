<!DOCTYPE html>
<html xmlns="https://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
  <html>
  <title>API Evangelist </title>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
  <!--[if lte IE 8]><script src="/assets/js/ie/html5shiv.js"></script><![endif]-->
  <link rel="stylesheet" href="/assets/css/main.css" />
  <link rel="stylesheet" href="/assets/css/bootstrap.min.css" />
  <!--[if lte IE 9]><link rel="stylesheet" href="/assets/css/ie9.css" /><![endif]-->
  <!--[if lte IE 8]><link rel="stylesheet" href="/assets/css/ie8.css" /><![endif]-->

  <!-- Icons -->
  <link rel="shortcut icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">
	<link rel="icon" type="image/x-icon" href="https://apievangelist.com/favicon.ico">

  <link rel="alternate" type="application/rss+xml" title="API Evangelist Blog - RSS 2.0" href="https://apievangelist.com/blog.xml" />
  <link rel="alternate" type="application/atom+xml" title="API Evangelist Blog - Atom" href="https://apievangelist.com/atom.xml">

  <!-- JQuery -->
  <script src="/js/jquery-latest.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/bootstrap.min.js" type="text/javascript" charset="utf-8"></script>
  <script src="/js/utility.js" type="text/javascript" charset="utf-8"></script>

  <!-- Github.js - http://github.com/michael/github -->
  <script src="/js/github.js" type="text/javascript" charset="utf-8"></script>

  <!-- Cookies.js - http://github.com/ScottHamper/Cookies -->
  <script src="/js/cookies.js"></script>

  <!-- D3.js http://github.com/d3/d3 -->
  <script src="/js/d3.v3.min.js"></script>

  <!-- js-yaml - http://github.com/nodeca/js-yaml -->
  <script src="/js/js-yaml.min.js"></script>

  <script src="/js/subway-map-1.js" type="text/javascript"></script>

  <style type="text/css">

    .gist {width:100% !important;}
    .gist-file
    .gist-data {max-height: 500px;}

    /* The main DIV for the map */
    .subway-map
    {
        margin: 0;
        width: 110px;
        height: 5000px;
        background-color: white;
    }

    /* Text labels */
    .text
    {
        text-decoration: none;
        color: black;
    }

    #legend
    {
    	border: 1px solid #000;
        float: left;
        width: 250px;
        height:400px;
    }

    #legend div
    {
        height: 25px;
    }

    #legend span
    {
        margin: 5px 5px 5px 0;
    }
    .subway-map span
    {
        margin: 5px 5px 5px 0;
    }

    </style>

    <meta property="og:url" content="">
    <meta property="og:type" content="website">
    <meta property="og:title" content="API Evangelist">
    <meta property="og:site_name" content="API Evangelist">
    <meta property="og:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    <meta property="og:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">

    <meta name="twitter:url" content="">
    <meta name="twitter:card" content="summary">
    <meta name="twitter:title" content="API Evangelist">
    <meta name="twitter:site" content="API Evangelist">
    <meta name="twitter:description" content="A network of research sites dedicated to the technology, business, and politics of APIs.">
    
      <meta name="twitter:creator" content="@apievangelist">
    
    <meta property="twitter:image" content="http://s3.amazonaws.com/kinlane-productions/api-evangelist/t-shirts/KL_InApiWeTrust-1000.png">


</head>

  <body>

			<div id="wrapper">
					<div id="main">
						<div class="inner">

              <header id="header">
	<a href="http://apievangelist.com" class="logo"><img src="https://kinlane-productions2.s3.amazonaws.com/api-evangelist/api-evangelist-logo-400.png" width="75%" /></a>
	<ul class="icons">
		<li><a href="https://twitter.com/apievangelist" class="icon fa-twitter"><span class="label">Twitter</span></a></li>
		<li><a href="https://github.com/api-evangelist" class="icon fa-github"><span class="label">Github</span></a></li>
		<li><a href="https://www.linkedin.com/company/api-evangelist/" class="icon fa-linkedin"><span class="label">LinkedIn</span></a></li>
		<li><a href="http://apievangelist.com/atom.xml" class="icon fa-rss"><span class="label">RSS</span></a></li>
	</ul>
</header>

    	        <section>
	<div class="content">

	<h3>The API Evangelist Blog</h3>
	<p>This blog is dedicated to understanding the world of APIs, exploring a wide range of topics from design to deprecation, and spanning the technology, business, and politics of APIs. <a href="https://github.com/kinlane/api-evangelist" target="_blank">All of this runs on Github, so if you see a mistake, you can either fix by submitting a pull request, or let us know by submitting a Github issue for the repository</a>.</p>
	<center><hr style="width: 75%;" /></center>
	
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/09/04/remembering-that-apis-are-used-to-reduce-everything-down-to-a-transaction/">Remembering That Apis Are Used To Reduce Everything Down To A Transaction</a></h3>
        <span class="post-date">04 Sep 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Remembering That APIs Are Used To Reduce Everything Down To A Transaction’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/reduce+everything+to+a+transaction_tr.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/reduce+everything+to+a+transaction_tr.png" width="45%" align="right" style="padding: 15px;" />
<p>This is our regular reminder that APIs are not good, nor bad, nor neutral. They are simply a tool in our technological toolbox, and something that is often used for very dark reasons, and occasionally for good. One of the most dangerous things I’ve found about APIs is just the general thought process that goes along with them, regarding how all roads lead to reducing, and distilling things down to a single transaction. APIs, REST, microservices, and other design patterns are all about taking something from our physical world, and redefining it as something that can be transmitted back and forth using the low cost request and response infrastructure of the web.

<p>No matter what you are designing your API for, your mission is to reduce everything to a simple transaction that can be exchanged between your server, and any other system, web, mobile, device, or network application. This digital resource could be a photo of your kids, a message to your mother, the balance of your bank account, your personal thoughts going into your notebook, the latest song you listened to, your DNA, your test results for cancer, or any other piece of relevant data, content, media, object, or other resource that is being sent or received online. APIs are all about reducing all of our meaningful digital bits to the smallest possible transaction, and then daisy chaining them together to produce some desired set of results.

<p>This API-ification of everything can be a good thing. It can make our lives better, but one of the negative side effects of this reducing of everything to a transaction, is that now that transaction can be bought and sold. The digitization of everything in our lives is rarely ever about making our lives better and whatever the reasons we are told up front, and almost always are about reducing that little piece of our lives to a transaction that can be quantified, have a value place on it, and then sold individually, or in bulk with millions of other transactions. As consumers of a digital reality, we rarely see the reasons why something around us are being digitized, and API-ified so that it can transacted online, resulting in something we’ve heard a lot–that we are the product.

<p>It’s easy to believe in the potential of APIs. It is easy to get caught up in the reducing of everyday things down to transactions. It takes discipline, and the ability to stop and consider the bigger picture on a regular basis to avoid being stuck in the strong under currents of the API economy. Making sure we are regularly asking ourselves if we want this piece of our reality digitized and reduced to a transaction, and what the potential negative consequences of this element of our existence being a transaction. Thinking a little more deeply about how we’d feel if someone was buying and selling the digital bits of our life, and are we only ok with this as long as it is someone else’s bits and bytes–demonstrating that APIs are winning, and humanity is losing in this game we’ve developed online.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/09/04/remembering-that-apis-are-used-to-reduce-everything-down-to-a-transaction/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/30/why-i-feel-the-department-of-veterans-affairs-api-effort-is-so-significant/">Why I Feel The Department Of Veterans Affairs Api Effort Is So Significant</a></h3>
        <span class="post-date">30 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Why I Feel The Department Of Veterans Affairs API Effort Is So Significant’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/federal-government/va/va-developer-portal.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/va/va-developer-portal.png" width="45%" align="right" style="padding: 15px;" />
<p>I have been working on API and open data efforts at the Department of Veterans Affairs (VA) for five years now. I’m passionate about pushing forward the API conversation at the federal agency because I want to see the agency deliver on its mission to take care of veterans. My father, and my step-father were both vets, and I lived through the fallout from my step-fathers two tours in Vietnam, exposure to the VA healthcare and benefits bureaucracy, and ultimately his passing away from cancer which he acquired from to his exposure to Agent Orange. I truly want to see the VA streamline as many of its veteran facing programs as they possibly can.

<p>I’ve been evangelizing for API change and leadership at the VA since I worked there in 2013. I’m regularly investing unpaid time to craft stories that help influence people I know who are working at the VA, and who are potentially reading my work. Resulting in posts like <a href="https://apievangelist.com/2017/10/26/my-response-on-the-department-of-veterans-affairs-rfi-for-the-lighthouse-api-management-platform/">my response to the VA’s RFI for the Lighthouse API management platform</a>, which included <a href="https://apievangelist.com/2018/02/24/department-of-veterans-affairs-lighthouse-platform-rfi-round-two/">a round two response a few months later</a>. Influence through storytelling is the most powerful tool I got in my API evangelist toolbox.

<p><strong>This Is An Amazon Web Services Opportunity</strong><br />
The most popular story on my blog is, “<a href="https://apievangelist.com/2012/01/12/the-secret-to-amazons-success-internal-apis/">The Secret to Amazon’s Success–Internal APIs</a>”. Which tells a story of the mythical transition of Amazon from an online commerce website to the cloud giant, who is now powering a significant portion of the web. The story is mostly fiction, but continues to be the top performing story on my blog six years later. I’ve heard endless conference talks about this subject, I’ve seen my own story framed on the wall in enterprise organizations in Europe and Australia, and as a feature link on internal IT portals. This is one of the most important stories we have in the API sector, and what is happening at the VA right now will become similar to the AWS story when we are talking about delivering government services a decade from now.

<p><strong>The VA Is Going All In On An API Vision</strong><br />
One of the reasons the VA will obtain the intended results from their API initiative is because they are going all in on APIs across the agency. The API effort isn’t just some sideshow going on in a single department or group. This API movement is being led out of the VA’s Digital Innovation center, but is being adopted, picked up, and moved forward by many different groups across the large government agency. When I did <a href="https://apievangelist.com/2018/06/18/va-lighthouse-landscape-analysis-and-roadmapping-project-report/">my landscape analysis for them</a>, I scanned as much of their public digital presence as possible in a two week timeframe, and provided them with a list of targets to go after. I see the scope of the results obtained from VA landscape analysis present in the APIs I’m seeing published to their portal, and in development by different groups, revealing in the beginnings of an agency-wide API journey.

<p><strong>The Use Of developer.va.gov Demonstrates The Scope</strong><br />
One way you can tell that the VA is going all in on an API vision, is their usage of the <a href="http://developer.va.gov">developer.va.gov</a> subdomain. This may seem like it is a trivial thing, but after looking at thousands of API operations, and monitoring some of them for eight years, the companies, organizations, institutions, and government agencies to dedicate a subdomain to their API programs are always more committed to them, and invest the appropriate amount of resources needed to be successful. These API leaders always stand out from the organizations that publish their API efforts as an entry in their help center or knowledge-base, or make it just a footnote in their online presence. The use of the developer.va.gov subdomain demonstrates the scope of investment going on over at the VA in my experience.

<p><strong>The VA Is Properly Funding Their API Efforts</strong><br />
One of the most common challenges I see API teams face is the lack of resources to do what they need to do. API teams that can’t afford to deliver across multiple stops along the API lifecycle, cutting corners on testing, monitoring, security, documentation, and other common building blocks of a successful API platform. Properly funding an API initiative, and making it a first class citizen within the enterprise is essential to success. The number one response an API platform gets rendered ineffective is due to a lack of resources to properly deliver, evangelize, and scale API operations. This condition often leaves API programs unable to effectively spread across large organizations, properly reach out to  partners, and generate the attention a public API program will need to be successful. From what I’ve seen so far, the VA is properly funding the expansion of the API vision at the agency, and will continue to do so for the foreseeable future.

<p><strong>The VA Is Providing Access To Meaningful API Resources</strong><br />
I’ve seen thousands of APIs get launched. Large enterprise always start with the safest resources possible. Learning by delivering resources that won’t cause any waves across the organization, which can be a good thing, but after a while, it is important that the resources you put forth do cause waves, and make change across an organization. The VA started with simple APIs like the VA Facilities API, but is quickly shifting gears into benefits, veteran validation, health, and other APIs that are centered around the veteran. I’m seeing the development of APIs that provide a full stack of resources that touch on every aspect of the veterans engagement with the VA. In order to see the intended results from the VA API efforts, they need to be delivering meaningful API resources, that truly make an impact on the veteran. From what I’m seeing so far, the VA is getting right at the heart of it, and delivering the useful API resources that will be needed across web, mobile, and device based applications that are serving veterans today.

<p><strong>There Is Transparency And Storytelling</strong><br />
Every one of my engagements with the VA this year has ended up on my blog. One of the reasons I stopped working within the VA back in 2013 was there were too many questions about being able to publish stories on my blog. I haven’t seen such questions of my work this year, and I’m seeing the same tone being struck across other internal and vendor efforts. The current API movement at the VA understands the significance of transparency, observability, and of doing much of the API work the VA out in the open. Sure, there is still the privacy and security apparatus that exists across the federal government, but I can see into what is happening with this API movement from the outside-in. I’m seeing the right amount of storytelling occurring around this effort, which will continue to sell the API vision internally to other groups, laterally to other federal agencies, and outwards to software vendors and contractors, as well as sufficiently involving the public throughout the journey.

<p><strong>Evolving The Way Things Get Done With Micro-Procurement</strong><br />
Two of the projects I’ve done with the VA have been micro-procurement engagements: 1) <a href="https://apievangelist.com/2018/06/18/va-lighthouse-landscape-analysis-and-roadmapping-project-report/">VA API Landscape Analysis</a>, and 2) <a href="https://apievangelist.com/2018/07/19/api-governance-models-in-the-public-and-private-sector/">VA API Governance Models In The Public And Private Sector</a>. Both of these projects were openly published to GitHub, opening up the projects beyond the usual government pool of contractors, then were awarded and delivered within two week sprints for less than $10,000.00. Demonstrating that the VA is adopting an API approach to not just changing the technical side of delivering service, but also working to address the business side of the equation. While still a very small portion of what will be needed to get things done at the VA, it reflects an important change in how technical projects can be delivered at the VA. Working to decompose and decouple not just the technology of delivering APIs at the VA, but also the business, and potentially the internal and vendor politics of delivering services at scale across the VA.

<p><strong>The VA Has Been Doing Their API Homework</strong><br />
As of the last couple of months, the VA is shifting their efforts into high gear with <a href="https://www.fbo.gov/index.php?s=opportunity&amp;mode=form&amp;id=989a6748f86f4851e30b6585b22ab137&amp;tab=core&amp;_cview=0">an API management</a>, as well as an <a href="https://www.fbo.gov/index.php?s=opportunity&amp;mode=form&amp;id=f7a6f0c12e1a4801df53d7f3fa239d93&amp;tab=core&amp;_cview=1">API development and operations</a> solicitation(s) to help invest in, and build capacity across the agency. However, before these solicitations were crafted the VA has been doing some serious homework. You can see this reflected in the RFI effort which started in 2017, and continued in 2018. <a href="https://github.com/department-of-veterans-affairs/VA-Micropurchase-Repo">You can see this reflected in the micro-procurement contracts that have been executed</a>, and are in progress as we speak. I’ve seen a solid year of the VA doing their homework before moving forward, but once they’ve started moving forward, they’ve managed to be able to shift gears rapidly because of the homework they’ve done to date.

<p><strong>Investing In An API Lifecycle And Governance</strong><br />
<img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/43_113_800_500_0_max_0_-5_-5.jpg" width="45%" align="right" style="padding: 15px;" />
I am actively contributing to, and tuning into the API strategy building going on at the VA, and I’m seeing investment into a comprehensive approach to delivering all APIs in a consistent way across a modern API lifecycle. Addressing API design, mocking, deployment, orchestration, management, documentation, monitoring, and testing in a consistent way using an OpenAPI 3.0 contract. Something that is not just allowing them to reliably deliver APIs consistently across different groups and vendors, but is also allowing them to develop a comprehensive API governance strategy to measure, report upon, and evolve their API lifecycle and strategy over time. Dialing in how they deliver services across the VA, by leveraging the development, management, and operational level capacity they are bringing to the table with the solicitations referenced above. This approach demonstrates the scope in which the VA API leadership understands what will be necessary to transform the way the VA delivers services across the massive federal agency.

<p><strong>Providing An API Blueprint For Other Agencies</strong><br />
What the VA is doing is poised to change the way the VA meets its mission. However, because it is being done in such a transparent and observable way, with every stop along the lifecycle being so well documented and repeatable, they are also developing an API blueprint that other federal agencies can follow. There are other healthy API blueprints to follow across the federal government, out of Census, Labor, NASA, CFPB, FDA, and others, but there is not an agency-wide, API definition driven, full life cycle, complete with governance blueprint available at the moment. The VA API initiative has the potential to be the blueprint for API change across the federal government, becoming the Amazon Web Services story that we’ll be referencing for decades to come. All eyes are on the VA right now, because their API efforts reflect an important change at the agency, but also an important change for how the federal government delivers services to it’s people.

<p>I am all in when it comes to support APIs at the VA. As I mentioned earlier, my primary motivation is rooted in my own experiences with the VA system, and helping it better serve veterans. My secondary motivation is all about contributing to, and playing a role in the implementation of one of the significant API platforms out there, which if done right, will change how our federal government works. I’m not trying to be hyperbolic in my storytelling around the VA API platform, I truly believe that we can do this. As always, I am working to be as honest as I can about the challenges we face, and I know that the API journey is always filled with twists and turns, but with the right amount of <a href="https://apievangelist.com/2017/07/18/diagramming-the-components-of-api-observability/">observability</a>, I believe the VA API platform can deliver on the vision being set by leadership at the agency, and why I find this work to be so significant.



</p></p></p></p></p></p></p></p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/30/why-i-feel-the-department-of-veterans-affairs-api-effort-is-so-significant/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/29/understanding-where-folks-are-coming-from-when-they-say-api-management-is/">Understanding Where Folks Are Coming From When They Say Api Management Is</a></h3>
        <span class="post-date">29 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Understanding Where Folks Are Coming From When They Say API Management Is’</p>

<p>image: http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/23<em>19_800_500_0_max_0</em>-5_-1.jpg</p>

<hr />

<p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/23_19_800_500_0_max_0_-5_-1.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I am always fascinated when I get push back from people about API management, the authentication, service composition, logging, analysis, and billing layer on the world of APIs. I seem to be find more people who are skeptical that it is even necessary anymore, and that it is a relic of the past. When I first started coming across the people making these claims earlier this year I was confused, but as I’ve pondered on the subject more, I’m convinced their position is more about the world of venture capital, and investment in APIs, that it is about APIs.

<p>People who feel like you do not need to measure the value being exchanged at the API layer aren’t considering the technology or business of delivering APIs. They are simply focused on the investment cycles that are occurring, and see API management as something that has been done, it is baked into the cloud, and really isn’t central to API-driven businesses. They perceive that the age of API management as being over, it is something the cloud giants are doing now, thus it isn’t needed. I feel like this is a symptom of tech startup culture being so closely aligned with investment cycles, and the road map being about investment size and opportunity, and rarely the actual delivery of the thing that brings value to companies, organizations, institutions, and government agencies.

<p>I feel this perception is primarily rooted in the influence of investors, but it is also based upon a limited understanding of API management, and seeing APIs being a about delivering public APIs, maybe with a complimenting a SaaS offering, and a free, pro, and enterprise tiers of access. When in reality API management is about measuring, quantifying, reporting upon, and in some cases billing for the value that is exchanged at the system integration, web, mobile, device, and network application levels. However, to think API operators shouldn’t be measuring, quantifying, reporting, and generating revenue from the digital bits being exchanged behind ALL applications, just demonstrates a narrow view of the landscape.

<p>It took me a few months to be able to see the evolution of API management from 2006 to 2016 through the eyes of an investment minded individual. Once the last round of consolidation occurred, Apigee IPO’d, and API management became baked into Amazon, Google, and Azure, it fell of the radar for these folks. It’s just not a thing anymore. This is just one example of how investment influences the startup road map, as well as the type of thinking that goes on amongst investor influence, painting an entirely different picture of the landscape, than what I see going on. Helping me understand more about where this narrative originates, and why it gets picked up and perpetuated within certain circles.

<p>To counter this view of the landscape, from 2006 to 2016 I saw a very different picture. I didn’t just see the evolution of Mashery, Apigee, and 3Scale as acquisition targets, and cloud consolidation. I also saw the awareness that API management brings to the average API provider. Providing visibility into the pipes behind the web, mobile, device, and network applications we are depending on to do business. I’m seeing municipal, state, and federal government agencies waking up to the value of the data, content, and algorithms they possess, and the potential for generating much needed revenue off commercial access to these resources. I’m working with large enterprise groups to manage their APIs using 3Scale, Apigee, Kong, Tyk, Mulesoft, Axway, and AWS API Gateway solutions.

<p>Do not worry. Authenticating, measuring, logging, reporting, and billing against the value flowing through our API pipes isn’t going anywhere. Yes it is baked into the cloud. After a decade of evolution, it definitely isn’t the early days of investing in API startups. But, API management is still a cornerstone of the API life cycle. I’d say that API definitions, design, and deployment are beginning to take some of the spotlight, but authentication, service composition, logging, metrics, analysis, and billing will remain an essential ingredient when it comes to delivering APIs of any shape or size. If you study this layer of the API economy long enough, you will even see some emerging investment opportunities at the API management layer, but you have to be looking through the right lens, otherwise you might miss some important details.



</p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/29/understanding-where-folks-are-coming-from-when-they-say-api-management-is/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/29/api-portals-designed-for-api-provider-and-api-consumers/">Api Portals Designed For Api Provider And Api Consumers</a></h3>
        <span class="post-date">29 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘API Portals Designed For API Provider And API Consumers’</p>

<p>image: http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/66<em>189_800_500_0_max_0</em>-5_-1.jpg</p>

<hr />

<p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/66_189_800_500_0_max_0_-5_-1.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I’ve been working a couple organizations who are struggling with providing information within their API developer portal intended for API publishers, pushing their API portal beyond just being for their API consumers. Some of the folks I’ve been working with haven’t thought about their API developer portals being for both API publishers and consumers, and asked me to weigh in on the pros and cons of doing this. Helping them understand how they can continue their journey towards not just being an API platform, but also an API marketplace.

<p>Some of the conversations we were having about providing API lifecycle materials to API developers, helping them deliver APIs consistently across all stops along lifecycle, focused on creating a separate portal for API publishers, decoupling them from where the APIs would be discovered and consumed. After some discussion, and consideration, it feels like it would be an unnecessary disconnect, to have API publishers going to a different location than where their APIs would end up being discovered, and integrated with. That having them actively involved in the publishing, presentation, and even support of, and engagement with consumers would benefit everyone involved.

<p>Think of it being like Rapid API, but a large company, organization, institution, or government agency. You can find APIs, and integrate with existing APIs, or you can also become an API publisher, and be someone who helps publish and manage APIs as well. You will have one account, but you can find documentation, usage information and other resources for the APIs you consume, but then you will also access information, and usage data on the APIs you’ve published. Pushing API developers within an organization to actively think about both sides of the API coin, and learn how to be both provider and consumer. Helping add to the catalog of APIs, but also helping evolve and grow the army of API people across an organization.

<p>We still have a lot of work ahead of us when it comes to fleshing out what type of information we should provide to API publishers, and how to cleanly separate the two worlds, but I feel the realization that a portal can be both for API publishers and consumers was an important one for these groups. I feel like it represents a milestone in the maturity and growth of their API programs, where the API developer portal has grown into something that everyone should be tuning into, and participating in. It isn’t just something that a single team, or handful of individuals managed, and it is has become something that is becoming a group effort. Sharing the load of operating the API portal, and keeping things up to date and active, further contributing to the potential success of the platform, shielding it from becoming yet another web service or API catalog that gets forgotten about on the network.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/29/api-portals-designed-for-api-provider-and-api-consumers/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/28/trying-to-define-an-algorithm-for-my-aws-api-cost-calculations-across-api/">Trying To Define An Algorithm For My Aws Api Cost Calculations Across Api</a></h3>
        <span class="post-date">28 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Trying To Define An Algorithm For My AWS API Cost Calculations Across API’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/4882162452_fa3126b38d_b_aqua.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/4882162452_fa3126b38d_b_aqua.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I am trying to develop a base base API pricing formula for determining what my hard costs are for each API I’m publishing using Amazon RDS, EC2, and AWS API Gateway. I also have some APIs I am deploying using Amazon RDS, Lambda, and AWS API Gateway, but for now I want to get a default base for determining what operating my APIs will cost me, so I can mark up and reliably generate profit on top of the APIs I’m making available to my partners. AWS has all the data for me to figure out my hard costs, I just need a formula that helps me accurately determine what my AWS bill will be per API.

<p>Math isn’t one of my strengths, so I’m going to have to break this down, and simmer on things for a while, before I will be able to come up with some sort of working formula. Here are my hard costs for what my AWS resources will cost me, for three APIs I have running currently in this stack:

<ul>
  <li><strong>AWS RDS</strong> - I am running a db.r3.large instance which costs me $0.29 per hour or 211.70 per month, with the bandwidth free to my Amazon EC2 instances in the same availability zone. I do not have any public access, so I don’t have any incoming or outgoing traffic, except from the EC2 instance.</li>
  <li><strong>AWS EC2</strong> - I am running a t2.large instance which costs me $0.0928 per hour or $67.74 per month with bandwidth out costing me $0.155 per GB. I’m curious if they have an Amazon EC2 to AWS API Gateway data consideration? I couldn’t find anything currently.</li>
  <li><strong>AWS API Gateway</strong> - Overall using AWS API Gateway costs me $3.50 per million API calls received, with the first 1GB costing me $0.00/GB, and costing me $0.09/GB for the next 9.999 TB.</li>
</ul>

<p>Across these three APIs, I am seeing an average of 5KB per responses, which is an important variable to use in these calculations. The <a href="https://calculator.s3.amazonaws.com/index.html">AWS API Calculator</a> helps me figure out your monthly bill across services, but it doesn’t help me break down what my hard costs are per API call. I need to establish a flat rate of what it costs for a single API call to exist. Each API will be in its own plan, so I can charge different rates for different APIs, but I need a baseline that I start with for each API call to make sure I’m covering my hard AWS costs. Sure, the more API calls I make, the more profitable I’ll be, but at some point I’ll also have to scale the infrastructure to keep a certain quality of service–there are a number of things to consider here.

<p>I envision my API pricing having the following components:

<ul>
  <li><strong>Base</strong> - A base cost to cover my AWS bill, considering AWS RDS, EC2, and API Gateway hard costs.</li>
  <li><strong>Resource</strong> - A price for covering investment in resource. Work finding, cleaning up, refining, and evolving.</li>
  <li><strong>Markup</strong> - The percentage of markup for each API call, allowing me to generate a profit from the resources I’m providing.</li>
  <li><strong>Partner</strong> - Provide a volume discount, charging light users more, and giving a break to my partners who are consumer larger volumes.</li>
</ul>

<p>I’m studying other pricing models from the telco, and software hosting spaces. I’ll also be doing some landscape analysis to see what people are charging for comparable API resources. I possess a wealth of data on what API providers and service providers are charging for their services. The trick will be finding comparable services to what I’m offering, and for the unique resources I possess, I’m going to have to be able to set my own price, and then test out my assumptions, and formula over time–until I find the sweet spot for covering my hard costs, and generating profit at some point from a specific service I’m offering.

<p>If you have an advice for me. Help on the math side of things, or examples from other industries, I’d love to hear more. I’ll be open sourcing and sharing everything I figure out, and tell the story of how it is being applied to each API I am publishing. I want the history to be present for each of my APIs, adding to my wider <a href="http://monetization.apievangelist.com/">API monetization</a> and <a href="http://plans.apievangelist.com/">API planning</a> research. In the end, I don’t think there is a perfect answer to what the pricing for an API should be. The best path forward involves covering your hard costs, and then experimenting over time to see what the market will bear. This is why AWS has gotten so good at doing this for cloud, because they have been doing this work for over a decade now. I am sure they have a lot of data, as well as experience understanding how to price API resources so they are both competitive, disruptive, and profitable.



</p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/28/trying-to-define-an-algorithm-for-my-aws-api-cost-calculations-across-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/28/reviewing-the-department-of-veterans-affairs-new-developer-portal/">Reviewing The Department Of Veterans Affairs New Developer Portal</a></h3>
        <span class="post-date">28 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Reviewing The Department Of Veterans Affairs New Developer Portal’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/federal-government/va/va-developer-portal.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/va/va-developer-portal.png" width="45%" align="right" style="padding: 15px;" />
<p>I wanted to take a moment and review t<a href="https://developer.va.gov/">he Department Of Veterans Affairs (VA) new developer portal</a>. Spending some time considering at how far they’ve come, what they published so far, and brainstorm on what the future might hold. Let me open by saying that I am working directly and indirectly with the VA on a variety of paid projects, but I’m not being paid to write about their API effort–that is something I’ve done since I worked there in 2013. I will craft a disclosure to this effect that I put at the bottom of each story I write about the VA, but I wanted to put out there in general, as I work through my thoughts on what is happening over at the VA.

<p><strong>The VA Has Come A Long Way</strong><br />
I wanted to open up this review with a nod towards how far the VA has come. There have been other publicly available APIs at the VA, as well as a variety of open data efforts, but the VA is clearly going all in on APIs with this wave. The temperature at VA in 2013 when it came to APIs was lukewarm at best. With the activity I’m seeing at the moment, I can tell that the temperature of the water has gone way up. Sure, the agency still has a long way to go, but from what I can tell, the leadership is committed to the agencies API journey–something I have not seen before.

<p><strong>Developer.VA.Gov Sends The Right Signal</strong><br />
It may not seem like much, but providing a public API portal at developer.va.gov sends a strong signal that the VA is seriously investing in their API effort. I see a lot of API programs, and companies who have a dedicated domain, or subdomain for their API operations are always more committed than people who make it just a subsection of their existing website, or existing as a help entry in a knowledge-base. It is important for federal agencies to have their own developer.[domain].gov portal that is actively maintained–which will be the next test for the VA’s resolve, keeping the portal active and growing.

<p><strong>The General Look And Feel Of The Portal</strong><br />
I like the minimalist look of the VA developer portal. It is simple. Easy on the eyes. I feel like the “site is currently under development” is unnecessary, because this should never cease to be. I like the “an official website of the United States government”, it is clean, and official looking. I’m happy to see the “Get help from Veterans Crisis Line”, and is something that should be on any page with services, data, or content for veterans. I like the flexible messaging area (FMA), where it says “Put VA Data to Work”. I’d like to see this section become an evolving FMA, with a wealth of messages rolling through it, educating the ecosystem about what is happening across the VA developer platform at any given moment.

<p><strong>Getting Started And Learning More</strong><br />
The learn more about VA APIs off the FMA area on home page drops me into benefits API overview, which happens to be the first category of APIs on the documentation page. I recommend isolating this to its own “getting started” page, which provides an overview of how to get started across all APIs. Providing background on the VA developer program, as well as the other building blocks of getting started with APIs, like requesting access, studying the API documentation, and the path to production for any application you are developing. The getting started for the VA developer portal should be a first class citizen, with its own page, and a logical presentation of exactly the building blocks developers will need to get started–then they can move onto documentation across all the API categories.

<p><strong>There Are Valuable APIs Available</strong><br />
Once you do actually begin looking at the API documentation available within the VA developer portal, you realize that there are truly valuable APIs available in there. Don’t get me wrong, the <a href="https://www.arlingtoncemetery.mil/Developers">Arlington National Cemetery API</a> is important, which has been the only publicly available API from the VA for several years, but when I think about VA APIs, I’m looking for resources that make a meaningful and significant impact on a vets life today:

<ul>
  <li><a href="https://developer.va.gov/explore/benefits/docs/benefits"><strong>Benefits Intake</strong></a> - Veterans Benefits Administration (VBA) document uploads.</li>
  <li><a href="https://developer.va.gov/explore/benefits/docs/appeals"><strong>Appeals Status</strong></a> - Enables approved organizations to submit benefits-related PDFs and access information on a Veteran’s behalf.</li>
  <li><a href="https://developer.va.gov/explore/facilities/docs/facilities"><strong>Facilities API</strong></a> - Use the VA Facility API to find relevant information about a specific VA facility. For each VA facility, you’ll find contact information, location, hours of operation and available services.</li>
  <li><a href="https://developer.va.gov/explore/health/docs/argonaut"><strong>Veterans Health API</strong></a> - [There is no concise description for this API, and what is there needs some serious taming, and pulling out as part of the portal.]</li>
  <li><a href="https://developer.va.gov/explore/verification"><strong>Veteran Verification</strong></a> - We are working to give Veterans control of their information – their service history, Veteran status, discharge information, and more – and letting them control who sees it.</li>
</ul>

<p>One minor thing, but will significantly contribute to the storytelling around VA APIs, is the consistent naming of APIs. Notice that only two of them have API in the title. I’m really not advocating for API to be in the title or not in the title. I am really advocating for consistency in naming, so that storytelling around them flows better. I lean towards using API in each title, just so that their titles in the documentation, OpenAPI contract behind, and everywhere these APIs travel are consistent, meaningful, and explain what is available.

<p>I like the organizing of APIs into the three categories of benefits, facilities, and health. I’d say veteran verification is a little out of place. Maybe have a veteran category, and it is the first entry? IDK. I’m thinking there needs to be a little planning for the future, and what constitutes a category, and some guidance on how things are defined, broken down, and categories, so that there is some thought put into it the API catalog as it grows. A little separation of concerns in categorization, that can maybe begin to contribute the overall microservices strategy across the VA.

<p><strong>The Path To Production For Alpha API Clients</strong><br />
I like the path to production information for alpha API clients, I felt like it should be its own dedicated page, as one building blocks of the getting started section. However, once I started scrutinizing, it seemed like it was a separate process potentially for each individual API or category of APIs. If it can be a standalone item, I’d make it one, and link to it from each individual API category, or individual API. It it can’t be, I’d figure out to make it an expandable section, or subsection of the docs. It isn’t something I want to have to scroll through when working with an API and its documentation. Sure, I want to be aware of it, and be able to understand it as part of on-boarding, and revisiting it at a later date, but I don’t need it to be part of the core documentation page–I just want to get at the interactive API documentation.

<p><strong>Self-Service Signup</strong><br />
The process for signing up seems smooth. I haven’t been approved for access, and the review process makes a lot of sense. I’ll invest time in a separate story taking a look at the signup and on-boarding process, as well as the path to production flow for API clients, but I wanted to lightly reference as part of the review. I’d say the one confusing piece was leaving the website for the signup form, and then being dropped back at https://www.oit.va.gov/developer/, without much of any information about what was happening. It was a little jarring, and the overall flow, and process needs some smoothing out. I get that we are just getting started here, so I’m too worried about this–I just wanted to make a note of it.

<p><strong>The Essentials Are There</strong><br />
Overall, the essentials are present at the VA developer area. It is a great start, and has the making of a good developer API portal. It just isn’t very mature, and you can tell things are just getting started. You can signup, get at API documentation, and understand what it takes to build an application on top of VA API resources. Adding to, refining, and further polishing what is there will take time, so I do not want to be too critical of what the VA has published–it is a much better start than I’ve seen out of other federal agencies.

<p>There are some other random items I’d like to reference, as well as brainstorm a little on what I’d like see invested in next, helping ensure the VA API portal provides what it needed for developers to be successful:

<ul>
  <li><strong>Terms of Service</strong> - It is good to see the basics of the TOS, and privacy policy there. I’d like to see more about privacy, security, and service level agreements (SLA).</li>
  <li><strong>Use Of GitHub Issues</strong> - I like the submission GitHub issues to request production access, and think it is a healthy use of GitHub issues forms, and is something the brings observability to the on-boarding process across the community.</li>
  <li><strong>Email Support</strong> - Beyond using GitHub issues for support and on-boarding, I see <a href="/cdn-cgi/l/email-protection" class="__cf_email__" data-cfemail="0a6b666f7224736b666f2746656f62784a7c6b246d657c">[email&#160;protected]</a> a lot across the site. I get why you want to be at the center of things, but email support should be made generic, and enable group ownership of the email support workflow.</li>
  <li><strong>Road Map</strong> - I’d like to see a roadmap of what is being planned, as well as a change log for what has already been accomplished.</li>
  <li><strong>Frequently Asked Questions</strong> - I’d like to see an FAQ page, with questions and answers broken down by category, allowing me to browse some of the common questions as I’m getting up to speed.</li>
  <li><strong>Code Samples &amp; SDKs</strong> - I’d like to see some more code samples in a variety of programming languages, either baked into the interactive documentation, or available on its own SDK / Code Libraries pages. I get if the VA doesn’t want to get in the business of doing this, but with an OpenAPI core, there are more options available out there to generate code samples, libraries, and SDKs. I think <a href="https://github.com/department-of-veterans-affairs/vets-api-clients/">this vets API client</a> effort needs to be pulled onto a code samples and SDKs page, and there be more investment in projects like this.</li>
  <li><strong>OpenAPI Download</strong> - I’d like to see a clear icon or button for downloading the OpenAPI 3.0 contract for each of the available APIs. I want to be able to use the OpenAPI definition for more than just documentation.</li>
  <li><strong>OpenAPI 3.0</strong> - I’m very happy to see OpenAPI 3.0 powering the API documentation, which I think is a little detail that shows the VA API team has been doing their homework.</li>
  <li><strong>Postman Collections</strong> - I’d like to see a Run in Postman button at the top right corner of each API’s documentation, allowing me to quickly load up each API into my Postman API integration and development environment.</li>
  <li><strong>Communications</strong> - I’d like to see a blog, a Twitter account, and emphasis on the VA Github account. As an API consumer I’d like to see that someone is home besides <a href="/cdn-cgi/l/email-protection" class="__cf_email__" data-cfemail="afcec3cad781d6cec3ca82e3c0cac7ddefd9ce81c8c0d9">[email&#160;protected]</a>, and be able to have regular asynchronous conversations, and may engage synchronously during API office hours, or other format.</li>
</ul>

<p>I’ll stop there. I have endless more ideas of what I’d like to see in the VA developer portal, but I’m just happy to see such a clean, informative portal up and running at developer.va.gov. I’m stoked they are working on delivering real APIs that offer value to the Veteran–that is why we are doing all of this, right? I’m curious to learn about what other APIs already exist and can just be hung within this portal, as well as what APIs are planned for the immediate road map. While there are still missing parts and pieces, what they’ve published is a damn fine start to the VA developer program.

<p>Next, I’m going to do a deep dive into what I’d like to see in the getting started page, as well as the path to production guidance. I’d also like to do some deep thinking on the production application (regular) check-in and review processes. I have a short list of concepts I want to flesh out, and questions I would like to answer in future posts. I just wanted to make sure I took a moment to review <a href="https://developer.va.gov/">the VA’s hard work on their new developer portal</a>. The publishing of their developer portal marks a significant milestone in the agency’s API journey, marking the spot where their API platform is beginning to shift into a higher gear.



</p></p></p></p></p></p></p></p></p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/28/reviewing-the-department-of-veterans-affairs-new-developer-portal/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/28/not-all-companies-are-interested-in-the-interoperability-that-apis-bring/">Not All Companies Are Interested In The Interoperability That Apis Bring</a></h3>
        <span class="post-date">28 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Not All Companies Are Interested In The Interoperability That APIs Bring’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/fence_dark_dali.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/fence_dark_dali.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I’ve learned a lot in eight years of operating API Evangelist. One of the most important things I’ve learned to do is separate my personal belief and interest in technology from the realities of the business world. I’ve learned that not all businesses are ready for the change that doing APIs bring, and that many businesses really aren’t interested in the interoperability, portability, and observability that APIs bring to the table. Despite what I may believe, APIs in the real world often have a very different outcome.

<p>I see the potential of having a public API developer portal where you publish all the digital resources your company offers. Providing self-service registration to access these digital resources at a fair, transparent, and pay for what you use pricing model. I get what this can do for companies when it comes to attracting developer talent to help deliver the applications that are needed for any platform to thrive. I’ve seen the benefits to the end-users of these applications when it comes to giving them control over their data, the ability to leverage 3rd party applications, while also better understanding, managing, and ultimately owning the digital resources they generate each day. I also regularly see how this all can be a serious threat to how some businesses operate, and work to reveal the illnesses that exist within many businesses, and the shady things the occur behind the firewall each day.

<p>I regularly see businesses pay lip service to the concept of APIs, but in reality, are more about locking things up, and slowing things down to their benefit, instead of opening up access, and streamlining anything. I’m not saying that businesses do this by default, and are always being led from the top down to behave this way, I am saying it gets baked into the fabric of how teams, groups, and individuals cells in the overall organizational organism. These cells learn to resist, fight back, appear like they are on board with this latest wave of how we deliver technology, but in reality, they are not interested in the interoperability that APIs bring to the table. There is just too much power, control, and revenue to be generated by locking things up, slowing things down, and making things hard to get.

<p>After eight years of doing this, plus another 22 years of working in the industry, I’m always skeptical of people’s motivation motivation behind doing APIs. Why do you think this resources is important enough to make accessible? Who will get access to this resources? What is the price of this resource? Is pricing observable across all tiers of access? Can we talk about your SLA? Can we talk about your road map? Why are you doing APIs? Who do they benefit? There are so many questions to be asked when getting at the soul of each company’s API efforts. Before you can truly understand if a company is truly interested in the interoperability that APIs bring to the table. Before you can begin to understand what their API journey will involve. Before you understand whether or not you want to do business with a company using their API, and make it something you bake into your own operations and applications.

<p>I write about this only to remind myself that some companies will have other plans. I write about this to remind myself to ask the hard questions of all the organizations I’m engaging with, all along the way. I tend to default to a belief that most people are straight up, and share their real intentions, yet I need a regular reminder that this really isn’t true. Most successful businesses are doing aggressive, shady, and manipulative things to get ahead. The concept of creating the best product, and running a smart business, and you’ll win, is a myth. I’m not saying it doesn’t happen, or can’t happen, I am saying it isn’t the normal mode of the business world–despite popular belief. This is all a reminder that just because a business has APIs, doesn’t mean their belief system around doing APIs reflects my vision, or the popular API community vision around what doing APIs is all about.



</p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/28/not-all-companies-are-interested-in-the-interoperability-that-apis-bring/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/28/helping-the-federal-government-get-in-tune-with-their-api-uptime-and/">Helping The Federal Government Get In Tune With Their Api Uptime And</a></h3>
        <span class="post-date">28 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Helping The Federal Government Get In Tune With Their API Uptime And’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/apimetrics/api-metrics-federal-government-1.png</p>

<hr />

<p>Nobody likes to be told that their APIs are unreliable, and unavailable on a regular basis. However, it is one of those pills that ALL APIs have to swallow, and EVERY API provider should be paying for an external monitoring service to tell us when are APIs are up or down. Having a monitoring service to tell us when our APIs are having problems, complete with a status dashboard, and history of our API's availability are essential building blocks of any API provider. If you expect consumers to use your API, and bake it into their systems and applications, you should committed to a certain level of availability, and offering a service level agreement if possible.

<p>My friends over at APImetrics monitor APIs across multiple industries, but we've been partnering to keep an eye on federal government APIs, in support of my work in DC. They've recently [shared an informative dashboard tracking on the performance of federal government APIs](https://apimetrics.io/us-government-api-performance-dashboard/), providing an interesting view of the government API landscape, and the overall reliability of APIs they provide.

<p align="center"><a href="https://apimetrics.io/us-government-api-performance-dashboard/"><img src="https://s3.amazonaws.com/kinlane-productions2/apimetrics/api-metrics-federal-government-1.png" width="95%" style="padding: 5px;" /></a>

<p>They continue by breaking down the performance of federal government APIs, including how the APIs perform across multiple North American regions across four of the leading cloud providers:

<p align="center"><a href="https://apimetrics.io/us-government-api-performance-dashboard/"><img src="https://s3.amazonaws.com/kinlane-productions2/apimetrics/api-metrics-federal-government-2.png" width="95%" style="padding: 5px;" /></a>

<p>Helping us visualize the availability of federal government APIs for the last seven days, by applying their <a href="https://apimetrics.io/2017/04/21/casc-score/">APImetrics CASC score</a> to each of the federal government APIs, and ranking their overall uptime and availability:

<p align="center"><a href="https://apimetrics.io/us-government-api-performance-dashboard/"><img src="https://s3.amazonaws.com/kinlane-productions2/apimetrics/api-metrics-federal-government-3.png" width="95%" style="padding: 5px;" /></a>

<p>I know it sucks being labeled as one of the worst performing APIs, but you also have the opportunity to be named one the best performing APIs. ;-) This is a subject that many private sector companies struggle with, and the federal government has an extremely poor track record for monitoring their APIs, let alone sharing the information publicly. Facing up to this stuff sucks, and you are forced to answer some difficult questions about your operations, but it is also something can't be ignored away when you have a public API

<p>You can [view the US Government API Performance Dashboard for July 2018 over at APImetrics](https://apimetrics.io/us-government-api-performance-dashboard/). If you work for any of these agencies and would like to have a conversation your API monitoring, testing, and performance strategy, I am happy to talk. I know the APImetrics team are happy to help to, so don't stay in denial about your API performance and availability. Don't be embarrassed. Tackle the problem head on, improve your overall quality of service, and then having an API monitoring and performance dashboard publicly available like this won't hurt nearly as much--it will just be a normal part of operating an API that anyone can depend on.



</p></p></p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/28/helping-the-federal-government-get-in-tune-with-their-api-uptime-and/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/27/provide-your-api-developers-with-a-forkable-example-of-api-documentation-in/">Provide Your Api Developers With A Forkable Example Of Api Documentation In</a></h3>
        <span class="post-date">27 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Provide Your API Developers With A Forkable Example of API Documentation In’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/va-working/va-demo-swagger-ui-documentation.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/va-working/va-demo-swagger-ui-documentation.png" width="45%" align="right" style="padding: 15px;" />
<p><a href="https://apievangelist.com/2018/08/24/how-should-teams-be-documenting-their-are-new-and-legacy-apis/">I responded about how teams should be documenting their APIs when they have both legacy and new APIs the other day</a>. I wanted to keep the conversation thread going with an example of one possible API documentation implementation. The best way to deliver API documentation guidance in any organization is to provide a forkable, downloadable example of whatever you are talking about. To help illustrate what I am talking about, I wanted to take one documentation solution, and publish it as a GitHub repository.

<p>I chose to go with a simple OpenAPI 3.0 defined API contract, driving a Swagger UI driven API documentation, hosted using GitHub Pages, and <a href="https://github.com/va-working/openapi-documentation">managed as a GitHub repository</a>. In my story about how teams should be documenting their APIs, I provided several API definition formations, and API documentation options–for this walk-through I wanted to narrow it down to a single combination, providing the minimum(alist) viable options possible using OpenAPI 3.0 and SwaggerUI. Of course, any federal agency implementing such a solution should wrap the documentation with their own branding, similar to the <a href="https://gsa.github.io/prototype-city-pairs-api-documentation/api-docs/">City Pairs API prototype out of GSA</a>, which <a href="https://cfpb.github.io/api/ccdb/">originated over at CFPB</a>.

<p>I used the VA Facilities API definition from the developer.va.gov portal for this sample. Mostly because it was ready to go, and relevant to the VA efforts, but also because it was using OpenAPI 3.0–I think it is worth making sure all API documentation moving forward supports is supporting the latest version of OpenAPI. <a href="https://va-working.github.io/openapi-documentation/">The API documentation is here</a>, the <a href="https://github.com/va-working/openapi-documentation/blob/master/openapi/openapi.json">OpenAPI definition is here</a>, and the <a href="https://github.com/va-working/openapi-documentation">Github repository is here</a>, showing what is possible. There are plenty of other things I’d like to see in a baseline API documentation template, but this provides a good first draft for a true minimum viable definition.

<p>The goal with this project is to provide a basic seed that any team could use. Next, I will add in some other building blocks, and implementation a ReDoc, DapperDox, or WSDLDoc version. Providing four separate documentation examples that developers can fork and use to document the APIs they are working on. In my opinion, one or more API documentation templates like this should be available for teams to fork or download and implement within any organization. All API governance guidance like this should have the text describing the policy, as well as one or many examples of the policy being delivered. Hopefully this projects shows an example of this in action.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/27/provide-your-api-developers-with-a-forkable-example-of-api-documentation-in/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/27/how-do-we-get-api-developers-to-follow-the-minimum-viable-api-documentation/">How Do We Get Api Developers To Follow The Minimum Viable Api Documentation</a></h3>
        <span class="post-date">27 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘How Do We Get API Developers To Follow The Minimum Viable API Documentation’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/va-working/api-documentation-guidance-1.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/va-working/api-documentation-guidance-1.png" width="45%" align="right" style="padding: 15px;" />
<p>After <a href="https://apievangelist.com/2018/08/24/how-should-teams-be-documenting-their-are-new-and-legacy-apis/">providing some guidance the other day on how teams should be documenting their APIs</a>, one of the follow up comments was: “Now we just have to figure out how to get the developers to follow the guidance!” Something that any API leadership and governance team is going to face as they work to implement new policies across their organization. You can craft the best possible guidance for API design, deployment, management, and documentation, but it doesn’t mean anyone is actually going to follow your guidance.

<p>Moving forward API governance within any organization represents the cultural frontline of API operations. Getting teams to learn about, understand, and implement sensible API practices is always easier said than done. You may think your vision of the organizations API future is the right one, but getting other internal groups to buy into that vision will take a significant amount of work. It is something that will take time, resources, and be something that will always be shifting and evolving over time.

<p><strong>Lead By Example</strong><br />
The best way to get developers to follow the minimum viable API documentation guidance being set forth is to do the work for them. Provide templates and blueprints of what you want them to do. Develop, provide, and evolve forkable and downloadable API documentation examples, with simple README checklists of what is expected of them. I’ve published <a href="https://va-working.github.io/openapi-documentation/">a simple example using the VA Facilities API definition published as OpenAPI 3.0 and Swagger UI to Github Pages</a>, with <a href="https://github.com/va-working/openapi-documentation">the entire thing forkable via the Github repository</a>. It is very bare bones example of providing API documentation guidance is a package that can be reused, providing API developers with a working example of what is expected of them.

<p><strong>Make It A Group Effort</strong><br />
To help get API developers on board with the minimum viable API documentation guidance being set forth, I recommend making it a group effort. Recruit help from developers to improve upon API documentation templates provided, and encourage them to extend, evolve, and push forward their individual API documentation implementations. Give API developers a stake in how you define governance for API documentation–not everyone will be up for the task, but you’d be surprised who will raise their hand to contribute if they are asked.
<p><img src="https://s3.amazonaws.com/kinlane-productions2/va-working/api-documentation-guidance-2.png" width="45%" align="right" style="padding: 15px;" />
<p><strong>Provide Incentive Model</strong><br />
This is something that will vary in effectiveness from organization to organization, but consider offering a reward, benefit, perk, or some other incentive to any group who adopts the API documentation guidance. Provide them with praise, and showcase their work. Bring exposure to their work with leadership, and across other groups. Brainstorm creatives ways of incentivizing development groups to get more involved. Establish a list of all development groups, track on ideas for incentivizing their participation and adoption, and work regularly to close them on playing an active role in moving forward your organization’s API documentation strategy.

<p><strong>Punish And Shame Others</strong><br />
As a last resort, for the more badly behaved groups within our organizations, consider punishing and shaming them for not following API documentation guidance, and contributing to overall API governance efforts. This is definitely not something you should not consider doing lightly, and should only be used in special cases, but sometimes teams will need smaller or larger punitive responses to their inaction. Ideally, teams are influenced by praise, and positive examples of why API documentation standards matter, but there will always be situations where teams won’t get on board with the wider organizational API governance efforts, and need their knuckles rapped.

<p><strong>Making Meaningful Change Is Hard</strong><br />
It will not be easy to implement consistent API documentation across any large organization. However, API documentation is often one of the most important stops along the API lifecycle, and should receive significant investment when it comes to API governance efforts. In most situations doing the work for developers, and providing them with blueprints to be successful will accomplish the goal of getting API developers all using a common approach to API documentation. Like any other stop along the API lifecycle, delivering consistent API documentation across distributed teams will take having a coherent strategy, with regular tactical investment to move everything forward in a meaningful way. However, once you get your API documentation house in order, many other stops along the API lifecycle will also begin to fall inline.



</p></p></p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/27/how-do-we-get-api-developers-to-follow-the-minimum-viable-api-documentation/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/27/do-not-miss-internal-developer-portals-developer-engagement-behind-the/">Do Not Miss Internal Developer Portals Developer Engagement Behind The</a></h3>
        <span class="post-date">27 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Do Not Miss Internal Developer Portals: Developer Engagement Behind the’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/events/apistrat-2018/Kristof-Van-Tomme.jpeg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/events/apistrat-2018/Kristof-Van-Tomme.jpeg" width="45%" align="right" style="padding: 15px;" />
<p>We are getting closer to <a href="https://events.linuxfoundation.org/events/apistrat-2018/">the 9th edition of APIStrat happening in Nashville, TN this September 24th through 26th</a>. <a href="https://events.linuxfoundation.org/events/apistrat-2018/program/schedule/">The schedule for the conference is up</a>, along with <a href="https://events.linuxfoundation.org/events/apistrat-2018/program/keynote_speakers/">the first lineup of keynote speakers</a>, and my drumbeat of stories about the event continues here on the blog. Next up in our session lineup is <em>“Do Not Miss Internal Developer Portals: Developer Engagement Behind the Firewall”</em> by Kristof Van Tomme (@kvantomme), Pronovix (@pronovix) on September 25th.

<p>Here is Kristof’s abstract for the API session:

<p><em>While there are a lot of talks and blogposts about APIs and the importance of an APIs Developer eXperience, most are about public API products. And while a lot of the best practices for API products are also applicable to private APIs, there are significant differences in the circumstances and trade-offs they need to make.
The most important difference is probably in their budgets: as potential profit centers, API products can afford to invest a lot more money in documentation and UX driven developer portal improvements. Internal APIs rarely have that luxury.</em>

<p><em>In this talk I will explain the differences between public and private APIs, introduce upstream DX, and explain how it can improve downstream DX. Introduce experience design (a.k.a. gamification) and Innersourcing (open sourcing practices behind the firewall) and describe how they could be used on internal developer portals.</em>

<p>Kristof is an expert in delivering developer portals and API documentation, making his talk a must attend session. <a href="https://events.linuxfoundation.org/events/apistrat-2018/attend/register/">You can register for the event here</a>, and there <a href="https://events.linuxfoundation.org/events/apistrat-2018/sponsor/">are still sponsorship opportunities available</a>. Don’t miss out on APIStrat this year–it is going to be a good time in Nashville as we continue the conversation we started back in 2012 with the initial edition of the API industry event in New York City.

<p>I am looking forward to seeing you all in Nashville next month!



</p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/27/do-not-miss-internal-developer-portals-developer-engagement-behind-the/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/24/may-contain-nuts-the-case-for-api-labeling-by-erik-wilde-dret-api-academy/">May Contain Nuts The Case For Api Labeling By Erik Wilde Dret Api Academy</a></h3>
        <span class="post-date">24 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘May Contain Nuts: The Case for API Labeling by Erik Wilde (@dret), API Academy’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/events/apistrat-2018/erik-wilde.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/events/apistrat-2018/erik-wilde.jpg" width="45%" align="right" style="padding: 15px;" />
<p>We are getting closer to <a href="https://events.linuxfoundation.org/events/apistrat-2018/">the 9th edition of APIStrat happening in Nashville, TN this September 24th through 26th</a>. <a href="https://events.linuxfoundation.org/events/apistrat-2018/program/schedule/">The schedule for the conference is up</a>, along with <a href="https://events.linuxfoundation.org/events/apistrat-2018/program/keynote_speakers/">the first lineup of keynote speakers</a>, and my drumbeat of stories about the event continues here on the blog. Next up in our session lineup is <em>“<a href="http://sched.co/FTQM">May Contain Nuts: The Case for API Labeling</a>“</em> by Erik Wilde (@dret), API Academy (@apiacademy) on September 25th.

<p>I’ll let Erik’s bio set the stage for what he’ll be talking about at APIStrat:

<p><em>Erik is a frequent speaker at both industry and academia events. In his current role at the API Academy, his work revolves around API strategy, design, and management, and how to help organizations with their digital transformation. Based on his extensive background in Web architecture and technologies, Erik combines deep expertise in protocols and representations with insights into API practices at today’s organizations.</em>

<p><em>Before joining API Academy and working in the API space full-time, Erik spent time at Siemens and EMC, in both cases working at ways how APIs could be used for their internal service ecosystems, as well as for better ways for customers to use services and products. Before that, Erik spent most of his life in academia, working at UC Berkeley and ETH Zürich. Erik received his Ph.D. in computer science from ETH Zürich, and his diploma in computer science from TU Berlin.</em>

<p>Erik nows his stuff, and can be found on the road with the CA API Academy, making this stop in Nashville, TN a pretty special opportunity. <a href="https://events.linuxfoundation.org/events/apistrat-2018/attend/register/">You can register for the event here</a>, and there <a href="https://events.linuxfoundation.org/events/apistrat-2018/sponsor/">are still sponsorship opportunities available</a>. Don’t miss out on APIStrat this year–it is going to be a good time in Nashville as we continue the conversation we started back in 2012 with the initial edition of the API industry event in New York City.

<p>I am looking forward to seeing you all in Nashville next month!



</p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/24/may-contain-nuts-the-case-for-api-labeling-by-erik-wilde-dret-api-academy/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/24/living-in-a-post-facebook-twitter-and-instagram-api-world/">Living In A Post Facebook Twitter And Instagram Api World</a></h3>
        <span class="post-date">24 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Living In A Post Facebook, Twitter, and Instagram API World’</p>

<p>image: http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/23<em>19_800_500_0_max_0</em>-5_-1.jpg</p>

<hr />

<p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/23_19_800_500_0_max_0_-5_-1.jpg" width="45%" align="right" style="padding: 15px;" />
<p>While Facebook, Twitter, and Instagram will always have a place in <a href="https://history.apievangelist.com/">my history of APIs</a>, I feel like we are entering a post Facebook, Twitter, and Instagram API world. All three platforms are going through serious evolutions, which includes tightening down the controls on API access, and shuttering of many APIs. These platforms are tightening things down for a variety of reasons, which are more about their business goals, than it is about the community. I’m not saying these APIs will go away entirely, but the era where where these API platforms ruled is coming to a close.

<p><a href="http://apievangelist.com/2018/08/20/we-need-you-api-developers-until-we-have-grown-to-a-certain-size/">The other day I articulated that these platform only needed us for a while</a>, and now that they’ve grown to a certain size do not need us anymore. While this is true, I know there is more to the story of why we are moving on in the Facebook, Twitter, and Instagram story. We can’t understand the transformation that is occurring without considering that these platform’s business models are playing out, and they (we) are reaping what they’ve sown with their free and open platform business models. It isn’t so much that they are looking to screw over their developers, they are just making another decision, in a long line of decisions to keep generating revenue from their user generated realities, and advertising fueled perception.

<p>I don’t fault Twitter, Facebook, and Instagram for fully opening their APIs, then closing them off over time. I fault us consumers for falling for it. I do fault Twitter, Facebook, and Instagram a little for not managing their APIs better along the way, but when your business model is out of alignment with proper API management, it is only natural that you look the other way when bad things are happening, or you are just distracted with other priorities. This is ultimately why you should avoid platforms who don’t have an API, or a clear business model for their platform. There is a reason aren’t having this conversation about Amazon S3 after a decade. With a proper business model, and API management strategy you deal with all the riff raff early on, and along the way–it is how this API game works when you don’t operate a user-exploitative business.

<p>Ultimately, living in a post Twitter, Facebook, and Instagram API world won’t mean much. The world goes on. There will be some damage to the overall API brand, and people will point to these platforms as why you shouldn’t do APIs. Twitter, Facebook, and Instagram will still be able to squeeze lots of advertising based revenue out of their platforms. Eventually it will make them vulnerable, and they will begin to lose market share being such a closed off ecosystem, but there will always be plenty of people willing to spend money on their advertising solutions, and believe they are reaching an audience. Developers will find other data source, and APIs to use in the development of web, mobile, and device applications.

<p>The challenge will be making sure that we can spot the API platforms early on who will be using a similar playbook to Twitter, Facebook, and Instagram. Will we push back on API provides who don’t have clear business models? Will we see the potential damage of purely eyeball based, advertising fueled platform growth? Will we make sound decisions in the APIs we adopt, or will we continue to just jump on whatever bandwagon that comes along, and willfully become sharecroppers on someone else’s farm. Will we learn from this moment, and what has happened in the last decade of growth for some of the most significant API platforms across the landscape today?

<p>To help paint a proper picture of this problem, let me frame another similar situation that is not the big bad Twitter and Facebook, that everyone loves bashing on. Medium. I remember everyone telling me in 2014 that I should move API Evangelist to Medium – I kicked the tires, but they didn’t have an API. A no go for me. Eventually they launched a read only API, and I began syndicating a handful of my stories there. I enjoyed some network effect. I would have scaled upon my engagement if there was write access to APIs, as well as other platform related data like my followers. I never moved my blog to Blogger, Tumblr, Posterous, or Medium, for all the same reasons. I don’t want to be trapped on any platform, and I saw the signs early on with Medium, and managed to avoid  any frustration as they go through their current evolution.

<p>I don’t use the Facebook API for much–it just isn’t my audience. I do use Twitter for a lot. I depend on Twitter for a lot of traffic and exposure. I would say the same for LinkedIn and Github. LinkedIn has been closing off their APIs for some time, but honestly it was never something that was ever very open. I worry about Github, especially after the Microsoft acquisition. However, I went into my Github relationship expecting it to be temporary, and because all my data is in a machine readable and portable format, I’m not worried about every having to migrate–I can’t do this with Facebook, Twitter, Instagram, or LinkedIn. I’m saddened to think about a post Twitter API world, where every API call is monetized, and there is no innovation in the community. It is coming though. It will be slow. We won’t notice much of it. But, it is happening.

<p>I know that Twitter, Facebook, and Instagram all think they are making the best decision for their business, and investors. I know they also think that they’ve done the best job they could have under the circumstances over the last decade. You did, within the vision of the world you had established. You didn’t for your communities. If Facebook and Twitter had been more strict and organized about API application reviews from early days, and had structured access tiers of free, as well as paid access early on, a lot fewer people would be complaining as you made those processes, and access tiers more strict. It is just that you didn’t manage anything for so long, and once the bad things happening began effecting the platform bottom line, and worrying investors, then you began managing your API.

<p>I know that Twitter, Facebook, and Instagram all think they will be fine. They will. However, over time they will become the next NBC, AOL, or other relic of the past. They will lose their soul, if they ever had one. And everyone on the Internet will be somewhere else, giving away their digital bits for free. This same platform model will play out over and over again in different incarnations, and the real test will be if we ever care? Will we keep investing in these platforms, building out their integrations, attracting new users, and keeping them engaged. Or, will we work to strike a balance, and raise the bar which platforms we sign up for, and ultimately depend on as part of our daily lives. I’m done getting pissed off about what Twitter, Facebook, and Instagram do, I”m more focused on evaluating and ranking all the digital platforms I depend on, and turn up or down the volume, based upon the signals they send me about what the future will hold.



</p></p></p></p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/24/living-in-a-post-facebook-twitter-and-instagram-api-world/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/24/how-should-teams-be-documenting-their-apis-when-you-have-both-legacy-and-new-apis/">How Should Teams Be Documenting Their Apis When You Have Both Legacy And New Apis</a></h3>
        <span class="post-date">24 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘How Should Teams Be Documenting Their APIs When You Have Both Legacy And New APIs?’</p>

<p>image: http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/109<em>214_800_500_0_max_0_1</em>-1.jpg</p>

<hr />

<p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/109_214_800_500_0_max_0_1_-1.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I’m continuing my work to help <a href="https://developer.va.gov/">the Department of Veterans Affairs (VA) move forward their API strategy</a>. One area I’m happy to help the federal agency with, is just being available to answer  questions, which I also find make for great stories here on the blog–helping other federal agencies also learn along the way. One question I got from the agency recently, is regarding how the teams should be documenting their APIs, taking into consideration that many of them are supporting legacy services like SOAP.

<p>From my vantage point, minimum viable API documentation should always include a machine readable definition, and some autogenerated documentation within a portal at a known location. If it is a SOAP service, WSDL is the format. If it is REST, OpenAPI (fka Swagger) is the format. If its XML RPC, you can bend OpenAPI to work. If it is GraphQL, it should come with its own definitions. All of these machine readable definitions should exist within a known location, and used as the central definition for the documentation user interface. Documentation should not be hand generated anymore with the wealth of open source API documentation available.

<p>Each service should have its own GitHub/BitBucket/GitLab repository with the following:

<ul>
  <li><strong>README</strong> - Providing a concise title and description for the service, as well as links to all documentation, definitions, and other resources.</li>
  <li><strong>Definitions</strong> - Machine readable API definitions for the APIs underlying schema, and the surface area of the API.</li>
  <li><strong>Documentation</strong> - Autogenerated documentation for the API, driven by its machine readable definition.</li>
</ul>

<p>Depending on the type of API being deployed and managed, there should be one or more of these definition formats in place:

<ul>
  <li><strong>Web Services Description Language (WSDL)</strong> - The XML-based interface definition used for describing the functionality offered by the service.</li>
  <li><strong>OpenAPI</strong> - The YAML or JSON based OpenAPI specification format managed by the OpenAPI Initiative as part of the Linux Foundation.</li>
  <li><strong>JSON Schema</strong> - The vocabulary that allows for the annotation and validation of the schema for the service being offered–it is part of OpenAPI specification as well.</li>
  <li><strong>Postman Collections</strong> - JSON based specification format created and maintained by the Postman client and development environment.</li>
  <li><strong>API Blueprint</strong> - The markdown based API specification format created and maintained by the Apiary API design environment, now owned by Oracle.</li>
  <li><strong>RAML</strong> - The YAML based API specification format created and maintained by Mulesoft.</li>
</ul>

<p>Ideally, OpenAPI / JSON Schema is established as the primary format for defining the contract for each API, but teams should also be able to stick with what they were given (legacy), and run with the tools they’ve already purchased (RAML &amp; API Blueprint), and <a href="https://apimatic.io/transformer">convert between specifications using API Transformer</a>.

<p>API documentation should be published to it’s GitHub/GitLab/BitBucket repository, and hosted using one of the service static project site solutions with one of the following open source documentation:

<ul>
  <li><a href="https://swagger.io/tools/swagger-ui/"><strong>Swagger UI</strong></a> - Open source API documentation driven by OpenAPI.</li>
  <li><a href="https://rebilly.github.io/ReDoc/"><strong>ReDoc</strong></a> - Open source API documentation driven by <strong>OpenAPI</strong>.</li>
  <li><a href="https://raml.org/developers/document-your-api">RAML</a> - Open source API documentation driven by RAML.</li>
  <li><a href="http://dapperdox.io/"><strong>DapperDox</strong></a> - DapperDox is Open-Source, and provides rich, out-of-the-box, rendering of your OpenAPI specifications, seamlessly combined with your GitHub flavoured Markdown documentation, guides and diagrams.</li>
  <li><a href="https://github.com/davidluckystar/wsdldoc"><strong>wsdldoc</strong></a> - The tool can be used to generate HTML documentation out of WSDL file.</li>
</ul>

<p>There are other open source solutions available for auto-generating API documentation using the core API’s definition, but these represent some of the commonly used solutions out there today. Depending on the solution being used to deploy or manage an API, there might be built-in, ready to go options for deploying documentation based upon the OpenAPI, WSDL, RAML or other using AWS API Gateway, Mulesoft, or other existing vendor solution already in place to support API operations.

<p>Even with all this effort, a repository, with a machine readable API definition, and autogenerated documentation still doesn’t provide enough of a baseline for API teams to follow. Each API documentation should possess the following within those building blocks:

<ul>
  <li><strong>Title and Description</strong> - Provide the concise description of what an API does from the README, and make sure it is based into the APIs definition.</li>
  <li><strong>Base URL</strong> - Have the base URL, or variable representation for a base URL present in API definitions.</li>
  <li><strong>Base Path</strong> - Provide any base path that is constant across paths available for any single API.</li>
  <li><strong>Content Types</strong> - List what content types an API accepts and returns as part of its operations.</li>
  <li><strong>Paths</strong> - List all available paths for an API, with summary and descriptions, making sure the entire surface area of an API is documented.</li>
  <li><strong>Parameters</strong> - Provide details on the header, path, and query parameters used for API path being documented.</li>
  <li><strong>Body</strong> - Provide details on the schema for the body of each API path that accepts a body as part of its operations.</li>
  <li><strong>Responses</strong> - Provide HTTP status code and reference to the schema being returned for each path.</li>
  <li><strong>Examples</strong> - Provide example requests and response for each API path being documented.</li>
  <li><strong>Schema</strong> - Document all schema being used as part of requests and responses for all APIs paths being documented.</li>
  <li><strong>Authentication</strong> - Document the authentication method used (ie. Basic Auth, Keys, OAuth, JWT).</li>
</ul>

<p>If EVERY API possesses its own repository, and README to get going, guiding all API consumers to complete, up to date, and informative documentation that is auto-generated, a significant amount of friction during the on-boarding process can be eliminated. Additionally, friction at the time of hand-off for any service from on team to another, or one vendor to another, will be significantly reduced–with all relevant documentation available within the project’s repository.

<p>API documentation delivered in this way provides a single known location for any human to go when putting an API to work. It also provides a single known location to find a machine readable definition that can be used to on-board using an API client like <a href="https://www.postman.com/">Postman</a>, <a href="https://luckymarmot.com/paw">PAW</a>, or <a href="http://insomnia.rest/">Insomnia</a>. The API definition provides the contract for the API documentation, but it also provides what is needed across other stops along the API lifecycle, like monitoring, testing, SDK generation, security, and client integration–reducing the friction across many stops along the API journey.

<p>This should provide a baseline for API documentation across teams. No matter how big or small the API, or how new or old the API is. Each API should have API documentation available in a consistent, and usable way. Providing a human and programmatic way for understanding what an API does, that can be use to on-board and maintain integrations with each application. The days of PDF and static API documentation are over, and the baseline for each APIs documentation always involves having a machine readable contract as the core, and managing the documentation as part of the pipeline used to deploy and manage the rest of the API lifecycle.



</p></p></p></p></p></p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/24/how-should-teams-be-documenting-their-apis-when-you-have-both-legacy-and-new-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/23/the-importance-of-postman-api-environment-files/">The Importance Of Postman Api Environment Files</a></h3>
        <span class="post-date">23 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘The Importance Of Postman API Environment Files’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/postman/postman-environments.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/postman/postman-environments.png" width="45%" align="right" style="padding: 15px;" />
<p>I’m a big fan of <a href="https://www.postman.com/">Postman</a>, and the power of their development environment, as well as their Postman Collection format. I think their approach to not just integrating with APIs, but also enabling the development and delivery of APIs has shifted the conversation around APIs in the last couple of years–not too many API service providers accomplish this in my experience. There are several dimensions to what Postman does that I think are pushing the API conversation forward, but one that has been capturing my attention lately <a href="https://www.postman.com/docs/v6/postman/environments_and_globals/manage_environments">are Postman Environment Files</a>.

<p>Using Postman, you can manage many different environments used for working with APIs, and if you are a pro or enterprise customer, you can export a file that represents an environment, making each of these API definitions more portable and collaborative. Managing the variety of environments for the hundreds of APIs I use is one of the biggest pain points I have. Postman has significantly helped me get a handle on the tokens and keys I use across the internal, as well as partner and public APIs that I depend on each day to operate API Evangelist.

<p>Postman environments allows me to define environments within the Postman application, and then share them as part of the pro / enterprise team experience. <a href="https://docs.api.postman.com/#a237ffbe-0444-b394-a2c4-b99f691931cf">You can also manage your environments through the Postman API</a>, if you need to more deeply integrate with your operations. The Postman Environment File makes all of this portable, sharable, and used across environments. It is one of the reasons that makes Postman Collections more valuable to some users, in specific contexts, because it has that run time aspect to what it does. Postman let’s you communicate effectively around the APIs you are deploying and integrating with, and solves relevant pain points like API environment management, that can stand in the way of integration.

<p>There aren’t many features of API service providers I get very excited about, but the potential of Postman as an environment management solution is significant. If Postman is able to establish itself as the broker of credentials at the API environment level, it will give them a significant advantage of other service providers. With the size of their developer base, having visibility at the environment level puts their finger on the pulse of what is going on in the API economy, from both an API provider and consumer perspective. With Postman Environment Files acting as a sort of key, or currency, that has to exist before any API transaction can be executed. And, as the number of APIs we depend on increases, the importance of having a strategy (and solution) for managing our environment will grow exponentially–putting Postman in a pretty sweet position.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/23/the-importance-of-postman-api-environment-files/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/23/getting-email-updates-from-the-api-providers-i-care-about-is-one-way-to-stay/">Getting Email Updates From The Api Providers I Care About Is One Way To Stay</a></h3>
        <span class="post-date">23 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Getting Email Updates From The API Providers I Care About Is One Way To Stay’</p>

<p>image: http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/68<em>158_800_500_0_max_0</em>-5_-1.jpg</p>

<hr />

<p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/68_158_800_500_0_max_0_-5_-1.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I do not like email. I do not have a good relationship with my inbox. However, it is one of those ubiquitous tools I have to use, and understand the value it can bring to my world. The goal is to not let my inbox control me too much, as my it is is often a task list that other people think they can control. With all that said, I’m finding renewed value in email newsletters, on several fronts. While I’d prefer to get updates via Atom, I’m warming up to receiving updates from API providers, and API service providers in my inbox.

<p>I am an active subscriber to the <a href="https://tinyletter.com/RESTAPINotes/">REST API Notes Newsletter</a>, <a href="https://apideveloperweekly.com/">API Developer Weekly</a>, and other relative newsletters. I’m also finding myself opening up more emails from the API providers, and service providers I’m registered with. Historically, I often see email as a nuisance, but I’m beginning to see emails from the companies I’m paying attention to as a healthy signal. Increasingly, it is a signal that I’m using to understand the overall health of a platform, and yet another signal that will go silent when a platform isn’t supporting their user base, and potentially running out of funding for their operations.

<p>I recently wrote a script that harvests emails from my inbox, and tracks on the communications occurring with API providers and service providers I am monitoring. At it’s most basic, it is a heartbeat that I can use to tell when an API provider or service provider is still alive. After that, I’m looking at harvest URLs, and other data, and use the signals to float an API provider or service provider up on my list. It can be tough to remember to tune into what is going on across hundreds and thousands of APIs, and any signal I can harvest to help companies float up is a positive thing. Hopefully it is something that will also incentivize API provider and service providers to tell more stories via email, as well as their blog and social media.

<p>Email is still one of my least favorite signals out there, but I’m beginning to realize there is still a lot of value to be found within my inbox. Having a regular newsletter is something I’m going to write about more, encouraging more API providers and service providers to provide. I think more companies, institutions, and government agencies feel comfortable telling stories via email, over a public blog. It may not be my preferred medium of choice, but I know that it takes a diverse set of channels to reach a large audience, and who am I to only use the ones I like the most.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/23/getting-email-updates-from-the-api-providers-i-care-about-is-one-way-to-stay/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/23/any-way-you-want-it-extending-swagger-ui-for-fun-and-profit-by-kyle-shockey/">Any Way You Want It Extending Swagger Ui For Fun And Profit By Kyle Shockey</a></h3>
        <span class="post-date">23 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Any Way You Want It: Extending Swagger UI for Fun and Profit by Kyle Shockey’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/events/apistrat-2018/kyle-shocky-smarbear.jpeg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/events/apistrat-2018/kyle-shocky-smarbear.jpeg" width="45%" align="right" style="padding: 15px;" />
<p>We are getting closer to <a href="https://events.linuxfoundation.org/events/apistrat-2018/">the 9th edition of APIStrat happening in Nashville, TN this September 24th through 26th</a>. <a href="https://events.linuxfoundation.org/events/apistrat-2018/program/schedule/">The schedule for the conference is up</a>, along with <a href="https://events.linuxfoundation.org/events/apistrat-2018/program/keynote_speakers/">the first lineup of keynote speakers</a>, and my drumbeat of stories about the event continues here on the blog. Next up in our session lineup is <em>“Any Way You Want It: Extending Swagger UI for Fun and Profit”</em> by Kyle Shockey (@kyshoc) of SmartBear Software (@SmartBear) on September 25th.

<p>Here is Kyle’s abstract for the session:

<p><em>Your APIs are tailored to your needs - shouldn’t your tools be as well? In this talk, we’ll explore how Swagger UI 3 makes it easier than ever to create custom functionality, and common use cases for the power that the UI’s plugin system provides.</em>

<p><em>Learn how to:</em>

<p><em>- Create plugins that extend existing features and define new functionality</em>
<em>- Integrate Swagger UI seamlessly by defining a custom layout</em>
<em>- Package and share plugins that can be reused by the community (or your organization)</em>

<p>Swagger UI has changed the conversation around how we document our APIs, and being able to extend the interface is an important part of keeping the API documentation conversation evolving, and APIStrat is where this type of discussion is happening. <a href="https://events.linuxfoundation.org/events/apistrat-2018/attend/register/">You can register for the event here</a>, and there <a href="https://events.linuxfoundation.org/events/apistrat-2018/sponsor/">are still sponsorship opportunities available</a>. Don’t miss out on APIStrat this year–it is going to be a good time in Nashville as we continue the conversation we started back in 2012 with the initial edition of the API industry event in New York City.

<p>I am looking forward to seeing you all in Nashville next month!



</p></p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/23/any-way-you-want-it-extending-swagger-ui-for-fun-and-profit-by-kyle-shockey/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/22/testing-and-meaningful-mocks-in-a-microservice-system-by-laura-medalia/">Testing And Meaningful Mocks In A Microservice System By Laura Medalia</a></h3>
        <span class="post-date">22 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Testing and Meaningful Mocks in a Microservice System by Laura Medalia’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/events/apistrat-2018/laura-medalia.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/events/apistrat-2018/laura-medalia.jpg" width="45%" align="right" style="padding: 15px;" />
<p>We are getting closer to <a href="https://events.linuxfoundation.org/events/apistrat-2018/">the 9th edition of APIStrat happening in Nashville, TN this September 24th through 26th</a>. <a href="https://events.linuxfoundation.org/events/apistrat-2018/program/schedule/">The schedule for the conference is up</a>, along with <a href="https://events.linuxfoundation.org/events/apistrat-2018/program/keynote_speakers/">the first lineup of keynote speakers</a>, and my drumbeat of stories about the event continues here on the blog. Next up in our session lineup is <em>“Testing and Meaningful Mocks in a Microservice System”</em>  by Laura Medalia (@codergirl__) of Care/Of on September 25th.

<p>Here is Laura’s abstract for the session:

<p><em>Laura will be talking about tooling for mocking microservice endpoints in a meaningful way using Open API specifications. She will cover how to set up microservice deployments processes so that with each versioned microservice deployed a mock of the service with up to date contracts will also be deployed. Laura will also show how to use tooling she and her team built to consume these lightweight mocks in unit tests and either get default mock responses or mock out custom responses for different test cases.</em>

<p>In an era where many API development groups are working to move to a design and mock first approach, this session will be key to our journey, and APIStrat is where we all need to be. <a href="https://events.linuxfoundation.org/events/apistrat-2018/attend/register/">You can register for the event here</a>, and there <a href="https://events.linuxfoundation.org/events/apistrat-2018/sponsor/">are still sponsorship opportunities available</a>. Don’t miss out on APIStrat this year–it is going to be a good time in Nashville as we continue the conversation we started back in 2012 with the initial edition of the API industry event in New York City.

<p>I am looking forward to seeing you all in Nashville next month!

<p><strong>Photo Credit:</strong> <a href="https://www.pintaram.com/u/codergirl_/1660376842954382073_2082485319">Laura Medalia on Pintaram</a>.



</p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/22/testing-and-meaningful-mocks-in-a-microservice-system-by-laura-medalia/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/22/searching-for-apis-that-possess-relevant-company-information/">Searching For Apis That Possess Relevant Company Information</a></h3>
        <span class="post-date">22 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Searching For APIs That Possess Relevant Company Information’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/cityscape_copper_circuit.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/cityscape_copper_circuit.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I’m evolving <a href="http://api.gallery.streamdata.io/">the search for the Streamdata.io API Gallery</a> I’ve been working on lately. I’m looking to move the basic keywords search that searches the API name and description, as well as the API path, summary, and description using a key word or phrase, to also be about searching parameters in a meaningful way. Each of the APIs in the Streamdata.io API have an OpenAPI definition. It is how I render each of the individual API paths using Jekyll and Github Pages. These parameters give me another dimension of data in which I can index, and use as a facet in my API gallery search.

<p>I am developing different sets of vocabulary to help me search against the parameters used across APIs, with one of them being focused on company related information. I’m trying to find APIs that provide the ability to add, update, and search against company related data, content, and execute algorithms that help make sense of company resources. There is no perfect way to search for API parameters that touch on company resources, but right now I’m looking for a handful of fields: <em>company, organization, business, enterprise, agency, ticker, corporate, and employer</em>. Returning APIs that have a parameter with any of those words in the path or summary, and weighting differently if it is in the description or tags for each API path.

<p>Next, I’m also tagging each API path that has a URL field, because this will allow me to connect the dot to a company, organization, or other entity via the domain. This is all I’m trying to do, is connect the dots using the parameter structure of an API. I find that there is an important story being told at the API design layer, and API search and discovery is how we are going to bring this story out. Connecting the dots at the corporate level is just one of many interesting stories out there, just waiting to be told. Pushing forward the conversation around how we understand the corporate digital landscape, and what resources they have available.

<p><a href="http://api.gallery.streamdata.io/">You can do a basic API search at the bottom of the Streamdata.io API Gallery main page</a>. I do not have my parameter search available publicly yet. I want to spend more time refining my vocabularies, and also look at searching the request and response bodies for each path–I’m guessing this won’t be as straightforward, as parameters has been. Right now I’m immersed in understanding the words we use to design our APIs, and craft our API documentation. It is fascinating to see how people describe their resources, and how they think (or don’t think) about making these resources available to other people. OpenAPI definitions provide a fascinating way to look at how APIs are opening up access to company information, establishing the digital vocabulary for how we exchange data and content, and apply algorithms to help us better understand the business world around us.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/22/searching-for-apis-that-possess-relevant-company-information/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/22/it-is-hard-to-go-api-define-first/">It Is Hard To Go Api Define First</a></h3>
        <span class="post-date">22 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘It Is Hard To Go API Define First’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/69<em>120_800_500_0_max_0_1</em>-1.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/69_120_800_500_0_max_0_1_-1.jpg" width="45%" align="right" style="padding: 15px;" />
<p>Last year I started saying API define first, instead of API design first. In response to many of the conversations out there about designing, then mocking, and eventually deploying your APIs into a production environment. I agree that you should design and iterate before writing code, but I feel like we should be defining our APIs even before we get to the API design phase. Without the proper definitions on the table, our design phase is going to be a lot more deficient in standards, common patterns, goals, and objectives, making it important to invest some energy in defining what is happening first–then iterate on the API definitions throughout the API lifecycle, not just design.

<p>I prefer to have a handful of API definitions drafted, before I move onto to the API design phase:

<ul>
  <li><strong>Title</strong> - A simple, concise title for my API.</li>
  <li><strong>Description</strong> - A simple, concise description for my API.</li>
  <li><strong>JSON Schema</strong> - A set of JSON schema for my APIs</li>
  <li><strong>OpenAPI</strong> - An OpenAPI for the surface area of my API.</li>
  <li><strong>Assertions</strong> - A list of what my API should be delivering.</li>
  <li><strong>Standards</strong> - What standards are being applied with this API.</li>
  <li><strong>Patterns</strong> - What common web patterns will be used with this API.</li>
  <li><strong>Goals</strong> - What are the goals for this particular API.</li>
</ul>

<p>I like having all of this in a GitHub repository before I get to work, actually designing my APIs. It provides me with the base set of definitions I need to go to be as effective as I can in my API design phase. Of course, each of these definitions will be iterated, added to, and evolved as part of the API design phase, and beyond. The goal is to just get a base set of building blocks on the workbench, properly setting the tone for what my API will be doing. Grounding my API work early on in the API lifecycle, in a consistent way that I can apply across many different APIs.

<p>The problem with all of this, is that it is easier said than done. I still like to hand code my APIs. It is something I’ve been doing for 20 years, and it is a habit that is hard to kick. When designing an API, often times I do not know what is possible, and I need to hack on the solution for a while. I need to hack on and massage some data, content, or push forward my algorithm a little. All of this has to happen before I can articulate the interface will look like. Sure, I might have some basic RESTful notions about what API paths will be, and the schema I’ve gathered will drive some of the conversation, but I still need to hack together a little goodness, before I can design.

<p>This is ok. With some APIs I will be able to define and then design without ever touching any code. While others I will still have to prototype at least a function to prove the concept behind the API. Once I have the proof of concept, then I can start crafting a sensible interface using OpenAPI, then mock, and work with the concept a little more within an API design phase. Ultimately, I do not think there is any RIGHT WAY to develop an API. I think there are healthier, and less healthier ways. I think there are more hardened, and proven ways, but I also think there should be experimental, and even legacy ways of doing things. My goal is to always make sure the process is as sensible and pragmatic as it can be, while meeting the immediate, and long term business goals of my company, as well as my partners.



</p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/22/it-is-hard-to-go-api-define-first/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/21/identifying-the-different-types-of-apis/">Identifying The Different Types Of Apis</a></h3>
        <span class="post-date">21 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Identifying The Different Types Of APIs’</p>

<p>image: http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/97<em>193_800_500_0_max_0</em>-5_-1.jpg</p>

<hr />

<p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/97_193_800_500_0_max_0_-5_-1.jpg" width="45%" align="right" style="padding: 15px;" />
<p>APIs come in many shapes and sizes. Even when APIs may share a common resource, the likelihood that they are similar in functional, will be slim. Even after eight years of studying APIs, I still struggle with understanding the differences, and putting APIs into common buckets. Think of the differences between two image APIs like Flickr and Instagram, but then also think about the difference between Twitter and Twilio–the differences are many, and a challenge to articulate.

<p>I’m pushing forward <a href="http://theapistack.com/">my API Stack</a>, and <a href="http://api.gallery.streamdata.io/">API Gallery work</a>, and I’m needing to better organize APIs into meaningful groups that I can add to the search functionality for each of API discovery services. To help me establish a handful of new buckets, I’m thinking more critically about the different types of API functionality I’m coming across, establishing seven new buckets:

<ul>
  <li><strong>General Data</strong> - You can get at data across the platform, users, and resources.</li>
  <li><strong>Relative Data</strong> - You can get at data that is relative to a user, company, or specific account.</li>
  <li><strong>Static Data</strong> - The data doesn’t change too often, and will always remain fairly constant.</li>
  <li><strong>Evolving Data</strong> - The data changes on a regular basis, providing a reason to come back often.</li>
  <li><strong>Historical Data</strong> - Provides access to historical data, going back an X number of. years.</li>
  <li><strong>Service</strong> - The API is offered as a service, or is provided to extend a specific service.</li>
  <li><strong>Algorithmic</strong> - The API provides some sort of algorithmic functionality like ML, or otherwise.</li>
</ul>

<p>Understanding the type of data an API provides is important to the work I’m doing. Streamdata.io caters to the needs of financial organizations, and they are looking for data to help them with their investment portfolio, but also have very particular opinions around the type of data they want. This first version of my API type list is heavily weighted towards data, but as I evolve in my thinking, I’m guessing the service and algorithmic buckets will expand and evolve as well.

<p>The APIs I am cataloging within this work spring fit into one or many of these buckets. They are meant to transcend the resource being made available, and the provider behind the service. I want to be able to search, filter, and organize APIs across many of the usual characteristics we use to track on. I’m wanting to go beyond the obvious resource focused characteristics, and move beyond the technology being applied. I’m looking to understand what you can do with an API, and be able to stack hundreds, or thousands of similar APIs side by side, and provide a new view of the landscape.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/21/identifying-the-different-types-of-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/21/describing-your-api-with-openapi-3-0-by-anthony-eden-aeden-dnsimple/">Describing Your Api With Openapi 3 0 By Anthony Eden Aeden Dnsimple</a></h3>
        <span class="post-date">21 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Describing Your API with OpenAPI 3.0 by Anthony Eden (@aeden), DNSimple’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/events/apistrat-2018/anthony-eden-dns-simple.jpeg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/events/apistrat-2018/anthony-eden-dns-simple.jpeg" width="45%" align="right" style="padding: 15px;" />
<p>We are getting closer to <a href="https://events.linuxfoundation.org/events/apistrat-2018/">the 9th edition of APIStrat happening in Nashville, TN this September 24th through 26th</a>. <a href="https://events.linuxfoundation.org/events/apistrat-2018/program/schedule/">The schedule for the conference is up</a>, along with <a href="https://events.linuxfoundation.org/events/apistrat-2018/program/keynote_speakers/">the first lineup of keynote speakers</a>, and my drumbeat of stories about the event continues here on the blog. Next up in our session lineup is <em>“Describing Your API with OpenAPI 3.0”</em> by Anthony Eden (@aeden), DNSimple (@dnsimple) on September 25th.

<p>Here is Christian’s abstract for the session:

<p><em>For the last 10 years, DNSimple has operated a comprehensive web API for buying, connecting, and operating domain names. After hearing about OpenAPI at APIStrat 2017, we decided to describe the DNSimple API using the OpenAPI v3 specification - this is the story of why we did it, how we did it, and where we are today.</em>

<p><em>By the end of this presentation you will have the tools you’ll need to evaluate your own API and decide if implementing OpenAPI makes sense for you, and if so, how you can get started. You’ll have a better understanding of the tools available to you to help write your OpenAPI 3 definition, as well the basics on how to write your own definition for your APIs.</em>

<p>We are all still working to make the switch from OpenAPI 2.0 to 3.0, and with APIStrat being owned and operated by the OpenAPI Initiative, it will definitely be the place to have face to face discussions that influence the road map for the API specification. <a href="https://events.linuxfoundation.org/events/apistrat-2018/attend/register/">You can register for the event here</a>, and there <a href="https://events.linuxfoundation.org/events/apistrat-2018/sponsor/">are still sponsorship opportunities available</a>. Don’t miss out on APIStrat this year–it is going to be a good time in Nashville as we continue the conversation we started back in 2012 with the initial edition of the API industry event in New York City.

<p>I am looking forward to seeing you all in Nashville next month!



</p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/21/describing-your-api-with-openapi-3-0-by-anthony-eden-aeden-dnsimple/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/21/algolia-kindly-provides-a-hacker-news-search-api/">Algolia Kindly Provides A Hacker News Search Api</a></h3>
        <span class="post-date">21 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Algolia Kindly Provides A Hacker News Search API’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algolia/hacker-news-algolia-search-api.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algolia/hacker-news-algolia-search-api.png" width="45%" align="right" style="padding: 15px;" />
<p>I was working on <a href="https://serverlessrepo.aws.amazon.com/applications/arn:aws:serverlessrepo:us-east-1:879370021840:applications~StreamData-IO-Hacker-News-New">a serverless app for Streamdata.io that takes posts to Hacker News and streams them into an Amazon S3 data lake</a>, and I came across the <a href="https://hn.algolia.com/api">Algolia powered Hacker News search API</a>. After being somewhat frustrated with the simplicity of the official Hacker News API, I was pleased to find <a href="https://hn.algolia.com/api">the search kindly provided by Algolia</a>.

<p>There is no search API available for the core Hacker News API, and the design leaves a lot to be desired, so the simplicity of Algolia’s API solution was refreshing. There is a lot of data flowing into Hacker News on a regular day, so providing a search API is pretty critical. Additionally,  Algolia’s ability to deliver such a simple, usable, yet powerful API on top of a relevant data source like Hacker News demonstrates the utility of what Algolia offers as a search solution–something I wanted to take a moment to point out here on the blog.

<p>I consider search to be an essential ingredient for any API. Every API should have a search element to their stack, allowing the indexing and searching of all API resources through a single path. Making Algolia a relevant API service provider in this area, enabling API providers to outsource the indexing and searching of their resources, and the delivery of a dead simple API for your consumers to tap into. This path forward is probably not for every API, as many weave specialized search throughout their API design, but for teams who are lacking in resources, and can afford to outsource this element–Algolia makes sense.

<p>Seeing Algolia in action, for a specific API I was integrating with helped bring their service front and center for me. I tend to showcase Elastic for deploying API search solutions, but it is a good to receive a regular reminder that Algolia does the same thing as a service. Their work on the Hacker News Search API provides a good example of they can do for you–sure, we can all build our own search solutions, but honestly, do you have the time? I’ll make sure and regularly highlight what Algolia is doing as part of <a href="http://search.apievangelist.com">my search API research</a>, and thanks Algolia! I really appreciate what you did for <a href="https://hn.algolia.com/api">the Hacker News API</a>, it made my work a lot easier.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/21/algolia-kindly-provides-a-hacker-news-search-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/20/we-need-you-api-developers-until-we-have-grown-to-a-certain-size/">We Need You Api Developers Until We Have Grown To A Certain Size</a></h3>
        <span class="post-date">20 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘We Need You API Developers Until We Have Grown To A Certain Size’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/castle-on-hill-edinburgh_feed_people.JPG</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/castle-on-hill-edinburgh_feed_people.JPG" width="45%" align="right" style="padding: 15px;" />
<p>I’m watching several fronts along the API landscape evolve right now, with large API providers shifting, shutting down, and changing how they do business with their APIs–now that they don’t need their ecosystem of developers as much. It is easy to point the finger at Facebook, Twitter, Google, and the other giants, but it really is a wider systemic, business of APIs illness that will continue to negatively impact the API universe. While this behavior is very prominent with the leading API providers right now, it is something we’ll see various waves of it’s influence on the tone of the entire API sector further on down the road.

<p>How API providers treat their consumers vary from industry to industry, and is different depending on the types of resources being made available. However, the issue of API providers treating their developers differently when they are just getting started versus once they grow to a certain size, is something that will continue to plague all areas of the API space. Companies change as they mature, and their priorities will no doubt evolve, but almost all will feel compelled to exploit developers early on so that they can grow their numbers–taking advantage of the goodwill and curiosity of the developer personality.

<p>The polarization of the API management layer is difficult to predict from the outside. I want to help new API providers out early on, but after eight years of doing this, and seeing many API providers and service providers evolve, and go away–I am left very skeptical of EVERYONE. I think many developers in the API space are feeling the same, and are weary of kicking the tires on new APIs, unless they have a specific project and purpose. I think the days of developers working for free within an API ecosystem are over. It is a concept that will still exist in some form, but along with each wave of new API startups taking advantage of this reality, then tightening things down later on down the road, eventually developers will change their behavior in response.

<p>The lesson is that if a API doesn’t have a clear business model early on–steer clear of their services. We can’t fault Twitter for working to monetize their platform now, and make their investors happy. We just should have seen it in the cards back in 2008. We can’t fault Facebook for working to please their shareholders, and protect their warehouse of inventory (Facebook Users) from malicious ecosystem players, we just should have seen this coming back in 2008. We can’t fault Google Maps for raising the prices on their digital assets developed by us giving them our data for the last decade, we should have know this would happen back in 2006. The business and politics of APIs is less straightforward than the technology of APIs is, and as we all know, the technology is a mess.

<p>I will keep calling out the offenses of API providers when feel strongly enough, but I’ve ranted about Facebook, Twitter, and Google a lot in the past. I feel like we should be skeptical of ALL API providers at this point, and assume the worst about them all–new or old. Expect them to change the prices down the road. Expect them to turn off the most valuable resources once we’ve helped them grow and mature them into a prized digital asset. This is how some companies will continue to think they can make their investments grow, and all of as API providers and consumers need to remain skeptical about our role in this process. Always making sure we remember that most API providers will no longer need us developers once they have grown to a certain size.



</p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/20/we-need-you-api-developers-until-we-have-grown-to-a-certain-size/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/20/the-redirect-url-to-confirm-selling-your-api-in-aws-marketplace-provides-us-with-a-positive/">The Redirect Url To Confirm Selling Your Api In Aws Marketplace Provides Us With A Positive</a></h3>
        <span class="post-date">20 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘The Redirect URL To Confirm Selling Your API In AWS Marketplace Provides Us With A Positive’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/amazon/aws-marketplace-aws-saas-seller-integration-guide.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/amazon/aws-marketplace-aws-saas-seller-integration-guide.png" width="45%" align="right" style="padding: 15px;" />
<p>I am setting up different APIs using the AWS API Gateway and then publishing them to the AWS Marketplace, as part of <a href="http://streamdata.io">my work with Streamdata.io</a>. I’m getting a feel for what the process is all about, and how small I can distill an API product to be, as part of the AWS Marketplace process. My goal is to be able to quickly define APIs using OpenAPI, then publish them to AWS API Gateway, and leverage the gateway to help me manage the entire business of the service from signup to discovery.

<p>As I was adding one of my first couple of APIs to the AWS Marketplace, and I found the instructions regarding the redirect URL for each API to be a good template. Each individual API service I’m offering will have its own subscription confirmation URL with the AWS API marketplace, with the relevant variables present I will need to scale the technical and business of delivering my APIs:

<ul>
  <li><strong>AWS Marketplace Confirmation Redirect URL:</strong> https://YOUR_DEVELOPER_PORTAL_API_ID.execute-api.[REGION].amazonaws.com/prod/marketplace-confirm/[USAGE_PLAN_ID]</li>
</ul>

<p>This URL is what my retail customers will be given after they click to subscribe to one of my APIs. Each API has its specific portal (present in URL), as well as being associated with a specific API usage plan within AWS API Gateway. Also notice that they have a variable for region, allowing me to deliver services by region, and scale up the technical side of delivering the various APIs I’m deploying–another important consideration when delivering reliable and performant services.

<p>Pointing out the URL for a signup process might seem like a small thing. However, because of AWS’s first mover advantage in the cloud, and their experience as an API pioneer, I feel like they are in a unique position to be defining the business layer of delivering APIs at scale in the cloud. The business opportunities available at the API management and marketplace layers within the AWS ecosystem are untapped, and represent the next generation of API management that is baked into the cloud. It’s an interesting place to be delivering and integrating with APIs, at this stage of the API economy.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/20/the-redirect-url-to-confirm-selling-your-api-in-aws-marketplace-provides-us-with-a-positive/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/20/microserviceing-like-a-unicorn-with-envoy-istio-kubernetes-with-christian/">Microserviceing Like A Unicorn With Envoy Istio Kubernetes With Christian</a></h3>
        <span class="post-date">20 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Microserviceing Like a Unicorn With Envoy, Istio &amp; Kubernetes With Christian’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/events/apistrat-2018/the-hardest-part-about-microserv.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/events/apistrat-2018/the-hardest-part-about-microserv.jpg" width="45%" align="right" style="padding: 15px;" />
<p>We are getting closer to <a href="https://events.linuxfoundation.org/events/apistrat-2018/">the 9th edition of APIStrat happening in Nashville, TN this September 24th through 26th</a>. <a href="https://events.linuxfoundation.org/events/apistrat-2018/program/schedule/">The schedule for the conference is up</a>, along with <a href="https://events.linuxfoundation.org/events/apistrat-2018/program/keynote_speakers/">the first lineup of keynote speakers</a>, and my drumbeat of stories about the event continues here on the blog. Next up in our session lineup is <em>“Microservice’ing Like a Unicorn With Envoy, Istio and Kubernetes”</em> by Christian Posta (@christianposta), Red Hat @RedHat on September 25th.

<p>Here is Christian’s abstract for the session:

<p><em>The exciting parts of APIs, unfortunately, happen when services actually try communicating and working together to accomplish some business function. The service-mesh approach has emerged to help make service communication boring. In particular, a project named Istio.io has garnered attention in the open-source community as a way of implementing the service mesh capabilities. These capabilities include pushing application-networking concerns down into the infrastructure: things like retries, load balancing, timeouts, deadlines, circuit breaking, mutual TLS, service discovery, distributed tracing and others.</em>

<p><em>As Istio becomes more popular and widely used, we’re going to see a lot of people put it into production for their API use cases. This talk will walk attendees through the Istio architecture, and more importantly, help them understand how it all works.</em>

<p>API delivery, integration, orchestration, and development of mesh networks all represent the next generation of doing APIs, and APIStrat is where we are having these discussions. <a href="https://events.linuxfoundation.org/events/apistrat-2018/attend/register/">You can register for the event here</a>, and there <a href="https://events.linuxfoundation.org/events/apistrat-2018/sponsor/">are still sponsorship opportunities available</a>. Don’t miss out on APIStrat this year–it is going to be a good time in Nashville as we continue the conversation we started back in 2012 with the initial edition of the API industry event in New York City.

<p>I am looking forward to seeing you all in Nashville next month!



</p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/20/microserviceing-like-a-unicorn-with-envoy-istio-kubernetes-with-christian/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/20/having-an-api-deprecation-page-like-evrythng-does/">Having An Api Deprecation Page Like Evrythng Does</a></h3>
        <span class="post-date">20 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Having An API Deprecation Page Like EVRYTHNG Does’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/catacombs_blue_circuit_3.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/catacombs_blue_circuit_3.jpg" width="45%" align="right" style="padding: 15px;" />
<p>The API providers I talk to regularly are rarely proactive when it comes to addressing API deprecation. Most API providers aren’t thinking about shutting down any service they deliver until they’ve actually encountered the need down the road. Many just begin their API journey, assuming their APIs will be a success, and they will have to support every version forever. However, once they encounter what it will take to support older versions, they begin to change their tune, something that often comes as an unexpected surprise to consumers.

<p>Shutting down old APIs will never be easy, but the process can be made easier with a little proactive communication. One example of this practice in action can be found over at <a href="https://developers.evrythng.com/docs/deprecation">the Internet of Things provider Evrythng, with their API deprecation page</a>. Which provides a pretty simple layout for an API deprecation page, beyond just a title and description of what the future might hold.

<p><em>API Status</em>

<p><em>The following status labels are applicable to APIs, features, or SDK versions depending on their current support status:</em>

<p>_- Preview - May change at any time.
<ul>
  <li>Stable - Fully released and stable. Will not change at short notice.</li>
  <li>Deprecated - No longer supported (and may have been replaced), and may be removed in the future at an announced date. Use not encouraged.</li>
  <li>Removed - Removed, and no longer supported or available._</li>
</ul>

<p><em>Communication</em>

<p><em>When a deprecation is announced, the details and any relevant migration information will be available on the following channels:</em>

<p>_- The Developer Blog.
<ul>
  <li>The @evrythngdev Twitter account.</li>
  <li>The relevant feature page on the Developer Hub.</li>
  <li>Enterprise customers may receive information by email to their specified EVRYTHNG contact, if applicable._</li>
</ul>

<p><em>Customers using one our SLAs can read <a href="https://evrythng.com/legal/sla/">the General section</a> for more information.</em>

<p>Evrythng provides us with some important building blocks for using as part of our overall API deprecation strategy. Something that EVERY API provide should be considering as they prepare to launch a new API. API deprecation shouldn’t be an afterthought, and if we communicate open and honestly about it from the beginning, our API consumers will be more forgiving. Surprising consumers down the road is the quickest way to piss people off, and get the tech blogosphere publishing those pitchfork and torches type of stories we see so often about API providers.

<p>I just finished sharing some API deprecation awareness as part of the API strategy for a federal government agency–they just hadn’t had an open discussion about it. API deprecation is one of those uncomfortable subjects, we all have to get used to discussing early on, whether we like it or not. It is something that will always be difficult, and leave some API consumers unhappy, but if done well, it can help reduce a lot of friction. You can visit <a href="http://deprecation.apievangelist.com/">my API deprecation research and storytelling</a> for more examples of how to do it right, and how you can avoid doing it wrong–bringing this important subject out into the open.



</p></p></p></p></p></p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/20/having-an-api-deprecation-page-like-evrythng-does/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/13/it-isnt-just-that-you-have-a-pdf-for-your-api-docs-it-is-because-it-demonstrates-that-you-do-not-use-other-apis/">It Isnt Just That You Have A Pdf For Your Api Docs It Is Because It Demonstrates That You Do Not Use Other Apis</a></h3>
        <span class="post-date">13 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘“It Isnt Just That You Have A PDF For Your API Docs, It Is Because It Demonstrates That You Do Not Use Other APIs”’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/pdf-everywhere.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/pdf-everywhere.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I look at a lot of APIs. I can tell a lot about a company, and the people behind an API from looking at their developer portal, documentation, and other building blocks of their presence. One of the more egregious sins I feel an API provider can make when operating their API is publishing their API documentation as a PDF. This is something that was acceptable up until about 2006, but over a decade after it shows that the organization behind an API hasn’t done their homework.

<p>The crime really isn’t the fact that an API provider is using a PDF for their documentation. I’m fine with API providers publishing a PDF version of their API, to provide a portable version of it. Where a PDF version of the documentation becomes a problem is when it is the primary version of the documentation, which demonstrates that the creators don’t get out much and haven’t used many other APIs. If an API team has done their homework, actually put other 3rd party APIs to work, they would know that PDF documentation for APIs is not the norm out in the real world.

<p>One of the strongest characteristics an API provider can possess, is an awareness of what other API providers are doing. The leading API providers demonstrate that they’ve used other APIs, and are aware of what API consumers in the mainstream are used to. Most mainstream API consumers will simply close the tab when they encounter an API that has a PDF document for their API. Unless you have some sort of mandate to use that particular API, you are going to look elsewhere. If an API provider isn’t up to speed on what the norms are for API documentation, and are outwardly facing, the chance they’ll actively support their API is always diminished.

<p>PDF API documentation may not seem like too big of a mistake to many enterprise, institutional, and government API providers, but it demonstrates much more than just a static representation of what an API can do. It represents an isolated, self-contained, non-interactive view of what an API can do. It reflects an API platform that is self-centered, and not really concerned with the outside world. Which often means it is an API platform that won’t always care about you, the API consumer. APIs in the age of the web are all about having an externalized view of the world, and understanding how to play nicely with large groups of developers outside of your firewall–when you publish a PDF version of your API docs, you demonstrate that you don’t get out much, and aren’t concerned with the outside world.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/13/it-isnt-just-that-you-have-a-pdf-for-your-api-docs-it-is-because-it-demonstrates-that-you-do-not-use-other-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/13/bringing-discovery-within-data-api-marketplaces-out-into-the-open/">Bringing Discovery Within Data Api Marketplaces Out Into The Open</a></h3>
        <span class="post-date">13 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Bringing Discovery Within Data API Marketplaces Out Into The Open’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/granaryfield_dali_three.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/granaryfield_dali_three.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I spend time reviewing each wave of data API marketplaces as they emerge on the landscape every couple of years. There are a number of reasons why these data marketplaces exist, ranging from supporting government agencies, NGOs, or for commercial purposes. One of the most common elements of API-driven data marketplaces that frustrates me is when they don’t do the hard work to expose the meta data around the databases, datasets, spreadsheets, and the raw data they are providing access to–making it very difficult to actually discover anything of interest.

<p>You can see a couple examples of this with <a href="https://docs.mlab.com/data-api/#list-databases">mLab</a>, <a href="http://apps.who.int/gho/data/node.resources.api">World Health Organization</a>, <a href="https://apidocs.data.world/api">Data.World</a>, and others. While these platforms provide (sometimes) impressive ability to manage data stores, but they don’t always do a good job exposing the meta data of their catalogs as part of the available APIs. Dynamically generating API endpoints, documentation, and other resources based upon the data that is being published to their platforms. Leaving developers to do the digging, and making the investment to understand what is available on a platform.

<p>Some of the platforms I encounter obfuscate their data metadata on purpose, requiring developers to qualified before they get access to valuable resources. Most I think, just do not put themselves in the position of an API consumer who lands on their developer page, and doesn’t know anything about an API. They understand the database, and the API, so it all makes sense to them, and they don’t have any empathy for anyone else who isn’t in the know. Which is a common trait of database centered people who speak in acronyms, and schema that they assume other people know, and do not spend much time thinking outside of that bubble.

<p>I could make a career out of deploying APIs on top of other data marketplace APIs. Autogenerating a more accessible, indexable, intuitive layer on top of what they’ve already deployed. I regularly find a wealth of data that is accessible through an API interface, but will most likely never be found by anyone. Before most developers will ever make the investment to onboard with an API, they need to understand what valuable resources are available. I can imagine many developers stumble across these data marketplaces, spend about 15 minutes looking around, maybe sign up for a key, but then give up because of the overhead involved with actually understanding what data is actually available.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/13/bringing-discovery-within-data-api-marketplaces-out-into-the-open/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/07/working-with-my-openapi-definitions-in-an-api-editor-helps-stabilize-them/">Working With My Openapi Definitions In An Api Editor Helps Stabilize Them</a></h3>
        <span class="post-date">07 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Working With My OpenAPI Definitions In An API Editor Helps Stabilize Them’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/openapi/swagger-editor-screenshot.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/openapi/swagger-editor-screenshot.png" width="45%" align="right" style="padding: 15px;" />
<p>I’m deploying three new APIs right now, using a new experimental serverless approach I’m evolving. One is a location API, another providing API access to companies, and the third involves working with patents. I will be evolving these three simple web APIs to meet the specific needs of some applications I’m building, but then I will also be selling retail and wholesale access to each API once they’ve matured enough. With all three APIs of these APIs, I began with a simple JSON schema from the data source, which I used to generate three rough OpenAPI definitions that will acts the contract seed for my three services.

<p>Once I had three separate OpenAPI contracts for the services I was delivering, I wanted to spend some time hand designing each of the APIs before I imported into AWS API Gateway, generating Lambda functions, loading in Postman, and used to support other stops along the API lifecycle. I still use <a href="https://editor.swagger.io/">a localized version of Swagger Editor for my OpenAPI design space</a>, but I’m working to migrate to <a href="https://mermade.github.io/openapi-gui/">OpenAPI-GUI</a> as soon as I can. I still very much enjoy the side by side design experience in Swagger Editor, but I want to push forward the GUI side of the conversation, while still retaining quick access to the RAW OpenAPI for editing.

<p>One of the reasons why I still use Swagger Editor is because of the schema validation it does behind the scenes. Which is one of the reasons I need to learn more about <a href="http://speccy.io/">Speccy</a>, as it is going to help me decouple validation from my editor, and all me to use it as part of my wider governance strategy, not just at design time. However, for now I am highly dependent on my OpenAPI editor helping me standardize and stabilize my OpenAPI definitions, before I use them along other stops along the API lifecycle. These three APIs I’m developing are going straight to deployment, because they are simple datasets, where I’m the only consumer (for now), but I still need to make sure my API contract is solid before I move to other stops along the API lifecycle.

<p>Right now, loading up an OpenAPI in Swagger Editor is the best sanity check I have. Not just making sure everything validates, but also making sure it is all coherent, and renders into something that will make sense to anyone reviewing the contract. Once I’ve spend some time polishing the rough corners of an OpenAPI, adding summary, descriptions, tags, and other detail, I feel like I can begin using to generate mocks, deploy in a gateway, and begin managing the access to each API, as well as the documentation, testing, monitoring, and other stops using the OpenAPI contract. Making this manual stop in the evolution of my APIs a pretty critical one for helping me stabilize each API’s definition before I move on. Eventually, I’d like to automate the validation and governance of my APIs at scale, but for now I’m happy just getting a handle on it as part of this API design stop along my life cycle.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/07/working-with-my-openapi-definitions-in-an-api-editor-helps-stabilize-them/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/07/twice-the-dose-of-vanick-digital-at-apistrat-in-nashville-tn-next-month/">Twice The Dose Of Vanick Digital At Apistrat In Nashville Tn Next Month</a></h3>
        <span class="post-date">07 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Twice The Dose Of Vanick Digital At APIStrat in Nashville, TN Next Month’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/vanick-digital/apistrat-2018-vanick-digital.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/vanick-digital/apistrat-2018-vanick-digital.png" width="45%" align="right" style="padding: 15px;" />
<p>We are kicking it into overdrive now that <a href="https://events.linuxfoundation.org/events/apistrat-2018/program/schedule/">the schedule is up for APIStrat in Nashville, TN this September 24th through 26th</a>. From now until the event at the end of September you are going to hear me talk about all the amazing speakers we have, the companies they work for, and the interesting things they are all doing with APIs. One of the perks of being a speaker or a sponsor at APIStrat–you get coverage on API Evangelist, a become part of the buzz around <a href="https://events.linuxfoundation.org/events/apistrat-2018/program/schedule/">the 9th edition of the API Strategy &amp; Practice Conference (APIStrat)</a>, now operated by the <a href="https://www.openapis.org/">OpenAPI Initiative (OAI)</a> and <a href="https://www.linuxfoundation.org/">the Linux Foundation</a>.

<p>Today’s post is about my friends over at <a href="https://www.vanick.com/">the digital solutions and API management agency Vanick Digital</a>. With APIStrat coming to their backyard, and their ability to capture the attention of the APIStrat program committee, Vanick Digital has two separate talks this year:

<ul>
  <li>
    <p><strong>Securing the Full API Stack by Patrick Chipman</strong> - APIs open up new channels for sharing and consuming data, but whenever you open a new channel, new security risks emerge. Additionally, APIs often involve a variety of new components, such as API gateways, in-memory databases, edge caches, facade layers, and microservice-aligned data stores that can complicate the security landscape. How and where do you apply the right controls to ensure your API and your data are secure? In this session, we’ll answer that question by identifying the different components commonly used in the delivery of API products. For each layer, we’ll discuss the security risks that can and should be mitigated there, along with best practice approaches (including ABAC, OAuth2, and more) to implement those mitigations.
  &lt;/li&gt;
  <li>
    <p><strong>What Do You Mean By “API as a Product”? by Lou Powell</strong> - You may have heard the term “API Product.” But what does it mean? In this talk I will introduce the concept and explain the benefits and challenges of transforming your organization to view your APIs as measurable products that expose your companies capabilities, creating agility, autonomy, and acceleration. Traditional product manufacturers create new product and launch them into the marketplace and then measure value; we will teach you to view your APIs in the same way. Concepts covered in this presentation will be designing APIs with Design Thinking, funding your product, building teams, marketing your API, managing your marketplace, and measuring success.
  &lt;/li&gt;
&lt;/ul&gt;

<p>Showcasing their skills as an API focused agency, by bringing it to the stage at APIStrat–smart! I am currently working with their team to understand how API Evangelist and Vanick Digital can work more closely together on projects. Helping me support the customers I’m reaching with my storytelling and workshops, delivering, scaling, and managing the day to day details I don’t have the time to provide for my customers. So it makes me happy to see them at APIStrat, sharing their wisdom, and demonstrating what they are capable of. If you are under resourced like many API providers are, I recommend coming to APIStrat and meeting with the team, or if it can’t wait until September, <a href="https://www.vanick.com/">feel free to reach out directly</a>–just let them know where you found them.

<p>APIStrat is seven weeks away, <a href="https://events.linuxfoundation.org/events/apistrat-2018/attend/register-2/">so make sure you get registered</a>. The workshop, session, and keynote lineup is locked up, but we still have a handful of sponsorship opportunities available. <a href="https://events.linuxfoundation.org/events/apistrat-2018/sponsor/">You can find the sponsorship prospectus on the web site</a>, or feel free to contact me directly and I’ll get you plugged in with the events team. Make sure you don’t miss out on an opportunity to be part of this ongoing API conversation that we’ve kept going since 2013–where API developers, architects, designers, and API business leaders, evangelists, advocates, and the API curious gather to discuss where the API space is headed. Now that APIStrat is operated by <a href="https://www.openapis.org/">the OpenAPI Initiative</a> it makes it the place to be if you want to contribute to the road map for the OpenAPI specification, and influence the direction of the API specification. No matter how you choose to get involved, we look forward to seeing you all in Nashville next month!



</p></p></p></li></p></li></ul></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/07/twice-the-dose-of-vanick-digital-at-apistrat-in-nashville-tn-next-month/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/07/i-am-speaking-in-washington-d-c-at-the-blue-button-2-0-developer-conference/">I Am Speaking In Washington D C  At The Blue Button 2 0 Developer Conference</a></h3>
        <span class="post-date">07 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘I Am Speaking In Washington D.C. At The Blue Button 2.0 Developer Conference’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/federal-government/blue-button/blue-button-api-docs.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/blue-button/blue-button-api-docs.png" width="45%" align="right" style="padding: 15px;" />
<p><a href="https://bluebutton.cms.gov/bb2dc18/">I’m heading to Washington D.C. this Monday to speak on the API life cycle as part of the Blue Button 2.0 Developer Conference</a>. We’ll be coming together in the Eisenhower Executive Office Building, within the west wing complex of the White House, to better understand how we can, “bring together developers to learn and share insights on how we can leverage claims data to serve the Medicare population.”

<p>The gathering will hear from CMS Administrator Seema Verma and other Administrator Leadership about Blue Button 2.0 and the MyHealthEData initiative, while also hosting a series of break sessions, which I’m part of:

<ul>
  <li><strong>Blue Button 2.0 and FHIR</strong> (where it’s all heading) with Mark Scrimshire and Cat Greim</li>
  <li><strong>MyHealthEData and Interoperability</strong> with Alex Mugge and Joy Day</li>
  <li><strong>Overview of Medicare Claims Data</strong> with Karl Davis</li>
  <li><strong>Medicare Beneficiary User Research</strong> with Allyssa Allen</li>
  <li><strong>Sync for Science</strong> with Josh Mandel and Andrew Bjonnes</li>
  <li><strong>API Design</strong> with Kin Lane</li>
</ul>

<p>Registration for the gathering is now closed, but if you are a federal govy, I’m sure you can find someone to get you in. I’m looking forward to seeing the CMS, HHS, and USDS folks again, as they are doing some amazing stuff with the Blue Button API, as well as hang out with some of the VA people I know will be there. The Blue Button API is one of the more important API blueprints we have out there in the healthcare space, as well as the federal government. I’ve been a champion on Blue Button since I contributed to the project back when I worked in DC back in 2013, and will continue to invest in its success in coming years.

<p>In my session I will be covering my API lifecycle and governance research as the API Evangelist, but I’m eager to talk with more folks involved with the Blue Button API about what is next, and better understand where <a href="https://www.hl7.org/fhir/">HL7 FHIR</a> is headed, while also developing my awareness of who is actively participating in the Blue Button API community. I’ll be in DC late Sunday night, through Monday, and I’m back to west coast on Tuesday AM. If you are around I’d love to connect, and if you want to tune in, <a href="https://bluebutton.cms.gov/bb2dc18/">I believe there will be a live stream of the event on the Blue Button API portal</a>.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/07/i-am-speaking-in-washington-d-c-at-the-blue-button-2-0-developer-conference/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/06/sap-and-being-late-to-the-api-game/">Sap And Being Late To The Api Game</a></h3>
        <span class="post-date">06 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘SAP And Being Late To The API Game’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/events/SAP/P5160110.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/events/SAP/P5160110.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I’ve been having regular meetings with the SAPI API team lately, talking through their presence in the API space, and throwing out ideas for what the future might hold. This isn’t a paid engagement, it is just something I’m interested in investing in on my own, but like many other API service providers in the space, we are exploring what partnership opportunities might there might be. Last week, <a href="http://apievangelist.com/2018/07/26/kicking-the-tires-on-the-sap-api-business-hub/">I wrote about their API Business Hub</a>, which I’ll keep exploring, but first I wanted to address perceptions in the industry that SAP is a little late to the game, when it comes to going all in on APIs.

<p>For me, the SAP API journey begins in 2010, when I left my role as VP of Technology at WebEvents Global, who runs all of SAP Events. By 2010, I had found success in my role by scaling up infrastructure using the AWS cloud, which was orchestrated using APIs, and I was beginning to get a taste of where things were going when it came to delivering digital resources to mobile applications using APIs. I helped lead the technology around SAPPHIRE, SAP’s flagship conference, as well as many other lesser events, and meetings. I had also gotten in trouble by SAP IT leadership for using the cloud, which they labeled a “hobby toy”, and told me I shouldn’t be using. However, I was delivering applications more quickly and cost effectively, so my leadership really couldn’t dismiss my usage of the cloud, and more specifically web APIs. Convincing me that web APIs were going to be more successful than the current web services strategy I was working under.

<p>After seeing the success I was having using APIs to deliver SAP events, and better understanding the potential for delivering global infrastructure in the cloud, and via the increasingly ubiquitous mobile devices we had in our pockets, I left my position and started API Evangelist–eight years later, I’m still doing it. The biggest difference between now and then, is back in 2010 I had to spend a lot of time explaining what is an API, and why someone should be doing it. In 2018, I don’t have to do that, people get it, and most conversations center around how to do APIs right. Everything was a lot more work earlier on, and while there is still so much work to be done, at least now I can get down to business, focusing on the challenges that large enterprise organizations face when doing APIs successfully at scale, and less about having to convince that APIs are a thing in the first place.

<p><img src="https://s3.amazonaws.com/kinlane-productions2/events/SAP/P5160117.jpg" width="45%" align="right" style="padding: 15px;" />
<p>When you talk to some people in the API space, they consider SAP a little late to the game as one of the leaders in the software industry. SalesForce, Amazon, Google, Microsoft all saw the signs early on, and IBM, Oracle, CA, HP, and others jumped on the bandwagon along the way, with SAP only joining the conversation in the last couple of years. There are plenty of SAP APIs, and a growing number of API focused services, but SAP just isn’t a player you hear about on a regular basis across the API sector. IBM, CA, and even Oracle have invested significant amounts into their presence in the API space, acquisitions of API talent and solutions, and overall mindshare when you think about APIs. While SAP has a significant amount of work ahead of them to turn up the volume on what they are doing with APIs, they couldn’t have picked a better time to step it up–with so many enterprise organizations finally realizing they need to go API first, SAP will see more return on every dollar they invest into their API operations.

<p>Now is the perfect time to be entering the API game if you are enterprise API service provider. The last decade of growth in the API sector has been established on the backs of many failed and successful API startups, and things are finally beginning to mature, with so many other enterprise organizations looking for direction when it comes to their API strategy. Financial, healthcare, government, and other mainstream sectors are trying to make sense of their digital assets, and APIs are key to this evolution. We are all trying to figure this out, at an individual, professional, and business entity level. In my opinion, any company looking to do APIs, or also sell services to the API sector, should be open and transparent about where they are in their API journey. Do not be embarrassed about being late to the party, not having everything figured out and in place, because this is the exact same position everyone else is at. Sure, there are some organizations that are further along in this journey, but 95% of the mainstream business world is just getting started in this is area.

<p>I’ve been doing API Evangelist for 8 years straight, and I’m still figuring things out. APIs are not a destination, they are a journey. Do not sweat being late for anything, just get to work investing in your journey. Get to work designing, developing, and operating the best in class APIs as possible, putting them to work internally, across your partner landscape, and make them publicly available when it makes sense. If you are selling your services to other API providers, get to work showing why your solutions matter, and are something they should be adopting. In both scenarios, get to work telling the story of your journey in real time. Be honest about how you got here, and where you are going. Tell your story regularly on and offline, and help provide a platform for your API consumers to tell their story. Do this for many year, and eventually, your brand, products, services, and personality will become more common in the ever expanding and evolving world of APIs.



</p></p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/06/sap-and-being-late-to-the-api-game/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/06/making-sure-my-api-dependencies-include-data-provenance/">Making Sure My Api Dependencies Include Data Provenance</a></h3>
        <span class="post-date">06 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Making Sure My API Dependencies Include Data Provenance’</p>

<p>image: http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/23<em>19_800_500_0_max_0</em>-5_-1.jpg</p>

<hr />

<p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/23_19_800_500_0_max_0_-5_-1.jpg" width="45%" align="right" style="padding: 15px;" />
<p><a href="http://locations.apievangelist.com/#Updates">I am publishing a new API for locations</a>. I am tired of needing some of the same location based resources across projects, and not having a simple, standardized API I can depend on. So I got to work finding the most accurate and complete data set I could find of cities, regions, and countries. I settled on using the complete, and easy to use <a href="https://github.com/prograhammer/countries-regions-cities">countries-regions-cities</a> project by <a href="https://github.com/prograhammer">David Graham</a>–providing a straightforward SQL script I can use as the seed for my locations API database.

<p>After crafting an API for this database using AWS API Gateway and Lambda, and working my way down <a href="https://apievangelist.com/2018/07/12/my-api-lifecycle-checklist-and-scorecard/">my API checklist</a>, it occurred to me that I wanted to include David Graham’s work as one of the project dependencies. Giving him attribution, while honestly acknowledging my project’s dependency on the data he provided. I’m working hard to include all dependencies within each of the microservices that I’m publishing, being mindful of every data, code, and human dependency that exists behind each service I deliver. Even if I don’t rely on regular updates from them, I still want to acknowledge their contribution, and consider attribution as one layer of my API dependency discussion.

<p>Having a dependency section of my API checklist has helped me evolve how I think about defining the dependencies my services have. I initially began tracking all other services that my microservices were dependent on, but then I quickly began adding details about the other software, data, and people the service depends on as well. I’m also pulling together a machine readable definition for tracking on my microservice dependencies. It will be something I include in the API discovery (APIs.json) document for each service, alongside the OpenAPI, and other specifications. Allowing me to track on the dependencies (and attribution) for all of my APIs, and API related artifacts that I am producing on a regular basis. Providing data provenance for each of my services, documenting the origins of all the data I’m using across my services, and making accessible via an API.

<p>For me, having the data provenance behind each service provides me with a nice clean inventory of all my suppliers. Understanding the data, services, open source code, and people I depend on to deliver a service is important to helping me make sense of my operations. For the people behind the data, services, and open source code I depend on it helps provide attribution, and showcase their valuable contribution to the services I offer. For partner and 3rd party consumers of my services, being observable about the dependencies that exist behind a service they are depending on, helps them make much more educated decisions around which services they put to work, and bake into their applications and systems. In the end, everyone is better off if I invest in data provenance as part of my wider API dependency efforts.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/06/making-sure-my-api-dependencies-include-data-provenance/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/08/06/automating-inequality-and-apis/">Automating Inequality And Apis</a></h3>
        <span class="post-date">06 Aug 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Automating Inequality (and APIs)’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/events/apistrat-2018/apistrat-virginia.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/events/apistrat-2018/apistrat-virginia.png" width="45%" align="right" style="padding: 15px;" />
<p><em>As we prepare for <a href="https://events.linuxfoundation.org/events/apistrat-2018/program/">APIStrat in Nashville, TN this September 24th through 26th</a>, I asked my partner in crime Audrey Watters (<a href="https://twitter.com/audreywatters">@audreywatters</a>) to write a post on the significance of Virginia Eubanks, the author of Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor keynoting the conference–she shared this story, of why her work is so significant, and why it is important for the API community to tune in.</em>

<p>Repeatable tasks can and should be automated – that’s an assertion that you’ll hear all the time in computing.

<p>Sometimes the rationale is efficiency – it’s cheaper, faster, “labor-saving.” Automation will free up time; it will make our lives easier. Or so we’re told.

<p>Sometimes automation is encouraged in order to eliminate human error or bias.

<p>Increasingly, automation is eliminating human decision-making altogether. And in doing so, let’s be clear, neither bias nor error are removed; rather they are often re-inscribed.  Automation – <em>algorithmic</em> decision-making – can obscure error; it can obscure bias.

<p>This push for more automated decision-making works hand-in-hand with the push for more data collection, itself a process that is already shaped by precedent and by politics. And all this, of course, is facilitated by APIs.

<p>APIs are commonly referred to as a “glue” of sorts – the implication, more often than not, is that APIs are simply a neutral technology holding larger technical systems together. But none of this is neutral – not the APIs and not the algorithms and not the databases.

<p>These technologies are never neutral in their design, development, or implementation. The systems that technologies exist in – organizationally, economically, politically, culturally – are never neutral either.

<p>It seems imperative that those building digital technologies begin to think much more critically about the implications of their work, recognizing that the existing inequalities in the analog systems are readily being ported to the digital sphere.

<p>This makes the work of one of the keynote speakers at this fall’s API Strategy and Practice conference so particularly timely: Virginia Eubanks is a political science professor at the University of Albany, SUNY and the author of Automating Inequality: How High-Tech Tools Profile, Police, and Punish the Poor. The book is a powerful work of ethnography, chronicling the ways in which data mining, predictive modeling, and algorithmic decision-making reproduce and even exacerbate inequalities in housing, health care, and social welfare services. “The digital poorhouse” Eubanks calls it.

<p>“When we talk about the technologies that mediate our interactions with public agencies today,” she writes, “we tend to focus on their innovative qualities, the ways they break with convention. Their biggest fans call them ‘disruptors,’ arguing that they shake up old relations of power, producing government that is more transparent, responsive, efficient, even inherently more democratic.” This argument overlooks the ways in which new technologies are necessarily entangled in old systems of power. Moreover, those building these technologies benefit from a privilege that both shields them from and blinds them to the ramifications of their work on those most marginalized politically and economically.

<p>Without a purposeful effort to address systemic inequalities, technologies will only make things worse. APIs will only make things worse. Instead, we must be part of the work of rethinking these old systems, listening to those on the margins, and reorienting our technological practices towards equity and justice.



</p></p></p></p></p></p></p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/08/06/automating-inequality-and-apis/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/07/31/we-need-your-help-moving-the-asyncapi-specification-forward/">We Need Your Help Moving The Asyncapi Specification Forward</a></h3>
        <span class="post-date">31 Jul 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘We Need Your Help Moving The AsyncAPI Specification Forward’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/asyncapi/asyncapi-define-your-message-driven-apis.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/asyncapi/asyncapi-define-your-message-driven-apis.png" width="45%" align="right" style="padding: 15px;" />
<p>We need your help moving <a href="https://www.asyncapi.com/">the AsyncAPI specification</a> forward. Ok, first, <a href="https://github.com/asyncapi/asyncapi">what is the AsyncAPI specification</a>? “The AsyncAPI Specification is a project used to describe and document Asynchronous APIs. The AsyncAPI Specification defines a set of files required to describe such an API. These files can then be used to create utilities, such as documentation, integration and/or testing tools.” AsyncAPI is a sister specification <a href="https://www.openapis.org/">to OpenAPI</a>, but instead of describing the request and response HTTP API landscape, AsyncAPI is describing the message, topic, event, and streaming API landscape across the HTTP and TCP landscape. It is how we are going to continue to ensure there is machine readable descriptions of this portion of the API landscape, for use in tooling and services.

<p>My friend Fran Mendez (@fmvilas) is the creator and maintainer of the specification, and he is doing way too much of the work on this important specification and he needs our help. Here is Fran’s request for our help to contribute:

<blockquote>
  <p><em>AsyncAPI is an open source project that’s currently maintained by me, with no company or funds behind. More and more companies are using AsyncAPI and the work needed is becoming too much work for a single person working in his spare time. E.g., for each release of the specification, tooling and documentation should be updated. One could argue that I should be dedicating full time to the project, but it’s in this point where it’s too much for spare time and very little to get enough money to live. I want to keep everything for free, because I firmly believe that engineering must be democratized. Also, don’t get me wrong, this is not a complaint. I’m going to continue running the project either with or without contributors, because I love it. This is just a call-out to you, the AsyncAPI lover. I’d be very grateful if you could lend a hand, or even raise your hand and become a co-maintainer. Up to you 😊</em>
<em>On the other hand, I only have good words for all of you who use and/or contribute to the project. Without you, it would be just another crazy idea from another crazy developer 😄</em>
<em>Thank you very much! 🙌</em>
<em>– Fran Mendez</em>
&lt;/blockquote&gt;

<p>When it comes to contributing to the AsyncAPI, <a href="https://github.com/asyncapi/contribute/blob/master/README.md">Fran has laid out some pretty clear ways in which he needs our help</a>, providing a range of options for you to pitch in and help, depending on what your skills are, and the bandwidth you have in your day.

<p><strong>1. The specification</strong>
There is always work to do in the spec. It goes from fixing typos to writing and reviewing new proposals. I try to keep releases small, to give time to tooling authors to update their software. If you want to start contributing, take a look at https://github.com/asyncapi/asyncapi/issues, pick one, and start working on it. It’s always a good idea to leave a comment in the issue saying that you’re going to work on it, just so other people know about it.

<p><strong>2. Tooling</strong>
As developers, this is sometimes the most straightforward way to contribute. Adding features to the existing tools or creating new ones if needed. Examples of tools are:

<ul>
  <li>Code generators (multiple languages):
    <ul>
      <li>https://github.com/asyncapi/generator</li>
      <li>https://github.com/asyncapi/node-codegen (going to be deprecated soon in favor of https://github.com/asyncapi/generator)</li>
    </ul>
  </li>
  <li>Documentation generators (multiple formats):
    <ul>
      <li>https://github.com/asyncapi/generator</li>
      <li>https://github.com/asyncapi/docgen (going to be deprecated soon in favor of https://github.com/asyncapi/generator)</li>
      <li>https://github.com/Mermade/widdershins</li>
      <li>https://github.com/asyncapi/asyncapi-node</li>
      <li>https://github.com/asyncapi/editor</li>
    </ul>
  </li>
  <li>Validation CLI tool (nobody implemented it yet)</li>
  <li>API mocking (nobody implemented it yet)</li>
  <li>API gateways (nobody implemented it yet)</li>
</ul>

<p>As always, usually the best way to contribute is to pick an issue and chat about it before you create a pull request.

<p><strong>3. Evangelizing</strong>
Sometimes the best way to help a project like AsyncAPI is to simply talk about it. It can be inside your company, in a technology meetup or speaking at a conference. I’ll be happy to help with whatever material you need to create or with arguments to convince your colleagues that using AsyncAPI is a good idea 😊

<p><strong>4. Documentation</strong>
Oh! documentation! We’re trying to convince people that documenting your message-driven APIs is a good idea, but we lack documentation, especially in tooling. This is often a task nobody wants to do, but the best way to get great knowledge about a technology is to write documentation about it. It doesn’t need to be rewriting the whole documentation from scratch, but just identifying the questions you had when started using it and document them.

<p><strong>5. Tutorials</strong>
We learn by examples. It’s a fact. Write tutorials on how to use AsyncAPI in your blog, Medium, etc. As always, count on me if you need ideas or help while writing or reviewing.

<p><strong>6. Stories</strong>
You have a blog and write about the technology you use? Writing about success stories, how-to’s, etc., really helps people to find the project and decide whether they should bet on AsyncAPI or not.

<p><strong>7. Podcasts/Videos</strong>
You have a Youtube channel or your own podcast? Talk about AsyncAPI. Tutorials, interviews, informal chats, discussions, panels, etc. I’ll be happy to help with any material you need or finding the right person for your interview.

<p><img src="https://s3.amazonaws.com/kinlane-productions2/asyncapi/asyncapi-example-streetlight.png" width="45%" align="right" style="padding: 15px;" />
<p>I’m going to take the liberty and add an 8th option, because I’m so straightforward when it comes to this game, and I know where Fran needs help.

<p><strong>8. Money <script type="math/tex"></script></strong>
AsyncAPI needs investment to help push forward, allowing Fran to carve out time, work on tooling, and pay for travel expenses when it comes to attending events and getting the word out about what it does. There is no legal entity setup for AsyncAPI, but I’m sure with the right partner(s) behind it, we can make something happen. Step up.

<p>AsyncAPI is important. We all need to jump in and help. I’ve been investing as many cycles as I can in helping learn about the specification, and tell stories about why it is important. I’ve been working hard to learn more about it so I can contribute to the roadmap. I’m using it as one of the key definition formats driving <a href="http://api.gallery.streamdata.io/">my Streamdata.io API Gallery work</a>, which is all driven using APIs.json, OpenAPI, and provides Postman Collections as well as AsyncAPI definitions when a message, topic, event, or streaming API is present. AsyncAPI is where OpenAPI (Swagger) was in 2011/2012, and with more investment, and a couple more years of adoption and maturing, it will be just as important for working with the evolving API landscape as OpenAPI and Postman Collections are.

<p>If you want to get involved with AsyncAPI, feel free to reach out to me. I’m happy to help you get up to speed on why it is so important. I’m happy to help you understand how it can be applied, and where it fits in with your API infrastructure. You are also welcome to just dive in, as <a href="https://github.com/asyncapi/">Fran has done an amazing job of making sure everything is available in the Github organization for the project</a>, where you can submit pull requests, and issues regarding whatever you are working on and contributing. Thanks for your help in making AsyncAPI evolve, and something that will continue to help us understand, quantify, and communicate about the diverse API landscape.



</p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></blockquote></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/07/31/we-need-your-help-moving-the-asyncapi-specification-forward/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/07/31/practical-secdevops-for-apis-from-42crunch-at-apistrat-in-nashville-this-fall/">Practical Secdevops For Apis From 42crunch At Apistrat In Nashville This Fall</a></h3>
        <span class="post-date">31 Jul 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Practical SecDevOps for APIs From @42Crunch At APIStrat In Nashville This Fall’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/42crunch/42-crunch-the-api-security-platform-for-the-enterprise.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/42crunch/42-crunch-the-api-security-platform-for-the-enterprise.png" width="45%" align="right" style="padding: 15px;" />
<p>We are gearing up for <a href="https://events.linuxfoundation.org/events/apistrat-2018/">the next edition of APIStrat in Nashville, TN this September 24th through 26th</a>. With the conference less than two months away, and <a href="https://events.linuxfoundation.org/events/apistrat-2018/program/schedule/">the schedule up</a>, I’m building momentum with my usual drumbeat about the speakers, and companies involved. So you’ll be reading a lot of stories related to APIStrat in coming weeks, where I’m looking to build awareness and attendance of the conference, but more importantly showcasing the individuals and companies who are supporting it and helping making the 9th edition of APIStrat amazing.

<p>One of innovative startups I’m partnering with right now, and who you will find speaking and sponsoring APIStrat is <a href="https://www.42crunch.com/">42Crunch</a>. Full disclosure, I’m regularly talking with 42Crunch regarding their road map, and I consider them an API Evangelist partner, however, this is because I find them to be one of the more progressive, and important API startups out there right now. 42Crunch is important in my opinion, because they are focusing on API security, a critical stop along the API lifecycle, and also because of the OpenAPI-driven, awareness building approach to delivering API security. 42Crunch isn’t just bringing API security solutions to the table for you to purchase as a service, they bring API security solutions to the table that help you invest in your internal API security practices–which is the most critical aspect of what they do in my opinion.

<p>42Crunch is focused on API security, but they are what I consider to be a full API lifecycle solution. Meaning they play nicely as one of the tools in your API lifecycle toolbox. Which begins with being OpenAPI-driven, and treating your API’s definition as a contract, but with 42Crunch it is about using this contract to empower your API team to make API security a first-class citizen across all stops along the API lifecycle. Not just at the API management layer, or as an after thought later on when you scan your infrastructure–42Crunch is baking security into your OpenAPI contract, proxying your APIs, and ensuring the right security policies are being applied consistently across the API lifecycle. Helping you think of API security all along the way from design to deployment, to management, testing, monitoring, and deprecation.

<p>If you want to learn more about what 42Crunch offers you should be registered for APIStrat, and joining us in Nashville, TN. My friend Isabelle Mauny will be giving her talk on <a href="https://events.linuxfoundation.org/events/apistrat-2018/program/schedule/">Practical SecDevOps for APIs</a>–here is the abstract for her talk: <em>In an ever agile world, API security must become a commodity. By working with security “ON” as early as possible, API developers can detect vulnerabilities when they are easy to fix. By continuously testing APIs for issues, they can ensure vulnerabilities do not sneak in later in the lifecycle. In this session, Isabelle presents a SecDevOps methodology and shares practical solutions for API security assessment, API protection and security monitoring.</em> You should be taking advantage of this opportunity to learn more about what 42Crunch has to offer, and speaking with Isabelle in person about where API security is going in 2018, because they are the people pushing forward the conversation as part of <a href="https://www.openapis.org/">the OpenAPI Initiative,</a> and with their API security services.

<p>Make sure you don’t miss out on the API conversation in Nashville this fall. <a href="https://events.linuxfoundation.org/events/apistrat-2018/attend/register-2/">Get registered for APIStrat</a>, and make sure you are participating in the ongoing API discussion that is APIStrat. While the conference will continue to be an environment where developers and business folk to gather and discuss the technology, business, and politics of APIs, this year will also have a music focus because of the venue. Making it something you will not want to miss out, with all the keynotes, sessions, workshops, and hallway and late night conversations with all API leaders from across the sector.



</p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/07/31/practical-secdevops-for-apis-from-42crunch-at-apistrat-in-nashville-this-fall/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/07/27/the-service-level-agreement-sla-definition-for-the-openapi-specification/">The Service Level Agreement Sla Definition For The Openapi Specification</a></h3>
        <span class="post-date">27 Jul 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘The Service Level Agreement (SLA) Definition For The OpenAPI Specification’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/legalstatue_light_dali.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/legalstatue_light_dali.jpg" width="45%" align="right" style="padding: 15px;" />
<p><a href="https://github.com/isa-group/SLA4OAI-Specification">I’m currently learning more about SLA4OAI, an open source standard for describing SLA in APIs</a>, which is based on the standards proposed by the OAI, adding an optional profile for defining SLA (Service Level Agreements) for APIs. “This SLA definition in a neutral vendor flavor will allow to foster innovation in the area where APIs expose and documents its SLA, API Management tools can import and measure such key metrics and composed SLAs for composed services aggregated way in a standard way.” Providing not just a needed standard for the API sector, but more importantly one that is built on top of an existing standard.

<p>SLA4OAI, provides an interesting way to define the SLA for any API, providing a set of objects that augment and can be paired with an OpenAPI definition using an x-sla vendor extension:

<ul>
  <li><strong><a href="https://github.com/isa-group/SLA4OAI-Specification/blob/master/Specification.md#522-contextobject">Context</a></strong> - Holds the main information of the SLA context.</li>
  <li><strong><a href="https://github.com/isa-group/SLA4OAI-Specification/blob/master/Specification.md#524-infrastructureobject">Infrastructure</a></strong> - Required Provides information about tooling used for SLA storage, calculation, governance, etc.</li>
  <li><strong><a href="https://github.com/isa-group/SLA4OAI-Specification/blob/master/Specification.md#525-pricingobject">Pricing</a></strong> - Global pricing data.</li>
  <li><strong><a href="https://github.com/isa-group/SLA4OAI-Specification/blob/master/Specification.md#526-metricsobject">Metrics</a></strong> - A list of metrics to use in the context of the SLA.</li>
  <li><strong><a href="https://github.com/isa-group/SLA4OAI-Specification/blob/master/Specification.md#528-plansobject">Plans</a></strong> - A set of plans to define different service levels per plan.</li>
  <li><strong><a href="https://github.com/isa-group/SLA4OAI-Specification/blob/master/Specification.md#5210-quotasobject">Quotas</a></strong> - Global quotas, these are the default quotas, but they could be overridden by each plan later.</li>
  <li><strong><a href="https://github.com/isa-group/SLA4OAI-Specification/blob/master/Specification.md#5211-ratesobject">Rates</a></strong> - Global rates, these are the default rates, but they could be overridden by each plan later.</li>
  <li><strong><a href="https://github.com/isa-group/SLA4OAI-Specification/blob/master/Specification.md#5212-guaranteesobject">Guarantees</a></strong> - Global guarantees, these are the default guarantees, but they could be overridden by each plan later.</li>
  <li><strong><a href="https://github.com/isa-group/SLA4OAI-Specification/blob/master/Specification.md#5218-configurationsobject">Configuration</a></strong> - Define the default configurations, later each plan can be override it.</li>
</ul>

<p>These objects provide all the details you will need to quantify the SLA for any OpenAPI defined API. In order to validate each of the SLAs, a separate Basic SLA Management Services is provided to implement the services that control, manage, report and track SLAs. Providing the reporting output you will need to understand whether or not each individual API is meeting its SLA. Providing a universal SLA format that can be used to create SLA templates, applied as individual API SLAs, and then leveraging a common schema for reporting on SLA monitoring, which can actually be used in conjunction with the API management layer of your API operations.

<p>I’m still getting familiar with the specification, but I’m impressed with what I’ve seen so far. There is a lot of detail available in there, and it provides all the context that will be needed to quantify, measure, and report upon API SLAs. My only critique at this point is that I feel the pricing, metrics, plans, and quotas elements should be broken out into a separate specification so that they can be used out of the context of just SLA management, and as part of the wider API management strategy. There are plenty of scenarios where you will want to be using these elements, but not in the context of SLA enforcement (ie. driving pricing and plan pages, billing and invoicing, and API discovery). Other than than, I’m pretty impressed with the work present in the specification.

<p>It makes me happy to see sister specifications emerge like this, rather than also baked directly into the OpenAPI. The way they’ve referenced the SLA from the OpenAPI definition, and the OpenAPI from the SLA definition is how these things should occur, in my opinion. It has been something I’ve done for years with <a href="http://apisjson.org/">the APIs.json format</a>, having seen a future where there are many sister or even competing specifications all working in unison. Which is why I feel the pricing and plan elements should be decoupled from the SLA object here. However, I think this is a great start to something that we are going to need if we expect this whole API thing to actually work at scale, and make sure the technology of APIs is in sync with the business of APIs. Without it, there will always been a gulf between the technology and business units, much like we’ve seen historically between IT and business groups across enterprise organizations today.



</p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/07/27/the-service-level-agreement-sla-definition-for-the-openapi-specification/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/07/26/thinking-deeply-about-other-people-using-your-api-is-the-most-valuable-lesson/">Thinking Deeply About Other People Using Your Api Is The Most Valuable Lesson</a></h3>
        <span class="post-date">26 Jul 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Thinking Deeply About Other People Using Your API Is The Most Valuable Lesson’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/federal-government/uspto/uspto-peds-api.png</p>

<hr />

<p>I am deploying a patent review API for a client, using data from <a href="https://ped.uspto.gov/peds/">the Patent Examination Data System (PEDS)</a>. You can download complete JSON or XML data from the United States Patent Office (USPTO), and they even have an API. So, why would I be launching yet another API? Well, because what they have is so cryptic, complex, and lacking in any schema or API design, there is value in me pushing the API conversation forward a bit by thinking deeply about how other people will potentially be using these resources–something the USPTO clearly hasn’t done.

<p>The <a href="https://ped.uspto.gov/api/swagger-ui.html">USPTO PEDS API</a> (that is more acronyms than you can shake a stick at) is a great example of how much database people, and developers take for granted as they operate within their little bubbles, without much concern for how the rest of the world views their work–take a look at the screenshot of thee USPTO PEDS API.

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/uspto/uspto-peds-api.png" align="center" width="90%" />

<p>There is only one telling sign on this page regarding what this API does–the email address for the contact, which has a uspto.gov address. Beyond that there is not a single sign of the resources available within this API, or the value they bring to the table. Even if you can extrapolate that this is a patent API, there is nothing to tell you that you can’t actually get patent data from this, you can only get meta data about the patents, reviewers, inventors, and the activity around the patent. For me, the API reflects many of the challenges developers and database people face when it comes to thinking out of their box, and effectively communicating with external consumers–which is the whole reason we do web APIs.

<p>I’m pretty well versed consuming patent data, and it took me several hours to get up to speed with this set of resources. I opted to not deal with the API, which is just an ElasticSearch index on top of a patent file store, and went directly to the full-size zipped up download. Something the average user will not have the knowledge, skills, and resources to always do. Which is why I feel there is value in me investing some schema and API design cycles into making the USPTO PEDS API a little more coherent, accessible, and usable by a wider audience using a simple web API. Moving it beyond the realm of wizards (database and developers), and making it something normal people, say patent attorneys, and other business folk can put to use in their work.

<p>The USTPO PEDS API reflects the divide between tech and business people. Some database people and developers will think the implementation is a good one, because it gives them the full download, as well as a customizable, ElasticSearch interface for querying what you want. Many though, will walk away, because they aren’t willing to make the several hour investment getting up to speed on the schema, so they can make their first API query, or load the full download into the database backend of their choosing. This is where an investment on API design begins to pay dividends, is reaching this wider audience of potential consumers who are unwilling to make the investment getting up to speed, or do not have the resources or knowledge to work with the full download or an ElasticSearch interface. Unless of course, your in the business of keeping data out of the hands of these people, which many folks are.

<p>I am a 30 year database professional. I get databases and querying solutions. What many GraphQL and ElasticSearch believers get wrong when they rely on these solutions for delivering publicly available APIs, is that they are unwilling to come to terms with the fact they can’t see their resources through the eyes of the public. They think everyone is like them, and thus want a full blown query interface to get at a known schema. They see API design as unnecessary work, when in reality, they are just unwilling to do the heavy lifting, and they are either consciously, or unconsciously passing that work off to each individual consumer. If you are keeping your APIs available for internal use amongst controlled group of developers this isn’t a problem, but if you are making your APIs available to a wider public audience, it ends up showing that you haven’t taken the time to think deeply about how others will be using your APIs, or that you just doo not care.



</p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/07/26/thinking-deeply-about-other-people-using-your-api-is-the-most-valuable-lesson/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/07/26/monolithic-serverless-wtf/">Monolithic Serverless Wtf</a></h3>
        <span class="post-date">26 Jul 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Monolithic Serverless? WTF?’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/statue-face-open-mouth_blue_circuit_5.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/statue-face-open-mouth_blue_circuit_5.png" width="45%" align="right" style="padding: 15px;" />
<p>While writing about <a href="http://apievangelist.com/2018/07/09/concerns-around-managing-many-microservice-repositories-and-going-with-a-mono-repo/">the discussions I’ve been having with folks around using monorepos to manage microservices</a>, <a href="https://medium.com/statics-and-dynamics/should-i-use-a-single-monolithic-lambda-function-or-multiple-lambda-functions-with-api-gateway-d99b0230f1e7">I came across this post about whether or not people should be using a single monolithic Lambda function or multiple lambda functions with the AWS API Gateway</a>. Again, surprising me about how lazy people are, and how difficult it is for people to think about things in a decoupled way. Which I think is the reason many people will go back to doing monolithic applications, and fail at microservices, not because it technically won’t work, it is just because it will be perceived as more work, and with a lack of imagination around how to work in a distributed way, people will give up.

<p>First, I do not think microservices is a good idea for all applications. Second, I don’t always subscribe to microservices meaning small or micro. I think a service mindset is good, and it is healthy to decouple, and reduce the surface area of your services, minimizing dependencies, but there are many situations where a super small microservice will be a bad idea. However, if you are going to do serverless microservices with Lambda and AWS API Gateway, I do not understand why you’d want a single monolithic function behind many different API paths. I’m guessing that people who think you should do monolithic serverless haven’t thought about sensible organization of their functions, and orchestration of them using the AWS CLI or API. They are managing them through the AWS dashboard and are thinking, <em>“man this is a lot of work, let’s just do a single function, with the routing built in.”</em>

<p>Similar to folks thinking a monorepo is a good idea over many different repos, without ever thinking about organizations using Github organizations, and orchestration using Git and the Github API, people aren’t getting creative with their Lamdba functions. People seem to be in love with brainstorming and dreaming about decoupled approaches to doing APIs, but when it comes to the hard work of actually doing it, and having an imagination when it comes to orchestration and reducing friction, people would rather just give up. I’m not 100% sold on serverless being the right use case for driving APIs, but I can tell you one thing, that having many different APIs with a single Lambda function behind it will not give you the granularity you need for understanding the performance, and functionality behind each API and service you are delivering–you are just going to create new problems that you won’t have the visibility into to be able to optimize.

<p>I’m reading a lot about microservices backlash lately. I’m guessing after about 1-2 more years of serverless, we will start seeing serverless backlash. While some of this backlash will be about folks using microservices and serverless for use cases that didn’t make sense, I’m guessing a significant amount will be because people can’t decouple their imagination, and think through the necessary organization and orchestration required to think about doing distributed applications at scale. Without it, they are going to fumble, struggle, and see decoupling as all about making extra work for themselves, and go back to the way they were doing things before. In my experience these folks are always on the hunt for easy solutions to their complex problems, and when you aren’t willing to invest the time into doing it right, and properly understanding all the moving parts, you are going to fail, and revert to what you know. The problem with this, is I’m guessing you are going to also fall prey to the next trend, and not have the capacity to understand what it is all about before going all in, yet again.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/07/26/monolithic-serverless-wtf/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/07/26/kicking-the-tires-on-the-sap-api-business-hub/">Kicking The Tires On The Sap Api Business Hub</a></h3>
        <span class="post-date">26 Jul 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Kicking The Tires On The SAP API Business Hub’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/sap/sap-api-business-hub.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/sap/sap-api-business-hub.png" align="right" width="45%" style="padding: 15px;" />
<p>I told the folks over at SAP that I would take a look at their <a href="https://api.sap.com">API Business Hub</a>. It isn’t paid work, just helping provide feedback on another addition to the <a href="http://discovery.apievangelist.com">API discovery</a> front, something I’m pretty committed to helping push forward in any way that I can. They’ve pulled together a pretty clean, OpenAPI driven catalog of useful APIs for the enterprise, so I wanted to make sure I kick the tires and size it up alongside the other API discovery work I am doing.

<p><a href="https://api.sap.com/themes/APICONTEN">The SAP API Business Hub is a pretty simple and clean catalog for searching and browsing applications, integrations, as well as APIs</a>–I am going to focus in on the API section. Which at first glance looks to have about 70 separate APIs, but then you notice each of them are just umbrellas for each API platform, and some of them contain many different API endpoints. Some of the APIs are simple language translation and text extraction resources, while others provide robust access to the SAP S/4HANA Cloud, SAP Ariba, and other SAP systems. You see a lot of SAP focused solutions, but then <a href="https://api.sap.com/themes/PartnerContent">you also see a handful of partner solutions</a> added via their platform partner program.

<p>I see the beginnings of a useful API catalog getting going on over at the SAP API Business Hub. Each API is well documented, and provides an OpenAPI definition for each API, complete with interactive documentation you can play within a sandbox environment. More than most API catalogs, marketplaces, and directories I profile have available. Allowing you to kick the tires and see what is going on, before working with the production version. They also provide you with a Java SDK to download for each API, something that could easily be expanded to support many different platforms, programming languages, and continuous integration cycles <a href="http://apimatic.io">with solutions like APIMATIC</a>. Making it more of a discovery, as well as integration marketplace.

<p>Like any API marketplace effort, SAP needs to drum up activity within their catalog. They need more partners signing up to add their APIs, as well as consumers being made aware of the resources published there–something that takes a lot of work, evangelism, and storytelling. Next, I’m going to go through their partner signup and see what I can do to add some of my API resources there, and tell some stories about how they might be able to improve upon the partner flow. I like that their marketplace is OpenAPI driven. I’m curious about how much of the API publishing process is machine readable, allowing API providers to easily add their resources, without a lot of manual form work–something most are going to not have the time and resources for. I’ll keep evaluating how the SAP API Business Hub overlaps with my other API discovery work on <a href="http://theapistack.com">the API Stack</a>, the <a href="http://api.gallery.streamdata.io/">Streamdata.io API Gallery</a>, <a href="https://www.postman.com/api-network/">Postman Network</a>, and partnerships with <a href="https://apis.guru/">APIs.guru</a>, <a href="http://apis.io">APIs.io</a>, and others–continuing to push forward the API discovery conversation after almost 8 years.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/07/26/kicking-the-tires-on-the-sap-api-business-hub/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/07/26/avoid-being-captain-obvious-when-documenting-your-api/">Avoid Being Captain Obvious When Documenting Your Api</a></h3>
        <span class="post-date">26 Jul 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Avoid Being Captain Obvious When Documenting Your API’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/captain_small.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/captain_small.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I read a lot of API documentation, and help review API portals for clients, and one of the most common rookie mistakes I see made, is people pointing out the obvious, and writing a bunch of fluffy, meaningless content that gets in the way of people actually using an API. When the obvious API industry stuff is combined with the assumed elements of what a company does, you end up with a meaningless set of obstacles that slow API integration down. Here is the most common thing I read when entering an API portal:

<p><em>“This is an API for querying data from the [Company X] platform, to get access to JSON from our system which allows you to get data from our system into yours using the web. You will need to write code to make calls to our APIs documented here on the page below. Our API uses REST to accept request and provide responses in a JSON format.”</em>

<p>I’ve read API after API that never tells you what the API does. It just assumes you know what the company does, and then goes into verbose explanations of what API, REST, JSON, and other things that should be intuitive if an API is well designed, and immediately accessible via an API. People tend to make to many assumptions about API consumers already knowing what a company does, while also assuming they known absolutely nothing about APIs, and burying actual API documentation behind a bunch of API blah blah blah, instead of just doing and being the API.

<p>It is another side effect of developers, database, and IT folk not being very good at thinking outside of their bubble. It goes beyond techies not having social skills, and is more about them not having to think about other people at all. They just don’t have the ability to put themselves in the shoes of someone landing on the home page of their developer portal, and not knowing anything about the company or the API, and asking themselves, “what does this person need?”. Which I get being something developers don’t think about with internal APIs, but publishing an API publicly, and not stepping back to think about what someone is going to need isn’t acceptable.

<p>Even with my experience, I still struggle to say exactly what needs to be said. There is no perfect introduction to a complex, often abstract set of APIs. However, you can invest a little more time thinking about what others will be needing, maybe run your portal by some external people for a little coherence testing. Most of all, just try to avoid being captain obvious, or captain assumption, and writing content that states the obvious while leaving out most of the critical details you take for granted. It really is the most important lessons we can take away from providing APIs, the ability for them to push us out of our boxes, from behind our firewalls, and have to engage with the real world.



</p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/07/26/avoid-being-captain-obvious-when-documenting-your-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/07/19/discover-profile-quantify-rank-and-publish-new-apis-to-the-streamdata-io/">Discover Profile Quantify Rank And Publish New Apis To The Streamdata Io</a></h3>
        <span class="post-date">19 Jul 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Discover, Profile, Quantify, Rank, And Publish New APIs To The Streamdata.io’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/art-museum/art-museum_blue_circuit_3.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/art-museum/art-museum_blue_circuit_3.jpg" width="45%" align="right" style="padding: 15px;" />
<p>About 60% of my work these days is building upon the last five years of my <a href="http://theapistack.com/">API Stack research</a>, with a focus on building out the <a href="http://api.gallery.streamdata.io/">Streamdata.io API Gallery</a>. We are fine tuning our approach for discovering new API-driven resources from across the landscape, while also profiling, quantifying, ranking, and publishing to the Streamdata.io API Gallery, The API Stack, and potentially other locations like the Postman Network, APIs.Guru, and other API discovery destinations I am working with. Helping us make sense of the increasingly noisy API landscape, while identifying the most valuable resources, and then profiling them to help reduce friction when it comes to potentially on-boarding and streaming data from each resource.

<h3 id="discover-new-api-driven-resources">Discover New API-Driven Resources</h3>
<p>Finding new APIs isn’t too difficult, you just have to Google for them. Finding new APIs in an automated way, with minimum human interaction becomes a little more difficult, but there are some proven ways to get the job done. There is no single place to go find new APIs, so I’ve refined a list of common place I use to discover new APIs:

<ul>
  <li><strong>Search Engines</strong> - Using search engine APIs to look for APIs based upon the vocabulary we’ve developed.</li>
  <li><strong>Github</strong> - Github provides a wealth of signals when it comes to APIs, and we use the Github API to discover interesting sources using our vocabulary.</li>
  <li><strong>Stack Overflow</strong> - Using the Stack Exchange API, we are able to keep an eye out for developers talking about different types of interesting APIs.</li>
  <li><strong>Twitter</strong> - The social network still provides some interesting signals when it comes to discussions about interesting APis.</li>
  <li><strong>Reddit</strong> - There are many developers who still use Reddit to discuss technical topics, and ask questions about the APIs they are using.</li>
</ul>

<p>Using the topic and entity vocabulary we’ve been developing, we can automate the discovery of new APIs across these sources using their APIs. Helping track on signals for the existing APIs we are keeping an eye on, but also quickly identify new APIs that we can add to the queue. Giving us the URL of companies, organizations, institutions, and government agencies who are doing interesting things with APIs.

<h3 id="profile-new-domains-that-come-in">Profile New Domains That Come In</h3>
<p>Our API discovery engine produces a wealth of URLs for us to look at to understand the potential for new data, content, and algorithmic API resources. Our profiling process begins with a single URL, which we then use as the seed for a series of automated jobs that help us understand what an entity is all about:

<ul>
  <li><strong>Description</strong> - Develop the most informative and concise description of what an entity does, including a set of rich meta tags.</li>
  <li><strong>Developer</strong> - Identify where their developer and API program exists, for quantifying what they do.</li>
  <li><strong>Blog</strong> - Find their blog, and supporting RSS feed so we can tune into what they are saying.</li>
  <li><strong>Press</strong> - Also find their press section, and RSS feed so we can tune into the press about them.</li>
  <li><strong>Twitter</strong> - Find their Twitter account so that we can tune into their social stream.</li>
  <li><strong>LinkedIn</strong> - Find their LinkedIn account so that we can tune into their social stream.</li>
  <li><strong>Github</strong> - Find their Github account so we can find more about what they are building.</li>
  <li><strong>Contact</strong> - Establish a way to contact each entity, in case we have any questions or need support.</li>
  <li><strong>Other</strong> - Identify other common building blocks like support, pricing, and terms of services that helps us understand what is going on.</li>
</ul>

<p>The profiling process provides us with a framework to understand what an entity is all about, and where they fit into the bigger picture of the API landscape. Most of the sources of information we profile have some sort of machine readable component, allowing us to further quantify the entity, and better understand the value they bring to the table.

<h3 id="quantify-each-entity">Quantify Each Entity</h3>
<p>Next up we want to quantify each of the entities we’ve profiled, to give us a better understanding of the scope of their operations, and further define where they fit into the API landscape. We are looking for as much detail about what they are up to so we can know where we should be investing our time and energy reaching out and developing deeper relationships.

<ul>
  <li><strong>API</strong> - We profile their APIs, generating an OpenAPI definition that describes the entire surface area of their APIs.</li>
  <li><strong>Applications</strong> - Define approximately how many applications are running on an API, and how many developers are actively using.</li>
  <li><strong>Blog</strong> - Pull all their blog posts, including the history, and actively pull on daily basis.</li>
  <li><strong>Press</strong> - Pull all their press releases, including the history, and actively pull on daily basis.</li>
  <li><strong>Twitter</strong> - Pull all their Tweets and mentions, including the history, and actively pull on daily basis.</li>
  <li><strong>Github</strong> - Pull all their repos, stars, followers, and commit history, understand more about what they are building.</li>
  <li><strong>Other</strong> - Pull other relevant signals from Reddit, Stack Overflow, AngelList, CrunchBase, SEC, Alexa Rank, ClearBit, and other important platform signals.</li>
</ul>

<p>By pulling all the relevant signals for any entity we’ve profiled, we can better understand the scope of their operations, and assess the reach of their network. Helping us further quantity the value and opportunity that exists with each entity we are profiling, before we spend much more time on integrating.

<h3 id="ranking-each-entity">Ranking Each Entity</h3>
<p>After we’ve profiled and quantify an entity, we like to rank them, and put them into different buckets, so that we can prioritize which ones we reach out to, and which ones we invest more resources in monitoring, tracking, and integrating with. We currently rank them on a handful of criteria, using our own vocabulary and ranking formula.

<ul>
  <li><strong>Provider Signals</strong> - Rank their activity and relevance based upon signals within their control.</li>
  <li><strong>Community Signals</strong> - Rank their activity based upon signals the community generates about them.</li>
  <li><strong>Analyst Signals</strong> - Rank their activity based upon signals from the analyst community.</li>
  <li><strong>StreamRank</strong> - Rank the activity of their data, content, and API-driven resources.</li>
  <li><strong>Topically</strong> - Understand the value of the activity based upon the topics that are available.</li>
</ul>

<p>Our ranking of each entity gives us an overall score derived from several different dimensions.  Helping us understand the scope, as well as the potential value for each set of APIs, allowing us to further prioritize which entities we invest more time and resources into, maximizing our efforts when it comes to deeper, more technical integrations, and streaming of data into any potential data lake.

<h3 id="publishing-apis-to-the-gallery">Publishing APIs To The Gallery</h3>
<p>Once an entity has been profiled, quantified, and ranked, we publish the profile to the gallery for discovery. Some of the more interesting APIs we hold back on a little bit, and share with partners and customers who are looking for interesting data sources via landscape analysis reports, but once we are ready we publish the entity to a handful of potential locations:

<ul>
  <li><strong><a href="http://api.gallery.streamdata.io/">Streamdata.io API Gallery</a></strong> - The distributed gallery owned and operated by Streamdata.io</li>
  <li><strong><a href="http://theapistack.com">The API Stack</a></strong> - My own research area for profiling APIs that I’ve run for five years.</li>
  <li><strong><a href="https://apis.guru/">APIs.guru</a></strong> - We are working on the best way to submit OpenAPI definitions to our friends here.</li>
  <li><strong><a href="https://www.postman.com/api-network/">Postman Network</a></strong> - For APIs that we validate, and generate working Postman Collections.</li>
  <li><strong><a href="http://apis.io">APIs.io</a></strong> - Publishing to the machine readable API search engine for indexing.</li>
  <li>Other - We have a network of other aggregation, discovery, and related sites we are working with.</li>
</ul>

<p>Because each entity is published to its own Github repository, with an <a href="http://apisjson.org">APIs.json</a>, <a href="https://www.openapis.org/">OpenAPI</a>, and Postman Collection defining its operations, once published, each entity becomes forkable. Making each gallery entry something anyone can fork, download and directly integrate into their existing systems and applications.

<h3 id="keep-discovering-profiling-quantifying-and-publishing">Keep Discovering, Profiling, Quantifying, and Publishing</h3>
<p>This work is never ending. We’ll just keep discovery, profiling, quantifying, and publishing useful APIs to the gallery, and beyond. Since we benchmark APIs, we’ll be monitoring APIs that go away and we’ll archive them in the listings. We’ll also be actively quantifying each entity, by tuning into their blogs, press, Twitter, and Github accounts looking for interesting activity about what they are doing. Keeping our finger on the pulse of what each entity is up to, as well as what the scope and activity within their community is all about.

<p>This project began as an API Evangelist project to understand how to keep up with the changing API space, and then evolved into a landscape analysis and lead generation tool for Streamdata.io, but now has become an engine for identifying valuable data and content resources. Providing a powerful discover engine for finding valuable data sources, but when combined with what Streamdata.io does, it also allows you to tune into the most important signals across all these entities being profiled, and stream the resulting data and signals into data lakes within your own existing cloud infrastructure, for use in training machine learning models, dashboards, and other relevant applications.



</p></p></p></p></p></p></p></p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/07/19/discover-profile-quantify-rank-and-publish-new-apis-to-the-streamdata-io/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/07/19/api-governance-models-in-the-public-and-private-sector/">Api Governance Models In The Public And Private Sector</a></h3>
        <span class="post-date">19 Jul 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘API Governance Models In The Public and Private Sector’</p>

<p>image: http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/68<em>113_800_500_0_max_0_1</em>-1.jpg</p>

<hr />

<p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/68_113_800_500_0_max_0_1_-1.jpg" width="45%" align="right" style="padding: 15px;" />
<p>This is a report for the Department of Veterans Affairs microconsulting project, “<a href="https://github.com/department-of-veterans-affairs/VA-Micropurchase-Repo/issues/18">Governance Models in Public and Private Sector</a>”. Providing an overview of API governance to help the VA, “understand, with the intention to adopt, best practices from the private and public sector, specifically for prioritizing APIs to build, standards to which to build APIs, and making the APIs usable by external consumers.” Pulling together several years of research conducted by industry analyst API Evangelist, as well as phone interviews with API practitioners from large enterprise organizations who are implementing API governance on the ground across the public and private sector, conducted by Skylight Digital.

<p>We’ve assembled this report to reflect the interview conversations we had with leaders from the space, helping provide a walk through of the types or roles and software architecture being employed to implement governance at large organizations. Then we walk through governance as it pertains to identifying possible APIs, developing standards around the delivery of APIs, how organizations are moving APIs into production, as well as presenting them to their consumers. Wrapping up with an overview of formal API governance details, as well as an acknowledgement that most API governance is rarely ever a fully formed initiative at this point in time. Providing a narrative for API governance, with a wealth of bulleted elements that can be considered, and assembled in the service of helping govern the API efforts across any large enterprise.

<h2 id="roles-within-an-organization">Roles Within An Organization</h2>
<p>There are many roles being used by organizations who are leading the conversation around the delivery of high quality, industry changing APIs. Defining the personalities that are needed to make change across large organizations when it comes to delivering APIs consistently at scale. While there may be many names for the specific roles leading the charge it is clear that these people are bringing a unique blend of skills to an organization, with an emphasis in a couple of key areas:

<ul>
  <li><strong>Leadership</strong> - Providing leadership for teams when it comes to APIs.</li>
  <li><strong>Innovation</strong> - A focus on innovation using APIs across the organization.</li>
  <li><strong>Communication</strong> - Facilitating communication across all teams, and projects.</li>
  <li><strong>Advisory</strong> - Acting as an advisor to existing leadership and management.</li>
  <li><strong>Strategy</strong> - Helping existing teams develop, evolve, and realize their strategy.</li>
  <li><strong>Success</strong> - Focusing on helping existing teams be successful when it comes to APIs.</li>
  <li><strong>Architect</strong> - Bringing a wide variety of software architectural skills to the table.</li>
  <li><strong>Coaching</strong> - Being a coach to existing teams, and decision makers across the organization.</li>
</ul>

<p>Bringing together a unique set of skills that range from the technical to deep knowledge of the business domain, into a concentrated, although sometimes distributed effort to bring change across an organization using APIs. Along with these roles, many large organizations are investing in new types of structure to help develop talent, take charge of new ideas, and move forward the enterprise wide API strategy, with a handful of common characteristics:

<ul>
  <li><strong>Labs</strong> - Treating API efforts as if it is a laboratory creating new experiments.</li>
  <li><strong>Center</strong> - Making it a center for API thinking, ideation, and for access to information.</li>
  <li><strong>Centralized</strong> - Keeping all efforts in a single group or organization within larger entity.</li>
  <li><strong>Distributed</strong> - Emphasis on keeping API knowledge distributed and not centralized at all.</li>
  <li><strong>Global</strong> - Acknowledging that APIs will need to be a global initiative for larger organizations.</li>
  <li><strong>Excellence</strong> - Focusing on the organization bring excellence to how APIs are delivered.</li>
  <li><strong>Embedded</strong> - Making sure there is API knowledge and expertise embedded in every group.</li>
</ul>

<p>Combing a unique set of skills and personalities, into a focused organization that takes the reins when it comes to leading API change, and digital transformation at an organization. While many of these efforts emphasize a center, or centralized presence, many are also realizing the importance of embedded and distributed approach, ensuring that talent, and ideas grow within existing teams, and are not seen as just some new, external, isolated group or initiative.

<p>There clearly is not a single role or organization structure that brings success to API efforts at scale across the enterprise, however there are clear patterns being applied in the early stages that can be emulated. Helping ensure that API knowledge and expertise is available and accessible by all groups across an organization, and all its geographic regions, ensuring that the entire enterprise is part of the conversation and moving forward in unison.

<h2 id="software-architecture-design">Software Architecture Design</h2>
<p>Governance is all about shaping and crafting the way we design and architect software, leveraging the web, and specifically web APIs to help drive the web, mobile, device, and network applications we depend on. There are a number of healthy, and not so healthy patterns across the landscape for considering as we look to shape and transform our software architecture, begin honest about the forces that influence what software is, does, and what it will ultimately become.

<h3 id="domain-awareness">Domain Awareness</h3>
<p>Software architecture is always a product of its environment, being influenced by a number of factors that already exist within any given domain. We are seeing a number of factors influence how large enterprises are investing and defining their software architecture. Here are a handful of the top areas of consideration when it comes to how the domain an enterprise exists within impact architecture:

<ul>
  <li><strong>Resources</strong> - The types of digital resources an enterprise already possess will drive software architecture, defining how it works, grows, expands, and shifts.</li>
  <li><strong>Schema</strong> - Existing schema define how data is stored, and often gathered and syndicated–even if this is abstracted away through other systems, it is still influencing architectural decisions at all levels.</li>
  <li><strong>Process</strong> - Existing business process are already in motion, driving current architecture, and is something that cannot immediately be changed, without having echoes of impact on future architectural decisions.</li>
  <li><strong>Industry</strong> - External industry factors are always emerging to shift how software architecture is crafted, providing design, development, and operational factors that need to be considered as architecture is refactored.</li>
  <li><strong>Regulatory</strong> - Beyond more organic industry influences, there are regulatory, legal, and other government considerations that will shift how software architecture will ultimately operate.</li>
  <li><strong>Definitions</strong> - The access and availability of machine readable definitions, schema, process, and other guiding structural elements that can help make software architecture operate more efficiently, or less efficiently in the absence of standardization, and portability.</li>
</ul>

<p>Domain expertise, awareness, and structure will always shape software architecture, and the decision making process that surrounds it. Making it an imperative for there to be an investment in internal capacity as well as leveraging external expertise and vendors when it comes to shaping the enterprise architectural landscape. Without the proper internal capacity, domain knowledge can be minimized, weakening the overall architecture of the digital infrastructure nutrients an enterprise will need to move forward.

<h3 id="legacy-considerations">Legacy Considerations</h3>
<p>We can never escape the past when it comes to the software architectural decisions we make, and it is important that we don’t just see legacy as a negative, and also view legacy as a historical artifact that should move forward. Maybe not always the same legacy code should be in forward motion, but the wisdom, knowledge, and lessons learned around the enterprise legacy should be on display. Here are a handful of the legacy considerations we’ve identified through our discussions.

<ul>
  <li><strong>Systems</strong> - Existing systems in operation have a significant influence over all current and future architectural decisions, making legacy system consideration a top player when it comes to decision making around software architecture conversations.</li>
  <li><strong>People</strong> - Senior staff who operate and sustain legacy system, or were around when they were developed possess a significant amount of power when it comes to influencing any new system architecture, and what gets invested in or not.</li>
  <li><strong>Partners</strong> - External partners who have significant history with the enterprise possess a great deal of voting power when it comes to what software architecture gets adopted or not.</li>
  <li><strong>Trauma</strong> - Legacy trauma from historical outages, breaches, and bad architectural decisions will continue to influence the future, especially when legacy teams still have influence over future projects.</li>
</ul>

<p>Systems, people, partners, and bad decisions made in the past will continue to drive, and often times haunt each wave of software architectural shifts. This influence cannot be ignored, abandoned, and needs to be transformed into positive effects on next generation investment in software architecture. Change will be inevitable, and legacy technical and cultural debt needs to be addressed, but not at the cost of repeating the mistakes of the past.

<h3 id="contemporary-considerations">Contemporary Considerations</h3>
<p>After legacy concerns, we all live in the reality have been given, and it is something that will continue to shape how we define our architecture. Throughout our discussions with companies, institutions, and government agencies regarding the challenges they face, and the current forces that shape their software architecture decisions, we found several recurring themes regarding contemporary considerations that were making the largest impact:

<ul>
  <li><strong>Talent Available</strong> - The talent available for designing, developing, and deploying of API infrastructure dictates what is possible at all stages.</li>
  <li><strong>Offshore Workers</strong> - The offshoring of work changes the governance dynamics, and requires strong processes, and a different focus when it comes to execution.</li>
  <li><strong>Mainstream Awareness</strong> - Keeping software architectural practices in alignment with mainstream practices helps shape software architecture decisions, allowing them to move forward at a healthier pace.</li>
  <li><strong>Internal Capacity</strong> - It has been stated several times that doing APIs at scale across the enterprise would not be possible without investing in internal capacity over outsourcing, or depending on vendor solutions.</li>
</ul>

<p>Modern practices continue shaping how we deliver our software architecture, defining how we govern the evolution of our infrastructure, and find the resources and talent to make it happen. Keeping software architecture practices in alignment with contemporary approaches helps streamline the road map, how teams work with partners, and can outsource and work with external entities to help get the job done as efficiently as possible.

<h3 id="technically-defined">Technically Defined</h3>
<p>The technology we adopts helps define and govern how software architecture is delivered, and evolved. There are many evolution trends in software architecture that has moved the conversation forward, allowing teams to be more agile, consistent, and efficient in doing what they do. As we studied the architectural approaches of leading API providers across the landscape, and engaged in conversations with a handful of them, we found several technologically defined views of how software architecture is influencing future generations and iterations.

<ul>
  <li><strong>Vendors</strong> - Specific vendors have their owning guiding principles to how software architecture gets defined, delivered, and governed. Often times given an outsized role in dictating what happens next.</li>
  <li><strong>Frameworks</strong> - Software and programming language frameworks dictate specific patterns, and govern how software is delivered, and lead the conversation on how it evolves. Software frameworks can possess a significant amount of dogma that will have a gravity all its own when it comes to evolving into the future.</li>
  <li><strong>Cloud Platforms</strong> - Amazon, Google, and Microsoft (Azure) have a strong voice in how software architecture is defined in the current climate, providing us with the services and tooling to govern the lifecycle. This control over how we define our infrastructure is only going to increase with their market dominance in the digital realm.</li>
  <li><strong>Continuous Integration / Deployment</strong> - CI/CD services and tooling have established a new way of defining software architecture, and establishing a pipeline approach to moving it forward, building in the hooks needed to govern every step of its evolution. Reducing the cycles from annual down to monthly, weekly, and even daily cycles of change.</li>
  <li><strong>Source Control</strong> - Github, Gitlab, and Bitbucket are defining how software is delivered, providing the vehicle for moving code forward, the hooks for governing each commit, and step forward any infrastructure makes as it is versioned, and evolved.</li>
</ul>

<p>These areas are increasingly governing how we design, develop, deploy, and manage our infrastructure. Providing us with the scaffolding we need to hang our technological infrastructure on, and gives us the knobs and levers we can pull to consistently orchestrate, and move forward increasingly complex and large enterprise software infrastructure, across many teams, and geographic regions. The decisions we make around the technology we use will stick with us for years, and continue to influence decisions even after it is gone.

<h2 id="business-defined">Business Defined</h2>
<p>When it comes to delivering software architecture, not everything is governed by the technical components, and much of what gets delivered and moved forward will defined by the business side of the equation. The amount of investment by a business into its overall IT, as well as more progressive groups, will determine what gets done, and what doesn’t. With the following elements of the business governing software architecture in several cases::

<ul>
  <li><strong>Budgets</strong> - How much money is allocated for a team to work when it comes to defining, deploying, managing, and iterating upon software architecture.</li>
  <li><strong>Investors</strong> - Many groups are influenced, driven, and even restricted by outside investors, determining what software architecture is prioritized, and even dictating the decisions around what is put to work.</li>
  <li><strong>Partners</strong> - External partners with strong influence over the business discussions that drive software infrastructure decision play a big role in the governance, or lack of governance involved.</li>
  <li><strong>Public Image</strong> - Often times the decisions that go into software architecture, and the governance of how it moves forward will be driven by the public image concerns around company, and its stakeholders.</li>
  <li><strong>Culture</strong> - The culture of a business will drive decisions being made when it comes to developing, managing, and governing software architecture, which can be more challenging to move forward than the technology in many cases.</li>
</ul>

<p>The governance of software architecture has to be in alignment with business objectives of an enterprise. Many groups choose to begin their API journeys based upon trends, or the desire of a small group, and have encountered significant friction when trying to bring in alignment with the wider enterprise business objectives. Groups that addressed business considerations earlier on in their strategy have done much better when it came to reducing friction, and eliminating obstacles from their road map.

<h3 id="observability">Observability</h3>
<p>Almost every discussion we’ve had around governance of software infrastructure has included mentions of the importance of observability across next generation iterations. Software designed, delivered, and supported in the darkness or in isolation either fails, or is destined to become the next generation of technical debt. There were several areas of emphasis when it came to ensuring the API driven infrastructure sees the light of day from day one, and continues to operate in a way that everyone involved can see what is happening.

<ul>
  <li><strong>Out in Open</strong> - Groups who operate out in the open, sharing their progress actively with other teams, and encouraging a transparent process find higher levels of success, adoption, and consistency across their architectural efforts.</li>
  <li><strong>Co-Discovery</strong> - Ensuring that before work begins, teams are working together to discovery new ideas, learn about alternative software solutions, and working together to create buy-in, and ultimately make decisions around what gets adopted.</li>
  <li><strong>Collaborative</strong> - While identified as sometimes being slower than traditional, more isolated efforts, teams who encouraged cross-team collaboration saw that their architectural decisions were sounder, more stable, and had more longevity.</li>
  <li><strong>Open Source</strong> - Following open source software development practices, and working with existing open source solutions helps ensure that enterprise software architecture last longer, has more support, and follows common standards over other more proprietary approaches.</li>
  <li><strong>Publicly</strong> - When it makes sense from a privacy and security standpoint, groups often articulate that being public by default helps ensure project teams behave differently, enjoy more accountability, and often attract external talent, domain expertise, and publicly opinion along the way.</li>
</ul>

<p>Enterprise organizations that push for observability by default find that teams tend to work better together, and have a more open attitude. Attracting the right personalities, encouraging regular communication, and thinking externally by default, not as something that happens down the road. Bringing much needed sunlight and observability into processes that can often be very complex and abstract, and pushing things to speak to a wider audience beyond developer and IT groups.

<h2 id="shared-process">Shared Process</h2>
<p>Having a shared process that can be communicated across teams, going beyond just technical teams, and is something that business groups, partners, 3rd party, and all other stakeholders can follow and participate in, is a regular part of newer, API-centric, software delivery life cycles. Possessing several core elements that help ensure the process for defining, designing, delivering and evolving software architecture is shared by all.

<ul>
  <li><strong>Contract</strong> - Crafting, maintaining, and consistently applying a common machine readable contract that is available in YAML format, is a common approach to ensuring there is a contract that can be used across all architectural projects, defining each solutions as an independent, business service.</li>
  <li><strong>Pipeline</strong> - Extending the machine readable service contracts with YAML defined pipeline definitions that ensure all software is delivered in a consistent, reproducible manner across many disparate teams.</li>
  <li><strong>Versioning</strong> - Establishing a common approach to versioning code, definitions, and other artifacts, providing a common semantic approach to governing how architecture is evolved in a shared manner that everyone can follow.</li>
</ul>

<p>Historically the software development and operation lifecycle is owned by IT, and development groups. Modern approaches to delivering software at scale is a shared process, including internal business and technical stakeholders, while also sharing the process with external partners, 3rd party developers, and the public. Bringing software architecture out of the shadows, and conducting it on the open web, making it more inclusive amongst all stakeholders, but done in a way that respects privacy and security along the way.

<h2 id="identifying-potential-apis">Identifying Potential APIs</h2>
<p>Once the architectural foundations have been laid, there are many ways in which large enterprises begin identifying the potential APIs that should be designed, deployed, and evolved supporting the many applications that will be depending on the underlying platform architecture. Depending on the organization and it’s priorities, the reasons for how new APIs are born will vary, resulting in different lifecycles, and resulting services being delivered across internal groups, partner stakeholders, and 3rd party developers.

<p>Throughout this research, we identified that there is no single approach to identifying which APIs should be delivered, but we did work to understand a variety of approaches in use across the landscape, and by the practitioners interviewed for this project. Establishing some common areas for answering the questions around what should be an API, why are we doing APIs, leading us to the how of doing APIs, and uncovering the pragmatic reasoning behind web, mobile, device, and other applications that APIs are driving across the landscape.

<h3 id="existing-realities">Existing Realities</h3>
<p>Our existing realities drive the need for APIs, and reflect where we should be looking to provide new services for internal stakeholders, partners, and potentially as new revenue streams for 3rd party developers. While some APIs may be entirely new solutions, it is most likely that APIs will be born out of the realities we are already dealing with on a daily basis, based upon the digital solutions we depend on each day. We identified the most common realities that enterprise group face when it comes to their present day digital transformation challenges.

<ul>
  <li><strong>Database</strong> - The existing databases in operation are the number one place groups are identifying potential resources for deployment of APIs. Exposing historically accumulated digital assets using the web, and making them available for use in new applications, to partners, and driving new types of data products for generating the next generation of revenue streams.</li>
  <li><strong>Website</strong> - Our existing websites reflect the last 20 years of the digital evolution of our enterprises, representing the digital resources we’ve accumulated and identified as being important for sharing with partners and the public. HTML representations of our digital assets are always the 101 of API deployment, and understand what should also be available as JSON, and other more sophisticated representations.</li>
  <li><strong>Integrations</strong> - Existing software integrations, system to system integrations, are represent a rich area for understanding how digital resources are delivered, accessed, and made available throughout existing applications and systems. Providing an important landscape for mapping out when understand what should be turned into APIs, having  a more consistent process applied, eliminating custom integrations, and standardizing how systems speak with one another.</li>
  <li><strong>Applications</strong> - Exiting web, mobile, desktop and other applications can have a variety of backend system connectivity solutions, but also might have a more custom, bespoke approach to doing APIs that is off the radar when it comes to governing how infrastructure evolves. Providing another rich area for mapping out the connections behind the common applications in use, understanding the internal, partner, and 3rd party APIs and other connections they use.</li>
  <li><strong>Services</strong> - APIs have been around for a while, and exist in a variety of formats. Legacy web services, RPC, FTP, and other API and messaging formats should be mapped out and included as part of the potential API evolutionary landscape. Taking existing services, and evolving them in a consistent manner with all other API-driven services, leveraging web technologies to consistently deliver and manage digital resources.</li>
  <li><strong>Spreadsheets</strong> - The majority of business in the world still happens within the spreadsheet. These portable data stores are emailed, shared, and spread around the enterprise, and represent a rich source of information when it comes to understanding which resources should be published as APIs.</li>
</ul>

<p>You can’t govern what isn’t mapped out and known. It becomes increasingly difficult to govern software infrastructure that exists across many open and proprietary formats, and delivered as custom one-off solutions. Governance begins with a known landscape, and the greatest impedance to functional governance across organizations are the unknowns. Not knowing a solution exists, or its architectural approach being something that isn’t part of the bigger picture, leaving it to be a lone actor, in a larger landscape of known services operating in concert.

<h3 id="public-presence">Public Presence</h3>
<p>Another reason for having an open, very public approach to selecting, delivering, and operating software infrastructure, is that it establishes a public presence, across web properties, social networks, and other platforms where enterprise organizations can build  community. There are a number of ways to identify potential new API resources by just being public, engaging with the community, and establishing API delivery life cycles that involve having a public presence.

<ul>
  <li><strong>User Requests</strong> - Actively soliciting web, mobile, and developer feedback, both internally and externally is a great way to learn about potentially new API resource opportunities. Leveraging existing users as a source of insight when it comes to what services would make applications better, and demonstrating the importance of investing in an API literate user base.</li>
  <li><strong>Partner Requests</strong> - Actively working with partners, conducting regular meetings regarding digital assets and transformation, seeking feedback on what types of services would improve upon existing solutions, and strengthen partner relations. Investing in existing partners, and using them as a way to evolve the API road map, and increase their dependency on enterprise resources.</li>
  <li><strong>Public Feedback</strong> - Engaging with the public via websites, social networks, forums, and at events, to understand what opportunities are out there when it comes to delivering new API resources. Tapping public awareness around specific topics, within particular domains, and considering suggestions for new APIs outside the enterprise firewall and the traditional box.</li>
  <li><strong>Media Coverage</strong> - Tuning on to popular tech blogs, mainstream media, and other public outlets to study and understand what opportunities are emerging for the delivery of new API services. Tuning into popular trends when it comes what is happening with APIs, or business sectors that might not have caught up to some of the modern approaches to delivering APIs.</li>
  <li><strong>Feedback Loops</strong> - Cultivating trusted feedback loops with existing users, social networks, and private messaging platforms. Investing in long term feedback loops that tap the knowledge and domain expertise of trusted professionals, who can bring fresh ideas to the table when it comes to which new APIs can benefit the enterprise.</li>
  <li><strong>Negative Consequences</strong> - One significant thing to note with having a public presence, is that not everything will be positive. There are serious privacy, security, and safety concerns when operating your APIs out in the public, and there should be plenty of consideration for how to do it in a healthy way. Acknowledging that not everyone in the public domain will have the enterprise’s best interest in mind.</li>
</ul>

<p>It isn’t easy soliciting feedback from the general public when it comes to determining the direction a platform road map should head. However, with some investment, curation, and cultivation, a more reliable source of insight regarding the direction of an API platform can be established. The API community across the public and private sector has grown significantly over the last decade, providing a wealth of knowledge talent that can be tapped, if you know where to look for it.

<h3 id="improvements">Improvements</h3>
<p>Moving beyond the “where to look for opportunities for APIs”, and moving into the why to find, as well as how to prioritize which resources should be turned into APIs, we hear a lot about investing in the overall improvement of the enterprise when it came to the motivations behind governance. Looking at many of the common incentives behind doing APIs, but also doing it in a more consistent and scalable way that supports the mission and forward motion of the enterprise.

<ul>
  <li><strong>Optimization</strong> - Seeking to optimize how applications are delivered, and services provided across teams. Providing consistent services that can be used across many internal groups, across partners, and that will improve the lives of 3rd party developers.</li>
  <li><strong>Common Patterns</strong> - Doing APIs, and pushing for governance to help identify common patterns across how software is designed, delivered, and managed. Working to extract the existing patterns in use, help standardize and establish the common patterns, and reenforce their usage across teams, and distributed groups.</li>
  <li><strong>Reusability</strong> - Encouraging reusability is the number one improvement we heard from different groups, and see across the landscape. Governing how software is not just delivered, but maximized, reused–ensuring the enterprise is maximizing software spend, as well as the revenue from services it delivers.</li>
  <li><strong>Acceleration</strong> - Investing in governance to help accelerate how applications are delivered, measuring, standardizing, and optimizing along the way to improve existing efforts, as well as new projects on the horizon. Increasing the speed of not just new services, but how the services are able to be put to work by developers and integrators.</li>
  <li><strong>Efficiency</strong> - Setting into motion patterns and processes that increase overall efficiency around how services are delivered, and how they enable teams to deliver on new projects, applications, integrations. Allowing IT, developers, and business users to benefit from an API focus.</li>
  <li><strong>Flexibility</strong> - Increasing the flexibility of how applications operate, and teams are able to work together and deliver across the enterprise. Encouraging the design and development of APIs that work together, and flexibly achieve organizational objectives.</li>
</ul>

<p>Providing a set of criteria that can be used to help prioritize which APIs get identified for delivery, and for evolution and versioning. If API resources help deliver on any of these areas, their benefit to the enterprise is increased, and it should be bumped up the list when looking for API opportunities. Always looking for how the enterprise can be improved upon, while also understanding which specific resources should be targeted for wrapping, and exposing as simple web APIs that can be used for both internal and external use cases.

<h3 id="challenges">Challenges</h3>
<p>The API journey is always full of challenges, and these areas of friction should be identified, and incorporated into the criteria for identifying new API solutions, as well as determining which APIs should be invested in and evolved. While some challenges can be minimized and overcome, many can also cause unnecessary friction throughout the API roadmap, making challenges something to be considered when putting together any API release strategy.

<ul>
  <li><strong>Education</strong> - What education is required when it comes to acquiring the resources behind any potential API, developing and standing up a proper API, as well as deploying, managing, and supporting an API. What challenges can be foreseen, and identified early on, helping weigh what investment in education, training, and learning along the API journey for any set of services.</li>
  <li><strong>Maturity</strong> - Understanding early on, and putting together a plan on what maturity will look like for any service, acknowledging that every service will begin in a juvenile state, and take time to harden, mature, and become something that is dependable, reliable, and usable in a production environment.</li>
  <li><strong>Isolation</strong> - Identifying resources that are being developed, maintained, and operated in isolation, and working to move them out into the mainstream. While also ensuring that any new services being developed avoid isolation, and are developed, evolved, and managed out in the open, ensuring that services never operate alone.</li>
  <li><strong>Management</strong> - Including management in discussions around which resources should be developed into APIs, including leadership in all conversations involving the targeting, evolution, and even deprecation of API services. Ensuring that the prioritization of API development is always on the radar of management, and there is a general awareness regarding the delivery of services.</li>
  <li><strong>Consistency</strong> - Realizing that while consistency is the goal, it may be an elusive, non-stop chase to actually realize consistency across teams. It should be a goal, but also realistically understanding that it won’t be easy to achieve, and while we want to strive for perfection, sometimes there will be good enough achieved for some services.</li>
  <li><strong>Reusability</strong> - Similar to consistency, reusability is an obvious goal, and should be worked towards, but it will also be elusive, and not always reliably achieved over time. There might still be redundancy for some services, and overlapping aspects of delivering services, while some areas reusability will be achievable.</li>
  <li><strong>Build It And They Will Come</strong> - There has been a significant amount of reflection regarding targeting, developing, and publishing APIs that were not needed based upon an “if you build it, they will come” mentality–where most often, nobody came, and the work was in vain.</li>
</ul>

<p>Challenges are a fact of life in the delivery of software, and evolving complex systems using APIs. Identifying challenges should be a natural part of targeting resources for delivery as APIs. Challenges can increase fiction when delivering service, and should be carefully evaluated before tackling the development of any new services. It is easy to identify the potential of new APIs, but it takes a more seasoned eye to understand potential challenges.

<p>New ideas for APIs will become numerous once you begin looking. Based upon existing resources, applications, and the feedback of internal groups, partners, and the public. Along with all of the possibilities that come along, a standardized, pragmatic approach to understanding the potential, value, as well as challenges with each potential idea should be part of the equation.
<p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/71_113_800_500_0_max_0_-1_-1.jpg" width="45%" align="right" style="padding: 15px;" />
<h2 id="defining-data-models--standards">Defining Data Models &amp; Standards</h2>
<p>To help realize and deliver upon governance at scale it will take heavy investment in standardizing data models, and incorporating existing patterns and standards throughout the API delivery lifecycle. Many enterprise API development groups are streamlining and standardization the delivery of APIs through the adoption and development of standards across operations, which is something that is also contributing to adoption, integration and removing friction for application developers.

<p>The adoption of common data models, interfaces, media types, and web standards helps contribute to the delivery of consistent APIs at scale, but they can also prove to be a challenge for some teams, and even been seen as a threat by others. There are a number of ways in which teams are pushing for standardization across their operations, and helping achieve more consistency, reuse, and the desired results across operations. Reflecting one of the strengths of web APIs, in that they employ web standards to achieve wider adoption, and the delivery of valuable resources at web scale.

<h3 id="core-definitions">Core Definitions</h3>
<p>A suite of approaches have emerged in the last decade for designing, developing, evolving, and applying common API patterns across the API lifecycle. These standardized approaches to defining and delivering APIs, using common machine readable specifications, and widely used patterns, have become central to API governance discussions. Providing the fuel for the growth of the API sector to serve mobile applications, as well as the growth of other emerging channels like voice, bot automation, and the connecting of everyday objects to the net. Helping the enterprise get more organized about how services are delivered across the organization at scale.

<ul>
  <li><strong>Resource-Defined</strong> - RESTful design patterns have provided a simple approach to taking corporate resources and defining them as an intuitive, reusable, potentially web-scale stack of API resources that can be used across a variety of applications. REST provides a philosophy that can be adopted across the enterprise to help organize digital resources as a reusable stack of resources that can be discovered and put to use across many channels.</li>
  <li><strong>Schema-Driven</strong> - JSON Schema is being used to take a variety of schema and standardize them for use in RESTful API resource delivery. Providing a reusable blueprint that can be used across the request and response model for all APIs. Deriving, and standardizing existing schema in use, and making available for usage in in newly developed, and evolving APIs allows for teams to achieve many of the objectives set out as part of modern API strategies.</li>
  <li><strong>Domain Driven</strong> - The business domain is used across the enterprise for guiding the identification, development, evolution, and standardization of a variety of API definitions in use across the enterprise. Lines of business, industry definitions, and a focus on the domain helps establish areas of concern, and the separation of services, allowing for the decoupling of enterprise resources used across systems, but working in unison to deliver a single set of business objectives.</li>
  <li><strong>Legacy Abstraction</strong> - Continue movements to decouple, redefine, and evolve legacy systems is pushing forward the identification of common patterns, and pushing to map, transform, and give them new life as newer web APIs. Taking legacy databases, system interfaces, and distilling the wisdom that exists across them, to help drive the development of common standards.</li>
  <li><strong>Vocabularies</strong> - API development teams are establishing common vocabularies based upon the standardized language already in use, but also essentially taking the slang that is used in bespoke systems and helping tame it, and add it to the common lexicon when it makes sense. Providing a standard language that can be used across the enterprise to talk about services, resources, and digital assets.</li>
  <li><strong>Discovery</strong> - Many groups expressed challenges around standards not seeing the desired adoption because other teams could not find existing schema, definitions, and other existing standards. Emphasizing the importance of comprehensive, actively maintained, and evangelized catalog of core definitions across the enterprise. Providing a single, or distributed location where everyone can find and publish their common definitions.</li>
</ul>

<p>The definitions coming out of existing API development efforts are being organized into catalog, and discovery systems that can be used to guide governance efforts. Mapping out the known landscape across the enterprise, and turning it into the common patterns that can be reused across the design, development, and operation of the next generation of APIs. Distilling down the essence of the enterprise so that it can become the building blocks of an API program, while also allowing each stop along the lifecycle to be quantified, measured, and considered as part of a wider governance strategy.

<h3 id="doing-business-on-the-web">Doing Business On The Web</h3>
<p>APis are built on the web. They use, and benefit from over 25 years of the evolution of the web. There are a number of elements to consider when working to identify and define common standards for use across the governance of any API program. While the API strategy should be rooted in definitions derived from the core of the enterprise, secondarily it should be embracing the web, and employing common patterns that make the web work to use as the foundation for delivering APIs.

<ul>
  <li><strong>Web Standards</strong> - The web is the foundation for the delivery of APIs. Most APIs will use HTTP as a transport, and be employing URLs, HTTP verbs, headers, parameters, and other common web standards. Web standards should be part of any governance strategy to help establish common patterns and definitions for use across operations.</li>
  <li><strong>Media Types</strong> - Media types are a fundamental part of the web, and help establish message formats that will be widely recognized outside the enterprise, encouraging the reuse and adoption of APIs that employ common media types. Allowing consumers to negotiate the format that makes the most sense to their team, and the types of applications they are looking to develop.</li>
  <li><strong>Industry Schema</strong> - Industry level schema are emerging and maturing for use across API operations. Specifications like FHIR, PSD2, and other schema, along with API design patterns are evolving to help support industry focused API operations, while encouraging reuse and interoperability across disparate groups.</li>
  <li><strong>Open Source</strong> - The usage of open source software, tooling, specifications, and processes are helping deliver on the API vision across the enterprise. Web APIs reflect the open source ethos, and plays well with the delivery of web APIs. Encouraging reuse, adoption, and bringing the observability necessary to help APIs succeed.</li>
</ul>

<p>APIs are all about doing business on the web. The web provides the platform in which any API program will operate. When it comes to defining schema, standards, and common patterns for use across API operations, the web is always the beginning of the conversation. While enterprise defined patterns will always be front and center, the standards used to operate the web should always trump localized definitions, and be given priority whenever possible. Don’t reinvent the wheel when it comes to the web, always reuse and implement what is already known.

<h3 id="under-the-influence">Under The Influence</h3>
<p>When learning about new standards, and considering which standards to adopt, it can be easy to find yourself under the influence of specific vendors, competing standards, programming communities, and other factors. Careful evaluation of standards is important, and an awareness of what some of the common elements are that may shift your opinions one way or another, or even obfuscate what is real and prevent you from achieving objectives.

<ul>
  <li><strong>Caught in Trends</strong> - Avoid getting caught up in the trend cycles that can often make it difficult to understand the hype around specific specifications. Do your research, understand best practices and adoption levels, and make the sensible decisions around the impact to your own efforts.</li>
  <li><strong>External Entities</strong> - While engaging with external entities, understand what their priorities are when it comes to standards and specifications. Consider what affinity may exist between enterprise objectives, and any external entity that is engaged with, and makes sure there is the right alignment, and influences are pushing efforts in the right direction.</li>
  <li><strong>Internal Demands</strong> - Similar to external entities, understand what the internal teams priorities are and don’t always assume internal requests will have the overall enterprise objectives in mind. Fully understanding what the awareness and motivation are around the implementation of specific standards, and how the fit into the overall strategy.</li>
  <li><strong>Feedback Loops</strong> - Ensure that feedback looks are diverse, and provide a wealth of opinions around what types of standards and specifications should be supported, providing the widest possible view of the landscape when it comes to adoption and investment.</li>
  <li><strong>Organic Change</strong> - Keeping an eye on vendor induced standards adoption over a more organic approach to the growth of standards, internally, as well as the outside community. Working to understand when a standard is artificially inflated, amplified, for alternative objectives beyond its core mission.</li>
</ul>

<p>There are plenty of currents to get caught up in when it comes to identifying, defining, and evolving standards. Not all will bear fruit, or realize the type of adoption they need to be successful. Establishing a balanced view of the landscape across internal, and external actors, while keeping counsel with a diverse set of voices can help ensure you understand which API specifications, standards, and definitions will help move the enterprise forward.

<h3 id="taking-the-lead">Taking The Lead</h3>
<p>While there are a number of ready to use standards available for the web, and organically grown out of the API community, these standards won’t always find their way into the enterprise. Leading organizations demonstrate that it takes a structured effort to define, disseminate, educate, and evolve standards across large organizations, with a number of proven tactics for taking the lead when it comes to standardizing API infrastructure across the enterprise.

<ul>
  <li><strong>Workshops</strong> - Organizing, conducting, and growing the number of workshops held to introduce individuals across many teams to a variety of common standards and specifications.</li>
  <li><strong>Discussions</strong> - Formalizing discussions around emerging standards, and those that are in use, to help push forward awareness, and adoption of standardized approaches across groups.</li>
  <li><strong>Collaboration</strong> - Push teams to work together when it comes to sharing the standards in use, showcasing the investment they’ve made, and working together to understand the tooling, services, and standards being used.</li>
  <li><strong>Event Storming</strong> - Putting event storming, a rapid, lightweight group modeling technique to help accelerate the identification, evolution, and adoption of standards that meet specific team’s needs.</li>
  <li><strong>Influencers</strong> - Identifying, investing in, and cultivating influencers who exist within current groups, and encourage them to evangelize and help spread the good word about standards across the enterprise.</li>
  <li><strong>Ask Questions</strong> - Always be asking questions about the standards, or lack of standards in use across the enterprise, pushing the conversation forward at all times when it comes to standards.</li>
  <li><strong>Challenge Assumptions</strong> - Making sure teams don’t get complacent, and the status quo is always being challenged, and that the internal domain should always be rising to a higher level of standardization whenever possible.</li>
</ul>

<p>It takes standards bodies to move forward common standards at the web and industry levels, and it takes the same approach to push forward the adoption and usage of standards within the enterprise. Leading enterprise organizations are able to quantify, measure and evolve the infrastructure in a more organized way through the adoption of common schemas, specifications, and standards. Providing a common vocabulary for all teams to use when designing, deploying, and managing services that can be used consistently across the enterprise, and its public interests.

<h2 id="development-to-production">Development to Production</h2>
<p>After understanding the roles needed to realize governance, more about the underlying platform architecture that is needed, how organizations can identify where the API opportunities are, and making sure groups are putting standards to work, we scrutinized how groups are moving APIs from development to production in a more structured way. Governing how teams are efficiently moving APIs from idea and design, to actually putting services to work in a production environment at scale across large teams. Documenting the lifecycle of a service, and the common elements of how enterprises are getting the job done on a regular basis.

<h3 id="well-defined">Well Defined</h3>
<p>To be able to deliver APIs at scale in a consistent way teams are relying on a well honed, well defined lifecycle that has been defined, proven, and evolved by senior teams. Forcing structure and rigor throughout the evolution of all services, putting governance in front of teams, and forcing them to execute in a consistent way if they expect to reach a production reality with their services. Focusing on a handful of structured formats for imposing governance at the deployment levels.

<ul>
  <li><strong>Contract</strong> - Requiring ALL services begin with a machine readable OpenAPI contract defining the entire surface area of the API and its schema. Leveraging the contract as the central truth for what the service will deliver, and how governance will be measured throughout the lifecycle of the service.</li>
  <li><strong>Process</strong> - Providing a well defined process for all developers laying out how any service moves from design to production, with as much detail regarding each step along the way. Helping all developers understand what they will face as they work to move their services forward in the enterprise.</li>
  <li><strong>Scorecard</strong> - Having a machine readable checklist and scorecard, with tooling to help each developer fork or establish an instance for their service. Providing a checklist of everything they need to consider, that allows them to check off what has been done, what is left to be done, and providing a definition that can be used to define and report upon governance along the way.</li>
  <li><strong>Cycles</strong> - Provide a variety of cycles that every service will need to go to before they will be production worthy, forcing developers to iterate upon their services, harden and mature them before they will be considered ready for production.</li>
  <li><strong>Reviews</strong> - Require all services go through a series of lifecycle reviews by other teams, pushing service owners to present their work to each review team, and work with them to satisfy any concerns, and make sure it meets all governance criteria.</li>
  <li><strong>Clinics</strong> - Providing developers with a variety of clinics where they can receive feedback on their work, improve upon their service, and improve the health of their work before submitting it for inclusion in a production environment.</li>
</ul>

<p>Enterprise organizations that provide structure for API development teams find it much easier to realize their governance aspirations. The scaffolding is already there to think about the consistency of services, and the face to face, and virtual scrutiny of services helps provide the environment for governance practices to be executed, enforced, and evolved before any service reaches a production state. A well defined API deployment lifecycle will help contribute to a well defined API governance practice.

<h3 id="virtualization">Virtualization</h3>
<p>One sign of enterprise groups who are further along in their governance journeys is when there are virtualized environments being put to use. Requiring all API developers to mock and iterate upon their APIs in a virtualized environment, presenting them as if they are real, before they ever are given a license to write any code, let alone reach a production state for their services.

<ul>
  <li><strong>Mocking</strong> - Creating mock APIs for all endpoints, virtualizing every aspect of the surface area of an API, allowing a service to be iterated upon early on in its lifecycle.</li>
  <li><strong>Data</strong> - Requiring virtualized and synthesized data be present for all mocked APIs, returning realistic data with responses, reflecting behavior encountered in a production environment.</li>
  <li><strong>Sandbox</strong> - Providing a complete labs and sandbox environment for developers to publish their mocked APIs into, reflecting what they’ll encounter in a production environment, but done in a much more safer and secure way.</li>
</ul>

<p>Virtualized environments provide an important phase in the journey for APIs moving from concept to reality. Establishing a safe environment for developers to iterate upon their work, encounter many of the challenges they’ll face in a public environment, without any of the potential for harm to users or the platform. Ensuring that when a service is ready for development, most of the rough edges have been worked out of the service contract, and for the team behind.

<h3 id="technology">Technology</h3>
<p>One of the most significant ways in which we’ve found enterprise groups governing the evolution of their APIs is through the technology they employ. This technology is providing much of the structure and discipline that organizations are depending on to help ensure that APIs are being developed, and ultimately deployed in a consistent manner. Bringing most of the governance to the table for some organizations who haven’t begun moving their governance strategy forward as a formal approach.

<ul>
  <li><strong>Authentication</strong> - Requiring standard-based approaches to authentication using Basic Auth, API keys, OAuth, and JWT. Ensuring the teams understand when to use which protocol, and how to properly configure, and use as part of larger API governance strategy.</li>
  <li><strong>Framework</strong> - Relying on the programming frameworks in use to inject discipline into the process, dictating the governance of how APIs are delivered before they are ready for a production environment.</li>
  <li><strong>Gateway</strong> - Applying the policies and structure necessary to govern API services as they are made available in a production environment. Many groups also had a sandbox or development edition of their gateway emulating many of the same elements that will be found in a production world.</li>
  <li><strong>Management</strong> - Similar to the gateway, groups are relying on their API management layers to help govern what APIs do, providing transformations, policy templates, and a wide variety of other standardization that occurs prior to being made available in production sense.</li>
  <li><strong>Vendor</strong> - The reliance on technology to deliver governance at the API deployment level gives a lot of control to vendors when it comes to governing the API lifecycle. If a vendor doesn’t provide a specific way of doing things, it may not exist within some groups. Dictating what is governance for many enterprise groups.</li>
  <li><strong>Tooling</strong> - Most groups have an arsenal of open source, and custom developed tooling for helping push code from development to production, validating, scanning, transforming, shaping, and hardening code and interfaces to be ready for production usage.</li>
  <li><strong>Encryption</strong> - Requiring encryption by default for storage, and in transport, using technology to ensure security is a default parameter for everything that is exposed publicly. Reducing the possibility of a breach, and minimizing the damage when one does occur.</li>
</ul>

<p>Demonstrating how important the technological choices we make, and the architectural planning we discussed earlier is to the overall API governance conversations. The services, tooling, and applications we adopt will either contribute, or will not contribute to our governance practices. Potentially enforcing governance for all APIs as they move from development to production, in a way that teams cannot circumvent, and often times don’t even notice is occurring behind the scenes.

<h3 id="orchestration">Orchestration</h3>
<p>Augmenting the core technology, there are a number of orchestration practices we found that help quantify and enforce governance on the road from development to production. Dictating how code, artifacts, and other elements included as part of the API lifecycle move forward, evolve, or possibly get held back until specific requirements are met to meet wider governance criteria.

<ul>
  <li><strong>Pipeline</strong> - CI/CD services, tooling, and mindset have introduced a pipeline workflow for many teams, standardizing the API delivery process as an executable, repeatable, measurable pipeline that can be realized across any team.</li>
  <li><strong>Stages</strong> - The defining of clear stages that exist after development, but before production, requiring quality assurance, security reviews, compliance audits, and other relevant governance practices to be realized.</li>
  <li><strong>Hooks</strong> - Well defined pre and post commit hooks for all service repositories, requiring that governance is applied throughout a service’s pipeline, and are default for all services, no matter which organization they emerge from.</li>
  <li><strong>Devops</strong> - Pushing that all teams are competent, and skilled enough to execute on behalf of their services from beginning to end, owning and executing at every stage of the life cycle. Reducing the need for reliance on special teams, and eliminating bottlenecks.</li>
  <li><strong>Logging</strong> - Identifying the logging capabilities of the entire stack for each service being delivered. Making sure logging is turned on for everything, and all logs are shipped to a central location for auditing, and when possible real time analysis and response.</li>
</ul>

<p>Orchestration provides some clarity on the automation side of moving services from development to production, while also enforcing governance along the way. Allowing for an assembly line delivery of consistent services, and the iteration of each version, in alignment with the overall governance strategy. Reducing the chance for human error, and increasing the chance for consistent execution of the enterprise API strategy at scale across many different teams.

<h3 id="the-legal-department">The Legal Department</h3>
<p>Beyond the technology, the legal department should have a significant influence over APIs going from development to production. Providing a structured framework that can generally apply across all services easily, but also providing granular level control over the fine tuning of legal documents for specific services and use cases. With a handful of clear building blocks in use to help govern the delivery of APIs from the legal side of the equation.

<ul>
  <li><strong>Terms of Services</strong> - Have universally applicable, and modular, as well as possibly machine readable, and human readable terms of service governing all services from a central location.</li>
  <li><strong>Privacy Policy</strong> - Have universally applicable, and modular, as well as possibly machine readable, and human readable privacy governing all services from a central location, protecting all platform users from harm.</li>
  <li><strong>Security Policy</strong> - Provide a comprehensive security policy that governs how services are secured, reflecting the technologies, checklists, tooling, and reviews that are in use by all team members, providing an overview for all stakeholders to understand.</li>
  <li><strong>Licensing</strong> - Establish clear code, data, and interface licensing to be used across the entire API stack, allowing developers to properly license their services, as well as understand the provenance for the systems and services they depend on.</li>
  <li><strong>Server Level Agreements</strong> - Have universally applicable, modular, as well as possibly machine readable, and human readable service level agreement (SLA) that can be applied across all services, measured, and reported upon as part of wider governance strategy.</li>
  <li><strong>Scopes</strong> - Define and publish the OAuth access scopes as part of a formal set of legal guidance regarding what data is accessible via services, and the role users and developers play in managing scope that is defined by the platform.</li>
</ul>

<p>The legal department will play an important role in governing APIs as they move from development to production, and there needs to be clear guidance for all developers regarding what is required. Similar to the technical, and business elements of delivering services, the legal components need to be easy to access, understand, and apply, but also make sure and protect the interests of everyone involved with the delivery and operation of enterprise services.

<h2 id="making-apis-available-to-consumers">Making APIs Available to Consumers</h2>
<p>The next step in the life cycle of properly governed APIs is making them available to consumer after they’ve been published to a production environment. The governing of APIs is not limited to the technical side of things, and this is where we begin understanding how to consistently deliver, measure, and understand the impact of API resources across the many consumers who are integrating the valuable resources into their applications. Shining a light on the business and politics of how digital assets are being put to use across the enterprise.

<p>This portion of this governance research is intended to provide a basic list of the building blocks used by enterprise groups to help reduce friction when putting APIs to work, but also make sense of how consumers are using API resources, establishing a feedback loop that guide the road map for the future of the platform. Taking us back to the beginning of this research and informing how we should be targeting the development of new APIs, the evolution of existing services, and in many cases the deprecation and archiving of services. Ensuring governance goes well beyond the technical details, and making sure they are benefitting the platform, as well as consumers.

<h3 id="known-consumers">Known Consumers</h3>
<p>Making your APIs available to consumers requires doing a lot of research on who you are marketing them to, and positioning yourself to speak to an intended audience. Tailoring not just the design of your APIs, but the overall presentation, messaging, and even portal, documentation, and other building blocks to speak to a particular audience. For many API providers, APIs might be made available to multiple audience, in a variety of ways, based upon knowing their customers, and presenting exactly the resources they are needing to get a specific job done.

<ul>
  <li><strong>Studies</strong> - Conducting regular stories about what internal, partner, and public user groups are using, and needing when it comes to developing applications, and integrating systems.</li>
  <li><strong>Landscape</strong> - Establishing an understanding of the industry landscape for the area being targeted by services, and regularly tuning into and refining the understanding of what consumers are using across that landscape.</li>
  <li><strong>B2B</strong> - Positioning the API to speak to a B2

</li></ul></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/07/19/api-governance-models-in-the-public-and-private-sector/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/07/16/tvmaze-uses-hal-for-their-api-media-type/">Tvmaze Uses Hal For Their Api Media Type</a></h3>
        <span class="post-date">16 Jul 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘TVMaze Uses HAL For Their API Media Type’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/tv-maze/tvm_api.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/tv-maze/tvm_api.png" width="45%" align="right" style="padding: 15px;" />
<p>One of the layers of the API universe where I come across an increased number Hypermedia APIs is in the movie, television, and entertainment space. Where having a more flowing API experience makes a lot of sense, and the extra investment in link relations will pay off. One example of this I recently came across was over at TVMaze, <a href="https://www.tvmaze.com/api">who has a pretty robust hypermedia API</a>, where they opted for using HAL as their media type.

<p>Like any good hypermedia should, TVMaze begins with its root URL: http://api.tvmaze.com, and provides a robust set of endpoints from there:
<p><img src="https://s3.amazonaws.com/kinlane-productions2/tv-maze/tvmaze-ha.png" width="40%" align="right" />
<h2 id="search">Search</h2>
<ul>
  <li><a href="http://www.tvmaze.com/api#show-search">Show Search</a></li>
  <li><a href="http://www.tvmaze.com/api#show-single-search">Show single search</a></li>
  <li><a href="http://www.tvmaze.com/api#show-lookup">Show Lookup</a></li>
  <li><a href="http://www.tvmaze.com/api#people-search">People search</a></li>
</ul>

<h2 id="schedule">Schedule</h2>
<ul>
  <li><a href="http://www.tvmaze.com/api#full-schedule">Full Schedule</a></li>
</ul>

<h2 id="shows">Shows</h2>
<ul>
  <li><a href="http://www.tvmaze.com/api#show-main-information">Show main information</a></li>
  <li><a href="http://www.tvmaze.com/api#show-episode-list">Show episode list</a></li>
  <li><a href="http://www.tvmaze.com/api#episode-by-number">Episode by number</a></li>
  <li><a href="http://www.tvmaze.com/api#episodes-by-date">Episodes by date</a></li>
  <li><a href="http://www.tvmaze.com/api#show-seasons">Show seasons</a></li>
  <li><a href="http://www.tvmaze.com/api#season-episodes">Season episodes</a></li>
  <li><a href="http://www.tvmaze.com/api#show-cast">Show cast</a></li>
  <li><a href="http://www.tvmaze.com/api#show-crew">Show crew</a></li>
  <li><a href="http://www.tvmaze.com/api#show-aka">Show AKA’s</a></li>
  <li><a href="http://www.tvmaze.com/api#show-index">Show index</a></li>
</ul>

<h2 id="people">People</h2>
<ul>
  <li><a href="http://www.tvmaze.com/api#person-main-information">Person main information</a></li>
  <li><a href="http://www.tvmaze.com/api#person-cast-credits">Person cast credits</a></li>
  <li><a href="http://www.tvmaze.com/api#person-crew-credits">Person crew credits</a></li>
</ul>

<h2 id="updates">Updates</h2>
<ul>
  <li><a href="http://www.tvmaze.com/api#show-updates">Show updates</a></li>
</ul>

<p>The TVMaze API isn’t an overly complex hypermedia API. I think it is simple, elegant, and shows how you can use link relations to establish a more meaningful experience for API consumers. Allowing you to navigate the large, ever-changing catalog of television shows, allowing the API client to do the heavy lifting of navigating the shows, schedules, and people involved with each production.

<p>There hasn’t been enough showcasing of the <a href="http://hypermedia.apievangelist.com">hypermedia APIs</a> available out there. Usually once a year I remember to give the subject some attention, or when I come across interesting ones like TVMaze. Hypermedia isn’t just an academic idea anymore, and is something that has gotten traction in a number of sectors, and I keep seeing signs of growth and adoption. I don’t think it will be the API solution most hypermedia believers envisioned it, but I do think it is a viable tool in our API toolbox, and for the right projects it makes a lot of sense.



</p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/07/16/tvmaze-uses-hal-for-their-api-media-type/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/07/16/if-a-search-for-swagger-or-openapi-doesnt-yield-results-i-try-for-a-postman/">If A Search For Swagger Or Openapi Doesnt Yield Results I Try For A Postman</a></h3>
        <span class="post-date">16 Jul 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘If A Search For Swagger or OpenAPI Doesnt Yield Results I Try For A Postman’</p>

<p>image: http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/80<em>86_800_500_0_max_0_1</em>-1.jpg</p>

<hr />

<p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/80_86_800_500_0_max_0_1_-1.jpg" width="45%" align="right" style="padding: 15px;" />
<p>While profiling any company, a couple of the Google searches I will execute right away are for “[Company Name] Swagger” and “[Company Name] OpenAPI”, hoping that a provide is progressive enough to have published an OpenAPI definition–saving me hours of work understanding what their API does. I’ve added a third search to my toolbox, if these other two searches do not yield results, searching for “[Company Name] Postman”, revealing whether or not a company has published a Postman Collection for their API–another sign of a progressive, outward thinking API provider in my book.

<p>A machine readable definition for an API tells me more about what a company, organization, institution, or government agency does, than anything else I can dig up on their website, or social media profiles. An OpenAPI definition or Postman Collection is a much more honest view of what an organization does, than the marketing blah blah that is often available on a website. Making machine readable definitions something I look for almost immediately, and prioritize profiling, reviewing, and understanding the entities I come across with a machine readable definition, over those that do not. I only have so much time in a day, and I will prioritize an entity with an OpenAPI or Postman, over those who do not.

<p>The presence of an OpenAPI and / or Postman Collection isn’t just about believing in the tooling benefits these definitions provide. It is about API providers thinking externally about their API consumers. I’ve met a lot of API providers who are dismissive of these machine readable definitions as trends, which demonstrates they aren’t paying attention to the wider API space, and aren’t thinking about how they can make their API consumers lives easier–they are focused on doing what they do. In my experience these API programs tend to not grow as fast, focus on the needs of their integrators and consumers, and often get shut down after they don’t get the results they thought they’d see. APIs are all about having that outward focus, and the presence of OpenAPI and Postman Collection are a sign that a provider is looking outward.

<p>While I’m heavily invested in OpenAPI (I am member), I’m also invested in Postman. More importantly, I’m invested in supporting well defined APIs that provide solutions to developers. When an API has an OpenAPI for delivering mocks, documentation, testing, monitoring, and other solutions, and they provide a Postman Collection that allows you to get up an running making API calls in seconds or minutes, instead of hours or days–it is an API I want to know more about. Making these potential searches the deciding factor between whether or not I will continue profiling and reviewing an API, or just flagging it for future consideration, and moving on to the next API in the queue. I can’t keep up with the number of APIs I have in my queue, and it is signals like this that help me prioritize my world, and get my work done on a regular basis.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/07/16/if-a-search-for-swagger-or-openapi-doesnt-yield-results-i-try-for-a-postman/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/07/16/for-every-competitor-you-keep-out-of-your-api-docs-you-are-keeping-twenty-new/">For Every Competitor You Keep Out Of Your Api Docs You Are Keeping Twenty New</a></h3>
        <span class="post-date">16 Jul 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘For Every Competitor You Keep Out Of Your API Docs You Are Keeping Twenty New’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/old-door-lock_smoking_cigarette.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/old-door-lock_smoking_cigarette.jpg" width="45%" align="right" style="padding: 15px;" />
<p>It is interesting for me to still regularly come across so many API providers who have a public API portals, but insist on keeping most of their documentation behind a login. Stating that they are concerned with competitors getting access to the design of their API and the underlying schema. Revealing some indefensible API business models, and general paranoia around doing business on the web. Something that usually is a sign for me of a business that is working really hard to maintain a competitive grip within an industry, without actually having to do the hard work of innovating and moving the conversation forward.

<p>Confident API providers know that you can put your API documentation out in the open, complete with schema, without giving away the farm. If your competition can take your API design, and underlying schema, and recreate your business–you should probably go back to the drawing board, and come up with a new business idea. Your API and schema definition is not your business. I’ve used this comparison may times–your API docs are like a restaurant menu. Can you imagine restaurants that kept them hidden until they were sure you are going to be a customer? If you think that your competition can read your menu and recreate all your dishes, then you won’t be in business very long, because your dishes probably weren’t that special to begin with.

<p>For every competitor you keep out of your API documentation, you are keeping twenty new customers out as well. I’m guessing that your savvy competitors are going to be able to get in anyways with a fake account, or otherwise. Don’t waste your time on hiding your API and keeping it out of the view of your potential customers–invest your energy in making sure your APIs kick ass. To use the restaurant analogy again, make sure ingredients are the best, and your processes, and your service are top notch. Don’t make your menu hard to get, it just shows how out of touch you are with the mainstream world of APIs, and your worst fears will come true–someone will come along and do what you do, but even better, and you will become irrelevant.

<p>Be proud of your APIs, and publish them prominently in your API portal. Make sure you have a OpenAPI definition handy, driving your documentation, tests, monitors, and other elements of your operations. Also make sure you have Postman Collections available, allowing your API definition to be portable and importable into the Postman client, allowing consumers to get up and running making calls in minutes, not hours or days. Get out of the way of your API consumers, don’t put up unnecessary, outdated obstacles in their way. I know that you feel you know best because you’ve been doing this for so long, and know your industry, but the world is moving on, and APIs are about doing business on the web in a much more open, accessible, and self-service way. If you aren’t moving in this direction, I’m guessing you won’t be doing what you do for much longer, because someone will come along who can move faster and be more open.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/07/16/for-every-competitor-you-keep-out-of-your-api-docs-you-are-keeping-twenty-new/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/07/16/concern-around-working-with-many-github-repositories/">Concern Around Working With Many Github Repositories</a></h3>
        <span class="post-date">16 Jul 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Concern Around Working With Many Github Repositories’</p>

<p>image: http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/114<em>69_800_500_0_max_0_1</em>-1.jpg</p>

<hr />

<p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/114_69_800_500_0_max_0_1_-1.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I’m regularly fascinated with API development teams I work with expressing their concern with working with many Github repositories. With all of the complexity I watch teams embrace when it comes to frameworks, scaffolding, continuous integration, deployment, and orchestration solutions, I’m lost on why many Github repositories suddenly become such a challenge. A Github organization is a folder, and a Github repository is a folder, that you can check out locally, on your server, or work with via an API, or Git. It is a distributed file store, that you can orchestrate with programmatically, and can be as logical or illogical as you design it to be.

<p>I work really hard to keep technical complexity to a minimum in my world, but this means limiting unnecessary vendor lock-in, and tech just because it is the latest trend. For me, Git is just a distributed file system, with version control built in, with Github providing a nice API and network effect layer that makes it compelling. Referencing a Github folder is just a matter of using its org and repo name, checking it out, and working with the standardized layout of information I have published there. Allowing me to work with, and orchestrate thousands of separate folders (repos), across almost 50 organizations (folders), in a consistent way, as a one person team. Something that has taken me about four years to setup, and fine tune, but has become essential to what I do on a daily basis.

<p>I’m not saying working with hundreds or thousands of individual Github repositories can’t be complex, or doesn’t take a significant amount of work. I’m just saying I’m intrigued by how technologists who manage large systems, adopt complex frameworks for delivering simple web solutions, and regularly make other complex technological investments, draw the line here. I see Github as a robust file store, with two doorways, 1) Git, and 2) API. I have a standardized structure to what I store on Github, something that is similar to my Amazon S3 store, or my backend of databases, and is something that takes discipline to maintain and keep from being unwieldy, but if I do the hard work it is possible. I’m guessing folks who see Github as a lot of work are seeing it through a web or desktop UI lens, and haven’t stopped to think about through an API or CLI lens–I find my AWS, Google, and other platforms to be complicated if I only look at them through the UI.

<p>I enjoy being able to checkout and work with the relevant repositories in my world, and automate my backend and front end systems to automate with the same repositories. My blog schedules, checks out, and publishes posts across 250 repositories. My curation system pulls from my Feedly every day, organizing what I’ve bookmarked and tagged across almost 500 repositories. My API system updates and publishes APIs.json, OpenAPI, and Postman collections across almost 4,000 separate repositories, as I discovery, profile, and make sense of the API landscape. I don’t see this as complexity, I see it as pretty simple, Git, Jekyll, HTML, CSS, JS, JSON, and YAML driven goodness. Something I can easily migrate off of Github if I wanted, and run on AWS, Google, Azure, or my own server infrastructure. For now I’m enjoying the network effect provided by Github, and the power of their API when it comes to more granular changes, and tuning into valuable signals that are available via the social platform.

<p>The conversations I’m having around the complexities of managing many Github repositories are a reminder for me of how technological complexity is relative. Some will see opportunity, while others see complexity, and vice versa. If you understand something, there is a lesser chance you will see complexity–you’ve made the investment already. For many, Github is a burden. I see it as liberating, and something to be orchestrated, providing me with some of the most significant signals I can find online–rivaling Twitter when it comes to driving my work. I can see managing Github through the web or desktop interface being pretty cumbersome, but once you’ve elevated beyond these tools, and work with repositories using the API and CLI, taking full advantage of the Git in Github, the landscape changes, and become less complex, and a more empowering experience. Something I don’t think everyone will fully realize, and be able to get beyond their view of Github as complex, difficult, and challenging. And, thats ok.



</p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/07/16/concern-around-working-with-many-github-repositories/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/07/12/my-api-lifecycle-checklist-and-scorecard/">My Api Lifecycle Checklist And Scorecard</a></h3>
        <span class="post-date">12 Jul 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘My API Lifecycle Checklist And Scorecard’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-blue-seal.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/api-evangelist-logos/api-evangelist-blue-seal.png" width="45%" align="right" style="padding: 15px;" />
<p>I am working on delivering a handful of new APIs, which I will be turning into products. I will be prototyping, developing, and operating them in production environments for myself, and for a handful of customers. To help guide my workflow, I’ve crafted this API lifecycle definition to help direct my efforts in an ongoing lifecycle approach.

<p><strong>Define</strong> - Define the problem and solution in a human and machine readable way.

<ul>
  <li><strong>Repository</strong> - Establish a Github repository.</li>
  <li><strong>README</strong> - Craft a README for the repository.</li>
  <li><strong>Title</strong> - Provide title for the service.</li>
  <li><strong>Description</strong> - Provide concise description for the service.</li>
  <li><strong>Goals</strong> - Establish goals for the service.</li>
  <li><strong>Schema</strong> - Organize loose, and JSON schema for the service.</li>
  <li><strong>OpenAPI</strong> - Establish an OpenAPI for the service.</li>
  <li><strong>Assertions</strong> - Craft a set of assertions for the service.</li>
  <li><strong>Team</strong> - Define the team behind the service.</li>
</ul>

<p><strong>Design</strong> - Establish a base set of design practices for the service.

<ul>
  <li><strong>Versioning</strong> - Determine how the code, schema, and the API be versioned.</li>
  <li><strong>Base Path</strong> - Set the base path for the service.</li>
  <li><strong>Path(s)</strong> - Define a set of resource paths for the service.</li>
  <li><strong>Verb(s)</strong> - Define which HTTP, and other verbs will be used for the service.</li>
  <li><strong>Parameters</strong> - Define a list of query parameters in use to work with service.</li>
  <li><strong>Headers</strong> - Define the HTTP Headers that will be used to work with the service.</li>
  <li><strong>Response(s)</strong> - Provide a resulting message and associated schema definition for the service.</li>
  <li><strong>Media Types</strong> - Define whether service will return CSV, JSON, and / or XML responses.</li>
  <li><strong>Status Codes</strong> - Define the available status code for each responses.</li>
  <li><strong>Pagination</strong> - Define how pagination will be handled for requests and responses.</li>
  <li><strong>Sorting</strong> - Define how sorting will be handled for requests and responses.</li>
</ul>

<p><strong>Database</strong> - Establish the base for how data will be managed behind the service.

<ul>
  <li><strong>Platform</strong> - Define which platform is in use to manage the database.</li>
  <li><strong>Schema</strong> - Establish the schema used for database based upon definition provided.</li>
  <li><strong>Location</strong> - Define where the database is located that supports services.</li>
  <li><strong>Logs</strong> - Define where the database logs are located that support services.</li>
  <li><strong>Backup</strong> - Define the database backup process and location for service.</li>
  <li><strong>Encryption</strong> - Define the encryption layer for the service database.</li>
</ul>

<p><strong>Storage</strong> - Establish how all objects will be stored for the service.

<ul>
  <li><strong>Platform</strong> - Define which platform is used to store objects.</li>
  <li><strong>Location</strong> - Define where objects are stored behind the service.</li>
  <li><strong>Access</strong> - Quantify how objects access is provided behind the service.</li>
  <li><strong>Backup</strong> - Define the backup process for objects behind the service.</li>
  <li><strong>Encryption</strong> - Define the encryption layer for stored objects.</li>
</ul>

<p><strong>DNS</strong> - Establish the DNS layer for this service.

<ul>
  <li><strong>Platform</strong> - Define which platform is used to operate DNS.</li>
  <li><strong>Prototype</strong> - Provide host for prototype of service.</li>
  <li><strong>Mock</strong> - Provide host for mock of service.</li>
  <li><strong>Development</strong> - Provide host for development version of service.</li>
  <li><strong>Production</strong> - Provide host for production version of service.</li>
  <li><strong>Portal</strong> - Provide host for the portal for this service.</li>
  <li><strong>Encryption</strong> - Define the encryption layer for service in transport.</li>
</ul>

<p><strong>Mocking</strong> - Provide a mock representation of this service.

<ul>
  <li><strong>Paths</strong> - Providing virtualized paths for the API driving service.</li>
  <li><strong>Data</strong> - Providing synthesized data behind each API response for service.</li>
</ul>

<p><strong>Deployment</strong> - Define the deployment scaffolding for this service.

<ul>
  <li><strong>Platform</strong> - Define the platform used to deploy this service.</li>
  <li><strong>Framework</strong> - Define the code framework used to deploy service.</li>
  <li><strong>Gateway</strong> - Define the gateway used to deploy service.</li>
  <li><strong>Function</strong> - Define the function(s) used to deploy service.</li>
  <li><strong>Containers</strong> - Define the container used to deploy service.</li>
  <li><strong>Pipeline</strong> - Define the pipeline in place to build and deploys service.</li>
  <li><strong>Location</strong> - Define where the service is deployed to.</li>
</ul>

<p><strong>Orchestration</strong> - Define how the service will be orchestrated.

<ul>
  <li><strong>Build</strong> - Define the build process for this service.</li>
  <li><strong>Hooks</strong> - Detail the pre and post commit hooks in use for this service.</li>
  <li><strong>Jobs</strong> - Define the jobs being executed as part of this service operations.</li>
  <li><strong>Events</strong> - Define the events that are in play to help operate this service.</li>
  <li><strong>Schedule</strong> - Details of the schedules used to orchestrate this service.</li>
</ul>

<p><strong>Dependencies</strong> - Providing details of the dependencies that exist for this service.

<ul>
  <li><strong>Service</strong> - Details of other services this service depends upon.</li>
  <li><strong>Software</strong> - Details of other software this service depends upon.</li>
  <li><strong>People</strong> - Details of other people this service depends upon.</li>
</ul>

<p><strong>Authentication</strong> - Details regarding authentication in use for this service.

<ul>
  <li><strong>Type</strong> - Define whether this service uses Basic Auth, API Keys, JWT, or OAuth for authentication.</li>
  <li><strong>Overview</strong> - Provide a location of the page that delivers an overview of this services authentication.</li>
</ul>

<p><strong>Management</strong> - Define the management layer for this service’s API.

<ul>
  <li><strong>Platform</strong> - Defining the platform used for the API management layer.</li>
  <li><strong>Administration</strong> - Provide a location for administrating the management layer.</li>
  <li><strong>Signup</strong> - Provide a location for users to signup for access to this service.</li>
  <li><strong>Login</strong> - Provide a location for users to login to access to this service.</li>
  <li><strong>Account</strong> - Provide a location for users to access a dashboard for this service.</li>
  <li><strong>Applications</strong> - Provide the location of applications that are approved to use service.</li>
</ul>

<p><strong>Logging</strong> - Define the logging layer for supporting this service.

<ul>
  <li><strong>Database</strong> - Define the logging for the database layer.</li>
  <li><strong>API</strong> - Define the logging for the API access layer.</li>
  <li><strong>DNS</strong> - Define the logging for the DNS layer.</li>
  <li><strong>Shipping</strong> - Define how longs are shipped or centralized for auditing.</li>
</ul>

<p><strong>Monetization</strong> - Define the costs associated with the delivery of this service.

<ul>
  <li><strong>Acquisition</strong> - Provide costs associated with acquisition of resources behind service.</li>
  <li><strong>Development</strong> - Provide costs associated with the development of this service.</li>
  <li><strong>Operation</strong> - Provide costs associated with the operation of this service.</li>
  <li><strong>Value</strong> - Provide a description of the value delivered by this service.</li>
</ul>

<p><strong>Plans</strong> - Define the operational plan for the business of this service.

<ul>
  <li><strong>Tiers</strong> - Define the tiers of access in place to support this service.</li>
  <li><strong>Elements</strong> - Define the elements of access for each tier for this service.</li>
  <li><strong>Paths</strong> - Define which API paths are available as part of each tier or service.</li>
  <li><strong>Metrics</strong> - Provide a list of metrics being used to measure service access.</li>
  <li><strong>Timeframes</strong> - Define the timeframes in use to measure access to this service.</li>
  <li><strong>Limits</strong> - Define what limitations and constraints are in place for this service.</li>
  <li><strong>Pricing</strong> - Define the monetary value in place to define the price for this service.</li>
</ul>

<p><strong>Portal</strong> - Define the public or private portal in use to present this service.

<ul>
  <li><strong>Hosting</strong> - Provide details on where this service portal is hosted.</li>
  <li><strong>Template</strong> - Define which graphical UI and brand template is in use for this portal.</li>
  <li><strong>Analytics</strong> - Define which analytics package is used to measure traffic for portal.</li>
</ul>

<p><strong>Documentation</strong> - Provide documentation for this service within portal.

<ul>
  <li><strong>Overview</strong> - Publish concise over for this service’s documentation.</li>
  <li><strong>Paths</strong> - Publish an interactive list of API paths available for service.</li>
  <li><strong>Examples</strong> - Provide as many examples of API requests in a variety of languages.</li>
  <li><strong>Definitions</strong> - Publish a list of schema definitions in use by this service.</li>
  <li><strong>Errors</strong> - Provide a list of available errors users will encounter for this service.</li>
</ul>

<p><strong>Getting Started</strong> - Provide a getting started page for this service within portal.

<ul>
  <li><strong>Overview</strong> - Provide an introduction to the getting started process for this service.</li>
  <li><strong>Signup</strong> - Provide a link to where users can signup for this service.</li>
  <li><strong>Authentication</strong> - Provide a link to the authentication overview for this service.</li>
  <li><strong>Documentation</strong> - Provide a link to the documentation for this service.</li>
  <li><strong>SDKs</strong> - Provide a link to where users can find SDKs and code libraries for this service.</li>
  <li><strong>FAQ</strong> - Provide a link to to the frequently asked questions for this service.</li>
  <li><strong>Support</strong> - Provide a link to where users can get support for this service.</li>
</ul>

<p><strong>SDKs</strong> - Providing code samples, libraries, or complete software development kits (SDKs).

<ul>
  <li><strong>PHP</strong> - Provide a PHP SDK.</li>
  <li><strong>Python</strong> - Provide a Python SDK.</li>
  <li><strong>Ruby</strong> - Provide a Ruby SDK.</li>
  <li><strong>Go</strong> - Provide a Go SDK.</li>
  <li><strong>Java</strong> - Provide a Java SDK.</li>
  <li><strong>C#</strong> - Provide a C# SDK.</li>
  <li><strong>Node.js</strong> - Provide a Node.js SDK.</li>
  <li><strong>JavaScript</strong> - Provide a JavaScript SDK.</li>
</ul>

<p><strong>FAQ</strong> - Publish a list of the frequently asked questions (FAQ) for this service.

<ul>
  <li><strong>Categories</strong> - Break all questions down by logical categories.</li>
  <li><strong>Questions</strong> - Publish a list of questions with answers within each category.</li>
  <li><strong>Ask Question</strong> - Provide a form for users to ask a new question.</li>
</ul>

<p><strong>Road Map</strong> - Provide a road map for the future of this service.

<ul>
  <li><strong>Private</strong> - Publish a private, internal version of entries for the road map.</li>
  <li><strong>Public</strong> - Publish a publicly available version of entries for the road map.</li>
  <li><strong>Suggest</strong> - Provide a mechanism for users to make suggestions for the road map.</li>
</ul>

<p><strong>Issues</strong> - Provide a list of currently known issues for this service.

<ul>
  <li><strong>Entries</strong> - Publish a list of all known issues currently outstanding.</li>
  <li><strong>Report</strong> - Provide a mechanism for users to report issues.</li>
</ul>

<p><strong>Change Log</strong> - Providing a historical list of what has changed for this service.

<ul>
  <li><strong>Outline</strong> - Publish a list of all road map and issue entries that have been satisfied for this service.</li>
</ul>

<p><strong>Communication</strong> - Establish a communication strategy for this service.

<ul>
  <li><strong>Blog</strong> - Provide a simple blog and update mechanism for this service.</li>
  <li><strong>Twitter</strong> - Provide the Twitter handle that is used as part of this service.</li>
  <li><strong>Github</strong> - Provide the Github account or organization behind this service.</li>
  <li><strong>Internal</strong> - Provide a location where internal communication is available.</li>
  <li><strong>External</strong> - Provide a location where public communication is available.</li>
</ul>

<p><strong>Support</strong> - Establish the support apparatus in place for this service.

<ul>
  <li><strong>Email</strong> - Define the email account used to support this service.</li>
  <li><strong>Issues</strong> - Provide a URL to the repository issues to support this service.</li>
  <li><strong>Tickets</strong> - Provide a URL to the ticketing system used to support this service.</li>
</ul>

<p><strong>Licensing</strong> - Provide a set of licensing in place for this service.

<ul>
  <li><strong>Server</strong> - Define how all backend server code is licensed for this service.</li>
  <li><strong>Data</strong> - Define how all data and schema is licensed for this service.</li>
  <li><strong>API</strong> - Define how the API definition is licensed for this service.</li>
  <li><strong>SDK</strong> - Define how all client code is licensed for this service.</li>
</ul>

<p><strong>Legal</strong> - Provide a set of legal documents guiding the service.

<ul>
  <li><strong>Terms of Service</strong> - Provide a terms of service for this service to operate within.</li>
  <li><strong>Privacy Policy</strong> - Provide a privacy policy for this service to operate within.</li>
  <li><strong>Service Level Agreement</strong> - Provide a service level agreement (SLA) for this service to operate within.</li>
</ul>

<p><strong>Monitoring</strong> - Defining the uptime monitoring for this service.

<ul>
  <li><strong>Monitors</strong> - Establish the monitors for this service.</li>
  <li><strong>Status</strong> - Provide real time details of monitor activity.</li>
</ul>

<p><strong>Testing</strong> - Defining the testing for this service.

<ul>
  <li><strong>Assertions</strong> - Provide details of the assertions being tested for.</li>
  <li><strong>Results</strong> - Provide real time details of testing activity.</li>
</ul>

<p><strong>Performance</strong> - Defining the performance monitoring for this service.

<ul>
  <li><strong>Tests</strong> - Provide details of the performance testing in place for this service.</li>
  <li><strong>Results</strong> - Provide real time details of performance testing activity.</li>
</ul>

<p><strong>Security</strong> - Defining the security practices in place for this service.

<ul>
  <li><strong>Overview</strong> - Provide the URL of the security practices overview page.</li>
  <li><strong>Policies</strong> - Define the security, IAM, and other policies are in place for this service.</li>
  <li><strong>Tests</strong> - Define the security tests that are conducted for this service.</li>
  <li><strong>Results</strong> - Provide real time details of security testing activity.</li>
</ul>

<p><strong>Discovery</strong> - Defining the discovery aspects for this service.

<ul>
  <li><strong>API Discovery</strong> - Publish an API Discovery (APIs.json) index for the project.</li>
  <li><strong>OpenAPI</strong> - Provide URL for all OpenAPI definitions and index in API discovery index.</li>
  <li><strong>Postman Collection</strong> - Provide URL for all Postman Collections and index in API discovery index.</li>
</ul>

<p><strong>Analysis</strong> - Define the analysis in play for this service.

<ul>
  <li><strong>Traffic</strong> - Document traffic to the service portal.</li>
  <li><strong>Usage</strong> - Document usage of APIs for the service.</li>
  <li><strong>Activity</strong> - Document other activity around the service.</li>
  <li><strong>SLA</strong> - Document whether the SLA was met for this service.</li>
</ul>

<p><strong>Stages</strong> - Define the stages that are applied to this lifecycle outline.

<ul>
  <li><strong>Prototype</strong> - When a prototype of this service is being developed.</li>
  <li><strong>Development</strong> - When a production instance of service is being developed.</li>
  <li><strong>Production</strong> - When a production instance of service is being operated.</li>
</ul>

<p><strong>Maintenance</strong> - Define the maintenance cycles applied to this lifecycle outline.

<ul>
  <li><strong>Daily</strong> - Provide a version of this outline that should be considered daily.</li>
  <li><strong>Weekly</strong> - Provide a version of this outline that should be considered weekly.</li>
  <li><strong>Monthly</strong> - Provide a version of this outline that should be considered monthly.</li>
  <li><strong>Releases</strong> - Provide a version of this outline that should be considered for each release.</li>
  <li><strong>Governance</strong> - Provide an outline of how this outline is measured, reported, and evolved.</li>
</ul>

<p>This outline gets executed differently depending on the service stage and maintenance cycle being executed. It is meant to provide a master checklist to consider from day one, and every other time this service is being versioned, maintained, and considered as part of my overall operations. Providing a living checklist, and scorecard rubric for how well this service is doing, depending on stage and maintenance dimensions.

<p>Ultimately the API Discovery (APIs.json) document is the heartbeat for this service checklist, which resides in the root of the Github repository will provide the machine readable index for reporting and governance, while also driving the human readable interface that is accessible via the service portal, which is driven by Jekyll running in Github Pages. Which is something I will publish more about next, as part of a working portal for one of the first services coming off the assembly line.



</p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/07/12/my-api-lifecycle-checklist-and-scorecard/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/07/11/ping-identity-acquires-elasticbeam-to-establish-new-api-security-solution/">Ping Identity Acquires Elasticbeam To Establish New Api Security Solution</a></h3>
        <span class="post-date">11 Jul 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Ping Identity Acquires ElasticBeam To Establish New API Security Solution’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/elastic-beam/image.ping.480.medium.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/elastic-beam/image.ping.480.medium.png" width="45%" align="right" style="padding: 15px;" />
<p>You don’t usually find me writing about API acquisitions unless I have a relationship with the company, or there are other interesting aspects of the acquisition that makes it noteworthy. <a href="https://www.pingidentity.com/en/company/press-releases-folder/2018/ping-identity-acquires-elastic-beam-launches-new-ai-driven-solution-to-secure-apis.html">This acquisition of Elastic Beam by Ping Identity</a> has a little of both for me, as I’ve been working with the Elastic Beam team for over a year now, and I’ve been interested in what Ping Identity is up to because of some research I am doing around open banking in the UK, and the concept of an industry level API identity and access management, as well as API management layer. All of which makes for an interesting enough mix for me to want to quantify here on the blog and load up in my brain, and share with my readers.

<p>From the press release, <em>“Ping Identity, the leader in Identity Defined Security, today announced the acquisition of API cybersecurity provider Elastic Beam and the launch of PingIntelligence for APIs.”</em> Which I think reflects some of the evolution of API security I’ve been seeing in the space, moving being just API management, and also being about security from the outside-in. The newly combined security solution, PingIntelligence for APIs, focuses in on automated API discovery, threat detection &amp; blocking, API deception &amp; honeypot, traffic visibility &amp; reporting, and self-learning–merging the IAM, API management, and API security realms for me, into a single approach to addressing security that is focused on the world of APIs.

<p>While I find this an interesting intersection for the world of APIs in general, where I’m really intrigued by the potential is <a href="https://www.pingidentity.com/en/company/press-releases-folder/2017/open-banking-selects-ping-identity-to-underpin-the-uks-open-bank.html">when it comes to the pioneering open banking API efforts coming out of the UK, and the role Ping Identity has played</a>. “Ping’s IAM solution suite, the Ping Identity Platform, will provide the hub for Open Banking, where all UK banks and financial services organizations, and third-party providers (TPPs) wanting to participate in the open banking ecosystem, will need to go through an enrollment and verification process before becoming trusted identities stored in a central Ping repository.” Which provides an industry level API management blueprint I think is worth tuning into.

<p>Back in March, <a href="https://apievangelist.com/2018/03/01/an-observable-industry-level-directory-of-api-providers-and-consumers/">I wrote about the potential of the identity, access management, API management, and directory for open banking in the UK to be a blueprint for an industry level approach to securing APIs in an observable way</a>. Where all the actors in an API ecosystem have to be registered and accessible in a transparent way through the neutral 3rd party directory, before they can provide or access APIs. In this case it is banking APIs, but the model could apply to any regulated industry, <a href="https://apievangelist.com/2018/03/26/a-regulatory-framework-for-facebook-and-other-platforms-is-already-in-place/">including the world of social media which I wrote about a couple months back as well after the Cambridge Analytics / Facebook shitshow</a>. Bringing API management and security out into the open, making it more observable and accountable, which is the way it should be in my opinion–otherwise we are going to keep seeing the same games being played we’ve seen with high profile breaches like Equifax, and API management lapses like we see at Facebook.

<p>This is why I find the Ping Identity acquisition of ElasticBeam interesting and noteworthy. The acquisition reflect the evolving world of API security, but also has real world applications as part of important models for how we need to be conducting API operations at scale. ElasticBeam is a partner of mine, and I’ve been talking with them and the Ping Identity team since the acquisition. I’ll keep talking with them about their road map, and I’ll keep understanding how they apply to the world of API management and security. I feel the acquisition reflects the movement in API security I’ve been wanting to see for a while, moving us beyond just authentication and API management, looking at API security through an external lens, exploring the potential of machine learning, but also not leaving everything we’ve learned so far behind.



</p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/07/11/ping-identity-acquires-elasticbeam-to-establish-new-api-security-solution/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/07/11/monetizing-your-device-location-data-with-lotadata/">Monetizing Your Device Location Data With Lotadata</a></h3>
        <span class="post-date">11 Jul 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Monetizing Your Device Location Data With LotaData’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/lotadata/lotadata-platform.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/lotadata/lotadata-platform.png" width="45%" align="right" style="padding: 15px;" />
<p>There are a lot of people making money off of the acquisition, organization, and providing access to data in our digital world. While I quietly tune into what the data monetization trends are, I am also actively looking for interesting approaches to generating revenue from data, but specifically with an eye on revenue sharing opportunities for the owners or stewards of that data. You know as opposed to just the exploitation of people’s data, and generating of revenue without them knowing, or including them in the conversation. To help counteract this negative aspect of the data economy, I’m always looking to highlight (potentially) more positive outcomes when it comes to making money from data.

<p>I was recently profiling the API of the <a href="https://www.lotadata.com/">people intelligence platform LotaData</a>, and I came across <a href="https://docs.lotadata.com/monetization.html">their data monetization program</a>, which provides an interesting look at how platforms can help data stewards generate revenue from data, but in a way that makes it accessible to individuals looking to monetize their own data as well. “LotaData’s AI platform transforms raw location signals into ‘People Intelligence’ for monetization, usually based upon the follow key attributes: latitude, longitude, timestamp, deviceID, and accuracy.”

<p><img src="https://s3.amazonaws.com/kinlane-productions2/lotadata/lotadata_permits_panel_image.png" width="45%" align="right" style="padding: 15px;" />
<p>Representing activity at a location and/or point in time, allowing LotaData’s to understand what is happening at specific places at scale, and develop meaningful insights and behavioral segments that other companies and government agencies will want to buy into. Some of the examples they provide are:

<ul>
  <li>Commuting daily on CalTrain from Palo Alto to San Francisco</li>
  <li>Mid-week date night at Nopa on the way back from work</li>
  <li>Sweating it out at Soul Cycle on Saturday mornings</li>
  <li>Taking the dog out for a walk on Sunday afternoons</li>
  <li>Season ticket holder for Warriors games at the Oakland Arena</li>
</ul>

<p>LotaData’s location-based insights and segments are entirely inferred from raw location signals, emphasizing that they do not access or collect any personally identifiable information (PII) from mobile phones–stating that they “do not and never will collect PII such as name, email, phone number, date of birth, national identifier, credit cards, or other sensitive information”. Essentially walking on the light side of the whole data acquisition and monetization game, and playing the honest card when it comes to the data economy.

<p>When it comes to the monetization of data, LotaData enables marketers, brands, city governments and enterprise businesses to purchase location-based insights–providing an extensive network of data buyers who are ready to purchase the insights generated from this type of data. Then the revenue generated from the sale of an insight is split proportionately and shared with the app developers who contributed their app data. With the SDK agreement with LotaData governing the payment terms, conditions and schedule for sharing revenue. However, if you are unable to integrated LotaData’s SDK in a mobile app for any reason, they can offer you alternative ways to share and monetize your location data:&lt;p&gt;<img src="https://s3.amazonaws.com/kinlane-productions2/lotadata/lotadata-ai-big-data-brain-platform.png" width="45%" align="right" style="padding: 15px;" />&lt;/p&gt;

<ul>
  <li><strong>Geo-Context API</strong> - The Geo-Context API is a simple script that you can embed in your mobile web sites and web apps. The script collects location data with explicit notice and permission obtained from end users.</li>
  <li><strong>Bulk Data Transfer</strong> - Customers that are proficient in collecting location signals from their mobile apps, websites or other services, can easily upload their historical location archives to LotaData’s cloud for analyzing, inferring and monetizing mobile user segments. The data can be transferred to LotaData by configuring the appropriate access policies for AWS S3 buckets.</li>
  <li><strong>Integration</strong> - LotaData can integrate with CRM and in-house data warehouse systems to ingest custom datasets or usage logs for deep analysis, enrichment and segmentation.
Questions?</li>
</ul>

<p>Providing a pretty compelling model for data providers to monetize their location based data. It is something I’ll be exploring more regarding how individuals can aggregate their own personal or professional data, as well as take advantage of the geo context API, bulk data transfer, or other integration opportunities. I have no idea how much money an individual or company could make from publishing data to LotaData, but the model provides an interesting approach that I think is worth exploring. It would be interesting to run a 30 to 90 test of tracking all of my location data, uploading it to LotaData, and then sharing the revenue details about what I can make with through a single provider like LotaData, as well as explore other potential providers so that you could sell your location data multiple times.

<p>In a world where our data is the new oil, I’m interested in any way that I can help level the playing field, and seeing how we can put more control back into the device owners hands. Allowing mobile phone, wearable, drone, automobile, and other connected device owners to aggregate and monetize their own data in a personal or professional capacity. Helping us all better understand the value of our own bits, and potentially generating some extra cash from its existence. I don’t think any of us are going to get rich doing this, but if we can put a little cash back in our own pockets, and limit the exploitation of our bits by other companies and device manufacturers, it might change the game to be a little more in our favor.



</p></p></p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/07/11/monetizing-your-device-location-data-with-lotadata/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/07/11/i-am-sorry-but-your-company-is-too-big-for-me-to-talk-to/">I Am Sorry But Your Company Is Too Big For Me To Talk To</a></h3>
        <span class="post-date">11 Jul 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘I Am Sorry, But Your Company Is Too Big For Me To Talk To’</p>

<p>image: http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/104<em>198_800_500_0_max_0</em>-5_-1.jpg</p>

<hr />

<p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/104_198_800_500_0_max_0_-5_-1.jpg" width="45%" align="right" style="padding: 15px;" />
<p>It is funny work with companies, organizations, institutions, and government agencies of all shapes and sizes, and learn all the weird practices they have, and the strange belief systems they’ve established. One day I will be talking to a 3 person startup, the next day I’ll be talking with a large bank, and after that I’ll be working with a group at a massive government agency. I have to be mindful of my time, make sure I’m meeting my mission, having an impact, as well as paying my bills, but for the most part I don’t have any entrenched rules about who I will talk to, or who I will share my knowledge with.

<p>One thing I chuckle at regularly is when I come across large organizations who will gladly talk with me, and tap my knowledge, but won’t work with some of the startups I work with, or the conferences I produce, because they are “too small”. They can’t waste their time working with small startups because it won’t bring the scope of revenue they need to justify the relationships, but they’ll gladly talk to me and welcome the exposure and knowledge I might bring. Sometimes I feel like telling organizations, “sorry you are just to large to work with, you are almost guaranteed to fail at this whole API thing, why should I bother?” I think I’ll say it sometimes jokingly, but not really interested in truly being a dick at that level.

<p>Most large organizations can’t figure out how to work with me in any long term anyway, because they are too bureaucratic and slow moving. Other large organizations have no problem figuring out how to get me past legal, and getting me paid, but some just can’t figure it out. I had one large enterprise group who follows my work, wanted to get me in really badly, but their on-boarding team needed proof that I was the API Evangelist going back every year since 2010, a letter from client, tax returns, or other proof that I was who I say I was–just so I could share my knowledge with them. Um, ok? You really are going to put up so many barriers to people coming into your organization and sharing knowledge? I’m guessing you aren’t going be very good at this whole API thing, with these types of barriers in the way.

<p>I know I can’t change the way large organization behave, but know I can influence their behavior. I’ve done it before, and I’ll keep doing it. Especially when large organizations reach out to me, asking to help them in their journey. At 99% of them I will have no impact, but it is the other 1% that I’m hoping to influence in some way. I can also regularly point out how silly their organizations are, even if the people I’m working with are well aware of the state of things. I know it isn’t how ALL large organizations have to behave because I do a lot of business with large entities, who are able to get me through legal, and able to pay me without problems. Somewhere along the way, certain organizations have made the decision to be more bureaucratic and the trick is going to be how do you begin unwinding this–this is what the API journey is all about.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/07/11/i-am-sorry-but-your-company-is-too-big-for-me-to-talk-to/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/07/10/using-openapi-and-json-patch-to-articulate-changes-for-your-api-road-map/">Using Openapi And Json Patch To Articulate Changes For Your Api Road Map</a></h3>
        <span class="post-date">10 Jul 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Using OpenAPI And JSON PATCH To Articulate Changes For Your API Road Map’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/downtheline_dali_three.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/downtheline_dali_three.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I’m doing a lot of thinking regarding how JSON PATCH can be applied because of my work with <a href="http://streamdata.io">Streamdata.io</a>. When you proxy an existing JSON API with Streamdata.io, after the initial response, every update sent over the wire is articulated as a <a href="https://tools.ietf.org/html/rfc6902">JSON PATCH</a> update, showing only what has changed. It is an efficient, and useful way to show what has changed with any JSON API response, while being very efficient about what you transmit with each API response, reducing polling, and taking advantage of HTTP caching.

<p>As I’m writing an OpenAPI diff solution, helping understand the differences between OpenAPI definitions I’m importing, and allowing me to understand what has changed over time, I can’t help but think that JSON PATCH would be a great way to articulate change of the surface area of an API over time–that is, if everyone loyally used OpenAPI as their API contract. Providing an OpenAPI diff using JSON PATCH would be a great way to articulate an API road map, and tooling could be developed around it to help API providers publish their road map to their portal, and push out communications with API consumers. Helping everyone understand exactly what is changing in way that could be integrated into existing services, tooling, and systems–making change management a more real time, “pipelinable” (making this word up) affair.

<p>I feel like this could help API providers better understand and articulate what might be breaking changes. There could be tooling and services that help quantify the scope of changes during the road map planning process, and teams could submit OpenAPI definitions before they ever get to work writing code, helping them better see how changes to the API contract will impact the road map. Then the same tooling and services could be used to articulate the road map to consumers, as the road map becomes approved, developed, and ultimately rolled out. With each OpenAPI JSON PATCH moving from road map to change log, keeping all stakeholders up to speed on what is happening across all API resources they depend on–documenting everything along the way.

<p>I am going to think more about this as I evolve my open API lifecycle. How I can iterate a version of my OpenAPI definitions, evaluate the difference, and articulate each update using JSON PATCH. Since more of my API lifecycle is machine readable, I’m guessing I’m going to be able to use this approach beyond just the surface area of my API. I’m going to be able to use it to articulate the changes in my API pricing and plans, as well as licensing, terms of service, and other evolving elements of my operations. It is a concept that will take some serious simmering on the back burners of my platform, but a concept I haven’t been able to shake. So I might as well craft some stories about the approach, and see what I can move forward as I continue to define, design, and iterate on the APIs that drive my platform and API research forward.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/07/10/using-openapi-and-json-patch-to-articulate-changes-for-your-api-road-map/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/07/10/people-do-not-use-tags-in-their-openapi-definitions/">People Do Not Use Tags In Their Openapi Definitions</a></h3>
        <span class="post-date">10 Jul 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘People Do Not Use Tags In Their OpenAPI Definitions’</p>

<p>image: http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/68<em>158_800_500_0_max_0</em>-5_-1.jpg</p>

<hr />

<p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/68_158_800_500_0_max_0_-5_-1.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I import and work with a number of OpenAPI definitions that I come across in the wild. When I come across a version 1.2, 2.0, 3.0 OpenAPI, I import them into my API monitoring system for publishing as part of my research. After the initial import of any OpenAPI definition, the first thing I look for is the consistent in the naming of paths, the availability of summary, descriptions, as well as tags. The naming conventions used is paths is all over the place, some are cleaner than others. Most have a summary, with fewer having descriptions, but I’d say about 80% of them do not have any tags available for each API path.

<p>Tags for each API path are essential to labeling the value a resource delivers. I’m surprised that API providers don’t see the need for applying these tags. I’m guessing it is because they don’t have to work with many external APIs, and really haven’t put much thought into other people working with their OpenAPI definition beyond it just driving their own documentation. Many people still see OpenAPI as simply a driver of API documentation on their portal, and not as an API discovery, or complete lifecycle solution that is portable beyond their platform. Not considering how tags applied to each API resource will help others index, categorize, and organize APIs based upon the value in delivers.

<p>I have a couple of algorithms that help me parse the path, summary, and description to generate tags for each path, but it is something I’d love for API providers to think more deeply about. It goes beyond just the resources available via each path, and the tags should reflect the overall value an API delivers. If it is a product, event, messaging, or other resource, I can extract a tag from the path, but the path doesn’t always provide a full picture, and I regularly find myself adding more tags to each API(if I have the time). This means that many of the APIs I’m profiling, and adding to my <a href="http://theapistack.com">API Stack</a>, <a href="http://api.gallery.streamdata.io/">API Gallery</a>, and other work isn’t as complete with metadata as they possibly could be. Something API providers should be more aware of, and helping define as part of their hand crafting, or auto-generation of OpenAPI definitions.

<p>It is important for API providers to see their OpenAPI definitions as more than just a localized, static feature of their platforms, and as a portable definition that will be used by 3rd party API service providers, as well as their API consumers. They should be linking their OpenAPI prominently from your API documentation, and not hiding behind the JavaScript voodoo that generates your docs. They should be making sure OpenAPI definitions are as complete as you possibly can, with as much metadata as possible, describing the value that it delivers. Loading up OpenAPI definitions into a variety of API design, documentation, discovery, testing, and other tooling to see what it looks like and how it behaves. API providers will find that tags are beginning to be used for much more than just grouping of paths in your API documentation, and it is how gateways are organizing resources, management solutions are defining monetization and billing, and API discovery solutions are using to drive their API search solutions–to just point out a couple of ways in which they are used.

<p>Tag your APIs as part of your OpenAPI definitions! I know that many API providers are still auto-generating from a system, but once they have published the latest copy, make sure you load up in one of the leading API design tools, and give that last little bit of polish. Think of it as that last bit of API editorial workflow that ensures your API definitions speak to the widest possible audience, and are as coherent as it possibly can. Your API definitions tell a story about the resources you are making available, and the tags help provide a much more precise way to programmatically interpret what APIs actually deliver. Without them APIs might not properly show up in search engine and Github searches, or render coherently in other API services and tooling. OpenAPI tags are an essential part of defining and organizing your API resources–give them the attention they deserve.



</p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/07/10/people-do-not-use-tags-in-their-openapi-definitions/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/07/10/my-moving-towards-a-modern-api-lifecycle-from-postcon-2018/">My Moving Towards A Modern Api Lifecycle From Postcon 2018</a></h3>
        <span class="post-date">10 Jul 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘My Moving Towards a Modern API Lifecycle From POST/CON 2018’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/talks/postcon/moving-towards-a-modern-api-lifecycle-postcon.png</p>

<hr />

<p>I gave a talk early in in June at POST/CON 2018 in San Francisco. The conference was a great mix of discussions reflecting the Postman community. <a href="https://www.youtube.com/playlist?list=PLM-7VG-sgbtCv6yx5Af3pGikTYIE3CAdu">You can find all the talks on Google</a>, including mine about moving towards a modern AP lifecycle.

<center><iframe width="560" height="315" src="https://www.youtube.com/embed/RmSFhySdLaE" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen=""></iframe></center>

<p>You can find <a href="http://apievangelist.com/">all the stop along what I consider to be a modern API lifecycle on the home page of API Evangelist</a>, with links to any of my research, services, tooling, and other storytelling I’ve done in each area.

<p>Thanks again to Postman for having me out!



</p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/07/10/my-moving-towards-a-modern-api-lifecycle-from-postcon-2018/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/07/10/long-running-api-requests-and-differential-api-responses/">Long Running Api Requests And Differential Api Responses</a></h3>
        <span class="post-date">10 Jul 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Long Running API Requests And Differential API Responses’</p>

<p>image: http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/80<em>140_800_500_0_max_0</em>-5_-5.jpg</p>

<hr />

<p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/80_140_800_500_0_max_0_-5_-5.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I am shifting my long running API operations from a PHP / EC2 based implementation to a more efficient Node.js / Lambda based solution, and I promised James Higginbotham (<a href="https://twitter.com/launchany">@launchany</a>) that I’d share a breakdown of my process with him a month or so back. I’m running 100+, to bursts of 1000+ long running API requests for a variety of purposes, and it helps me to tell the narrative behind my code, introducing some coherence into the why and how of what I’m doing, while also sharing with others along the way. <a href="http://apievangelist.com/2018/01/19/developing-a-microservice-to-orchestrate-long-running-background-server-sent-events/">I had covered my earlier process a little bit in a story a few months ago</a>, but as I was migrating the process, I wanted to further flesh out, and make sure I wasn’t mad.

<p>The base building block of each long running API request I am making is HTTP. The only difference between these API requests, and any others I am making on a daily basis, is that they are long running–I am keeping them alive for seconds, minutes, and historically hours. My previous version of this work ran as long running server side jobs using PHP, which I monitored and kept alive as long as I possibly could. My next generation scripts will have a limit of 5 minutes per API request, because of constraints imposed by Lambda, but I am actually find this to be a positive constraint, and something that will be helping me orchestrate my long running API requests more efficiently–making them work on a schedule, and respond to events.

<p>Ok, so why am I running these API calls? A variety of reasons. I’m monitoring a Github repository, waiting for changes. I’m monitoring someone’s Twitter account, or a specific Tweet, looking for a change, like a follow, favorite, or retweet. Maybe I’m wanting to know when someone asks a new question about Kafka on Stack Overflow, or Reddit. Maybe I’m wanting to understand the change schedule for a financial markets API over the course of a week. No matter the reason, they are all granular level events that are occurring across publicly available APIs that I am using to keep an eye on what is happening across the API sector. Ideally all of these API platforms would have webhook solutions that would allow for me to define and subscribe to specific events that occur via their platform, but they don’t–so I am doing it from the outside-in, augmenting their platform with some externally event-driven architecture.

<p>An essential ingredient in what I am doing is <a href="http://streamdata.io">Streamdata.io</a>. Which provides me a way to proxy any existing JSON API, and turn into a long running / streaming API connection using Server-Sent Events (SSE). Another essential ingredient of this is that I can choose to get my responses as JSON PATCH, which only sends me what has changed after the initial API response comes over the pipes. I don’t receive any data, unless something has changed, so I can proxy Github, Twitter, Stack Overflow, Reddit, and other APIs, and tailor my code to just respond to the differential updates I receive with each incremental update. I can PATCH the update to my initial response, but more importantly I can take some action based upon the incremental change, triggering an event, sending a webhook, or any other action I need based upon the change in the API space time continuum I am looking for.

<p>My previous scripts would get deployed individually, and kept alive for as long as I directed the jobs manager. It was kind of a one size fits all approach, however now that I’m using Lambda, each script will run for 5 minutes when triggered, and then I can schedule to run again every 5 minutes–repeating the cycle for as long as I need, based upon what I’m trying to accomplish. However, now I can trigger each long running API request based upon a schedule, or based upon other events I’m defining, leveraging AWS Cloudwatch as the logging mechanism, and AWS Cloudwatch Events as the event-driven layer. I am auto-generating each Node.js Lambda script using OpenAPI definitions for each API, with a separate environment layer driving authentication, and then triggering, running, and scaling the API streams as I need, updating my AWS S3 Lake(s) and AWS RDS databases, and pushing other webhook or notifications as I need.

<p>I am relying heavily on Streamdata.io for the long running / streaming layer on top of any existing JSON API, as well as doing the differential heavy lifting. Every time I trigger a long running API request, I’ll have to do a diff between it’s initial response, and the previous one, but every incremental update for the next 4:59 is handled by Streamdata.io. Then AWS Lambda is doing the rest of the triggering, scaling, logging, scheduling, and event management in a way more efficient way than I was previously with my long running PHP scripts running as background jobs on a Linux EC2 server. It is a significant step up in efficiency and scalability for me, allowing me to layer on an event-driven layer on top of existing 3rd party API infrastructure I am depending on to keep me informed of what is going on, and keep my network of API Evangelist research moving forward.



</p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/07/10/long-running-api-requests-and-differential-api-responses/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/07/09/using-plain-language-in-your-api-paths/">Using Plain Language In Your Api Paths</a></h3>
        <span class="post-date">09 Jul 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Using Plain Language In Your API Paths’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/power_colorful_blocks.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/power_colorful_blocks.jpg" width="45%" align="right" style="padding: 15px;" />
<p>It is tough to help developers think outside of the world they operate within. Most software is still developed and managed within silos, knowing it’s inner workings will never be seen by anyone outside of the team. This mode of operation is a rich environment for poor code quality, and teams with port communication. This is one of the reasons I’ve embraced web APIs, after running software development teams since the 1990s, I’ve been put in charge of some pretty dysfunctional teams, and some pretty unwieldy legacy codebases, so once I started working out in the open using web APIs, I did’t want to go back. Web APIs aren’t the cure for all of our technology problems, but it does begin to let some sunlight in on some messed up ways of doing things.

<p>One common illness I still see trickling out of API operations are developers not using plain language. Speaking in acronyms, code, and other cryptic ways of articulating the resources they are exposing. I came across a set of API resources for managing a DEG the other day. You could  add, updated, delete and get DEGs. You can also pull analytics, history, and other elements of a DEG. I spent about 10-15 minutes looking around their developer portal, documentation, and even Googling, but never could figure out what a DEG was. Nowhere in their documentation did they ever tell consumers what a DEG was, you just had to be in the know I guess. The API designer (if that occurred) and developer had never stopped to consider that maybe someone would stumble across their very public API and not know what a DEG was. Demonstrating how us developers have trouble thinking outside our silos, and thinking about what others will need.

<p>There is no reason that your API paths shouldn’t be plain language, using common words. I’m not even talking about good RESTful resource design, I’m simply talking about looking at the URI for an API and being able to understand what it is because it used words we can understand. If you have trouble pausing, and stepping back, and thinking what some random 3rd party developer will interpret your API paths as, I recommend printing them out and sharing them with someone that isn’t on your team, and familiar with the resources you work with. Even if your APIs aren’t going to be public, someday you will be gone, and maybe your documentation isn’t up to date, and someone will have to reverse engineer what your API does. There is no reason your API should hide what it does, and not speak for itself, providing an intuitive, plain language description fo the value it possesses.

<p>I look at hundreds of APIs each month. I push myself to understand what an API does in seconds, or minutes. When I spend 10-15 minutes unsuccessfully to understand what an API does, there is a problem with its design. I’m not talking about good API design, I’m just talking about coherent API design. There is no reason you should have an acronym in your API path. I don’t care how short-lived, or internal you view this API. These are often times the APIs that end up sticking around for generations, and becoming part of the technical debt future teams will have to tackle. Don’t be part of the problem in the future. Speak in plain language, and make your API paths speak for themselves. Make them speak to as wide as possible audience as you can. Make them reach outside of your developer circles, and become something any human can copy and paste, and put to work as part of their daily routine.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/07/09/using-plain-language-in-your-api-paths/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/07/09/operating-your-api-in-the-cloud-kill-zone/">Operating Your Api In The Cloud Kill Zone</a></h3>
        <span class="post-date">09 Jul 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Operating Your API In The Cloud Kill Zone’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/beachclouds_blue_circuit.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/beachclouds_blue_circuit.jpg" width="45%" align="right" style="padding: 15px;" />
<p>When you operate your application within the API ecosystem of a large platform, depending on the platform, you might have to worry about the platform operator copying, and emulating what you do. Twitter has long been accused of sharecropping within their ecosystem, and other larger platforms have come out with similar features to what you can find within their API communities. Not all providers take the ideas, it is also very common for API platforms to acquire talent, features, and applications from their ecosystems–something that Twitter has done regularly. Either way, API ecosystems are the R&amp;D, and innovation labs for many platforms, where the latest features get proven.

<p>As the technology playing field has consolidated across three major cloud providers, AWS, Azure, and Google, this R&amp;D and innovation zone, has become more of a cloud kill zone for API providers. Where the cloud giants can see the traction you are getting, and decide whether or not they want to launch a competing solution behind the scenes. Investors are tuning into this new cloud kill zone, and in many cases opting not to invest in startups who operate on a cloud platform, afraid that the cloud giant will just come along and copy a service, and begin directly competing with companies operating within their own ecosystem.  Making it a kill zone for API providers, who can easily be assimilated into the AWS, Azure, or Google stack, and left helpless do anything but wither on the vine, and die.

<p>Much like other API ecosystems, AWS, Azure, and Google all have the stats on who is performing across their platforms, and they know which solutions developers are demanding. Factoring in the latest growth trends into their own road maps, and making the calculations around whether they will be investing in their own solutions, or working to partner, and eventually acquire a company operating with this new kill zone. The 1000 lb cloud gorillas carry a lot of weight in regards to whether or not they choose to partner and acquire, or just crush a startup. I’m guessing there are a lot of factors they consider along the way that will contribute to whether or not they play nicely or not. There are no rules to this game, and they really can do whatever they want with as much market share and control over the resources as they all possess. It will be interesting to begin tracking on acquisitions and partnerships across all players to better understand the score.

<p><a href="https://apievangelist.com/2017/08/31/the-api-space-is-in-the-tractor-beam-of-the-cloud-giants-now/">I wrote last year about how the API space is in the tractor beam of the cloud providers now</a>, and it is something I think will only continue in coming years. It will be hard to deploy, scale, and operate your API without doing it on one of the cloud platforms, or multiple cloud platforms, forcing all API providers to operate within the cloud kill zone. Exposing all new ideas to share their analytics with their platform overlords, and open them up for being copied, or at least hopefully acquired. Which is something that will stunt investment in new APIs, making it harder for them to scale and grow on the business side of things. Any way you look at it, the cloud providers have the upper hand when it comes to cherry picking the best ideas and features, with AWS having a significant advantage in the game with their dominant cloud market position. It will be pretty hard to do APIs in the next decade without AWS, Azure, and Google knowing what you are doing, and having the last vote in whether you are successful or not.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/07/09/operating-your-api-in-the-cloud-kill-zone/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/07/09/i-love-the-api-enthusiasm-predix-but-please-publish-an-api-style-guide-for/">I Love The Api Enthusiasm Predix But Please Publish An Api Style Guide For</a></h3>
        <span class="post-date">09 Jul 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘I Love The API Enthusiasm Predix, But Please Publish An API Style Guide For’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/predix/predix-diagram.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/predix/predix-diagram.png" width="45%" align="right" style="padding: 15px;" />
<p>I was profiling <a href="https://www.predix.io/api">the volume of API from the Internet of Things platform Predix</a> this last week. Luckily they have OpenAPI definitions for each of the APIs, something that makes my life a lot easier. As, they have a wealth of APIs available, doing an amazing amount of work when it comes to connecting devices to the Internet–I love their enthusiasm for putting out APIs. My only critical feedback for them after working my way through their API definitions, is they should invest some time to develop an API design guide, and distribute across their teams. The wild variances in definition and design of their APIs made me stumble a number of times while learning about what they do.

<p>While looking through the definitions for the Predix APIs, I found many inconsistent patterns between them, and you could tell that they had different teams (or individuals) working across the suite of APIs. The inconsistencies ranged from the naming, description, and how the meta data was provided for each API, all the way to acronyms used in API paths, and other things that prevented me from understand what an API did all together. While I am stoked they provide OpenAPI definitions for all of their APIs, I still struggled to understand what was possible with many of their APIs. It kind of feels like they need an external editor to review each API definition before it leaves the door, as well as some sort of automated validation using JSON schema, that would work against a common set of API design standards.

<p>I can tell that Predix has an extremely powerful stack of Internet of Things API resources. They have insight, predictive, and event-driven layers, and a wealth of resources for device operators to put to work. They just need another layer of API design polish on their APIs, as well as ensuring their API documentation reflects this design polish, helping bring it all home. If they did, I’m guessing they would see their adoption numbers increase. It can be tough to come into someone’s world and understand the value they bring to the table, even with simple API resources, but with something as robust and complex as what Predix is up to, even an experienced integrator like me is having trouble getting up to speed on what was possible. A little API design and documentation polish would go a long way to reduce the friction for new consumers getting up to speed.

<p>I struggle with making sure some of my writing gets the editing love before it gets out the door. I also struggle with making sure my own API definitions and designs get the love they need before they see the light of day. As a one person show I just do not have the resources it always takes to deliver at the scope I need. So I fully understand the challenge of small startups when it comes to investing in proper API design across their operations–you just don’t always have the time to slow down and invest in a common API design guide, and the training and awareness across teams. I don’t want to shame the Predix team, as I can tell they’ve invested a lot into their APIs. I just want to make sure they understand that a little investment in API design will go a long ways in helping them better achieve their goals as an Internet of Things API provider.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/07/09/i-love-the-api-enthusiasm-predix-but-please-publish-an-api-style-guide-for/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/07/09/concerns-around-managing-many-microservice-repositories-and-going-with-a-mono/">Concerns Around Managing Many Microservice Repositories And Going With A Mono</a></h3>
        <span class="post-date">09 Jul 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Concerns Around Managing Many Microservice Repositories And Going With A Mono’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/lacloudydaypano_blue_circuit.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/lacloudydaypano_blue_circuit.jpg" width="100%" align="center" />
<p>About half of the teams I work with on microservices strategy are beginning to freak out about the number repositories they have, and someone is regularly bringing up the subject of having a mono repo. Which is usually a sign for me that a group is not ready for doing the hard work involved with microservices, but also shows a lack of ability to think, act, and respond to things in a distributed way. It can be a challenge to manage many different repositories, but with a decoupled awareness of the sprawl that can exist, and some adjustments and aggregation to your strategy it can be doable, even for a small team.
<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/lacloudydaypano_blue_circuit_3.jpg" width="100%" align="center" />
<p>The most import part of sticking to multiple repositories is for the sake of the code. Keeping services decoupled in reality, not just name is extremely important. Allowing the code behind each service to have its own repository, and build a pipeline that keeps things more efficient, nimble, and fast. Each service you layer into a mono repo will be one more chunk of time needed when it comes to builds, and understanding what is going on with the codebase. I know there are a growing number of strategies for managing mono repos efficiently, but it is something that will begin to work against your overall decoupling efforts, and you are better off having a distributed strategy in place, because code is only the first place you’ll have to battle centralization, in the name of a more distributed reality.
<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/lacloudydaypano_blue_circuit_4.jpg" width="100%" align="center" />
<p>Github, Gitlab, and Bitbucket all have an API, which makes all of your repositories accessible in a programmatic way. If you are building microservices, and working towards a distributed way of doing things, it seems like you should be using APIs to aggregate and automate your reality. It is pretty easy to setup an organization for each grouping of microservices, and setup a single master or control repository where you can aggregate information, and activity across all repository–using Github Pages (or other static implementation) as a central dashboard, and command center for managing and containing microservice sprawl. Your repository structure should reflect your wider microservices organization strategy, and all the moving parts should be allowed to operated in a distribution fashion, not just the code, but also the conversation, support, and other essential elements.
<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/lacloudydaypano_blue_circuit_5.jpg" width="100%" align="center" />
<p>I’m spending more time learning about Kubernetes, and studying how microservices are being orchestrated. Like other aspects of the API world, I’m going to focus on not just the code, but also the other communications, support, dependencies, security, testing, and critical building blocks of delivering APIs. I feel like many folks Im talking with are getting hung up on the distributed nature of everything else, while trying to distribute and decouple their code base. Microservices are definitely not easy to do, and decoupling isn’t an automatic solution to all our problems. From what I am seeing, it is opening up more problems than it is solving in some of the organizations I am working with, and causing a lot of anxiety about the scope of what teams will have to tackle when trying to find success with microservices across their increasingly distributed organizations.



</p></p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/07/09/concerns-around-managing-many-microservice-repositories-and-going-with-a-mono/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/06/29/mayors-governors-and-lawmakers-tech-companies-are-getting-rich-mining-your-constituents-data/">Mayors Governors And Lawmakers Tech Companies Are Getting Rich Mining Your Constituents Data</a></h3>
        <span class="post-date">29 Jun 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Mayors, Governors, And Lawmakers: Tech Companies Are Getting Rich Mining Your Constituents Data’</p>

<p>image: http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/64<em>99_800_500_0_max_0</em>-5_-1.jpg</p>

<hr />

<p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/64_99_800_500_0_max_0_-5_-1.jpg" width="45%" align="right" style="padding: 15px;" />
<p>It has been a fascinating and eye opening experience sitting at the intersection of tech startups and the web, mobile, and device applications they’ve built over the last decade. In 2010 I was captivated by the power of APIs to deliver resources to developers, and end-users. In 2018, I’m captivated by the power of APIs to mine end-users like they are just a resource, with the assistance of the developer class. A dominant white male class of people who are more than willing to look the other way when exploitation occurs, and make for the perfect “tools” to be exploited by the wealthy investor class.

<p>While I do not have much hope for diversity efforts in tech, or the bro culture waking up, I do have hope for city and state/provincial lawmakers to wake up to the exploitation that is going on. I’ve seen hints of cities waking up to the mining that has been occurring by Facebook and Google over the last decade. The open exploitation and monetization of a city’s and state’s most precious resources–their constituents. While some cities are still swooning over having Amazon set up shop, or Facebook to build a data center, these company’s web, mobile, and device applications have infiltrated their districts been probing, mining, extracting, and shipping value back to offshore corporate headquarters.

<p>You can see this occurring with Google Maps, which has long been a darling of the API community. We were all amazed at the power of this new mapping resource, something us developers could never have built on our own. We all integrated it into our websites, and embedded it into our mobile applications. We could use it to navigate and find where we were going, completely unaware of the power of the application to mine data from our local transit authorities, businesses, as well as track the location of all of us at each moment. Google Maps was the perfect trojan horse to invade our local communities, extract value, only leaving us with a handful of widgets and embeddable apps to keep us hooked, and working for the Google machine–always giving as little back as possible.

<p>Facebook is probably the highest profile example, connecting our families and communities, while it also disrupted our local news, and information channels, as well as take control over our elections. While connecting us all at the local level, we failed to see we were being connected to the Facebook corporate machine, reminiscence of the Matrix movie of the 1990s. Now we are just mindlessly scrolling, clicking, and emotionally responding, where we are simultaneously being mined, tracked, influenced, nudged, and directed. Something that was once done out in the open for many years through a public API program, but is slowly being closed up and done privately behind closed doors, so that a new regulatory show can be performed to demonstrate that Facebook really cares.

<p>I’m spending more time in Europe, having conversations with regulators and business leaders about a more sensible future driven by APIs. Having conversations with city leaders about the value of their data, content, and algorithms. Discussing the value of their constituents personal data, privacy, and security. Talking about the imperialist nature of Facebook, Google, Twitter, Amazon, and Microsoft, and how they invade, conquer, then extract value from our communities. Helping mayors, governors, and other lawmakers realize the value they have before it is gone, and helping them realize that they can take control over their digital resources using APIs, and gain an upper hand in the conversations that are already occurring across the web.



</p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/06/29/mayors-governors-and-lawmakers-tech-companies-are-getting-rich-mining-your-constituents-data/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/06/29/looking-for-sponsors-for-apistrat-2018-in-nashville-tn-this-september/">Looking For Sponsors For Apistrat 2018 In Nashville Tn This September</a></h3>
        <span class="post-date">29 Jun 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Looking For Sponsors For APIStrat 2018 In Nashville, TN This September’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/apistrat/apistrat-conference-sponsorship-prospectus.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/apistrat/apistrat-conference-sponsorship-prospectus.png" width="45%" align="right" style="padding: 15px;" />
<p>We are building up to <a href="https://events.linuxfoundation.org/events/apistrat-2018/">the 9th edition of API Strategy &amp; Practice (APIStrat) happening in Nashville, Tennessee this September 24th through 26th</a>. As part of the build up we are looking for sponsors to help make the event happen, bringing the API community together once again to share stories from the trenches, and discuss healthy practices that are allowing companies, organizations, institutions, and government agencies make an impact when it comes to their API operations.

<p><a href="http://events17.linuxfoundation.org/events/apistrat">The 2017 edition of APIStrat in Portland, OR</a> was a huge success, and help complete the transition of APIStrat to be part of the OpenAPI Initiative (OAI). After seven editions, and four years of operation exclusively by 3Scale and API Evangelist, the event has matured and will continue growing under the guidance of the OAI, and the community that has evolved around the OpenAPI specification. Presenting an opportunity for other API providers, and API service providers to get involved by <a href="https://www.openapis.org/membership/join">joining as an OAI member</a> and / or <a href="https://events.linuxfoundation.org/events/apistrat-2018/sponsor/">sponsoring APIStrat</a>, and joining the conversation that has been going on in the community since early 2013.

<p><a href="https://events.linuxfoundation.org/events/apistrat-2018/sponsor/">You can download the APIStrat conference prospectus from the Linux Foundation / OAI event website</a>, and there is a form to submit to learn more about sponsoring. You can also email  <a href="/cdn-cgi/l/email-protection" class="__cf_email__" data-cfemail="87e6f7eef4f3f5e6f3f4f7e8e9f4e8f5f4efeef7f4c7ebeee9f2ffe1e8f2e9e3e6f3eee8e9a9e8f5e0">[email&#160;protected]</a> if you’d like to get plugged in. Feel free to also reach out to me as well, as I’m in charge of trying to drum up sponsors, and expand our base beyond just the OAI membership, and the companies who stepped up last year. Helping API providers and service providers understand what a community event APIStrat is, and help it differentiate from the other API, and tech-focused conferences that are happening.

<p>I’m definitely biased, as I help start and grow the conference, but after running tech events for over a decade, it was important to me that APIStrat grow into a community event about ideas, and less about vendors and product pitches. It is a great opportunity for API providers, and API service and tooling providers to actually rub elbows with developers who are building on top of their APIs, and putting their tools and services to work. The keynotes, sessions, and workshops are always great, but the hallway conversations are always where the magic happens for me. Please step up and help make sure the event continues to grow, and help sponsor APIStrat in Nashville. If you do, I promise to cover your APIs here on the blog, and help tell the story of the impact you are making on the community leading up to the event this September in Tennessee!!



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/06/29/looking-for-sponsors-for-apistrat-2018-in-nashville-tn-this-september/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/06/29/graphql-and-rest-differences-explained-with-burgers/">Graphql And Rest Differences Explained With Burgers</a></h3>
        <span class="post-date">29 Jun 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘GraphQL And REST Differences Explained With Burgers’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/graphql/DgsXLk_X4AEKiJJ.jpg</p>

<hr />

<p><a href="https://twitter.com/NikkitaFTW/status/1011928066816462848"><img src="https://s3.amazonaws.com/kinlane-productions2/graphql/DgsXLk_X4AEKiJJ.jpg" width="45%" align="right" style="padding: 15px;" /></a>GraphQL folks keep on with the GraphQL vs REST narratives, rather than a REST and / or GraphQL narrative lately with <a href="https://twitter.com/NikkitaFTW/status/1011928066816462848">a recent burger meme/narrative</a>. Continuing to demonstrate their narrow view of the landscape, and revealing the short lived power of an adversarial approach to community building. I get why people do this, because they feel they are being clever, and that the click response from the echo chamber re-enforces it, but ultimately it is something that won’t move the conversation forward, but it does get them kudos within their community–which is what many of them live for.

<p>I’ll start with the usual disclaimer. I actually like GraphQL, and prescribe it as part of my API toolbox. However, rather than a REST vs GraphQL approach, I sell it as REST and GraphQL, depending on the developer audience we are trying to reach with our efforts. Whether or not you use GraphQL on your platform is completely based upon knowing your developers, and working with a group that understands the resources before offered–something the GraphQL community continues their failure to see. Also their adversarial marketing tactics has lost me several GraphQL projects in government because it comes off as being a trend, and not something that will be around very long.

<p>With that said, I think this meme tells a great story about GraphQL, and demonstrates the illnesses of not the technology, but the ideology and beliefs of the community. I had a couple of thoughts after seeing the Tweet, and reviewing the replies:

<p>1) I thought it was an anti-GraphQL meme at first. Demonstrating that you can build a horrible burger with some very well known ingredients. <a href="https://www.theverge.com/2017/10/30/16569346/burgergate-emoji-google-apple">Spoofing on the burger emoji drama that has been going on in recent years.</a>. I mean, is the lettuce the plate in the GraphQL burger?

<p>2) Like GraphQL, the food choices demonstrate that GraphQL works well in very controlled environments. Where there are known ingredients, and your clients/customers/developers know the ingredients, and know what they want. Hell yeah GraphQL is a better choice in this environment. The problem is you are selling it as a a better solution than REST in general. I hate to tell you, but most of the business getting done in the world IS NOT FAST FOOD.

<p>3) The meme demonstrates the whole fast food, limited world view of many technologists who work with known ingredients, and think everyone is just like them. This tool works for me, and everyone is just like me, so what I use is cool, and everyone should use the tools and the process that I do. A common perspective out of the white bread (bun?) world of technology.

<p>4) Let’s take this GraphQL meme and begin applying it to an Ethiopian, Greek, or French menu. Let’s take it and apply to a BBQ, catering, or maybe home cooked family gathering. Try applying it when you get a food basket from your local community supported agriculture (CSA), where you have no idea the ingredients that are coming, and you’ll have to adjust based upon the season and whatever is available to you that week. Maybe do the same for a food shelter and pantry, does everyone get it their way?

<p>5) There are some restaurants in New York I’d love to take you to, and have you ask for it your way. I’d love to see you get yelled out of the place when you think you know more than the chef, and you always should have things your way. Really, you know more than someone who has been cooking for years, and your fast food loving, unsophisticated tastes are going to dictate what gets served? Get outta here!!

<p>6) I love API to restaurant menu analogies. I wrote one <a href="https://apievangelist.com/2014/05/23/restaurant-menus-as-analogy-for-api-copyright/">to support the Oracle v Google copyright case</a>, which <a href="https://apievangelist.com/2014/05/23/restaurant-menus-as-analogy-for-api-copyright/">the Google lawyer referenced in the latest round</a>. There are many ways you can use restaurants and food to make API comparisons, and educate people about the potential of APIs. I’m sorry though, this one just wasn’t sophisticated enough to really bring home the potential of APIs, and it was more about reflecting this same unsophisticated approach of people marketing and telling stories around GraphQL.

<p>I’ll say it again, and again, and again. I’m not anti-GraphQL. I’m against ya’ll saying it is a replacement for REST. Stop it. It’s dumb. It shows your lack of awareness of the larger API world. It shows you live in tech isolation, where you think everyone wants it your way. Most developers I know do not have a clue as to what they want. They don’t understand the existing schema being used, and need menus, and hand-crafted buffets. Sure, there are development groups who know exactly what they need, and have a full grasp on the schema and resource models being used, but this isn’t EVERYONE!! Stop it. I get GraphQL, but I’m getting tired of coming across new APIs I don’t understand at all, and being expected to just know what I want. I love GraphQL for Github because I KNOW GITHUB. I don’t love GraphQL for <a href="http://docs.openstates.org/en/latest/api/v2/">the OpenStates API</a>, because I have no clue what the schema and model is for their API–please do the extra work to document your resources, and provide me intelligent, well-crafted paths to get at your valuable data.

<p>Instead of bashing REST, how about thinking more about REST as a starter. Having feedback loops in place to get to your audience. Sure, if it is all internal development, in service of a known group of React developers, go for it–use GraphQL! However, if it is a public API, start with REST, establish feedback loops, and get to know your audience. If enough developers are requesting a query language (GraphQL isn’t the only show in town), and it makes sense in your roadmap, then offer GraphQL alongside REST, but not instead of REST. GraphQL works in a known known, and sometimes a known unknown environment, but not in an unknown unknown environment. The community needs to wake up and realize this. Stop selling it as a replacement to REST, and realize it is just another tool in the API toolbox. Y’all are just hurting your cause, and running some people off with this regular REST v GraphQL storytelling. In the end, you are just showing your lack of knowledge and respect for the web–just like Facebook does.

<p>P.S. Anyone who has their feelings hurt by this post, needs to get out more. Maybe change jobs, move to a new city and industry. You need to see and experience more than you have currently.



</p></p></p></p></p></p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/06/29/graphql-and-rest-differences-explained-with-burgers/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/06/26/why-is-api-versioning-in-the-path-still-the-dominant-pattern/">Why Is Api Versioning In The Path Still The Dominant Pattern</a></h3>
        <span class="post-date">26 Jun 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Why Is API Versioning In The Path Still The Dominant Pattern?’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/los-angeles-downtow-freeway_blue_circuit_5.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/los-angeles-downtow-freeway_blue_circuit_5.jpg" width="45%" align="right" style="padding: 15px;" />
<p>API versioning is almost always one of the top attended discussions at conferences I help organize, and one of the first questions I get in the QA sessions at workshops I conduct. People want to understand the “right way” to version, when it my experience there is rarely ever a “right way” to version your APIs. There are commonly held practices regarding sensible ways to version your APIs, as well as dominant patterns for how you version APIs, but there isn’t any 100% solid answer to the question, despite what many folks might say.

<p>In my experience, the most commonly held approach to properly versioning your APIs (if you are going to), is to put the major and minor version in your header and / or combine it with content-type negotiation via your header. However, even with this knowledge being widely held, the most dominant pattern for versioning your APIs is sticking it in the URL of your API. I know many API providers who put the version in the header, despite many on their team fully being aware that it is something that should be put in the header. So, why is this? Why do people still do it the “wrong way”, even though then know how to do it the “right way”?

<p>I feel like this phenomenon reflects the wider API space, and how upside down many API belief systems are. People put the version in the URL because it is easier for them, and it is easier for their developers to understand. While headers are a native aspect of developing using the web, they are still very foreign and unknown to most developers. While this shows the lack of web literacy that is rampant amongst developers, it also demonstrates why simple web APIs have dominated the landscape–they are easy for a wide segment of developers to understand. An aspect of why this whole API thing has worked that many technologists overlook, and take for granted as they try to push the next trend or solution on the sector.

<p>While conducting workshops, I always teach the more sensible patterns around versioning, but I can’t always sell them as the “right way”. Because I don’t see a “right way”. I see people trying to get a job done, and reach a wide audience. I see people trying to keep things simple for their developers, and taking the path of least resistance. I see a whole lot of web literacy education that needs to occur across the tech sectors and in school. I just don’t see any perfect answer to the API versioning debate. I see a whole lot of interesting and useful patterns, and I see people doing the best they can with what they have. Reflecting why APIs work so well, because they are scrappy, often times simple, and allow people to get business done on the web using low cost, easy to understand approaches to making resources available.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/06/26/why-is-api-versioning-in-the-path-still-the-dominant-pattern/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/06/26/not-liking-openapi-fka-swagger-when-you-have-no-idea-what-it-does/">Not Liking Openapi Fka Swagger When You Have No Idea What It Does</a></h3>
        <span class="post-date">26 Jun 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Not Liking OpenAPI (fka Swagger) When You Have No Idea What It Does’</p>

<p>image: http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/64<em>181_800_500_0_max_0_1</em>-1.jpg</p>

<hr />

<p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/64_181_800_500_0_max_0_1_-1.jpg" width="45%" align="right" style="padding: 15px;" />
<p>People love to hate in the API space. Ok, I guess its not exclusive to the API space, but it is a significant aspect of the community. I receive a regular amount of people hating on my work, for no reason at all. I also see people doing it to others in the API space on a regular basis. It always makes me sad to see, and have always worked to try to be as nice as I can to counteract the male negativity and competitive tone that often exists. While I feel bad for the people on the receiving end of all of this, I often times feel bad for the people on the giving end of things, as they are often not the most informed and up to speed folks, who seem to enjoy opening their mouth before they understand what is happening.

<p>One thing I notice regularly, is that these same people like to bash on is OpenAPI (fka Swagger). I regularly see people (still) say how bad of an idea it is, and how it has done nothing for the API space. One common thread I see with these folks, which prevents me from saying anything to them, is that it is clear they really don’t have an informed view of what OpenAPI is. Most people spend a few minutes looking it, maybe read a few blog posts, and then establish their opinions about what it is, or what it isn’t. I regularly find people who are using it as part of their work, and don’t actually understand the scope of the specification and tooling, so when someone is being vocal about it and doesn’t use actually it, it is usually pretty clear pretty quickly how uninformed they are about the specification, tooling, and scope of the community.

<p>I’ve been tracking on it since 2011, and I still have trouble finding OpenAPI specifications, and grasping all of the ways it is being used. When you are a sideline pundit, you are most likely seeing about 1-2% of what OpenAPI does–I am a full time pundit in the game and I see about 60%. The first sign that someone isn’t up to speed is they still call it Swagger. The second sign is they often refer to it as documentation. Thirdly, they often refer to code generation with Swagger as a failure. All three of these views date someone’s understanding to about a 2013 level. If someone is forming assumptions, opinions, and making business decisions about OpenAPI, and being public about it, I’d hate to see what the rest of their technology views look like. In the end, I just don’t even feel like picking on them, challenging them on their assumptions, because their regular world is probably already kicking their ass on a regular basis–no assistance is needed.

<p>I do not feel OpenAPI is the magical solution to fix all the challenges the API space, but it does help reduce friction at almost every stop along the API lifecycle. In my experience, 98% of the people who are hating on it do not have a clue what OpenAPI is, or what it does. I used to challenge folks, and try to educate them. Over the years I’ve converted a lot of folks from skeptics to believers, but in 2018, I think I’m done. If someone is openly criticizing it, I’m guessing it is more about their relationship to tech, and their lack of awareness of delivering APIs at scale, and they probably exist in a pretty entrenched position because of their existing view of the landscape–they don’t need me piling on. However, if people aren’t aware of the landscape, and ask questions about how OpenAPI works, I’m always more than happy to help open their eyes to how the API definition is serving almost every stop along the API lifecycle from design to deprecation, and everything in between.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/06/26/not-liking-openapi-fka-swagger-when-you-have-no-idea-what-it-does/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/06/26/a-public-selfservice-api-platform-as-a-competitive-advantage/">A Public Selfservice Api Platform As A Competitive Advantage</a></h3>
        <span class="post-date">26 Jun 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘A Public Self-Service API Platform as a Competitive Advantage’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/old-gas-pumps.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/old-gas-pumps.jpg" width="45%" align="right" style="padding: 15px;" />
<p>When it comes to providing data, content, and even ML and AI models via APIs, having a public platform will become a competitive advantage. I know that many companies see it as giving away something, especially when your resources and business model are not defensible, but in reality having a publicly available, 24/7 operational, self-service solution will give you an edge over your more proprietary approaches to making resources available on the web. Sure, your competition will be able to often get in there without friction, but so will your customers–how many customers vs. competitors do you have?

<p>I know many companies believe in the power of a sales team to be able to squeeze every last penny out of would be customers, but a sales only approach leaves a significant amount of self-service revenue on the table. Throughout the course of our busy days, many IT decision makers just do not have the time for the phone calls and lunches involved with the traditional sales process. Sure, there are some IT decision makers who fill their schedule with these types of conversations, but there are a growing number who depend on self-service, SaaS approaches to getting business done on a daily basis–look at the growth of Amazon Web Services over the last decade if you need a reference point.

<p>If you think a public API platform involves giving away your intellectual property in 2018, you are severely behind the times on where the sector has been headed for about a decade. Far enough behind that you may not be able to play catch up at the speed in which things are shifting. A public portal, documentation, and other resources does not mean you are giving anything away. Even having a free tier doesn’t mean that you are giving away the farm. Modern API management solutions allow you to generate leads, let developers kick the tires, while also still being able to charge what the market will bear for your data, content, and algorithms. You can also still have a sales force that will swoop in on leads, and close the deals when it makes sense.

<p>Even with a self-service API, and robust documentation, code samples and SDKs, API providers still have to work hard to reduce friction when on boarding–providing OpenAPI definitions, Postman collections, connectors, plugins, and platform specific development kits to make integration quick and painless. If you don’t even have a public self-service presence you are just getting in the way of integration, and a growing number of your customers will just choose to go with your competitors who have opted to get out their way. The companies who don’t have self-service in their DNA won’t be able to compete in the new landscape, making it essential to be able to do business out in the open, in a self-service way, essential to staying competitive in the new API-driven landscape.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/06/26/a-public-selfservice-api-platform-as-a-competitive-advantage/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/06/22/staying-informed-of-api-changes-using-streamdata-io/">Staying Informed Of Api Changes Using Streamdata Io</a></h3>
        <span class="post-date">22 Jun 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Staying Informed of API Changes Using Streamdata.io’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/twitter/twitter-automation.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/twitter/twitter-automation.png" width="45%" align="right" style="padding: 15px;" />
<p>My friend James Higginbotham (<a href="https://twitter.com/launchany?lang=en">@launchany</a>) was sharing his frustration with being able to stay in tune with changes to a variety of APIs. Like me, James works to stay in tune with a variety of signals available via platforms like Twitter, Github, and other commonly used services. These platforms don’t always properly signal when things are updated, changed, or advanced, making it difficult to understand the granular changes that occur like likes, votes, edits, and other common events that occur via highly active platforms.

<p>This challenge is why the evolution towards a more event-driven approach to operating an API platform is not just more efficient, it gives users what they need. Using event-driven architectural approaches like Webhooks, and real times streams. This is one of the reasons I’m interested in what <a href="http://streamdata.io">Streamdata.io</a> does, beyond them helping support me financially, is that they allow me to focus on the event-driven shift that is occurring with many leading API providers, and needs to be brought to the attention of other platforms. Helping API providers be more efficient in what they are doing, while also meeting the needs of the most demanding customers like James and myself.

<p>It is easy to think Streamdata.io is just about streaming real time data. This is definitely a large aspect of what the SaaS solution does, but the approach to using Server-Sent Events (SSE), with incremental updates using JSON Patch adds another useful dimension when it comes to understanding what has changed. You can proxy an existing HTTP API that returns a JSON response using Streamdata.io, and the first response will look just like any other, but every pushed response after that will be a JSON Patch of just what has changed. Doing the heavy lifting of figuring out what has changed in each API response and only sending you the difference, and allowing you to focus only on what has changed, and not having to rely on timestamps, and other signals within the JSON response to understand what the difference is from the previous API response.

<p>Using Streamdata.io you don’t have to keep polling an API asking if things have changed, you just proxy the API and you get pushed changes via an HTTP stream. You also don’t have to sort through each response and try to understand what changed, you just take the JSON Patch response, and it tells you what has changed. I’m going to create a draft blueprint for James of how to do this, that he can use across a variety of APIs to establish multiple API connections using long running, server-side API streams for a variety of topics. Allowing him to monitor many different APIs, and stay in tune with what changes as efficiently as possible. Once I craft a generic blueprint, I’m going to apply to Twitter and see if I can increase the efficiency of my Twitter monitoring, by turning their REST APIs into real time feeds using <a href="http://streamdata.io">Streamdata.io</a>.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/06/22/staying-informed-of-api-changes-using-streamdata-io/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/06/21/working-to-keep-programming-language-dogma-at-edges-of-the-api-conversation/">Working To Keep Programming Language Dogma At Edges Of The Api Conversation</a></h3>
        <span class="post-date">21 Jun 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Working To Keep Programming Language Dogma At Edges Of The API Conversation’</p>

<p>image: http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/28<em>68_800_500_0_max_0</em>-5_-1.jpg</p>

<hr />

<p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/28_68_800_500_0_max_0_-5_-1.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I’m fascinated by the dominating power of programming languages. There are many ideological forces at play in the technology sector, but the dogma that exists within each programming language community continues to amaze me. The potential absence of programming language dogma within the world of APIs is one of the reasons I feel it has been successful, but alas, other forms of dogma tends to creep in around specific API approaches and philosophies, making API evangelism and adoption always a challenge.

<p>The absence of programming languages in the API design, management, and testing discussion is why they have been so successful. People in these disciplines have ben focused on the language agnostic aspects of just doing business with APIs. It is also one of the reasons the API deployment conversation still is still so fragmented, with so many ways of getting things done. When it comes to API deployment, everyone likes to bring their programming language beliefs to the table, and let it affect how we actually deliver this API, and in my opinion, why API gateways have the potential to make a comeback, and even excel when it comes to playing the role of API intermediary, proxy, and gateway.

<p>Programming language dogma is why many groups have so much trouble going API first. They see APIs as code, and have trouble transcending the constraints of their development environment. I’ve seen many web or HTTP APIs called Java API, Python APIs, or reflect a specific language style. It is hard for developers to transcend their primary programming language, and learn multiple languages, or think in a language agnostic way. It is not easy for us to think out of our boxes, and consider external views, and empathize with people who operate within other programming or platform dimensions. It is just easier to see the world through our lenses, making the world of APIs either illogical, or something we need to bend to our way of doing things.

<p>I’m in the process of evolving from my PHP and Node.js realm to a Go reality. I’m not abandoning the PHP world because many of my government and institutional clients still operate in this world, and I’m using Node.js for a lot of serverless API stuff I’m doing. However I can’t ignore the Go buzz I keep coming stumbling upon. I also feel like it is time for a paradigm shift, forcing me out of my comfort zone and push me to think in a new language. This is something I like to do every five years, shifting my reality, keeping me on my toes, and forcing me to not get too comfortable. I find that this keeps me humble and thinking across programming languages, which is something that helps me realize the power of APIs, and how they transcend programming languages, and make data, content, algorithms, and other resources more accessible via the web.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/06/21/working-to-keep-programming-language-dogma-at-edges-of-the-api-conversation/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/06/21/do-not-try-to-service-all-the-stops-along-the-api-lifecycle-as-an-api-service/">Do Not Try To Service All The Stops Along The Api Lifecycle As An Api Service</a></h3>
        <span class="post-date">21 Jun 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Do Not Try To Service All The Stops Along The API Lifecycle As An API Service’</p>

<p>image: http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/66<em>189_800_500_0_max_0</em>-5_-1.jpg</p>

<hr />

<p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/66_189_800_500_0_max_0_-5_-1.jpg" width="45%" align="right" style="padding: 15px;" />
<p>One thing I see a lot from API service providers who are selling their services to the API sector, is that once they find success servicing one stop along the API lifecycle, they often want to service other additional stops. I don’t have a problem with API service providers delivering across multiple stops along the API lifecycle, however I do caution of trying to expand across too many stops, and potentially doing any of them poorly, rather than partnering with other more specialized API service providers to help you focus on what you do best.

<p>I’m a big advocate for encouraging API providers to service one to five stops along the API lifecycle well, and then partner for helping deliver the rest of the stops. I know that all your investors are encouraging to take as many pieces of the puzzle as you possibly can, but there is more money in doing a handful of things really well, over doing many things poorly. Try to be an expert in a handful of specialized areas, over being a generalist. Then make sure your platform is as interoperable as possible, while investing in your partner program to attract the best of breed API service providers to your platform.

<p>This balance between focusing on a handful of stops and partnering is why <a href="http://plugin.apievangelist.com/">I emphasize and study  common approaches to delivering plugins</a>. All platforms should invest in plugin infrastructure, to allow for extending their reach beyond the stops that a platform services. Feature creep, and platform bloat is a real challenge, especially when you have investors whispering in your ear to keep building, and a very vocal, but often long-tail group of users demanding solutions to their unique problems. Plugin and connector architecture is how you help manage this reality, and provide a relief valve for delivering too many features as part of your platform, while also bringing in potential partners who can help extend what your platforms in a way that allows you to keep doing what you do best.

<p>I see a big push going on from many legacy API service providers, as well as some of the next generation of startups bringing services and tooling to the space. I feel like many people desire a single solution to do everything, but then fail to realize that every platform that has attempted this in the past ends up failing, because you can’t be everything to everyone. I want my API service providers to stick to doing a handful of things well, but then acknowledge that I will also be using several other tools to what get I need accomplished on a daily basis. Ideally all of my tools are interoperable with import and export capabilities, as well as a suite of API driven connectors, and plugins that allow me to keep all of my services and tooling working together in concert. For this reality to occur we all have to resist the temptation to lock our customers in, and put down delusions that we can serve all stops along a modern API lifecycle all by ourselves.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/06/21/do-not-try-to-service-all-the-stops-along-the-api-lifecycle-as-an-api-service/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/06/19/adding-a-lead-to-salesforce-using-the-rest-api/">Adding A Lead To Salesforce Using The Rest Api</a></h3>
        <span class="post-date">19 Jun 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Adding A Lead To SalesForce Using The REST API’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/salesforce/salesforce-rest-api.png</p>

<hr />

<p><a href="https://developer.salesforce.com/page/Salesforce_APIs"><img src="https://s3.amazonaws.com/kinlane-productions2/salesforce/salesforce-rest-api.png" width="45%" align="right" style="padding: 15px;" /></a>
<p>I spend a lot of time talking about <a href="https://developer.salesforce.com/page/Salesforce_APIs">the SalesForce API</a>, using it as a reference for where the API evolution began 18 years ago, but it has been a long time since I’ve actually worked with the SalesForce API. Getting up and running with any API, especially iconic APIs that we all should be familiar with, is always an enlightening experience for me. Going from zero to understanding what is going on and actually achieving the API call(s) you want, is really what this game is all about.

<p>As part of some work I’m doing <a href="http://streamdata.io">with Streamdata.io</a> I needed to be able to add new leads into SalesForce, and I thought it would be a good time for me to get back into the saddle with the SalesForce REST API–so I volunteered to tackle the integration. The SalesForce API wasn’t as easy to get up and running as many simpler APIs I onboard with is, as the API docs isn’t as modern as I’d expect, and what you need is buried behind multiple clicks. Once you find what you are looking for, and click numerous times, you begin to get a feel for what is going on, and the object model in use becomes a little more accessible.

<p>In addition to finding what you need with the SalesForce REST API, you have to make sure you have a handle on the object structure and nuance of SalesForce itself. For this story, I am just working with one object–Leads. I’m using PHP to work with the API, and to begin I wanted to be able to get leads, to be able to see which leads I currently have in the system:

<script src="https://gist.github.com/kinlane/aa472c9338a491f204ce77cb9e35fecb.js"></script>

<p>I will add pagination, and other elements in the future. For now, I just wanted to be able to get the latest leads I have in the system to help with with some checks on what is being added. Now that I can check to see what leads are in the system, I wanted to be able to add a lead, with the following script:

<script src="https://gist.github.com/kinlane/dc45901494369ca601a36ccdabc96086.js"></script>

<p>I am only displaying some of the default fields available for this example, and you can add other custom fields based upon which values you wish to add. Once I have added my lead, I wanted to be able to update with a PATCH API call:

<script src="https://gist.github.com/kinlane/9bdb62b79b1f0f04b60a182a9ed4a743.js"></script>

<p>Now I am able to add, update, and get any leads I’m working with via the SalesForce API. The project gave me a good refresher for what is possible with the SalesForce API. The API is extremely powerful, and something I want to be up to speed on so that I can intelligently respond to questions I get. I wish the SalesForce API team would spend some time modernizing their API portal and documentation, providing a more coherent separation between the different flavors of their API, and provide OpenAPI driven documentation, as well as Postman Collections. It would have saved me hours of working through their API docs, and playing around with different API calls in Postman before I was able to successfully OAuth, and make my first call against the accounts and leads API endpoints.

<p>While I think SalesForce remains a worthwhile API to showcase when I talk about the history of APIs, and the power of providing web APIs, their overall documentation and approach is beginning to fall behind the times. SalesForce possesses many of the building blocks I recommend other API providers operate, and are very advanced in some of their support and training efforts, but their documentation, which is the biggest pain point for developers, leaves a lot to be desired. I’m used to having to jump through hurdles to get up and running APIs, so the friction for me was probably less than a newer API developer would experience. I could see some of the domain instance url, versioning, and available API paths proving to be a significant hurdle if you didn’t understand what was going on. Something that could be significantly minimized with some simpler, more modern API docs, and OpenAPI and Postman Collections available.



</p></p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/06/19/adding-a-lead-to-salesforce-using-the-rest-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/06/18/va-api-landscape-analysis-and-roadmapping-project-report/">Va Api Landscape Analysis And Roadmapping Project Report</a></h3>
        <span class="post-date">18 Jun 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘VA API Landscape Analysis and Roadmapping Project Report’</p>

<p>image: https://api-evangelist.github.io/va-api-landscape/images/lighthouse.png</p>

<hr />

<p align="center"><img src="https://api-evangelist.github.io/va-api-landscape/images/lighthouse.png" width="90%" align="center" />

<p>This report summarizes Skylight’s evaluation of the VA’s public datasets, which exist within the <a href="https://va.gov/">va.gov</a> web domain, as well as an analysis of what types of data representatives of the Veteran community expressed would be most useful and valuable to Veterans and their supporters if made more digitally accessible and available by the VA. This report also outlines potential resources that can be turned into application programming interfaces (APIs) as part of the VA’s Lighthouse platform initiative, and actions the VA should consider to move forward successfully.

<h2 id="landscape-analysis">Landscape analysis</h2>

<h3 id="the-purpose">The purpose</h3>

<p>APIs are the next evolution in the web, and shouldn’t be thought of as the latest tech trend or vendor solution. The first phase of the web was about delivering data and content to humans using a browser. The second phase of the web is about delivering that same data and content to other applications and algorithms using APIs.

<p>With that said, the purpose of this landscape analysis is, in effect, to assist the VA in evaluating the data and content that they’ve made available during the first phase of the web. This, in turn, will help set the stage for the VA to make smart investments in phase two of their web presence.

<p>The VA has already signaled they’re committed to investing in the second phase of their web presence with the announcement of the <a href="https://www.oit.va.gov/developer/">Lighthouse API platform initiative</a>. Our landscape analysis will help ensure that the Lighthouse program is aware of what types of data and content that the VA has already identified as important to serving the Veteran community. This visibility will allow the Lighthouse program to bring these resources into alignment with the development and operation of their API platform.

<h3 id="the-process">The process</h3>

<p>To help the VA evaluate the landscape that defines their web presence, we employed a “low-hanging-fruit” process that involved identifying the resources that exist across their web properties. That process relied on a spidering script, which we ran for two weeks (and continue to run). To begin the process, we seeded the script by giving it the root URL for the va.gov domain. The script then proceeded to:

<ul>
  <li>
    <p>Parse every URL on the page and store it in a database;
  &lt;/li&gt;
  <li>
    <p>Count every table on the page, and the number of rows that exist in the table;
  &lt;/li&gt;
  <li>
    <p>Count every form that exists on the page; and
  &lt;/li&gt;
  <li>
    <p>Extract the title from the meta tags for each page.
  &lt;/li&gt;
&lt;/ul&gt;

<p>The script then iterated and repeated this for every URL it found on any web page, working to identify each of the following types of data resources:

<ul>
  <li>
    <p>HTML table with more than 10 rows
  &lt;/li&gt;
  <li>
    <p>HTML form
  &lt;/li&gt;
  <li>
    <p>CSV file
  &lt;/li&gt;
  <li>
    <p>XML file
  &lt;/li&gt;
  <li>
    <p>JSON file
  &lt;/li&gt;
  <li>
    <p>XLS/XLSX file
  &lt;/li&gt;
&lt;/ul&gt;

<p>The script ignored any URLs external to the seed domain (va.gov) and many common web objects (for example, images, Word docs, and videos).

<p>As each page was processed, the script tried to identify potential data resources to deliver as an API by parsing several elements from them:

<ul>
  <li>
    <p>The title of the page a file was published on,
  &lt;/li&gt;
  <li>
    <p>The name of the file itself, and
  &lt;/li&gt;
  <li>
    <p>Occasionally a sample of the data.
  &lt;/li&gt;
&lt;/ul&gt;

<p>We took the list of words extracted from this process, and sorted and grouped them by the number of times the word appeared, helping us understand the overall presence of each potential resource. Sometimes this produced a lot of meaningless words, but we worked to filter those out, leaving only the meaningful data resources.

<p>After running the script for a couple of weeks, we spidered nearly 1/3 of the URLs (out a total of 4M+) targeted for processing. That was enough progress to start painting an interesting picture of the VA’s web presence and existing data resources, as described in the sections that follow.

<h3 id="vas-web-presence">VA’s web presence</h3>

<p>The VA has a sprawling web presence, spanning multiple domains and subdomains. We focused our analysis on everything within the va.gov domain. In the future, we can extend the analysis to other domains, but for now we focused on the core VA web presence.

<h4 id="domain-sprawl">Domain sprawl</h4>

<p>The VA’s web presence is spread across a mix of domain levels, including top-level domain and subdomain levels — program, state/region, and city. Domains play a role in providing addresses in the browser so that users can find the resources they need, as well as providing similar addressing for applications to find the resources they need via APIs. The VA domain sprawl reflects the growth of the VA’s web presence, and the lack of overall strategy when it comes to providing web address and routing to all VA resources. The current strategy (or lack thereof) represents the need for location- and program-related resource discovery, whether it’s in the browser, or for other applications via APIs.

<p>We assume there are other domains that haven’t been indexed by our spider, as we were only able to index less than 1/3 of the targeted URLs during our two-week timebox. We can continue indexing and updating numbers beyond this period in order to paint an even more complete picture. Ideally, this would extend beyond the core va.gov domain. Domain and subdomains play an important role in determining how APIs will be accessed, and have a downstream impact on the overall API design, affecting both API path and parameter design. This makes domain and subdomains a top-level consideration early on in the VA’s API journey.

<h4 id="program-domains">Program domains</h4>

<p>After the top-level domains of va.gov and www.va.gov, the most common approach to defining domains is by program, providing the addressing needed for organizing information by relevant programs. We have identified <a href="https://api-evangelist.github.io/va-api-landscape/domains/programs-tag-list/">133 individual program-related domains</a>.

<p align="center"><img src="https://api-evangelist.github.io/va-api-landscape/images/tag-cloud/domains-programs.png" width="90%" align="center" />

<p>While there aren’t consistent naming conventions used in crafting these subdomains, it does demonstrate the prominence of programs, research, and other related groupings used across the VA web presence for organizing resources.

<h4 id="state-domains">State domains</h4>

<p>Beyond program-related domains, state/region level domains are being used to organize data and content for presentation to consumers. Only <a href="https://api-evangelist.github.io/va-api-landscape/domains/state-tag-list/">22 subdomains</a> are represented currently, but the practice demonstrates the prominence of these locations when it comes to organizing information.

<p align="center"><img src="https://api-evangelist.github.io/va-api-landscape/images/tag-cloud/domains-state.png" width="90%" align="center" />

<p>Some states are just paths within the top-level VA domains, while others exist within regional subdomains, with the rest possessing their own subdomain. This demonstrates the importance of states and regions, but also the inconsistency of how domains or paths are used to organize information.

<h4 id="city-domains">City domains</h4>

<p>Lastly, you find many city-related subdomains being used to organize data and content, providing another dimension on how resources are being organized, while demonstrating the dominance of specific cities. We have identified <a href="https://api-evangelist.github.io/va-api-landscape/domains/city-tag-list/">120 individual city-related domains</a>.

<p align="center"><img src="https://api-evangelist.github.io/va-api-landscape/images/tag-cloud/domains-city.png" width="90%" align="center" />

<p>Like states, there isn’t a consistent pattern in which cities have their own subdomain, with others existing as a path within state subdomains or top-level domains. The approach to using cities as part of subdomain DNS addressing further demonstrates the importance of location when it comes to the organization of data and content.

<h3 id="website-outline">Website outline</h3>

<p>As part of the spidering the va.gov domain across the 278 subdomains that exist, over 4M individual URLs were identified, with slightly less than 1/3 of these URLs evaluated for potential data sources to-date. Across these URLs, we took the base path and grouped them by the number of pages and data files that exist.

<p align="center"><img src="https://api-evangelist.github.io/va-api-landscape/images/tag-cloud/path.png" width="90%" align="center" />

<p>While there are many other paths in use across the VA websites, these paths reflect the top paths in use to deliver data and content. Providing a look at what the most relevant resources are when it comes to providing web access to data and content, which is something that should be considered when delivering the same data and content to other applications.

<h4 id="data-vocabulary">Data vocabulary</h4>

<p>After assessing the titles of HTML pages and the names of files, it’s clear there’s no consistent vocabulary in use across VA resources. This, combined with the use of key phrases, acronyms, and singular and plural variances, make it difficult to cleanly identify resources. We opted to use just keywords over phrases and to not expand acronyms as part of the process due to the difficulty in consistently identifying resources.

<p>Even with the difficulties in identifying some resources, we were still able to paint a fairly compelling picture of the resources being exposed as common data formats across VA web properties. That’s because we were able to isolate, group, and identify words that are most commonly associated with resources. From there, we were able to establish some resource lists, which we have organized visually as tag clouds and tag lists.

<h3 id="data-resources">Data resources</h3>

<p>Data is available across VA websites in a variety of formats. We focused on a handful of easy to identify formats, reflecting the low-hanging-fruit aspect of this landscape analysis. While there’s data locked up in simple text files and zipped packages, we chose to look for the easiest to identify and the easiest to publish data sources. Data that’s published by humans usually take the form of CSV files, spreadsheets, and HTML tables. Data that’s published by systems usually take the form of JSON and XML.

<h4 id="data-formats">Data formats</h4>

<p>Each of type of format that we targeted provides a different story as to the type of resources being published. Publication implies that those resources carry some level of value and importance to VA stakeholders, and, potentially, to Veterans, their supporters, and other consumers of this information. We worked to harvest all the data available from several formats, but also worked to identify the top resources available from each type.

<h4 id="csv-files">CSV files</h4>

<p>We discovered <a href="https://api-evangelist.github.io/va-api-landscape/csv/">534 CSV files</a> containing a variety of data. By parsing the titles of the web pages these CSV files were linked from, and the names of some of the files, we identified handful of top resource types present across these files.

<p align="center"><img src="https://api-evangelist.github.io/va-api-landscape/images/tag-cloud/csv.png" width="90%" align="center" />

<p>CSV files tell a particular story because they were most likely published by people working at the VA, who exported the files from spreadsheets and made them available on the website for a reason. This makes them relevant to the VA’s API conversation. You can view a <a href="https://api-evangelist.github.io/va-api-landscape/csv/tag-list/">list of CSV resources</a>, as well as a <a href="https://api-evangelist.github.io/va-api-landscape/csv/">complete list of CSV files</a> on the GitHub repository.

<h4 id="xlsxlsx-files">XLS/XLSX files</h4>

<p>We identified <a href="https://api-evangelist.github.io/va-api-landscape/xls/">6,077 spreadsheets</a> containing a variety of data. After parsing these files for semantic meaning, we identified handful of top resource types present across these files.

<p align="center"><img src="https://api-evangelist.github.io/va-api-landscape/images/tag-cloud/xls.png" width="90%" align="center" />

<p>Similar to CSV files, the presence of spreadsheets tell a very human story. Spreadsheets are the #1 source of data on the web, and reflects the data management and publishing practices across the VA. After evaluating what types of resources are available across these spreadsheets, we have been considering the use of spreadsheets as a data source, as well as a data publishing tool. You can view a <a href="https://api-evangelist.github.io/va-api-landscape/xls/tag-list/">list of XLS/XLSX resources</a>, as well as a <a href="https://api-evangelist.github.io/va-api-landscape/xls/">complete list of XLS/XLSX files</a> on the GitHub repository.

<h4 id="json-files">JSON files</h4>

<p>We identified <a href="https://api-evangelist.github.io/va-api-landscape/json/">467 JSON files</a> containing a variety of data. Unlike the CSV and spreadsheet data sources, JSON files likely represent a more modern systems approach to publishing data and a whole another set of data sources, which should be considered when deploying APIs.

<p align="center"><img src="https://api-evangelist.github.io/va-api-landscape/images/tag-cloud/json.png" width="90%" align="center" />

<p>JSON reflects the latest evolution of data publishing at the VA. But they are only a small subset of the data being made available across VA web properties. This implies they have only become a recent priority when it comes to publishing data in a format that is consumable by developers and computers. You can view a <a href="https://api-evangelist.github.io/va-api-landscape/json/tag-list/">list of JSON resources</a>, as well as a <a href="https://api-evangelist.github.io/va-api-landscape/json/">complete list of JSON files</a> on the GitHub repository.

<h4 id="xml-files">XML files</h4>

<p>We found <a href="https://api-evangelist.github.io/va-api-landscape/xml/">3,099 XML files</a> containing a variety of data. Like JSON files, XML files represent system-generated publication of data. Unlike JSON, however, XML reflects an older systems approach to data publication. And are likely being generated by legacy systems that’ll be important to interface with over the course of the VA’s API journey.

<p align="center"><img src="https://api-evangelist.github.io/va-api-landscape/images/tag-cloud/xml.png" width="90%" align="center" />

<p>XML represents a large portion of the data being published across VA web properties. This list of priority resources represents a significant part of the system-based publishing of data occurring at the VA. And provides a large snapshot of the systems that should be evolved as part of the deployment of APIs. You can view a <a href="https://api-evangelist.github.io/va-api-landscape/xml/tag-list/">list of XML resources</a>, as well as a <a href="https://api-evangelist.github.io/va-api-landscape/xml/">complete list of XML files</a> on the GitHub repository.

<h4 id="html-tables">HTML tables</h4>

<p>We identified <a href="https://api-evangelist.github.io/va-api-landscape/table/">8,393 pages that had tables on them with over 10 rows</a>. These tables represent potentially valuable data and should be considered as part of the VA’s API deployment conversations.

<p align="center"><img src="https://api-evangelist.github.io/va-api-landscape/images/tag-cloud/table.png" width="90%" align="center" />

<p>While HTML tables tell a story about top resources that VA stakeholders thought website users needed access to, these tables also represent data that was published with potential search engine optimization (SEO) in mind. In other words, someone wanted the data to be indexed by search engines in order to make it more readily accessible. You can view a <a href="https://api-evangelist.github.io/va-api-landscape/table/tag-list/">list of table resources</a>, as well as a <a href="https://api-evangelist.github.io/va-api-landscape/table/">complete list of pages containing tables</a> on the GitHub repository.

<h4 id="html-forms">HTML forms</h4>

<p>We identified <a href="https://api-evangelist.github.io/va-api-landscape/form/">9,439 pages with more than one form present</a>, which is usually just a basic search. Similar to HTML tables, these forms provide a window into how the VA is making data available for users to search, explore, and consume in the browser. This, in turn, tells another story of what types of resources are published to VA websites.

<p align="center"><img src="https://api-evangelist.github.io/va-api-landscape/images/tag-cloud/form.png" width="90%" align="center" />

<p>HTML forms often times provide a search mechanism for other table, CSV, JSON, XML, and spreadsheet resources, many of which are listed in the sections above. HTML forms tell their own story as to how and why data are being published across VA websites. And offer another source of resources that are being made available and should be considered as part of the VA’s API deployment efforts.

<h4 id="datagov">Data.gov</h4>

<p>The only external source of data that we analyzed was data.gov, which hosts a number of VA data resources. While somewhat out-of-date, the VA datasets on data.gov tell an important part of the story that should be considered as part of the Lighthouse efforts. There are a lot of lessons to be learned from how data.gov has been used, beyond just understanding what resources have been published there.

<p align="center"><img src="https://api-evangelist.github.io/va-api-landscape/images/tag-cloud/data-gov.png)" width="90%" align="center" />

<p>The resources published to data.gov reflect the VA’s recent past when it comes to making data resources available and accessible via manual downloads and APIs. We think that the most important lesson that the VA should take away from its experience with data.gov is that the VA should own all the data and API resources and syndicate them as part of other external efforts. That way the VA owns the full scope of the effort, which will ultimately result in the VA being more invested in API operations. You can view a <a href="https://api-evangelist.github.io/va-api-landscape/data-gov/tag-list/">list of data.gov resources</a>, as well as a <a href="https://api-evangelist.github.io/va-api-landscape/data-gov/">complete list of data files</a> on the GitHub.

<h3 id="humanizing-the-data">Humanizing the data</h3>

<p>To give us a more human perspective on what types of data resources are most valuable to Veterans and their supporters, <a href="https://www.eventbrite.com/e/veterans-data-needs-prioritization-workshop-tickets-46761726583">we facilitated a series of online workshops</a> using <a href="https://mural.co/">Mural</a>. About 50 people total participated across all three workshops, with about 40% reporting as Veterans and 60% non-Veterans. During these workshops, we employed the <a href="https://articles.uie.com/kj_technique/">KJ technique</a> for establishing group priorities. The KJ technique relies on a focus question to drive the results of the workshop. We used the following focus question:

<p>“What types of data, content, and other resources would be most useful to Veterans and their supporters if the VA could make them more available and accessible on the web, mobile devices, and other platforms?”

<p>The following images capture the results of each workshop:

<p align="center"><img src="https://api-evangelist.github.io/va-api-landscape/images/veteran-data-needs-prioritization-workshop-1-results.png" width="90%" align="center" />

<p align="center"><img src="https://api-evangelist.github.io/va-api-landscape/images/veteran-data-needs-prioritization-workshop-2-results.png" width="90%" align="center" />

<p align="center"><img src="https://api-evangelist.github.io/va-api-landscape/images/veteran-data-needs-prioritization-workshop-3-results.png" width="90%" align="center" />

<p>The yellow cards represent all the ideas, in response to the focus question, that everyone brainstormed. As you can see, these yellow cards were organized into like groups. The blue cards represent descriptive labels that participants gave to each group. The black circles with numbers represent the votes that the participants casted when asked which group labels they thought best answered the focus question. We weighted Veteran votes 2x more heavily than votes from non-Veterans.

<p>You may notice things that seem out of place in the final results (for example, yellow cards that look like they belong to another category). This is largely due to the timeboxed nature of the activities. In other words, not everything could be made perfect, but that doesn’t detract from the overall usefulness of the results.

<p>Given the fact that the results were spread across three different workshop sessions, we took the additional step of normalizing the groupings and merging the votes.

<ol>
  <li>
    <p>Directory of Services/Resources – <strong>34 votes</strong>
  &lt;/li&gt;
  <li>
    <p>Mental Health – <strong>21 votes</strong>
  &lt;/li&gt;
  <li>
    <p>Personal Healthcare Data – <strong>20 votes</strong>
  &lt;/li&gt;
  <li>
    <p>Personalized Self-Service Portal – <strong>20 votes</strong>
  &lt;/li&gt;
  <li>
    <p>Benefits – <strong>13 votes</strong>
  &lt;/li&gt;
  <li>
    <p>Peer Support Networking – <strong>8 votes</strong>
  &lt;/li&gt;
  <li>
    <p>Family Support Networking – <strong>9 votes</strong>
  &lt;/li&gt;
  <li>
    <p>Real-Time Status – <strong>8 votes</strong>
  &lt;/li&gt;
  <li>
    <p>Patient Experience Data – <strong>7 votes</strong>
  &lt;/li&gt;
  <li>
    <p>Military-to-Civilian Transition – <strong>7 votes</strong>
  &lt;/li&gt;
  <li>
    <p>Ratings and Calculators – <strong>6 votes</strong>
  &lt;/li&gt;
  <li>
    <p>Appointments – <strong>6 votes</strong>
  &lt;/li&gt;
  <li>
    <p>Medical Healthcare – <strong>5 votes</strong>
  &lt;/li&gt;
  <li>
    <p>Housing Assistance – <strong>5 votes</strong>
  &lt;/li&gt;
  <li>
    <p>Public Accountability and Awareness – <strong>4 votes</strong>
  &lt;/li&gt;
  <li>
    <p>Service History Data – <strong>4 votes</strong>
  &lt;/li&gt;
  <li>
    <p>Veteran Status Verification – <strong>0 votes</strong>
  &lt;/li&gt;
  <li>
    <p>Statistical Analysis and Machine Learning – <strong>0 votes</strong>
  &lt;/li&gt;
  <li>
    <p>Metadata Support – <strong>0 votes</strong>
  &lt;/li&gt;
  <li>
    <p>Documentation – <strong>0 votes</strong>
  &lt;/li&gt;
&lt;/ol&gt;

<p>It’s entirely possible that these groupings could be further normalized, or even some of the ideas within the original groups split out into separate groups. Some groupings could even be disregarded as irrelevant (for example, Metadata Support). However, we didn’t want to dilute the results of what the participants came up with. Somewhat surprising is the low number of votes for Medical Health. That may be a result of lacking the right type of participant representation in the workshops, at least for that particular category.

<h3 id="summary-of-the-landscape-analysis"><strong>Summary of the landscape analysis</strong></h3>

<p>The landscape analysis, which only processed about 1/3 of the URLs targeted for spidering over the course of a two-week period, revealed about 20,216 data files. This work produced a lot of data to wrangle and make sense of. Each of the data formats made available tell their own story about what types of data has been published to VA websites and why. The number of times a resource has been published using a particular data format (CSV, XML, JSON, XLS/XLSX, etc.) serves as a vote for making that resource available and accessible on the web.

<p>Despite the huge amount of information to work with, we believe that our analysis provides valuable insight into some of the most relevant data resources, based on years of publication to VA websites. The top resources identified from all of the URLs, file formats, tables, and forms all point to data resources that should be considered turning into APIs. If these data resources were considered a priority when publishing to the VA’s websites, then there’s a good chance that they should be considered priorities when it comes to publishing via APIs as part of the Lighthouse initiative.

<h2 id="lighthouse-program-considerations-moving-forward">Lighthouse program considerations moving forward</h2>

<h3 id="resources-to-prioritize">Resources to prioritize</h3>

<p>After spending time with all of the data uncovered during the landscape analysis, we began to see patterns emerge from across all the resources being published to the VA’s web properties, as well as those resources identified by people during our facilitated workshops. So based on analysis of the available data, we recommend that the VA Lighthouse program give consideration to prioritizing the following 25 resources:

<ol>
  <li>
    <p><strong>Healthcare Facilities –</strong> Up-to-date information on hospitals, clinics, and other healthcare facilities.
  &lt;/li&gt;
  <li>
    <p><strong>Organizations –</strong> Details on any type of organization that services Veterans and their families.
  &lt;/li&gt;
  <li>
    <p><strong>Services –</strong> Services being offered by the VA, healthcare facilities, and other organizations.
  &lt;/li&gt;
  <li>
    <p><strong>Programs –</strong> Programs being offered by the VA, healthcare facilities, and other organizations.
  &lt;/li&gt;
  <li>
    <p><strong>Resources –</strong> Content, video, and other resources providing healthcare, outpatient, and other relevant content.
  &lt;/li&gt;
  <li>
    <p><strong>Schedules –</strong> The schedules of healthcare facilities, organizations, services, and programs being offered.
  &lt;/li&gt;
  <li>
    <p><strong>Events –</strong> Calendar and details of relevant events that service Veterans around the country.
  &lt;/li&gt;
  <li>
    <p><strong>Benefits –</strong> Details of the benefits being offered to Veterans, including elements of the process involved.
  &lt;/li&gt;
  <li>
    <p><strong>Performance –</strong> Performance details for the healthcare facilities, organizations, services, and programs.
  &lt;/li&gt;
  <li>
    <p><strong>Insurance –</strong> Home, auto, and healthcare insurance information that Veterans can take advantage of.
  &lt;/li&gt;
  <li>
    <p><strong>Loans –</strong> Information on home, auto, and other types of loans available to Veterans and their families.
  &lt;/li&gt;
  <li>
    <p><strong>Grants –</strong> Grants for education, businesses, projects, and other Veteran-focused efforts.
  &lt;/li&gt;
  <li>
    <p><strong>Education –</strong> Educational opportunities and information available to Veterans and their families.
  &lt;/li&gt;
  <li>
    <p><strong>Training –</strong> Specific training opportunities available that Veterans can take advantage of.
  &lt;/li&gt;
  <li>
    <p><strong>Jobs –</strong> Job postings that Veterans can apply to and use to guide their career.
  &lt;/li&gt;
  <li>
    <p><strong>Human Resources –</strong> VA human resources and related information in support of VA employees and Veterans.
  &lt;/li&gt;
  <li>
    <p><strong>Forms –</strong> Directory, access, and management of forms and the data that’s stored within them.
  &lt;/li&gt;
  <li>
    <p><strong>Budgets –</strong> Budget information on healthcare facilities, organizations, programs, and services.
  &lt;/li&gt;
  <li>
    <p><strong>Statistics –</strong> Statistics and data on all aspects of VA operations, and anything that impacts Veterans.
  &lt;/li&gt;
  <li>
    <p><strong>Cemeteries –</strong> Details of the cemeteries, and the Veterans who are laid to rest at all locations.
  &lt;/li&gt;
  <li>
    <p><strong>News –</strong> News that impacts Veterans from across any source and is relevant to the community.
  &lt;/li&gt;
  <li>
    <p><strong>Press –</strong> Press releases from the VA and related organizations and programs.
  &lt;/li&gt;
  <li>
    <p><strong>Research –</strong> Information and other resources produced as part of specific Veteran-related research.
  &lt;/li&gt;
  <li>
    <p><strong>Surveys –</strong> Centralized organization, access to, and the results of Veteran and program-related surveys.
  &lt;/li&gt;
  <li>
    <p><strong>FOIA –</strong> Process and information related to Freedom of Information Act (FOIA) efforts occurring at VA.
  &lt;/li&gt;
&lt;/ol&gt;

<p>These resources represent what was harvested and analyzed as part of our landscape analysis, merging many of the patterns present across individual datasets. They’re organized using a REST-centric approach to turning data into API resources, which allows for data access via HTTP. Many of the keywords identified as part of the landscape analysis have been rolled-up into higher level areas — such as PTSD, mental health, and suicide — would exist across services, programs, and resources.

<p>These suggested resources are derived from about 65% of the top-level resources identified across all the top paths, file formats, tables, and forms. They represent a nice cross-section of resources across all the data formats, but also reflect the general web presence of the VA. Our list also provides a coherent stack of resources that could be developed, deployed, and maintained in support of the central veteran APIs, offering personalized and generalized data experiences that would benefit Veterans and their families.

<h3 id="centralize-focus-on-the-veteran">Centralize focus on the Veteran</h3>

<p>From a data perspective, the most important resource above all is the Veteran and their personal data. Therefore, the identity and healthcare record of a Veteran should be at the front and center of any API deployed as part of the Lighthouse API platform initiative. This requires full knowledge and accurate information about a Veteran. In others words, in order for the Lighthouse’s APIs to work well, there must be a robust identity and access management in place, as well as detailed, layered, portable, and usable Veteran profiles.

<h3 id="increase-personalization">Increase personalization</h3>

<p>One thing that became evident during our work is the need for greater personalization of data across almost every resource that we identified. Where there’s value in having general information available (for example, medical facilities), this data becomes exponentially more valuable when it’s personalized, localized, and made more relevant to the Veteran who is browsing, searching, engaging. Therefore, there are two types of engagement models with the resources that we propose: (1) general access without knowledge of the Veteran and (2) personalized knowledge of the Veteran via custom configuration settings that determine the relevancy of the data and content when these are made available via APIs and within applications.

<p>There are existing portal efforts such as <a href="https://www.vets.gov/">vets.gov</a> that are available as part of the VA’s online presence. The personalization efforts occurring there should be reflected across the design and operation of the Lighthouse’s APIs. By designing APIs to operate in generalized or personalized mode, this would empower API developers to act on behalf of a Veteran using OAuth tokens. If a token’s present, each API will act in a more personalized manner and allow for localization based upon a Veteran’s preferences and history of interactions. This personalization layer should act as a bridge between the core healthcare record of a Veteran and the other resources that we outlined above.

<h3 id="writing-not-just-reading-data">Writing, not just reading data</h3>

<p>Many of the resources that we identified represent read-only access to data and content. It’s important to note that getting access to data and content is useful; however, a significant portion of the resources that we harvested and gathered through conversations with the community will require the ability to write information via APIs. Forms, surveys, and other feedback loops will need to allow for APIs that not only GET data, but also POST and PUT data as part of their operations. This additional operations will round-off the Lighthouse’s stack of resources, helping to ensure that services provide a two-way street for engaging with the VA community.

<p>In addition to reading, the ability to write data and content will be a deciding factor in whether applications built on top of the APIs will deliver meaningful value to Veterans and their supporters. If information is only being pushed outwards, many applications will be seen as having little to no value to users, developers, and operators of the Lighthouse API platform. To help ensure meaningful value is delivered to everyone involved, all applications should be capable of sharing usage data and feed analytics and support feedback loops between users and the platform operators. With the ability to write data, the APIs will lack meaning and substance, and will contribute to lack of adoption and integration.

<h3 id="focus-on-the-source-of-the-data">Focus on the source of the data</h3>

<p>A common misconception in conducting a landscape analysis such as the one we performed is to assume that the data discovered can be published via any APIs that are deployed as part of the next phase of work. That’s rarely the situation, because most of the discovered data is just published snapshots derived from existing data sources. This is certainly the case with the VA. Much of the data we discovered is unusable in its current state due to lack of normalization, duplication, being out of date, and other noise and clutter. Many of the XML and JSON files identified provider a much cleaner option for transforming into web APIs. However, with any resource identified, it’s more desirable to integrate the original source of data than relying on published snapshots.

<p align="center"><img src="https://api-evangelist.github.io/va-api-landscape/images/api-landscape-layers.png" width="90%" align="center" />

<p>Even after coming to a consensus on the data resources to transform into APIs, the next phase of work should focus on identifying the data sources for each of the targeted resource areas, and not relying on published data that already exist across websites. While it’s tempting, and sometimes necessary to rely on published data for the source of API data, it increases the chance that an API will eventually become dormant, out-of-date, and cause many of the issues that we’ve seen play out with the existing VA datasets. Our landscape analysis came at the resource prioritization from an external, public perspective. We recommend a subsequent, more internal landscape analysis to identify the data sources for important resource types emerging from this landscaping effort.

<h3 id="improve-domain-management">Improve domain management</h3>

<p>Moving forward, it’d be logical to have a standardized approach to naming subdomains for both web and API properties in support of VA operations. Establishing a common approach to naming city, state, regional, program, research, and other resource areas would provide human- and machine-readable access to these resources. This might be difficult to do for web properties with so much legacy infrastructure, but the API platform provides an opportunity to establish a standardized approach moving forward.

<h3 id="leverage-common-data-formats">Leverage common data formats</h3>

<p>Our landscape analysis revealed a lack of a consistency when it comes to vocabulary, schema, and data formats. Most of the data published is derived from an existing system or represents a human-directed process. There’s a significant amount of fluff and noise surrounding these valuable data and a lack of consistent naming and field types.

<p>Based upon the resources that we’ve identified for your consideration, there are a handful of existing data formats the VA should consider. Some of these are already underway, while others are not currently reflected in the Lighthouse’s efforts, but are used by other government entities to publish data in a consistent manner.

<ul>
  <li>
    <p><strong>Fast Healthcare Interoperability Requirements (FHIR) –</strong> FHIR is already in-motion at the VA, but worth highlighting here. FHIR provides an anchor for why common data schema formats are relevant to other resources beyond Veteran healthcare records.
  &lt;/li&gt;
  <li>
    <p><strong>Open Referral (211) –</strong> Open Referral is a common schema and API specification for defining human services, including organizations, locations, and services, along with all the supporting information and metadata that goes with this core set of resources.
  &lt;/li&gt;
  <li>
    <p><strong>Open311 –</strong> Open311 is a common data format for reporting problems and issues at the municipal level, but can easily be adopted for establishing feedback loops at any level of government. It provides a common schema for how large volumes of information get submitted via API infrastructure.
  &lt;/li&gt;
  <li>
    <p><strong>Schema.org –</strong> A common schema vocabulary that provides object definitions for almost every resource identified throughout this landscape analysis, and the recommended list of resources above.
  &lt;/li&gt;
&lt;/ul&gt;

<p>There are undoubtedly other open data formats that can be leveraged. Common microformats and other RFCs should also be considered, but these can be addressed during the define and design stages of the API development lifecycle, once individual resources have been decided upon. Common formats help ensure resources are interoperable and reusable across VA groups; they also help bring teams together to speak in a common language, using a common dictionary, which will go a long ways to standardizing how data is published and consumed.

<h3 id="improve-analytical-information">Improve analytical information</h3>

<p>We had hoped there would have been more analytical information available to help rank the resource data that we identified. We did incorporate the ranking information available from data.gov as an input into our resource prioritization. However, we relied mostly on publication frequency and the overall occurrence of each keyword to help weight relevance. The existence of a word in a path, title, and file name gives it a weight, which can be amplified for every occurrence, providing us with adequate levels of prioritization, grouping, and organization to help us understand each topics importance. If a topic exists frequently across VA web properties, and exists as a sectional grouping, and title of data file, it has importance and relevance.

<p>The lack of analytics, or access to current analytics, across existing VA data sources demonstrates the importance of having a consistent and comprehensive analytics strategy across the VA’s data. There should be download counts for all machine-readable files. And, more importantly, real-time analytics for the consumption of this data via simple web APIs. There should be regional- and program-related ata. We should have personalized data that reflects what’s most important to Veterans. We should understand what’s relevant what isn’t through strategically-designed analytics across web and API operations. The lack of analytics is why we’re working to identify relevant data sources, so those can be made more available and analytics become the default — not an afterthought.

<h3 id="continue-refining-the-landscape-analysis">Continue refining the landscape analysis</h3>

<p>Our landscape analysis produced a lot of information that was messy and difficult to work with. We can continue to make another pass, which would involve refining indexes, optimizing title and filename parsing, and developing key phrase, plural word, and other dictionaries to make the results much more refined.

<p>There was a lot of data to harvest, process, and make sense of in a two-week sprint. However, we feel that we were able to do a good job of making sense of what was captured. Another sprint could easily be spent sorting through all of the data targeted, separating quality datasets from the more messier ones. Creating a dictionary to translate words and rehydrate acronyms would be very useful to help make further sense of what’s available in CSV, XLS/XLSX, JSON, and XML files. More work could also be done around forms: identifying the types available; defining their search mechanisms; defining what input parameters they allow (whether GET or POST); and unlocking further details on how they store data. Form and table data often have a direct connection to backend databases, which make them more valuable than some of the published data files.

<p>All of the data from the landscape analysis has been published to GitHub, minus the primary index of harvested and processed URLs. Those are too big to publish as JSON to GitHub, but we’ll evaluate how best to provide access to each site index using a solution such as Amazon AMI. We also started experimenting with a secondary spider solution in order to generate an index of the VA website and can publish those indexes as separate GitHub repositories within a single GitHub repository when completed. We feel like these newer indexes could provide a much richer approach to understanding the data and content across the VA’s web properties. And allow for other researchers and analysts to fork and work to make sense of the data that they contain.

<h3 id="incorporate-user-research">Incorporate user research</h3>

<p>It’s critical that any further landscape analysis focused on uncovering valuable data resources from across the VA’s web presence is combined with user research activities, such as the series of design workshops that we conducted. Doing so will provide a human perspective on what’s most important to Veterans and their supporters.

<p>We strongly encourage the Lighthouse program to conduct a similar workshop activity to the one that we ran, leveraging the VA’s much stronger outreach capability in order to attract an even larger and more diversified representation of the Veteran community’s data resource needs.

<p>We also recommend that the Lighthouse program consider using the <a href="https://en.wikipedia.org/wiki/Service_blueprint">service blueprinting technique</a> as a way to help identify and prioritize specific APIs for deployment. For example, a service blueprint could be created for a specific interaction that Veterans have with the VA, such as trying to find information on healthcare facilities. It’s likely that any service blueprints you want to create could be acquired by chunking the work into multiple micro-purchases. At the very least, we recommend trying to do at least one as an experiment. Once specific APIs are identified, you could then map them against a 2x2 prioritization matrix, based on how high they score against two main criteria: Veteran Experience Impact (y-axis) and Readiness to Execute (x-axis).

<h2 id="conclusion-this-journey-is-just-beginning">Conclusion: this journey is just beginning</h2>

<p>The landscape analysis for the VA doesn’t end here. Just like the resulting API effort, the evaluation of the VA’s web presence should be an ongoing process. Work should continue to help identify what datasets are being published to the VA’s web properties, and to incorporate these datasets into API operations or to replace them with API-driven solutions. In the long term, there shouldn’t be any tables, forms, CSV files, JSON files, XML files, or XLS/XLSX files without a direct connection to the API platform. Eventually all data should be derived from a federated, but standardized, set of API platforms that are designed, deployed, and managed consistently as part of the VA Lighthouse effort.

<p>Hopefully the work conducted here provides a base of resources for the Lighthouse program to consider as it moves forward. Ideally, everything uncovered as part of this work eventually becomes an API, or part of a suite of APIs. We understand that this won’t be a reality anytime soon, but we worked diligently to uncover the most valuable resources and to provide a concise list of data resources that could be turned into APIs and used to begin driving web, mobile, and desktop applications that serve Veterans and their families. There’s a wealth of resources available to Veterans across the VA’s websites. The challenge now is how do we ensure these resources deliver value consistently across many platforms? A simple, consistent, and usable API stack is the answer.

<h2 id="lessons-to-share">Lessons to share</h2>

<p>This project is one of the VA’s first experiments using the microconsulting model in support of the Lighthouse initiative. Sharing what went well, what didn’t, and what could we have been done better — all in the name of continuous improvement — is the responsibility of everyone involved in order to make not only the Lighthouse initiative a success, but the microconsulting model as well. With that said, here’s what we have to share:

<ul>
  <li>
    <p>We thoroughly enjoyed working on this as a micro-project. We felt that the short-timeboxed, tightly-scoped nature of the work focused our efforts on executing only the most essential activities, giving even more meaning to inherently impactful work. As <a href="https://en.wikipedia.org/wiki/Parkinson%27s_law">Parkinson’s Law</a> states, “work expands so as to fill the time available for its completion.”
  &lt;/li&gt;
  <li>
    <p>Looking back, our approach to this project involved some known unknowns (and some unknown unknowns) from a technical standpoint. In particular, the question of how well our spidering process could scale to handle the VA’s enormous web footprint. It would have been best to propose conducting an <a href="http://agiledictionary.com/209/spike/">agile “spike”</a> activity as a small micro-project in order to gain risk-reducing knowledge.
  &lt;/li&gt;
  <li>
    <p>For micro-projects under a tight schedule and for which there are external dependencies (for example, scheduling interviews or workshops with external participants), some lead time may be necessary before formally kicking off the project.
  &lt;/li&gt;
  <li>
    <p>Those people who participated in our facilitated workshops expressed extreme gratitude for the opportunity to contribute to the progress of the Lighthouse program. Working in the open and co-creating with the public will not only foster an engaged community of supporters, but will also lead to better quality outcomes.
  &lt;/li&gt;
  <li>
    <p>While our workshop activities were extremely valuable in giving us a human perspective on our landscape analysis, we felt that there could have been even greater representation from the Veteran community. We should have been more proactive about leveraging the VA’s outreach capability to draw in an even more dense and diverse group of Veterans and their supporters.
  &lt;/li&gt;
&lt;/ul&gt;



</p></li></p></li></p></li></p></li></p></li></ul></p></p></p></p></p></p></p></p></p></p></p></p></p></li></p></li></p></li></p></li></ul></p></p></p></p></p></p></p></p></p></p></p></p></p></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></ol></p></p></p></p></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></p></li></ol></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></p></li></p></li></p></li></ul></p></p></p></li></p></li></p></li></p></li></p></li></p></li></ul></p></p></li></p></li></p></li></p></li></ul></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/06/18/va-api-landscape-analysis-and-roadmapping-project-report/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/06/18/the-importance-of-openapi-tooling/">The Importance Of Openapi Tooling</a></h3>
        <span class="post-date">18 Jun 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘The Importance Of OpenAPI Tooling’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/openapi/OpenAPI_Pantone.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/openapi/OpenAPI_Pantone.png" width="45%" align="right" style="padding: 15px;" />
<p>In my world, OpenAPI is always a primary actor, and the tooling and services that put it to work are always secondary. However, I’d say that 80% of the people I talk with are the opposite, putting OpenAPI tooling in a primary role, and the OpenAPI specification in a secondary role. This is the primary reason that many still see Swagger tooling as the value, and haven’t made the switch to the concept of OpenAPI, or understand the separation between the specification and the tooling.

<p>Another way in which you can see the importance of OpenAPI tooling is the slow migration of OpenAPI 2.0 to 3.0 users. Many folks I’ve talked to about OpenAPI 3.0 tell me that they haven’t made the jump because of the lack of tooling available for the specification. This isn’t always about the external services and tooling that supports OpenAPI 3.0, it is also about the internal tooling that supports it. It demonstrates the importance of tooling when it comes to the evolution, and adoption of OpenAPI. It demonstrates the need for the OAI community to keep investing in the development and evangelism of tooling for the latest version.

<p>I am going to work to invest more time into rounding up OpenAPI tooling, and getting to know the developers behind them, as I prepare APIStrat in Nashville, TN. I’m also going to invest in my own migration to OpenAPI 3.0. The reason I haven’t evolved isn’t because of lack tooling, it is because of a lack of time, and the cognitive load involved with thinking new ways. I fully grasp the differences between 2.0 and 3.0, but I just don’t have intuitive knowledge of 3.0 in the way I do for 2.0. I’ve spent hundreds of hours developing around 2.0, and I just don’t have the time in my schedule to make similar investment in 3.0–soon!

<p>If you need to get up to speed on the latest when it comes to OpenAPI 3.0 tooling I recommend checking out <a href="http://openapi.tools/">OpenAPI.Tools</a> from Matt Trask (<a href="https://twitter.com/matthewtrask">@matthewtrask</a>) and Crashy McCiderface (aka Phil Sturgeon) (<a href="https://twitter.com/philsturgeon">@philsturgeon</a>). It is the best source of OpenAPI tooling out there right now. If you are still struggling with the migration from 2.0 to 3.0, or would like to see a specific solution developed on top of OpenAPI 3.0, I’d love to hear from you. I’m working to help shape the evolution of the OpenAPI tooling conversation, as well as tell stories about what tools are available, or should be available, and how they are can be put to work on the ground at companies, organizations, institutions, and government agencies.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/06/18/the-importance-of-openapi-tooling/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/06/08/people-still-think-apis-are-about-giving-away-your-data-for-free/">People Still Think Apis Are About Giving Away Your Data For Free</a></h3>
        <span class="post-date">08 Jun 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘People Still Think APIs Are About Giving Away Your Data For Free’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/adam-smith_dali_three.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/adam-smith_dali_three.jpg" width="45%" align="right" style="padding: 15px;" />
<p>After eight years of educating people about sensible API security and management, I’m always amazed at how many people I come across who still think public web APIs are about giving away access to your data, content, and algorithms for free. I regularly come across very smart people who say they’d be doing APIs, but they depend on revenue from selling their data and content, and wouldn’t benefit from just putting it online for everyone to download for free.

<p>I wonder when we stopped thinking the web was not about giving everything away for free? It is something I’m going to have to investigate a little more. For me, it shows how much education we still have ahead of us when it comes to informing people about what APIs are, and how to properly manage them. Which is a problem, when many of the companies I’m talking to are most likely doing APIs to drive internal systems, and public mobile applications. They are either unaware of the APIs that already exist across their organization, or think that because they don’t have a public developer portal showcasing their APIs, that they are much more private and secure than if they were openly offering them to partners and the public.

<p>Web API management has been around for over a decade now. Requiring ALL developers to authenticate when accessing any APIs, and the ability to put APIs into different access tiers, limit that the rate of consumption, while logging and billing for all API consumption isn’t anything new. Amazon has been extremely public about their AWS efforts, and the cloud isn’t a secret. The fact that smart business leaders see all of this and do not see that APIs are driving it all represents a disconnect amongst business leadership. It is something I’m going to be testing out a little bit more to see what levels of knowledge exist across many fortune 1000 companies, helping paint of picture of how they view the API landscape, and help me quantify their API literacy.

<p>Educating business leaders about APIs has been a part of my mission since I started API Evangelist in 2010. It is something that will continue to be a focus of mine. This lack of awareness is why we end up with damaging incidents like the Equifax breach, and the Cambridge Analytica / Facebook scandal. Its how we end up with so many trolls on Twitter, and an out of balance API ecosystems across federal, state, and municipal governments. It is a problem that we need to address in the industry, and work to help educate business leaders around common patterns for securing and managing our API resources. I think this process always begins with education and API literacy, but is a symptom of the disconnect around storytelling about public vs private APIs, when in reality there are just APIs that are secured and managed properly, or not.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/06/08/people-still-think-apis-are-about-giving-away-your-data-for-free/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/06/04/the-rockstar-committees-we-have-assembled-to-make-apistrat-nashville-rock/">The Rockstar Committees We Have Assembled To Make Apistrat Nashville Rock</a></h3>
        <span class="post-date">04 Jun 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘The Rockstar Committees We Have Assembled To Make APIStrat Nashville Rock!!’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/apistrat/apistrat-nashville-header.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/apistrat/apistrat-nashville-header.jpg" width="45%" align="right" style="padding: 15px;" />
<p>It is APIStrat time again! This time it is in Nashville, Tennessee! We are in the early stages of the event, but we are getting close to the deadline of the call for papers. We’ve assembled another rockstar ensemble for this round to help us steer the event, and review talk submissions once the CFP process has closed. I just wanted to take a moment and recognize the folks who are helping out and make sure they get the recognition they deserve.

<p>First up are the six members of the APIStrat steering committee, playing different leadership roles in the conference, making sure everything gets done by September:

<ul>
  <li>Darrel Miller (<a href="https://twitter.com/darrel_miller">@darrel_miller</a>)– Workshop Chair - <a href="https://www.microsoft.com">Microsoft</a></li>
  <li>Erin McKean (<a href="https://twitter.com/emckean">@emckean</a>) – Program Chair - <a href="https://www.ibm.com/">IBM</a></li>
  <li>Kin Lane (<a href="https://twitter.com/kinlane">@kinlane</a>) – Marketing Chair - <a href="https://apievangelist.com">API Evangelist</a></li>
  <li>Lorinda Brandon (<a href="https://twitter.com/lindybrandon">@lindybrandon</a>) – Community Chair - <a href="https://developer.capitalone.com/">Capital One DevExchange</a></li>
  <li>Steven Willmott (<a href="https://twitter.com/njyx">@njyx</a>) – Chair - <a href="https://www.redhat.com">Red Hat</a>.</li>
  <li>Tim Burks (<a href="https://twitter.com/timburks">@timburks</a>) – Program Vice Chair - <a href="https://www.google.com/">Google</a></li>
</ul>

<p>Then we have assembled nineteen folks on the program committee who will be reviewing your talk submissions before you can get on stage at APIStrat in Nashville:

<ul>
  <li>Amy Palamountain (<a href="https://twitter.com/ammeep">@ammeep</a>) - <a href="https://github.com/">GitHub</a></li>
  <li>Ash Hathaway (<a href="https://twitter.com/Ash_Hathaway">@Ash_Hathaway</a>) - <a href="https://pivotal.io/">Pivotal</a></li>
  <li>Carly Thelander (<a href="https://twitter.com/CarlyNThelander">@CarlyNThelander</a>) - <a href="https://localmilkrun.com/">MilkRun</a></li>
  <li>David Biesack (<a href="https://twitter.com/davidbiesack">@davidbiesack</a>) - <a href="https://www.apiture.com/">Apiture</a></li>
  <li>Gail Frederick (<a href="https://twitter.com/screaminggeek">@screaminggeek</a>) - <a href="https://www.ebay.com/">eBay</a></li>
  <li>George Atala (<a href="https://twitter.com/gmatala">@gmatala</a>) - <a href="https://quanterocapital.com/">Quantero Capital</a></li>
  <li>Glenn Block (<a href="https://twitter.com/gblock">@gblock</a>) - <a href="">Auth0 https://auth0.com/</a></li>
  <li>Gregory Koberger (<a href="https://twitter.com/gkoberger">@gkoberger</a>) - <a href="https://readme.io/">ReadMe</a></li>
  <li>James Higginbotham (<a href="https://twitter.com/launchany">@launchany</a>) - <a href="https://launchany.com">LaunchAny</a></li>
  <li>Jennifer Riggins (<a href="https://twitter.com/jkriggins">@jkriggins</a>) - <a href="http://ebranding.ninja/">the eBranding Ninja</a></li>
  <li>Joey French (<a href="https://twitter.com/josephtfrench">@josephtfrench</a>) - <a href="https://intrinio.com/">Intrinio</a></li>
  <li>Kyle Dallaire (<a href="https://twitter.com/kyledallaire">@kyledallaire</a>) - <a href="https://recruitingdigital.capitalone.com/">Capital One Digital</a></li>
  <li>Melissa Jurkoic (<a href="https://twitter.com/melissa_jurkoic">@melissa_jurkoic</a>) - <a href="https://www.amadeus-hospitality.com/">Amadeus Hospitality</a></li>
  <li>Phil Sturgeon (<a href="https://twitter.com/philsturgeon">@philsturgeon</a>) - <a href="https://www.wework.com">WeWork</a></li>
  <li>Sai Vennam (<a href="https://twitter.com/sai_vennam">@sai_vennam</a>) - <a href="">IBM https://www.ibm.com/</a></li>
  <li>Shelby Switzer (<a href="https://twitter.com/switzerly">@switzerly</a>) - <a href="https://www.healthify.us/">Healthify</a></li>
  <li>Skip Hovsmith (<a href="https://twitter.com/skiphovsmith">@skiphovsmith</a>) - <a href="https://www.approov.io">Critical Blue</a></li>
  <li>Taylor Barnett (<a href="https://twitter.com/taylor_atx">@taylor_atx</a>) - <a href="https://stoplight.io/">Stoplight</a></li>
  <li>Tessa Mero (<a href="https://twitter.com/TessaMero">@TessaMero</a>) - <a href="https://www.cisco.com/">Cisco</a></li>
  <li>Yina Arenas (<a href="https://twitter.com/yina_arenas">@yina_arenas</a>) - <a href="https://developer.microsoft.com/en-us/graph">Microsoft Graph</a></li>
</ul>

<p>Than you to everyone for helping do the hard work of making sure APIStrat not only continues, but continues to represent the wider API community. Everyone is doing this work because they care about the community, and want to make the event as good as, or better than it has been in the past. This is the 9th edition of APIStrat, spanning New York, San Francisco, Amsterdam, Chicago, Berlin, Austin, Boston, Portland, and now Nashville! It has been a pretty wild ride.

<p>While we have everyone we need for these committees, we still need help in other areas. First, get your talk submitted before the CFP closes next week. Second, we need your financial support, so make sure you consider sponsoring APIStrat, and help make sure Nashville rocks. Beyond that we can use some help spreading the word. We are looking to grow the event beyond the usual 500 threshold, helping expand participation in the event, as well as the OpenAPI Initiative. If you want to help, feel free to ping me anytime, and I’ll see you in Nashville.



</p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/06/04/the-rockstar-committees-we-have-assembled-to-make-apistrat-nashville-rock/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/06/04/making-connections-at-the-api-management-layer/">Making Connections At The Api Management Layer</a></h3>
        <span class="post-date">04 Jun 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Making Connections At The API Management Layer’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-connect-plug.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/bw-icons/bw-connect-plug.png" width="45%" align="right" style="padding: 15px;" />
<p>I’ve been evaluating API management providers, and this important stop along the API lifecycle in which they serve for eight years now. It is a space that I’m very familiar with, and have enjoyed watching it mature, evolve, and become something that is more standardized, and lately more commoditized. I’ve enjoyed watching the old guard (3Scale, Apigee, and Mashery) be acquired, and API management be baked into the cloud with AWS, Azure, and Google. I’ve also had fun learning about Kong, Tyk, and the next generation API management providers as they grow and evolve, as well as some of the older players like Axway as they work to retool so that they can compete and even lead the charge in the current environment.

<p>I am renewing efforts to study what each of the API management solutions provide, pushing forward my ongoing API management research, understanding what the current capacity of the active providers are, and potentially they are pushing forward the conversation. One of the things I’m extremely interested in learning more about is the connector, plugin, and extensibility opportunities that exist with each solution. Functionality that allows other 3rd party API service providers to inject their valuable services into the management layer of APIs, bringing other stops along the API lifecycle into management layer, allowing API providers to do more than just what their API management solution delivers. Turning the API management layer into much more than just authentication, service plan management, logging, analytics, and billing.

<p>Over the last year I’ve been working with <a href="https://www.elasticbeam.com/">API security provider ElasticBeam</a> to help make sense of what is possible at the API management layer when it comes to securing our APIs. ElasticBeam can analyze the surface area of an API, as well as the DNS, web, API management, web server, and database logs for potential threats, and apply their machine learning models in real time. Without direct access at the API management layer, ElasticBeam is still valuable but cannot respond in real-time to threats, shutting down keys, blocking request, and other threats being leveraged against our API infrastructure. Sure, you can still respond after the fact based upon what ElasticBeam learns from scanning all of your logs, but without being able to connect directly into your API management layer, the effectiveness of their security solution is significantly diminished.

<p>Complimenting, but also contrasting ElasticBeam, I’m also working with <a href="http://streamdata.io">Streamdata.io</a> to help understand how they can be injected at the API management layer, adding an event-driven architectural layer to any existing API. The first part of this would involve turning high volume APIs into real time streams using Server-Sent Events (SSE). With future advancements focused on topical streaming, webhooks, and WebSub enhancements to transform simple request and response APIs into event-driven streams of information that only push what has changed to subscribers. Like ElasticBeam, Streamdata.io would benefit being directly baked into the API management layer as a connector or plugin, augmenting the API management layer with a next generation event-driven layer that would compliment what any API management solution brings to the table.

<p>Without an extensible connector or plugin layer at the API management layer you can’t inject additional services like security with ElasticBeam, or event-driven architecture like Streamdata.io. I’m going to be looking for this type of extensibility as I profile the features of all of the active API management providers. I’m looking to understand the core features each API management provider brings to the table, but I’m also looking to understand how modern these API management solutions are when it comes to seamlessly working with other stops along the API lifecycle, and specifically how these other stops can be serviced by other 3rd party providers. Similar to my regular rants about API service providers always having APIs, you are going to hear me rant more about API service providers needing to have connector, plugin, and other extensibility features. API management service providers can put their APIs to work driving this connector and plugin infrastructure, but it should allow for more seamless interaction and benefits for their customers, that are brought to the table by their most trusted partners.



</p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/06/04/making-connections-at-the-api-management-layer/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/06/04/i-will-be-discussing-the-government-api-lifecycle-at-devnation-federal-in-dc/">I Will Be Discussing The Government Api Lifecycle At Devnation Federal In Dc</a></h3>
        <span class="post-date">04 Jun 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘I Will Be Discussing The Government API Lifecycle At DevNation Federal In DC’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/talks/devnation-federal.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/talks/devnation-federal.png" width="45%" align="right" style="padding: 15px;" />
<p>I’m kicking off a busy week of travel and <a href="https://devnationfederal.org/">talks this week in DC with a discussion about delivering microservices at federal agencies at DevNation Federal on Tuesday, June 5th, 2018</a>. I was invited by <a href="https://www.redhat.com/en">Red Hat</a> to come speak about the work I’m doing as API Evangelist across federal agencies. You can find me in the afternoon lineup, sharing my stories title “The Tech, Business, and Politics of APIs In Federal Government”. Focusing on information gathered as part of my research, workshops, and consulting across the public and private sector.

<p>My talk reflects my work to motivate federal agencies to do APIs over the last five years, and help pollinate the ideas and practices I gather from across the private sector, and understand which ones will work in the public sphere. Not everything about doing APIs at startups and in the enterprise translates perfectly to delivering APIs in the federal government, but there are many practices that will help agencies better serve the people. My goal is to open up discussion with government employees and contractors, to help figure out what works and what doesn’t–sharing stories along the way.

<p>Let me know if you are going to be at <a href="https://devnationfederal.org/">DevNation Federal</a> let me know. I am happy to make some time to talk, and hear what you are up to with APIs. I depend on these hallway conversations to populate my blog with stories. I’ll be in town around noon, and there until around 5 or so until I head over to the DC API Meetup for my second talk of the day. Thanks to <a href="https://www.redhat.com/en">Red Hat</a> for having me out. I enjoy doing talks for Red Hat events, as they tend to reflect more of the audience I’m looking for, with a focus on more open source, and a little less proprietary focus when it comes to delivering government technology. I’ll see you in Washington D.C. on Tuesday!



</p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/06/04/i-will-be-discussing-the-government-api-lifecycle-at-devnation-federal-in-dc/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/06/04/catch-me-at-the-dc-api-user-group-in-washington-dc-this-tuesday-evening/">Catch Me At The Dc Api User Group In Washington Dc This Tuesday Evening</a></h3>
        <span class="post-date">04 Jun 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Catch Me At The DC API User Group in Washington DC This Tuesday Evening’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/talks/dc-api-user-group.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/talks/dc-api-user-group.png" width="45%" align="right" style="padding: 15px;" />
<p>After I speak at DevNation Federal in Washington DC this Tuesday, I am going to give a similar talk at <a href="https://www.meetup.com/DC-Web-API-User-Group/">the DC API API User Group</a> that evening. I love going to the Meetups in DC, partly because my good friend Gray Brooks runs the event, but also because <a href="https://apievangelist.com/2012/08/18/api-craft-washington-dc/">I’ve been working to jumpstart API conversations in Washington DC since 2012 when I held the first DC edition of API Craft</a>. I was on a mission to jumpstart API Craft gatherings around the country that year, and it makes me happy to see the API Meetup culture continuing to thrive in DC, where other places it has died out.

<p>At the DC API Meetup I’ll be giving a variation of my talk that I’m giving earlier that day at DevNation Federal. Talking about the technology, business, and politics of doing APIs, with an emphasis on a consistent and repeatable API lifecycle. I’ll be reworking my regular material in light of current projects I’m working on at the federal level including with the VA, FDIC, HHS, and beyond. Sharing stories about how a microservice approach can help make government services more agile, flexible, and delivered in smaller more bite sized chunks–helping move the IT conversation forward across federal agencies.

<p>If you can’t make it to DevNation Federal, I recommend you head out <a href="https://www.meetup.com/DC-Web-API-User-Group/">to the DC API User Group later that evening</a>. I’d love to get a chance to hang out with you and talk about APIs. I’m always impressed with the folks who turn out for the DC API Meetup, consistently providing a fresh opportunity to discuss APIs and the impact they are making across the federal government. Organizer Gray Brooks has his finger on the pulse of what is going on across agencies, way beyond what I am capable of from the outside-in. I look forward to hanging out in DC, and hope you can make it out Tuesday to talk some APIs with me.



</p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/06/04/catch-me-at-the-dc-api-user-group-in-washington-dc-this-tuesday-evening/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/05/31/when-the-right-api-solution-is-not-always-the-sensible-one/">When The Right Api Solution Is Not Always The Sensible One</a></h3>
        <span class="post-date">31 May 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘When The Right API Solution Is Not Always The Sensible One’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/art-museum_light_dali.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories-new/art-museum_light_dali.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I conducted an API workshop at <a href="https://www.genscape.com/">Genscape</a> in Boston yesterday. I thoroughly enjoyed the conversation with the technical folks there, and found their pragmatic, yet educated views on APIs refreshing. I spent the day going through the usual stops along the API lifecycle, but found some of our breakout conversations during the API design section to be what has stuck with me on my train ride back to New York. Specifically the discussions around versioning, content negotiation and the overall design of paths, parameters, and potentially providing query language layers on top of APIs.

<p>After going through much of my textbook API design patterns for paths, and parameters, then working my through content negotiation and headers, we kept finding ourselves talking about what their clients were capable of. Bringing the API design discussion back around to why REST is still dominate in my opinion–simplicity, and lowering the bar for consumers. Time after time we found ourselves talking about their target consumers, the spreadsheet wielding data analyst, and how the bar needed to be kept low to make sure their needs were being met. Sure, we can quickly get academic and theoretical with the design practices, but if they fall on deaf ears, and API consumers do not adopt and use the API-driven tools, does it matter?

<p>I know that all of us API “thawt liters” would rather folks just do things the right way, but this isn’t always the reality on the ground. Most people within real world businesses don’t have the time or luxury to learn the right way of doing things, and take the time to disrupt their flow with new ways of doing things. Most of the time we need to just get people the data and other resources they need, in the tooling they are comfortable with (spreadsheet cough cough), and get out of their way. Sure, we should still be introducing people to new concepts, and working to strike a balance when it comes to the API design patterns we adopt, but it should be about striking a balance between reality on the ground, and the future we want to see.

<p>During the workshop we kept coming back around to simple, plain language, URIs that provide flexibility with path variables, as well as potentially relevant query parameters, that allow people to get CSV and JSON representations of the resources they need. With Excel as the target client, once again I find myself minimizing header usage, and maximizing the paths, and simplifying the data models in which folks can retrieve via APIs. I see this in industry after industry, and across the government agencies I am working with. I know that we all want our APIs to reflect the best possible patterns, and leverage the latest technology, but many of the people who will be potentially consuming our APIs, just want to get at the resources we are serving up, and do not have the time and the bandwidth to get on board with anything too far beyond their current mode of getting business done.

<p>None of this will shift me evangelizing the “proper way to design APIs”, but it reminds me (once again), at how immovable the business world can actually be. APIs are having a significant impact on how we develop web, mobile, desktop, and device applications, but one of the reasons web APIs have found so much success is that they are simple, scrappy, and flexible. Being able to serve up valuable data and other resources through simple URLs, allowing anyone to take them and put to use in the application of their choice has fed the explosion in the API we see across the landscape today. It is why we still see just as many poorly designed APIs in 2018 as we saw in 2010. It is because the best design doesn’t always win. Sometimes you just need the right design for the job, and the one that will make sense to the audience who will be consuming it. It is why hypermedia, GraphQL, and other design patterns will continue to be the choice of some practitioners, but simple, plain, REST will keep dominating the landscape.



</p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/05/31/when-the-right-api-solution-is-not-always-the-sensible-one/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/05/31/how-should-we-be-organizing-all-of-our-microservices/">How Should We Be Organizing All Of Our Microservices</a></h3>
        <span class="post-date">31 May 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘How Should We Be Organizing All Of Our Microservices?’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/containership_deep_connections.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/containership_deep_connections.jpg" width="45%" align="right" style="padding: 15px;" />
<p>A question I get regularly in my API workshops is, “how should we be organizing all of our microservices?” To which I always recommend they tune into what <a href="http://www.apiacademy.co/">the API Academy team is up to</a>, and then I dance around give a long winded answer about how hard it is for me to answer that. I think in response, I’m going to start asking for a complete org chart for their operations, list of all their database schema, and a list of all their clients and the industries they are operating in. It will still be a journey for them, or me to answer that question, but maybe this response will help them understand the scope of what they are asking.

<p>I wish I could provide simple answers for folks when it came to how they should be naming, grouping, and organizing their microservices. I just don’t have enough knowledge about their organization, clients, and the domains in which they operate to provide a simple answer. It is another one of those API journeys an organization will have to embark on, and find their own way forward. It would take so much time for me to get to know an organization, its culture, resources, and how they are being put to use, I hesitate to even provide any advice, short of pointing them to what the API Academy team publishes books, and provides talks on. They are the only guidance I know that goes beyond the hyped of definition of microservices, and actually gets at the root of how you do it within specific domains, and tackle the cultural side of the conversation.

<p>A significant portion of my workshops lately have been about helping groups think about delivering services using a consistent API lifecycle, and showing them the potential for API governance if they can achieve this consistency. Clearly I need to back up a bit, and address some of the prep work involved with making sure they have an organizational chart, all of the schema they can possibly bring to the table, existing architecture and services in play, as well as much detail on the clients, industries, and domains in which they operate. Most of my workshops I’m going in blind, not knowing who will all be there, but I think I need a section dedicated to the business side of doing microservices, before I ever begin talking about the technical details of delivering microservices, otherwise I will keep getting questions like this that I can’t answer.

<p>Another area that is picking up momentum for me in these discussions is a symptom of of the lack of API discovery, and directly related to the other areas I just mentioned. You need to be able to deliver APIs along a lifecycle, but more importantly you need to be able to find the services, schema, and people behind them, as well as coherently speak to who will be consuming them. Without a comprehensive discovery, and the ability to understand all of these dependencies, organizations will never be able to find the success they desire with microservices. They won’t be any better than the monolithic way many organizations have been doing things to date, it will just be much more distributed complexity, which will achieve the same results as the monolithic systems that are in place today.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/05/31/how-should-we-be-organizing-all-of-our-microservices/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/05/30/apistrat-nashville-call-for-papers-closes-at-the-end-of-next-week/">Apistrat Nashville Call For Papers Closes At The End Of Next Week</a></h3>
        <span class="post-date">30 May 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘APIStrat Nashville Call For Papers Closes At The End Of Next Week’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/apistrat/apistrat-2.png</p>

<hr />

<p><a href="https://events.linuxfoundation.org/events/apistrat-2018/"><img src="https://s3.amazonaws.com/kinlane-productions2/apistrat/apistrat-2.png" width="45%" align="right" style="padding: 15px;" /></a>
<p>I know, time flies and we are all very busy, but have you submitted your talk for APIStrat Nashville? The <a href="https://events.linuxfoundation.org/events/apistrat-2018/cfp/">call for papers</a> closes next Friday, June 8, 2018 at 11:59 PM PST! This is the second edition of the conference being run by <a href="https://www.openapis.org/">the OpenAPI community</a>, and is something you aren’t going to want to miss. We are bringing together the usual API community suspects, but also working to build upon the work that is going on in the fast growing OpenAPI community. The conference continues to be its usual voice of the API community, and won’t be all about the OpenAPI specification, but it it will definitely be where you want to be showcasing any groundbreaking work you are doing around the specification.

<p>The CFP process for APIStrat Nashville asks three main questions:

<ol>
  <li>How will the audience benefit from your presentation?</li>
  <li>Why should YOU be the one to give this talk? You have a unique story. Tell it.</li>
  <li>Be prepared to explain how this fits into the API ecosystem.</li>
</ol>

<p>After answering these questions we are looking for talks in the following areas of doing APIs: <em>API Design, API For the Greater Good, API Management, API SDKs &amp; Clients, API Security, API Success Stories, API Transformations, API Testing, API Usability, At The Protocol Level, Digital Transformation, GraphQL, Hypermedia, Machine Learning, Microservices, REST, RPC systems, and Standards &amp; Definitions</em>. However, don’t let these topics put you in a box. We want you to think about whatever you think will make the biggest impact on the audience, and hopefully the wider API community.

<p>Do not wait until the last moment to get your talk submitted! Last year, as the program chair I spoke with a number of people who had waited too long, and didn’t get their talk in front of the program committee early on. As soon as the submission are close they will begin spending time reviewing the talks, and while we may consider accepting late submissions, they are unlikely to get the attention that the earlier submissions will. If you have any questions about past events, or what might speak to this years audience, feel free to reach out to me personally. While I’m not in charge of the program committee this year, I’m still happy to help answer questions, and share my thoughts on what will catch their attention. Here is the link to the APIStrat CFP page for when you are ready, and I look forward to hearing what you have to say this year in Nashville!



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/05/30/apistrat-nashville-call-for-papers-closes-at-the-end-of-next-week/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/05/29/api-coordination-and-communication-across-federal-government-agencies/">Api Coordination And Communication Across Federal Government Agencies</a></h3>
        <span class="post-date">29 May 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘API Coordination And Communication Across Federal Government Agencies’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/gray-brooks/gray-brooks-microphone.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/gray-brooks/gray-brooks-microphone.jpg" width="45%" align="right" style="padding: 15px;" />
<p>Most of this API stuff isn’t technical. For APIs to work at scale within a single company, a wider industry, or across government agencies, you need people who are committed to evangelism, coaching, and communication around everything that is occurring across API ecosystems. We often times get caught up in our work, and operating within our silos and forget to email, call, and just walk next door sometimes to share stories of what is going on. If you are a regular reader of my work you know how I feel about storytelling, and just how important it is to all of this working or not. Which is why I like to make sure that I showcase the storyelling of other evangelists who are working their magic, and spreading API knowledge within their domains.

<p>I’m a big fan of what the GSA is up to with APIs across the federal government, and specifically what my hero Gray Brooks does to support API efforts acorss multiple federal agencies. He does a lot of legwork, but one thing that has stood out for me is his work to establish an <a href="https://digital.gov/communities/apis/">API community of practice</a>, and share relevant API stories on the US Government APIs Google Group. To help highlight what he is up to, and hep amplify the knowledge he puts out there, I wanted to <a href="https://groups.google.com/forum/#!topic/us-government-apis/xQQvLajni08">share his latest post from the Google Group</a> in it’s entirety because it is, well, beautiful.



</p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/05/29/api-coordination-and-communication-across-federal-government-agencies/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/05/29/a-microprocurement-package-for-api-monitoring-and-testing/">A Microprocurement Package For Api Monitoring And Testing</a></h3>
        <span class="post-date">29 May 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘A Microprocurement Package For API Monitoring And Testing’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/federal-government/va/microconsulting-work-state-va-api-landscape-analysis.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/va/microconsulting-work-state-va-api-landscape-analysis.png" width="45%" align="right" style="padding: 15px;" />
<p><a href="https://github.com/department-of-veterans-affairs/VA-Micropurchase-Repo/issues/6">I’m kicking off a micro-procurement project with the Department of Veterans Affairs (VA) this week</a>. I’m going to to be conducting one of <a href="https://apievangelist.com/2016/04/13/formalizing-my-approach-to-identifying-the-low-hanging-api-fruit/">my API low hanging fruit campaigns</a> for them, where I help them identify the best possible data sets available across their public websites for turning into APIs. The project is one of many small projects the federal agency is putting out there to begin working on their agency wide API platform they are calling Lighthouse. Laying the groundwork for better serving veterans through a robust stack of microservices that each do one thing really well, but can be used in concert to deliver the applications the agency needs to meet their mission.

<p>While not specifically a project to develop a microservice. The landscape analysis project is an API focused research project, that is testing a new procurement model called microprocurement. At my government consulting agency partnership <a href="http://skylight.digital/">Skylight Digital</a>, my partner in crime Chris Cairns has been pushing for a shift in how government tackles technology projects, pushing them to do in smaller chunks that essentially can be put on the credit card in less than 10K increments. So far we’ve been doing consulting, research, and training related projects like how to create a bug bounty program, and internal evangelism strategies. Now we are kicking our campaign into high gear and pushing more agencies to think about microprocurement for microservices–the VA was the first agency to buy into this new way of thinking about how government IT gets delivered.

<p>I am working with all of my partners to help me craft potential services that would fit into the definition of a microprocurement project. I’ve been working to educate people about the process so that more API experts are on hand to respond <a href="https://github.com/department-of-veterans-affairs/VA-Micropurchase-Repo/issues">when agencies publish microprocurement RFPs on Github like the VA did</a>, but I also want to make sure and have a suite of microprocurement packages that federal agencies can choose from as well. I’ve been working with my partner Dave O’Neill over at <a href="https://apimetrics.io/">APIMetrics</a> to provide me with some detail on an API testing microprocurement package that federal agencies could buy into for less than 10K, providing the follow value:

<ul>
  <li>API discovery and creation of a Postman collection</li>
  <li>Annual monitoring for up to 10 APIs including the following:</li>
  <li>Weekly and monthly emailed quality statements – CASC scores and SLOs/SLA attainment</li>
  <li>Interface to named operations tooling for alerts</li>
  <li>Public dashboards for sharing API status and SLA with all stakeholders.</li>
</ul>

<p>Providing a pretty compelling annual API monitoring package that government agencies could put on the credit card, and help ensure the quality of service provided across federal agencies when it comes to providing APIs. Something that is desperately needed across federal agencies who in my experience are mostly conducting “API nonitoring”, where they are not monitoring anything. Ideally, ALL federal agencies provide an SLA with the public and private APIs they are serving up, and <a href="https://apimetrics.io/">relying on outside entities like APIMetrics to be doing the monitoring from an external perspective, and potentially even different regions of the US</a>–ensuring everyone has access to government services.

<p>I’ll publish anther story after I kick off my work with the VA. I’ll also keep beating the drum about microprocurement opportunities like this one with APIMetrics. If you are an API service provider who has an interesting package you think federal agencies would be interested in–let me know. Also, if you are a government agency and would like to learn more about how to conduct microprocurement projects feel free to drop me a line, or reach out to Chris Cairns (@cscairns) over at <a href="https://skylight.digital/">Skylight Digital</a>–he can set you up. We are just getting going with this types of projects, but I’m pretty optimistic about the potential they can bring to the table when it comes to delivering IT projects–an approach that reflects the API way of doing things in small, meaningful chunks, and won’t break the bank.



</p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/05/29/a-microprocurement-package-for-api-monitoring-and-testing/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/05/16/every-moment-is-potentially-an-api-story/">Every Moment Is Potentially An Api Story</a></h3>
        <span class="post-date">16 May 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Every Moment Is Potentially An API Story’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/skylight/skylight-home.png</p>

<hr />

<p><a href="https://skylight.digital/"><img src="https://s3.amazonaws.com/kinlane-productions2/skylight/skylight-home.png" width="45%" align="right" style="padding: 15px;" /></a>
<p>I was on a call for a federal government API platform project with my partner in crime Chris Cairns (<a href="https://twitter.com/cscairns">@cscairns</a>) of <a href="https://skylight.digital/">Skylight.Digital</a>. We were back channeling in our Slack channel during the call, when he said, <em>“I always imagine you participating in these things, finding topics you haven’t covered or emphasized from a certain angle, and then writing a blog post in real time.”</em> He was right, I had taken notes on a couple of new angles regarding the testing, monitoring, and understanding performance of APIs involved with federal government projects.

<p>The single conference call resulted in about seven potential stories in my notebook. I’m guessing that only four of them will actually end up being published. However, the conversation does a good job at highlighting my style for generating stories on the blog, which is something that allows me to publish 3-5 posts a day–keeping things as active as I possibly can, generating traffic, and bringing attention to my work. If you’ve ever worked closely with me, you know that I can turn anything into a story, even a story like this, about how I create stories. ;-)

<p>Think of API Evangelist as my public workbench where I work through the projects I’m tackling each day. It is how I make sense of my research, the projects I am working on, and the conversations I am having. All of which generates SEO exhaust, which brings more attention to my work, extends its reach, and ultimately brings in more work. It is a cycle that helps me work through my ideas in a way that forces me to make them more coherent (hopefully) along the way, while also making them immediately available to my partners, customers, and readers.

<p>Chris has been pushing this concept forward and advocating that we be public with as many of the Skylight.Digital responses and proposals as we possibly can, which resulted in me publishing both of our responses to <a href="http://apievangelist.com/2017/10/26/my-response-on-the-department-of-veterans-affairs-rfi-for-the-lighthouse-api-management-platform/">the first</a>, and <a href="http://apievangelist.com/2018/02/24/department-of-veterans-affairs-lighthouse-platform-rfi-round-two/">the second</a> Department of Veteran Affairs (VA) RFIs. He has also been pushing other Skylight.Digital partners to be public with their proposals and responses, which folks were skeptical about at first, but then it started making sense when they see the effect it has, and the publicity it can generate.

<p>This approach to storytelling isn’t something you can do effectively right out of the gate. It takes practice, and regular exercising–I have eight years of practice. However, I feel it is something that anyone can do eventually. You just have to find your own way of approaching it, and work on establishing your own voice and style. It is something that I’ve found to be essential to how I do business, but also something I find to be personally rewarding. I enjoy working through my ideas, and telling stories. It is always the one aspect of my work that I look forward to doing each day, and I feel like something is missing the days I don’t get to craft any posts. I really enjoy that every moment has the potential to be an API story, it really makes each day an adventure for me.



</p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/05/16/every-moment-is-potentially-an-api-story/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/05/16/api-evangelist-is-partnering-with-bridge-software-to-tell-more-api-integration/">Api Evangelist Is Partnering With Bridge Software To Tell More Api Integration</a></h3>
        <span class="post-date">16 May 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘API Evangelist Is Partnering With Bridge Software To Tell More API Integration’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/bridge-software/bridge-software-screenshot.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/bridge-software/bridge-software-screenshot.png" width="45%" align="right" style="padding: 15px;" />
<p>I have been intrigued by the people who reached out after <a href="http://apievangelist.com/2018/04/09/creating-a-productive-api-industry-partner-program/">I published a story about my partner program</a>. One of the companies that reached out was the API integration agency <a href="https://www.bridgesoftware.com/">Bridge Software</a>, who reflects the next generation of software development groups who are emerging to focus exclusively on API integration.

<p>I’ve had several calls with the Bridge Software team this week, discussing the possible ways in which we can work together. I’m regularly in need of software development resources that I can refer projects to, and I’m also interested in hearing more interesting stories about how businesses are integrating with APIs. Of course, Bridge Software is looking for more customers, which is something I can definitely help with. So we started brainstorming more about how they can regularly feed me stories that I can develop into posts for API Evangelist, and I can help send new business their way.

<p>One of the most valuable resources I possess as the API Evangelist, is access to real world stories from the trenches of API providers and integrators. These sources of stories have been the bread and butter for API Evangelist for eight years. I have depended on the stories that businesses share with me to generate the 3,276 posts I have published on the blog since 2010. So I am stoked to have a new source of fresh stories, from a team who is neck deep in API integrations each day, and will be actively gathering stories from their regular developer team meetings.

<p>I’m looking forward to working with the Bridge Software team, and getting new stories from the API integration trenches. I’m also looking forward to assisting them with finding new customers, and helping them with deliver on their API integration needs. If you are looking for help with API integrations, Bridge Software has a pretty sophisticated approach to delivering integrations that has test driven development at its core. Feel free to reach out to the team, but make sure and tell them where you heard about them, to get special treatment. ;-) Also, make sure you stay tuned for more stories about the API integrations stories they will be sharing with me on a regular basis, which I will be weaving into my regular cadence of storytelling here on the blog.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/05/16/api-evangelist-is-partnering-with-bridge-software-to-tell-more-api-integration/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/05/15/opportunity-for-openapidriven-open-source-testing-performance-security-and/">Opportunity For Openapidriven Open Source Testing Performance Security And</a></h3>
        <span class="post-date">15 May 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Opportunity For OpenAPI-Driven Open Source Testing, Performance, Security, And’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/openapi/openapi-icons-gears.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/openapi/openapi-icons-gears.png" width="45%" align="right" style="padding: 15px;" />
<p>I’ve been on five separate government reflated projects lately where finding modular OpenAPI-driven open source tooling has been a top priority. All of these projects are microservice-focused and OpenAPI-driven, and are investing significant amounts of time looking open source tools that will help with design governance, monitoring, testing, and security, and interact with the Jenkins pipeline. Helping government agencies find success as their API journey picks up speed, and the number of APIs grows exponentially.

<p>Selling to the federal government can be a long journey in itself. They can’t always use the SaaS solutions many of us fire up to get the job done in our startup or enterprise lives. Increasingly government agencies are depending on open source solutions to help them move projects forward. Every agency I’m working with is using OpenAPI (Swagger) to drive their API lifecycle. While not all have gone design (define) first, they are using them as the contract for mocking, documentation, testing, monitoring, and security. The teams I’m working with are investing a lot of energy looking for, vetting, and testing out different open source modules on Github–with varying degrees of success.

<p>Ideally, there was an OpenAPI-driven marketplace, or federated set of marketplaces like <a href="http://openapi.tools/">OpenAPI.Tools</a>. I’ve had <a href="http://openapi.toolbox.apievangelist.com/">one for a while</a>, but haven’t kept up to date–I will invest some time / resources into it soon. My definition of an OpenAPI tool marketplace would be that it is OpenAPI-driven, and open source. I’m fine with there being other marketplaces of OpenAPI-driven services, but I want a way to get at just the actively maintained open source tools. When it comes to serving government this is an important, and meaningful distinction. I’d also like to encourage many of the project owners to ensure there is CI/CD integration, as well as make sure their projects are actively supported, and they are willing to entertain commercial implementations.

<p>While there wouldn’t always be direct commercial opportunities for open source tooling owners to engage with federal agencies, there would be through contractors and subcontractors. Working for federal agencies is a maze of forms and hoop jumping, but working with contractors can be pretty straightforward if you find the right ones. I don’t think you will get rich developing OpenAPI-driven tooling that serves the API lifecycle, but I think with the right solutions, support, and team behind them, you can make a decent living developing them. Especially as the lifecycle expands, and the number of services being delivered grows, the need for specialized, OpenAPI-driven tools to apply across the API lifecycle is only going to increase. Making it something I’ll be writing more stories about as I hear more stories from the API trenches.

<p>I’m going to try and spend time working with Phil Sturgeon (<a href="https://twitter.com/philsturgeon">@philsturgeon</a>) and Matt Trask (<a href="https://twitter.com/matthewtrask">@matthewtrask</a>) on API.Tools, as well as give my own toolbox some love. If you have an open source OpenAPI-driven tool you’d like to get some attention feel free to ping me, and make sure its part of API.Tools. Also, if you have a directory, catalog, or marketplace of tools you’d like to showcase, ping me as well, I’m all about supporting diversity of choice in the space. I have multiple federal agencies ear right now when it comes to delivering along the API lifecycle, and I’m happy to point agencies and their contractors to specific tools, if it makes sense. Like I said, there won’t always be direct revenue opportunities, but they are implementations that will undoubtedly lead to commercial opportunities in the form of consulting, advising, and development opportunities with the contractors and subcontractors who are delivering on federal agency contracts.



</p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/05/15/opportunity-for-openapidriven-open-source-testing-performance-security-and/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/05/15/general-services-administration-gsa-needs-help-testing-their-fedbizopps-api/">General Services Administration Gsa Needs Help Testing Their Fedbizopps Api</a></h3>
        <span class="post-date">15 May 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘General Services Administration (GSA) Needs Help Testing Their FedBizOpps API’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/federal-government/FedBizOppsLogo.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/federal-government/FedBizOppsLogo.jpg" width="45%" align="right" style="padding: 15px;" />
<p><a href="https://www.nextgov.com/it-modernization/2018/05/gsa-needs-vendor-volunteers-test-fedbizopps-api/148136/">The General Services Administration (GSA) has an open call for help to test the new FedBizOpps API</a>. Setting a pretty compelling precedent for releasing APIs in the federal government, slowly bringing federal agencies out of their shell, and moving the API conversation forward in government. Hopefully it will be something that other federal agencies, and other levels of government will consider as they move forward on their API journeys.

<p>It is good to see federal agencies reaching out to ask for help in making sure APIs are well designed, deliver value, and operate as expected. In my experience, most government agencies are gun-shy when it comes to seeking outside help, and accepting criticism when it comes to their work. The GSA is definitely the most progressive on this front, but they are leading by example, showing other agencies what is possible, and something that hopefully will spread. Something I am always keen to support with my storytelling here on API Evangelist.

<p>One thing I’d caution federal agencies on when seeking outside feedback in this area, is to be mindful of fatigue in the private sector when it comes to working for free. API providers have been encouraging developers to document, test, develop code libraries, and other essential aspects of operating APIs for years now, and many developers are growing weary of this exploitation by companies who can afford to pay, but choose not to. While it is good that government is getting on the API bandwagon here, but be aware that you are about 5+ years behind, and the tides are shifting.

<p>I’d recommend thinking about how micro-procurement models emerging at the Department of Veterans Affairs (VA) and other agencies can be applied to testing, performance, security, and other needs agencies will have. Think about how you can create $500, $1,000, and other opportunities for professional testers to make money. Of course, you are going to have to establish a way of vetting developers, but once you do, you are going to get a higher level work than you would for free. It might take more effort to lay the groundwork, but it will be something that will pay off down the road with a community of external professionals who can hep tackle micro-tasks that emerge while developing and operating government APIs.

<p>Overall, I’m stoked to see stuff like this come out of the government. I’m just eager to see it spread to other federal agencies, shifting how digital services are delivered. I’m also eager to see more mock or virtualized APIs exposed publicly, long before any code is actually written. Bringing in outside opinions early on, allowing APIs to be more efficiently delivered–ensuring they are more mature, without the costly evolutionary and versioning process. I’m going to push more projects I’m involved in to follow GSA’s lead, and solicit outside testers to join in and provide feedback at critical steps in the API delivery process. The more agencies do this, the more likely they will to have a trusted group of outside developers, to step up when they need help, and have tasks like this to accomplish.



</p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/05/15/general-services-administration-gsa-needs-help-testing-their-fedbizopps-api/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/05/15/cautiously-aware-of-how-apis-can-be-used-to-feed-government-privatization/">Cautiously Aware Of How Apis Can Be Used To Feed Government Privatization</a></h3>
        <span class="post-date">15 May 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Cautiously Aware Of How APIs Can Be Used To Feed Government Privatization’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/white-house-window_atari_missle.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/white-house-window_atari_missle.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I’m working to define APIs at the Department of Veterans Affairs (VA) on several fronts right now. I’ve <a href="http://apievangelist.com/2017/10/26/my-response-on-the-department-of-veterans-affairs-rfi-for-the-lighthouse-api-management-platform/">provided not just one</a>, but <a href="http://apievangelist.com/2018/02/24/department-of-veterans-affairs-lighthouse-platform-rfi-round-two/">two detailed reponses</a> to their Lighthouse API platform RFIs. Additionally, I’m providing guidance to different teams who are working on a variety of projects occurring simultaneously to serve veterans who receive support from the agency.

<p>As I do this work, I am hyper aware of the privatization machinations of the current administration, and the potential for APIs to serve these desires. APIs aren’t good, bad, nor are they neutral. APIs reflect the desires of their owners, and the good or bad they can inflict is determined by their consumers. Because APIs are so abstract, and are often buried within web and mobile applications, it can be difficult to see what they are truly doing on a day to day basis. This is why we have API management solutions in place to help paint a picture of activity in real time, but as we’ve seen with the Facebook / Cambridge Analytica, it requires the API provider to be paying attention, and to possess incentives that ensure they will actually care about paying attention, and responding to negative events.

<p>By re-defining existing VA internal systems as APIs, we are opening up the possibility that digital resources can be extracted, and transferred externally. Of course, if these systems remain the sources of data, VA power and control can be retained, but it also opens up the possibilities for power to eventually be transferred externally–reversing the polarities of government power and putting it into private hands. Depending on your politics, this could be good, or it could be bad. Personally, I’m a fan of a balance being struck, allowing government to do what it does best, and allowing private, commercial entities to help fuel and participate in what is happening–both with proper oversight. This reflects the potential benefits APIs can bring, but this good isn’t present by default, and could shift one way or the other at any moment.

<p>Early on, APIs appeared as a promising way to deliver value to external developers. They held the promise of delivering unprecedented resources to developers, thing they would not normally be able to get access to. Facebook promised democratization using it’s social platform. While silently mining and extracting value from it’s application end-users, executed by often unknowing, but sometimes complicit developers. A similar transfer of power has occurred with Google Maps, which promised unprecedented access to maps for developers, as well as web and mobile maps for cities, businesses, and other interests. Over time we’ve seen that Google Maps APIs have become very skilled at extracting value from cities, business, and end-users, again executed unknowingly, and knowingly by developers. This process has been playing out over and over throughout the last decade of the open data evolution occurring across government at all levels.

<p>Facebook and Google have done this at the platform level, extracting value and maintaining control within their operations, but the open data movement, Facebook and Google included, have been efficiently extracting value from government, and giving as little back as it possibly can. Sure, one can argue that government benefits from the resulting applications, but this model doesn’t ensure that the data and content stewards within government retain ownership or control over data, and benefit from revenue generated from the digital resources–leaving them struggling with tight budgets, and rarely improving upon the resources. This situation is what I fear will happen at the Department of Veterans Affairs (VA), allowing privatization supporters to not actually develop applications that benefit veterans, but really focusing on extracting value possessed by the VA, capturing it, and generating revenue from it, without ever giving back to the VA–never being held accountable for actually delivering value to veterans.

<p>I have no way of testing the political motivations of people that I am working with. I also don’t want to slow or stop the potential of APIs being used to help veterans, and ensure their  health and insurance information can become more interoperable. I just want to be mindful of this reality, and revisit it often. Openly discussing it as I progress in my work, and as the VA Lighthouse platform evolves.  Sometimes I regret all the API evangelism I have done, as I witness the damage they can do. However, the cat is out of the bag, and I’d rather be involved, part of the conversation, and pushing for observability, transparency, and open dialogue about the realities of opening up government digital resources to private entities via APIs. Allowing people who truly care about serving veterans to help police what is going on, maximize the value and control over digital resources into the hands of people who are truly serving veteran’s interests, and minimizing access by those who are just looking to make money at veteran’s expense.



</p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/05/15/cautiously-aware-of-how-apis-can-be-used-to-feed-government-privatization/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/05/15/api-lifecycle-seminars-in-new-york-city/">Api Lifecycle Seminars In New York City</a></h3>
        <span class="post-date">15 May 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘API Lifecycle Seminars In New York City’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/seminars/DaptHzuWkAMjlKY.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/seminars/DaptHzuWkAMjlKY.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I’ve had a huge demand for putting on API seminars for a variety of enterprise groups lately. Helping bring my API 101, history, lifecycle, and governance knowledge to the table. I’ve conducted seminars in the UK and France in April, and have more in Virginia, Massachusetts, and California in May. I’ve been working with a variety of companies, institutions, and government agencies to plan even more internal seminars, which is something that is proving to be challenging for some groups who are just getting started on their API journey.

<p>While groups are keen on me coming visit to share my API knowledge, some of them are having trouble getting me through legal, get me into their payment systems, and making everyone’s schedules work. Some of them don’t have a problem figuring it out, while others are facing significant amounts of bureaucracy and friction due to the size, complexity, and legacy of their organizations. To help simplify the process for organizations to participate in my seminars I am going to begin planning a rolling schedule of seminars beginning in New York City.

<p>Every two weeks, on Tuesday and Wednesday I’ll be planning to do a 2-day API lifecycle seminars in NYC. I’m going to be focusing on delivering 12 stops along the API lifecycle, targeting my readers and customers who have already embarked on their API journey:

<p>Day One:

<ul>
  <li><strong>Definition</strong> - The basics of OpenAPI, Postman, and using Github to manage API definitions.</li>
  <li><strong>Design</strong> - Entry level API design, including touching on API governance and guidance.</li>
  <li><strong>Deployment</strong> - Looking at the big world of how APIs can deployed on-premise, and the clouds.</li>
  <li><strong>Virtualization</strong> - Understanding mocking, and API, as well as data virtualization.</li>
  <li><strong>Authentication</strong> - Thinking about the common patterns of authentication for API consumption.</li>
  <li><strong>Management</strong> - Covering the services, tools, and approaches to managing APIs in operation.</li>
  <li><strong>Discussion</strong> - Open discussion about anything covered throughout the day, and beyond.</li>
</ul>

<p>Day Two:

<ul>
  <li><strong>Documentation</strong> - Demonstrating how to deliver portals, documentation, and other resources.</li>
  <li><strong>Testing</strong> - Providing information on the monitoring and testing of API infrastructure.</li>
  <li><strong>Security</strong> - Walking through API security beyond just API management and authentication.</li>
  <li><strong>Support</strong> - Discussing the importance of providing direct and indirect support for APIs.</li>
  <li><strong>Evangelism</strong> - Looking at how to evangelize your APIs internally and externally.</li>
  <li><strong>Integration</strong> - Thinking briefly about about API integration concerns for API providers.</li>
  <li><strong>Discussion</strong> - Open discussion about anything covered throughout the day, and beyond.</li>
</ul>

<p>I’m kicking off this seminar series in New York City, with the following dates proposed to get the ball rolling:

<ul>
  <li><a href="https://www.eventbrite.com/e/api-evangelist-api-lifecycle-seminar-june-12th-edition-tickets-46113642145">June 12th &amp; 13th 2018 from 8:30 AM to 4:30 PM each day.</a></li>
  <li><a href="https://www.eventbrite.com/e/api-evangelist-api-lifecycle-seminar-june-26th-edition-tickets-46114150666">June 26th &amp; 27th 2018 from 8:30 AM to 4:30 PM each day.</a></li>
</ul>

<p>I’m going to be charging $1,000.00 per person for the two day seminars. I’m setting the minimum bar for attendance to be 5 people, otherwise I won’t be scheduling the seminar, and will be pushing forward to the next dates. I’ll make sure and let everyone know at least 2 weeks in advance if we don’t get the expected attendance. All seminars are open to anyone who would like to attend, but I am also happy to conduct private group seminars, with minimum of five people in attendance. My goal is to conduct a regular cadence of seminars, that people know they can plan on, and participate in as it works for their schedule, but also provide more stability for my schedule as well.
<p><img src="https://s3.amazonaws.com/kinlane-productions2/kin-lane/kin-lane-apidays-paris-2018.jpg" align="right" width="30%" />
<p>The objective with these seminars is to make it easier for companies, institutions, and government agencies to participate in my seminars, while also forcing them out of their bubbles. I’m finding the toll of me coming onsite can vary widely, and the value to seminar attendees is lower when they aren’t forced to leave their bubbles. So, I am looking to force people out of their usual domains, so that they begin thinking about how they’ll be doing APIs, and playing nicely with others–even if they are only intended for internal use. I find the process helps you to think outside your normal daily operations, and is easier for me to not have to navigate coming on site to deliver seminars.

<p>I’ve published both initial dates to Eventbrite to test the waters, and we’ll see if I can shift some of the demand I’m getting to this format. I will also keep pushing dates out every other week, and in July I’ll probably start looking at dates in Washington DC, then London, Paris, and probably some other west coast US locations. If you have interest in participating in my API lifecycle seminars and / or have specific requests for venues I’d love to hear from you. I’m looking to continue formalizing my process as well as schedule, and bringing my seminars wherever they are need, but doing so in a more organized fashion that helps you get your team what they need, but will also be something that is more sustainable for me in the long term.



</p></p></p></p></p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/05/15/api-lifecycle-seminars-in-new-york-city/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/05/14/using-jekyll-to-look-at-microservice-api-definitions-in-aggregate/">Using Jekyll To Look At Microservice Api Definitions In Aggregate</a></h3>
        <span class="post-date">14 May 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Using Jekyll To Look At Microservice API Definitions In Aggregate’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/kin-mountain_dali_three.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/kin-mountain_dali_three.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I’ve been evaluating microservices at scale, using their OpenAPI definitions to provide API design guidance to each team using Github. <a href="http://apievangelist.com/2018/05/14/looking-at-20-microservices-in-concert/">I just finished a round of work where I took 20 microservices</a>, and evaluated each OpenAPI definition individually, and made design suggestions via an updated OpenAPI definition that I submitted as a Github issue. I was going to submit as a pull request, but I really want the API design guidance to be a suggestions, as well as a learning experience, so I didn’t want them to feel like they had to merge all my suggestions.

<p>After going through all 20 OpenAPI definitions, I took all of them and dumped them into a single local repository where I was running Jekyll via localhost. Then using Liquid I created a handful of web pages for looking at the APIs across all microservices in a single page–publishing two separate reports:

<ul>
  <li><strong>API Paths</strong> - An alphabetical list of the APIs for all 20 services, listing the paths, verbs, parameters, and headers in a single bulleted list for evaluation.</li>
  <li><strong>API Schema</strong> - An alphabetical list of the APIs for all 20 services, listing the schema definitions, properties, descriptions, and types in a single bulleted list for evaluation.</li>
</ul>

<p>While each service lives in its own Github repository, and operates independently of each other, I wanted to see them all together, to be able to evaluate their purpose and design patterns from the 100K level. I wanted to see if there was any redundancy in purpose, and if any services overlapped in functionality, as well as trying to see the potential when all API resources were laid out on a single page. It can be hard to see the forest for the trees when working with microservices, and publishing everything as a single document helps flatten things out, and is the equivalent of climbing to the top of the local mountain to get a better view of things.

<p>Jekyll makes this process pretty easy to do by dumping all OpenAPI definitions into a single collection, which I can then navigate easily using Liquid on a single HTML page. Providing a comprehensive report of the functionality across all microservices. I am calling this my microservices monolith report, as it kind of reassembles all the individual microservices into a single view, letting me see the collective functionality available across the project. I’m currently evaluating these service from an API governance perspective, but at a later date I will be taking a look from the API integration perspective, and trying to understand if the services will work in concert to help deliver on a specific application objective.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/05/14/using-jekyll-to-look-at-microservice-api-definitions-in-aggregate/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/05/14/sports-data-api-simulations-from-sportradar/">Sports Data Api Simulations From Sportradar</a></h3>
        <span class="post-date">14 May 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Sports Data API Simulations From SportRadar’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/sportradar/sportradar-simulations.png</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/sportradar/sportradar-simulations.png" width="45%" align="right" style="padding: 15px;" />
<p>I’m a big fan of API sandboxes, labs, and other virtualization environments. <a href="https://streamdata.io/blog/sandbox-default-feature-banking-apis/">API sandboxes should be default in heavily regulated industries like banking</a>. I also support the virtualization of schema and data used across API operations, <a href="http://apievangelist.com/2018/05/01/synthetic-healthcare-records-for-your-api-using-synthea/">like I am doing at the Department of Veterans Affairs (VA), with synthetic healthcare data</a>. I’m very interested in anything that moves forward the <a href="http://virtualization.apievangelist.com/">API virtualization</a> conversation, so I found <a href="https://developer.sportradar.com/files/indexFootball.html#nfl-official-api-v2-simulations">the live sporting API simulations over at SportRadar</a> very interesting.

<p>SportRadar’s <em>“live simulations give you the opportunity to test your code against a simulation of live data before the preseason starts or any time! Our simulation system replays select completed games allowing you to view our API feeds as if they were happening live.”</em> Here are the details of their NFL Official API simulations that run every day:

<ul>
  <li>11:00 am - Data is reset for the day’s simulations.</li>
  <li>1:00 pm - PST week 1 games will run – Oakland at Houston, Detroit at Seattle, Miami at Pittsburgh, and New York at Green Bay</li>
  <li>2:00 pm – PST week 2 games will run - Seattle at Atlanta, Houston at New England, Green Bay at Dallas, and Pittsburgh at Kansas City</li>
  <li>3:00 pm – PST week 3 games will run – Green Bay at Atlanta and Pittsburgh at New England</li>
  <li>4:00 pm – PST week 4 games will run – New England vs Atlanta</li>
</ul>

<p>Providing a pretty compelling evolution in the concept API virtualization when it comes to event data, or data that will happen on a schedule. I wanted to write up this story to make sure I bookmarked this as part of my API virtualization research. If I don’t write about it, it doesn’t happen. As I’m working to include API sandboxes, lab environments, and other API virtualization approaches in my consulting, I’m keen to add new dimensions like API simulation, which provide great material for workshops, presentations, and talks.

<p>I feel like ALL APIs should have some sort of sandbox to play in while developing. Not just heavily regulated industries, or those with sensitive production data and content. It can be stressful to have to develop against a live environment, and having a realistic sandbox, labs, and simulations which provide complete copies of production APIs, along with real world synthetic data can help developers be more successful in getting up and running. I’ll keep profiling interesting approaches like this one out of SportRadar, and add to <a href="http://virtualization.apievangelist.com/">my API virtualization research</a>, helping round off my toolbox when it comes to helping API providers develop their sandbox environments.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/05/14/sports-data-api-simulations-from-sportradar/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/05/14/measuring-value-exchange-around-government-data-using-api-management/">Measuring Value Exchange Around Government Data Using Api Management</a></h3>
        <span class="post-date">14 May 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Measuring Value Exchange Around Government Data Using API Management’</p>

<p>image: http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/16_33_800_500_0_max_0_1_1-5.jpg</p>

<hr />

<p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/16_33_800_500_0_max_0_1_1-5.jpg" width="45%" align="right" style="padding: 15px;" />
<p><a href="http://apievangelist.com/2018/04/27/venture-capital-obfuscating-the-opportunities-for-value-exchange-at-the-api-management-level/">I’ve written about how the startup community has driven the value exchange that occurs at the API management layer down a two lane road with API monetizatin and plans</a>. To generate the value that investors have been looking for, we have ended up with a free, pro, enterprise approach to measuring value exchanged with API integration, when in reality there is so much more going on here. Something that really becomes evident when you begin evaluating the API conversations going on across government at all levels.

<p>In conversation after conversation I have with government API folk I hear that they don’t need API management reporting, analysis, and billing features. There is a perception that government isn’t selling access to APIs, so API management measurement isn’t really needed. Missing out on a huge opportunity to be measuring, analyzing, and reporting upon API usage, and requiring a huge amount of re-education on my part to help API providers within government to understand how they need to be measuring value exchange at the API management level.

<p>Honestly, I’d love to see government agencies actually invoicing API consumers at all levels for their usage. Even if the invoices aren’t in dollars. Measuring API usage, reporting, quantifying, and sending an invoice for usage each month would help bring awareness to how government digital resources are being put to use (or not). If all data, content, and algorithms in use across federal, state, and municipal government were available via APIs, then measured, rate limited, and reported upon across all internal, partner, and public consumers–the awareness around the value of government digital assets would increase significantly.

<p>I’m not saying we should charge for all access to government data. I’m saying we should be measuring and reporting upon all access to government data. The APIs, and a modern API management layer is how you do this. We have the ability to measure the value exchanged and generated around government digital resources, and we shouldn’t let misconceptions around how the private sector measures and generates revenue at the API layer interfere with government agencies benefitting from this approach. If you are publishing an API at a government agency, I’d love to learn more about how you are managing access to your APIs, and how you are reporting upon the reading and writing of data across applications, and learn more about how you are sharing and communicating around these consumption reports.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/05/14/measuring-value-exchange-around-government-data-using-api-management/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/05/14/looking-at-20-microservices-in-concert/">Looking At 20 Microservices In Concert</a></h3>
        <span class="post-date">14 May 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Looking At 20 Microservices In Concert’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/losangeles-from-observatory_free_woman.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/losangeles-from-observatory_free_woman.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I checked out the Github repositories for twenty microservices of one of my clients recently, looking understand what is being accomplished across all these services as they work independently to accomplish a single collective objective. I’m being contracted with to help come in blindly and provide feedback on the design of the APIs being exposed across services, and help provide guidance on their API lifecycle, as well as eventually API governance when things have matured to that level. Right now we are addressing pretty fundamental definition and design issues, but eventually we’ll hopefully graduate to the next level.

<p><a href="http://apievangelist.com/2018/04/30/a-readme-for-your-microservice-github-repository/">A complete and up to date README for each microservice is essential to understanding what is going on with a service</a>, and a robust OpenAPI definition is critical to breaking down the details of what each API delivers. When you aren’t part of each service’s development team it can be difficult to understand what each service does, but with an up to date README and OpenAPI, you can get up to speed pretty quickly. If an service is well documented via its README, and the API is well designed, and the surface area is reflected in it’s OpenAPI, you can go from not knowing what a service does to, understanding its value within hopefully minutes, not hours.

<p>When each service possesses an OpenAPI it becomes possible to evaluate what they deliver at scale. You can take all APIs, their paths, headers, parameters, and schema and out them in different ways so that you can begin to paint a picture of what they deliver in aggregate. Bringing all the disparate services back together to perform together in a sort of monolith concert, while still acknowledging they all do their own thing independently. Allowing us to look at how many different service can be used in concert to deliver a single application, or potentially a variety of application instances. Thinking critically about each independent service, but more importantly how they all work together.

<p>I feel like many groups are still struggling with decomposing their monolithic systems into separate services, and while some are doing so in a domain-driven way, few are beginning to invest in understanding how they move forward with services in concert to deliver on application needs. Many of the groups I’m working with are so focused on decomposing and tearing down, they aren’t thinking too critically about how they will make all of this begin work together again. I see monolith systems working like a massive church organ which take a lot of maintenance, and require a single (or handful) of knowledgeable operators to play. Where microservices are much more like an orchestra, where every individual player has a role, but they play in concert, directed by a conductor. I feel like most groups I’m talking with are just beginning the process of hiring a conductor, and have a bunch of musicians roaming around–not quite ready to play any significant productions quite yet.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/05/14/looking-at-20-microservices-in-concert/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/05/10/api-discovery-is-for-internal-or-external-services/">Api Discovery Is For Internal Or External Services</a></h3>
        <span class="post-date">10 May 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘API Discovery is for Internal or External Services’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/vancouver_light_dali.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/vancouver_light_dali.jpg" width="45%" align="right" style="padding: 15px;" />
<p>The topic of API discovery has been picking up momentum in 2018. It is something I’ve worked on for years, but with the number of microservices emerging out there, it is something I’m seeing become a concern amongst providers. It is also something I’m seeing more potential vendor chatter, looking to provide more services and tooling to help alleviate API discovery pain. Even with all this movement, there is still a lot of education and discussion to occur on the subject to help bring people up to speed on what is API discovery.

<p>The most common view of what is API discovery, is when you need to find an API for developing an application. You have a need for a resource in your application, and you need to look across your internal and partner resources to find what you are looking for. Beyond that, you will need to search for publicly available API resources, using Google, Github, ProgrammableWeb, and other common ways to find popular APIs. This is definitely the most prominent perspective when it comes to API discovery, but it isn’t the only dimension of this problem. There are several dimensions to this stop along the API lifecycle, that I’d like to flesh out further, so that I can better articulate across conversations I am having.

<p>Another area that gets lumped in with API discovery is the concept of service discovery, or how your APIs will find their backend services that they use to make the magic happen. Service discovery focuses on the initial discovery, connectivity, routing, and circuit breaker patterns involved with making sure an API is able to communicate with any service it depends on. With the growth of microservices there are a number of solutions like Consul that have emerged, and cloud providers like AWS are evolving their own service discovery mechanisms. Providing one dimension to the API discovery conversation, but different from, and often confused with front-end API discovery and how developers and applications find services.

<p>One of the least discussed areas of API discovery, but is one that is picking up momentum, is finding APIs when you are developing APIs, to make sure you aren’t building something that has already been developed. I come across many organizations who have duplicate and overlapping APIs that do similar things due to lack of communication and a central directory of APIs. I’m getting asked by more groups regarding how they can be conducting API discovery by default across organizations, sniffing out APIs from log files, on Github, and other channels in use by existing development teams. Many groups just haven’t been good at documenting and communicating around what has been developed, as well as beginning new projects without seeing what already exists–something that will only become a great problem as the number of microservices grows.

<p>The other dimension of API discovery I’m seeing emerge is discovery in the service of governance. Understand what APIs exist across teams so that definitions, schema, and other elements can be aggregated, measured, secured, and governed. EVERY organization I work with is unaware of all the data sources, web services, and APIs that exist across teams. Few want to admit it, but it is a reality. The reality is that you can’t govern or secure what you don’t know you have. Things get developed so rapidly, and baked into web, mobile, desktop, network, and device applications so regularly, that you just can’t see everything. Before companies, organizations, institutions, and government agencies are going to be able to govern anything, they are going to have begin addressing the API discovery problem that exists across their teams.

<p>API discovery is a discipline that is well over a decade old. It is one I’ve been actively working on for over 5 years. It is something that is only now getting the discussion it needs, because it is a growing concern. It will be come a major concern with each passing day of the microservice evolution. People are jumping on the microservices bandwagon without any coherent way to organize schema, vocabulary, or API definitions. Let alone any strategy for indexing, cataloging, sharing, communicating, and registering services. I’m continuing my work on APIs.json, and the API Stack, as well as pushing forward my usage of OpenAPI, Postman, and AsyncAPI, which all contribute to API discovery. I’m going to continue thinking about how we can publish open source directories, catalogs, and search engines, and even some automated scanning of logs and other ways to conduct discovery in the background. Eventually, we will begin to find more solutions that work–it will just take time.



</p></p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/05/10/api-discovery-is-for-internal-or-external-services/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/05/09/api-gateway-as-a-node-and-moving-beyond-backend-and-frontend/">Api Gateway As A Node And Moving Beyond Backend And Frontend</a></h3>
        <span class="post-date">09 May 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘API Gateway As A Node And Moving Beyond Backend and Frontend’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/docks_copper_circuit.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/docks_copper_circuit.jpg" width="45%" align="right" style="padding: 15px;" />
<p>The more I study where the API management, gateway, and proxy layer is headed, the less I’m seeing a front-end or a backend, I’m seeing just a node. A node that can connect to existing databases, or what is traditionally considered a backend system, but also can just as easily proxy and be a gateway to any existing API. A lot has been talked about when it comes to API gateways deploying APIs from an existing database. There has also been considerable discussion around proxying existing internal APIs or web services, so that we can deploy newer APIs. However, I don’t think there has been near as much discussion around proxying other 3rd party public APIs–which flips the backend and frontend discuss on its head for me.

<p>After profiling the connector and plugin marketplaces for a handful of the leading API management providers I am seeing a number of API deployment opportunities for Twilio, Twitter, SendGrid, etc. Allowing API providers to publish API facades for commonly used public APIs, and obfuscate away the 3rd party provider, and make the API your own. Allowing providers to build a stack of APIs from existing backend systems, private APIs and services, as well as 3rd party public APIs. Shifting gateways and proxies from being API deployment and management gatekeepers for backend systems, to being nodes of connectivity for any system, service, and API that you can get your hand on. Changing how we think about designing, deploying, and managing APIs at scale.

<p>I feel like this conversation is why API deployment is such a hard thing to discuss. It can mean so many different things, and be accomplished in so many ways. It can be driven by a databases, or strictly using code, or be just taking an existing API and turning it into something new. I don’t think it is something that is well understood amongst developer circles, let alone in the business world. An API gateway can be integration just as much as it can be about deployment. It can be both simultaneously. Further complexing what APIs are all about, but also making the concept of the API gateway a more powerful concept, continuing to renew this relic of our web services past, into something that will accommodate the future.

<p>What I really like about this notion, is that it ensures we will all be API providers as well as consumers. The one-sided nature of API providers in recent years has always frustrated me. It has allowed people to stay isolated within their organizations, and not see the world from the API consumer perspective. While API gateways as a node won’t bring everyone out of their bubble, it will force some API providers to experience more pain than they have historically. Understanding what it takes to to not just provide APIs, but what it takes to do so in a more reliable, consistent and friendly way. If we all feel the pain our integration and have to depend on each other, the chances we will show a little more empathy will increase.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/05/09/api-gateway-as-a-node-and-moving-beyond-backend-and-frontend/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/05/09/1000-lb-gorilla-monolith-and-microservices/">1000 Lb Gorilla Monolith And Microservices</a></h3>
        <span class="post-date">09 May 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘1000 LB Gorilla Monolith and Microservices’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/gorilla-monolith.png</p>

<hr />

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions2/gorilla-monolith.png" width="75%" />

<p align="center"><img src="https://s3.amazonaws.com/kinlane-productions2/gorilla-microservices.png" width="75%" />



</p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/05/09/1000-lb-gorilla-monolith-and-microservices/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/05/08/making-the-openapi-contract-friendlier-for-developers-and-business/">Making The Openapi Contract Friendlier For Developers And Business</a></h3>
        <span class="post-date">08 May 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Making the OpenAPI Contract Friendlier For Developers and Business’</p>

<p>image: http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/64<em>98_800_500_0_max_0_1</em>-1.jpg</p>

<hr />

<p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/64_98_800_500_0_max_0_1_-1.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I was in a conference session about an API design tool today, and someone asked if you could get at the OpenAPI definition behind the solution. They said yes, but quickly also said that the definition is boring and that you don’t want to be in there, you want to be in the interface. I get that service providers want you to focus on their interface, but we shouldn’t be burying, or abstracting away the API contract for APIs, we should always be educating people about it, an bringing it front and center in any service, tooling, or conversation.

<p>Technology folks burying or devaluing the OpenAPI definition with business users is common, but I also see technology folks doing it to each other. Reducing OpenAPI to be just another machine readable artifact alongside other components of delivering API infrastructure today. I think this begins with people not understanding what OpenAPI is, but I think it is sustained by people’s view of what is technological magic and should remain in the hands of the wizards, and what should be accessible to a wider audience. If you limit who has access and knowledge, you can usually maintain a higher level of control, so they use your interface in the case of a vendor, or they come to you develop and build an API in the case of a developer.

<p>There is nothing in a YAML OpenAPI definition that business users won’t be able to understand. OpenAPIs aren’t anymore boring than a Word document or Spreadsheet. If you are a stakeholder in the service, you should be able to read, understand, and engage with the OpenAPI contract. If we teach people to be afraid of the OpenAPI definitions we are repeating the past, and maintaining the canyon that can exist between business and IT/Developer groups. If you are in the business of burying the OpenAPI definition, I’m guessing you don’t understand the portable API lifecycle potential of this API contract, and simply see it as a config, documentation, or other technical artifiact. Or you are just in the business of maintaining control and power by being the gatekeeper for the API contract, similar to how we see database people defend their domain.

<p>Please do not devalue or hide away the OpenAPI contract. It isn’t your secret sauce. It isn’t boring. It isn’t too technical. It is the contract for how a service will work, that will speak to business and technical groups. It is the contract that all the services and tools you will use along the API lifecycle will understand. It is fine to have the OpenAPI right behind the scenes, but always provide a button, link, or other way to quickly see the latest version, and definitely do not scare people away or devalue it when you are talking. If you are doing APIs, you should be encouraging, and investing in everyone being able to have a conversation around the API contract behind any service you are putting forward.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/05/08/making-the-openapi-contract-friendlier-for-developers-and-business/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/05/08/constraints-introduced-by-supporting-standardized-api-definitions-and-schema/">Constraints Introduced By Supporting Standardized Api Definitions And Schema</a></h3>
        <span class="post-date">08 May 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Constraints Introduced By Supporting Standardized API Definitions and Schema’</p>

<p>image: https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/server-cloud1_feed_people.jpg</p>

<hr />

<p><img src="https://s3.amazonaws.com/kinlane-productions2/algo-rotoscope/stories/server-cloud1_feed_people.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I’ve had a few API groups contact me lately regarding the challenges they are facing when it comes to supporting organizational, or industry-wide API definitions and schema. They were eager to support common definitions and schema that have been standardized, but were getting frustrated by not being able to do everything they wanted, and having to live within the constraints introduced by the standardized definitions. Which is something that doesn’t get much discussion by those of use who are advocating for standardization of APIs and schema.

<p><strong>Web APIs Come With Their Own Constraints</strong><br />
We all want more developers to use our APIs. However, with more usage, comes more responsibility. Also, to get more usage our APIs need to speak to a wider audience, something that common API definitions and schema will help with. This is why web APIs are working, because they speak to a wider audience, however with this architectural decision we are making some tradeoffs, and accepting some constraints in how we do our APIs. This is why REST is just one tool in our toolbox, so we can use the right tool, establish the right set of constraints, to allow our APIs to be successful. The wider our API toolbox, the wider the number options we will have available when it comes to how we design our APIs, and what schema we can employ

<p><strong>Allowing For Content Negotiation By Consumers</strong><br />
One way I’ve encouraged folks to help alleviate some of the pain around the adoption of common API definitions and schema is to provide content negotiation to consumers, allowing them to obtain the response they are looking for. If people want the standardized approaches they can choose those, and if they want something more precise, or custom they can choose that. This also allows API providers to work around the API standards that have become bottlenecks, while still supporting them where they matter. Having the best of both worlds, where you are supporting the common approach, but still able to do what you want when you feel it is important. Allowing for experimentation as well as standardization using the same APIs.

<p><strong>Participate In Standards Body and Process</strong><br />
Another way to help move things forward is to participate in the standards body that is moving an API definition or schema forward. Make sure you have a seat at the table so that you can present your case for where the problems are, and how to improve on the design, definition, and schema being evolved. Taking a lead in creating the world you want to see when it comes to API and schema standards, and not just sitting back being frustrated because it doesn’t do what you want it to do. Having a role in the standards body, and actively participating in the process isn’t easy, and it can be time consuming, but it can be worth it down the road and helping you better achieve your goals when it comes to your APIs operating as you aspect, as well as the wider community and industry you are serving.

<p>Delivering APIs at scale won’t be easy. To reach a wide audience with your API it helps to be speaking a common vocabulary. This doesn’t always allow you to move as fast as you’d like, and do everything exactly as you envision. You will have to compromise. Operate within constraints. However, it can be worth it. Not just for your organization, but for the overall health of your community, and the industry you operate in. You never know, with a little patience, collaboration, and communication, you might learn even new approaches to defining your APIs and schema that you didn’t think about in isolation. Also, experimentation with new patterns will still be important, even while working to standardize things. In the end, a balance between standardized and custom will make the most sense, and hopefully alleviate your frustrations in moving things forward.



</p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/05/08/constraints-introduced-by-supporting-standardized-api-definitions-and-schema/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/05/08/api-design-either-the-provider-does-the-work-or-the-consumer-will-have-to/">Api Design Either The Provider Does The Work Or The Consumer Will Have To</a></h3>
        <span class="post-date">08 May 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘API Design, Either the Provider Does the Work or the Consumer Will Have To’</p>

<p>image: http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/109<em>201_800_500_0_max_0_1</em>-5.jpg</p>

<hr />

<p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/109_201_800_500_0_max_0_1_-5.jpg" width="45%" align="right" style="padding: 15px;" />
<p>I’m always fascinated by the API design debate, and how many entrenched positions there are when it comes to the right way of doing it. Personally, I don’t see any right way of doing it, I just see many different ways to put the responsibility on the provider or the consumer shoulders, or sharing the load between them. I spend a lot of time profiling APIs, crafting OpenAPI definitions that try to capture 100% of the surface area of an API. Something that is pretty difficult to do when you aren’t the provider, and you are just working from existing static documentation. It is just hard to find every parameter and potential value, exhaustively detailing what is possible when you use an API.

<p>When designing APIs I tend to lean towards exposing the surface area of an API in the path. This is my personal preference for when I’m using an API, but I also do this to try and make APIs more accessible to non-developers. However, I regularly get folks who freak out at how many API paths I have, preferring to have the complexity at the parameter level. This conversation continues with the GraphQL and other query language folks who prefer to craft more complex queries that can be passed in the body, to define exactly what is desired. I do not feel there is a right way of doing this, but it does reflect what I said early about balancing the load between provider and consumer.

<p>The burden for defining and designing the surface area of an API resides in the providers court–only the provider truly knows the entire surface area. Then I’d add that when you offload the responsibility to the consumer using GraphQL, you are limiting who will be able to craft a query, putting API access beyond the reach of business users, and many developers. I feel that exposing the surface area of an API in the URL makes sense to a lot of people, and puts it all out in the open. Unless you are going to provide every possible enum value for all parameters, this is the only way to make 100% of the surface area of an API visible and known. However, depending on the complexity of an API, this is something that can get pretty unwieldy pretty quickly, making parameters the next stop be defining and designing the surface area of your API.

<p>I know my view of API design doesn’t sync with many API believers, across many different disciplines. I’m not concerned with that. I’m happy to hear all the pros and cons of any approach. My objective is to lower the bar for entry into the API game, not raise the bar, or hide the bar. I’m all for pushing API providers to be more responsible for defining the surface area of their API, and not just offloading it on consumers to do all the work, unless they implicitly ask for it. In the end, I’m just a fan of simple, elegantly designed APIs that are intuitive and well documented using OpenAPI. I want ALL APIs to be accessible to everyone, even non-developers. I want them accessible to developers, minimizing the load on them to understand what is happening, and what all the possibilities are. I just don’t want to spend too much time on-boarding with an API. I just want to go from discovery to exploration in as little time as possible.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/05/08/api-design-either-the-provider-does-the-work-or-the-consumer-will-have-to/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/05/07/turning-the-stack-exchange-questions-api-into-25-separate-tech-topic-streaming/">Turning The Stack Exchange Questions Api Into 25 Separate Tech Topic Streaming</a></h3>
        <span class="post-date">07 May 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘Turning The Stack Exchange Questions API Into 25 Separate Tech Topic Streaming’</p>

<p>image: http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/80<em>168_800_500_0_max_0_1</em>-1.jpg</p>

<hr />

<p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/80_168_800_500_0_max_0_1_-1.jpg" width="40%" align="right" style="padding: 15px;" />
<p><a href="http://streamdata.io">I’m turning different APIs into topical streams using Streamdata.io</a>. I have been profiling hundreds of different APIs as part of my work to build out the <a href="http://api.gallery.streamdata.io/">Streamdata.io API Gallery</a>, and as I’m creating OpenAPI definitions for each API, I’m realizing the potential for event and topical driven streams across many existing web APIs. One thing I am doing after profiling each API is that I benchmark them to see how often the data changes, applying what we are calling StreamRank to each API path. Then I try to make sure all the parameters, and even enum values for each parameter are represented for each API definition, helping me see the ENTIRE surface area of an API. Which is something that really illuminates the possibilities surrounding each API.

<p>After profiling <a href="https://api.stackexchange.com/docs/questions#order=desc&amp;sort=activity&amp;tagged=php&amp;filter=default&amp;site=stackoverflow&amp;run=true">the Stack Exchange Questions API</a>, I began to see how much functionality and data is buried within a single API endpoint, and was something I wanted to expose and make much easier to access. Taking a single OpenAPI definition for the Stack Exchange Questions API:

<script src="https://gist.github.com/kinlane/ab5c043fc2aa08fc233935e57e24d217.js"></script>

<p>Then exploding it into 25 separate tech topic streaming APIs. Taking the top 25 enum value for the tags parameter for the Stack Overflow site, and exploding into 25 separate streaming API resources. To do this, I’m taking each OpenAPI definition, and generating an AsyncAPI definition to represent each possible stream:

<script src="https://gist.github.com/kinlane/cab2b55c14b8fe0b6dab3ef9d198e2d2.js"></script>

<p>I’m not 100% sure I’m properly generating the AsyncAPI currently, as I’m still learning about how to use the topics and streams collections properly. However, the OpenAPI definition above is meant to represent the web API resource, and the AsyncAPI definition is meant to represent the streaming edition of the same resource. Something that can be replicated for any tag, or any site that is available via the Stack Exchange API. Turning the existing Stack Exchange API into streaming topic APIs, that people can subscribe to only the topics they are interested in receiving updates.

<p>At this point I’m just experimenting with what is possible with OpenAPI and AsyncAPI specifications, and understanding what I can do with some of the existing APIs I am already using each day. I’m going to try and turn this into a prototype, delivering streaming APIs for all 25 of the top Stack Overflow tags. To demonstrate what is possible on Stack Exchange, but also to establish a proof of concept that I can apply to other APIs like Reddit, Github, and others. Then eventually automating the creation of streaming topic APIs using the OpenAPI definitions for common APIs, and <a href="http://streamdata.io">the Streamdata.io service</a>.



</p></p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/05/07/turning-the-stack-exchange-questions-api-into-25-separate-tech-topic-streaming/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  
	 	
    
			
        <h3><a href="https://apievangelist.com/2018/05/07/graphql-just-get-out-of-my-way-and-give-me-what-i-want/">Graphql Just Get Out Of My Way And Give Me What I Want</a></h3>
        <span class="post-date">07 May 2018</span>
        <hr />

<p>published: true</p>

<p>layout: post</p>

<p>title: ‘GraphQL, Just Get Out Of My Way And Give Me What I Want’</p>

<p>image: http://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/16<em>38_600_500_0_max_1_0</em>-2.jpg</p>

<hr />

<p><img src="https://kinlane-productions2.s3.amazonaws.com/algorotoscope/builder/filtered/16_38_600_500_0_max_1_0_-2.jpg" width="45%" align="right" style="padding: 15px;" />
<p>One of the arguments I hear for why API providers should be employing GraphQL is that they should just get out of developers way, and let them build their own queries so that they can just get exactly the data that they want. As an application developer we know what we want from your API, do not have us make many different calls, to multiple endpoints–just give us one API and let us ask for exactly what we need. It is an eloquent, logical argument when you operate and live within a “known bubble”, and you know exactly what you want. Now, ask yourself, will every API consumer know what they want? Maybe in some scenarios, every API consumer will know what they want, but in most situations, developers will not have a clue what they want, need, or what an API does.

<p>This is where the GraphQL as a replacement for REST argument begins breaking down. In a narrowly defined bubble, where every developers knows the schema, knows GraphQL, and knows they want–GraphQL can make a lot of sense. In the autodidact alpha developer startup world this argument makes a lot of sense, and gets a lot of traction. However, not everyone lives in this world, and in this real world, API design can become very important. Helping people understand what is possible, learn the schema behind an API, and become more familiar with an API, until they have a better understanding of what they might want. I’m not saying GraphQL doesn’t have a place when you have a significant portion of your audience knowing what they want, I’m just saying that you shouldn’t leave everyone else behind.

<p>To further turn this argument on its head, as a developer, if I know what I want, why make me build a query at all? Just give me a single URL with what I want! Don’t make me do the heavy lifting, and work to craft a query, just give me a single URL with minimal authentication, and let me get what I need in one click. Sure, it will take some time before you can craft enough URLs to meet everyone’s needs, but it will be worth it for those who you can. You can always craft new paths based upon requests, and yes, you can also augment your web APIs with a GraphQL endpoint for those neediest, most demanding developers who love building queries, and know exactly what they want. I guess my point is that there are a lot of definitions of knowing what you want, and how APIs can satisfy that, and not everyone will be in the same mindset as you are.

<p>Now I have to bury in this last paragraph the fact that I’m not anti-GraphQL. I am anti-GraphQL being sold as a replacement for simpler, resource centered web APIs (aka REST). So if you are going to come at me with the Y U HATE GraphQL response–don’t. I’m just trying to show GraphQL believers that they are leaving some people behind with a GraphQL only approach, unless you are 100% sure that ALL your API consumers know GraphQL, know your schema, and know what they want. GraphQL is an important tool in the API toolbox, but it isn’t the one size fits all tool it is often sold as. After much contemplation, and working on the ground within enterprise groups, I am trying to put GraphQL to work in more use cases where it makes sense, and before I can do this I have to push back much of the misinformation that has been peddled about it, and undo the damage I’me seeing on the ground.



</p></p></p></p></p>

        <ul class="actions" style="text-align: center;">
          <li><a href="https://apievangelist.com/2018/05/07/graphql-just-get-out-of-my-way-and-give-me-what-i-want/" class="button big">Details</a></li>
        </ul>
      	<center><hr style="width: 75%;" /></center>
				
    
  

<p align="center"><a href="http://apievangelist.com/archive/"><strong>View Previous Posts Via Archives</strong></a></p>

  </div>
</section>

              <footer>
	<hr>
	<div class="features">
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="75%" style="padding: 15px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
	<hr>
	<p align="center">
		relevant work:
		<a href="http://apievangelist.com">apievangelist.com</a> |
		<a href="http://adopta.agency">adopta.agency</a>
	</p>
</footer>


            </div>
          </div>

          <div id="sidebar">
            <div class="inner">

              <nav id="menu">
  <header class="major">
    <h2>Menu</h2>
  </header>
  <ul>
    <li><a href="/">Homepage</a></li>
    <li><a href="/blog/">Blog</a></li>
    <li><a href="/about/">About</a></li>
  </ul>
</nav>

              <section>
	<div class="mini-posts">
		<header>
			<h2 style="text-align: center;"><i>API Evangelist Sponsors</i></h2>
		</header>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.postman.com" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/partners/postman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://tyk.io/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/tyk/tyk-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.openapis.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/openapi.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://www.asyncapi.com/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/asyncapi/asyncapi-horiozontal.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://json-schema.org/" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/specifications/json-schema.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
		<article>
			<div class="content">
				<p align="center"><a href="https://github.com/postmanlabs/newman" target="_blank"><img src="https://kinlane-productions2.s3.amazonaws.com/postman/newman-logo.png" width="75%" style="padding: 5px; border: 1px solid #000;" /></a></p>
			</div>
		</article>
	</div>
</section>


            </div>
          </div>

      </div>

<script src="/assets/js/skel.min.js"></script>
<script src="/assets/js/util.js"></script>
<!--[if lte IE 8]><script src="assets/js/ie/respond.min.js"></script><![endif]-->
<script src="/assets/js/main.js"></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-1119465-51"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-1119465-51');
</script>


</body>
</html>
